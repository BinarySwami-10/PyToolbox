modulex Imported

PreviewInterest in digital image processing methods stems from two principal application areas: improvement of pictorial information for human interpretation; and
processing of image data for storage, transmission, and representation for autonomous machine perception. This chapter has several objectives: (1) to define
the scope of the field that we call image processing; (2) to give a historical perspective of the origins of this field; (3) to give you an idea of the state of the art
in image processing by examining some of the principal areas in which it is applied; (4) to discuss briefly the principal approaches used in digital image processing; (5) to give an overview of the components contained in a typical,
general-purpose image processing system; and (6) to provide direction to the
books and other literature where image processing work normally is reported.GAM What Is Digital Image Processing?An image may be defined as a two-dimensional function, f(x, y), where x and
y are spatial (plane) coordinates, and the amplitude of fat any pair of coordinates (x, y} is called the intensity or gray level of the image at that point. When
x,y, and the intensity values of f are alt finite, discrete quantities, we call the
image a digital image. The field of digital image processing refers to processing
digital images by means of a digital computer. Note that a digital image is composed of a finite number of elements, each of which has a particular location23
1.2 & The Origins of Digital Image Processing 25and, in addition, encompasses processes that extract attributes from images, up
to and including the recognition of individual objects. As an illustration to clarify these concepts, consider the area of automaled analysis of text. The
processes of acquiring an image of the area containing the text, preprocessing
that image, extracting (segmenting) the individual characters, describing the
characters in a form suitable for computer processing, and recognizing those
individual characters are in the scope of what we call digital image processing
in this book. Making senseof the content of the page may be viewed as being in
the domain of image analysis and even computer vision, depending on the lieve)
of complexity implied by the statement “making sense.” As will become evident
shortly, digital image processing, as we have defined it, is used successfully in a
broad range of areas of exceptional social and economic value. The concepts
developed in the following chapters are the foundation for the methods used in
those application areas.ERX The Origins of Digital Image ProcessingOne of the first applications of digital images was in the newspaper industry, when pictures were first sent by submarine cable between London and
New York. Introduction of the Bartlane cable picture transmission system
in the early 1920s reduced the time required to transport a picture across
the Atlantic from more than a week to less than three hours. Specialized
printing equipment coded pictures for cable transmission and then reconstructed them at the receiving end. Figure |.1 was transmitted in this way
and reproduced on a telegraph printer fitted with typefaces simulating a
halftone pattern.Some of the initial problems in improving the visual quality of these early
digital pictures were related to the selection of printing procedures and the
distribution of intensity levels. The printing method used to obtain Fig. J.1 was
abandoned toward the end of 1921 in favor of a technique based on photographic reproduction made from tapes perforated at the telegraph receiving
terminal. Figure 1.2 shows an image obtained using this method. The improvements over Fig. 1.1 are evident, both in tonal quality and in resolution.FIGURE 1.1 A
digital picture
produced in 1921
from a coded tape
by a telegraph
printer with
special type faces.
(McFarlane.’)  ‘Refcrences in the Bibliography al the end of the book are listed ie adphabetical order In authors’ less
names.
22 About the AuthorsGonzalez is author or co-author of over 100 technical articles, two edited
books, and four textbooks in the fields of pattern recognition, image processing, and robotics. His books are used in over 1000 universities and research institutions throughout the world. He is listed in the prestigious Marquis Who's
Who in America, Marquis Who's Who in Engineering, Marquis Who's Who in
the World, and in 10 other national and international biographical citations. He
is the co-holder of two U.S. Patents, and has been an associate editor of the
[EEE Transactions on Systems, Man and Cybernetics, and the International
Journal of Computer and Information Sciences. He is a member of numerous
professional and honorary societies, including Tau Beta Pi, Phi Kappa Phi, Eta
Kappa Nu, and Sigma Xi. He is a Fellow of the IEEE.Richard E. WoodsRichard E. Woods earned his B.S., M.S., and Ph.D. degrees in Electrical
Engineering from the University of Tennessee, Knoxville. His professional
experiences range from entrepreneurial to the more traditional academic,
consulting, governmental, and industrial pursuits. Most recently, he founded
MedData Interactive, a high technology company specializing in the development of handheld computer systems for medical applications. He was also a
founder and Vice President of Perceptics Corporation, where he was responsible for the development of many of the company’s quantitative image analysis
and autonomous decision-making products.Prior to Perceptics and MedData, Dr. Woods was an Assistant Professor of
Electrical Engineering and Computer Science at the University of Tennessee
and prior to that, a computer applications engineer at Union Carbide Corporation. As a consultant, he has been involved in the development of a number
of special-purpose digital processors for a variety of space and military agencies, including NASA, the Ballistic Missile Systems Command, and the Oak
Ridge National Laboratory,Dr. Woods has published numerous articles related to digital signal processing and is a member of several professional societies, including Tau Beta Pi,
Phi Kappa Phi, and the JEEE. In 1986, he was recognized as a Distinguished
Engineering Alumnus of the University of Tennessee.
24 Ceaptes 1 @ Introductionand value. These elements are called picture elements, image elements, pels, and
pixels. Pixel is the term used most widely to denote the elements of a digital
image. We consider these definitions in more forma! terms in Chapter 2.Vision is the most advanced of our senses, so it is not surprising that images
play the single most important role in human perception. However, unlike humans, who are- limited to the visual band of the electromagnetic (EM) spectrum, imaging machines cover almost the entire EM spectrum, ranging from
gamma to radio waves. They can operate on images generated by sources that
humans are not accustomed to associating with images. These include ultrasound, electron microscopy, and computer-generated images. Thus, digital
image processing encompasses a wide and varied field of applications.There is no general agreement among authors regarding where image
processing stops and other related areas, such as image analysis and computer vision, start. Sometimes a distinction is made by defining image processing
as a discipline in which both the input and output of a process are images. We
believe this to be a limiting and somewhat artificial boundary. For example,
under this definition, even the trivial task of computing the average intensity
of an image {which yields a single number) would not be considered an
image processing Operation. On the other hand, there are fields such as compuier vision whose ultimate goal is to use computers to emulate human vision, including iearning and being able to make inferences and take actions
based on visual inputs. This area itself is a branch of artificial intelligence
(AI) whose objective is to emulate human intelligence. The field of AL is in
its earliest stages of infancy in terms of development, with progress having
been much slower than originally anticipated. The area of image analysis
(also called image understanding) is in between image processing and computer vision.There are no clear-cut boundaries in the continuum from image processing
at one end to computer vision at the other. However, one useful paradigm is
to consider three types of computerized processes in this continuum: low-,
mid-, and high-level processes. Low-level processes involve primitive operations such as image preprocessing to reduce noise, contrast enhancement, and
image sharpening. A low-level process is characterized by the fact that both
its inputs and outputs are images. Mid-level processing on images involves
tasks such as segmentation (partitioning an image into regions or objects), description of those objects to reduce them to a form suitable for computer processing, and classification (recognition) of individual objects. A mid-level
process is characterized by the fact that its inputs generally are images, but its
outputs are attributes extracted from those images (e.g., edges, contours, and
the identity of individual objects). Finally. higher-level processing involves
“making sense” of an ensemble of recognized objects, as in image analysis, and,
at the far end of the continuum, performing the cognitive functions normally
associated with vision.Based on the preceding comments, we see that a logical place of overlap between image processing and image analysis is the area of recognition of individual regions or objects in an image. Thus, what we call in this book digital
image processing encompasses processes whose inputs and outputs are images
26 Chapter 1 & IntroductionFIGURE 1.2 A
digital picture
made in 1922
from a tape
punched after the
signals had
crossed the
Atlantic twice.
(McFarlane.}FIGURE 1.3
Unretouched
cable picture of
Generals Pershing
and Foch,
transmitted in
1929 from
London to New
York by 15-tone
equipment.
(McFarlane.) The early Bartlane systems were capable of coding images in five distinct
levels of gray. This capability was increased to 15 levels in 1929. Figure 1.3 is
typical of the type of images that could be obtained using the 15-tone equipment. During this period, introduction of a system for developing a film plate
via light beams that were modulated by the coded picture tape improved the
reproduction process considerably,Although the examples just cited involve digital nnages, they are not considered digital image processing results in the context of cur definition because computers were not involved in their creation. Thus, the history of
digital image processing is intimately tied to the development of the digital
computer. In fact, digital images require so much storage and computational
power that progress in the field of digital image processing has been dependent on the development of digital computers and of supporting technologies
that include data storage, display, and transmission.The idea of a computer goes back fo the invention of the abacus in Asia
Minor, more than 5000 years ago. More recently, there were developments in
the past two centuries that are the foundation of what we call a compuler today,
However, the basis for what we call a prvdern digital computer dates back to
only the 1940s with the introduction by John von Neumann of two key concepts: (1) a memory to hold a stored program and data, and (2} conditional
branching. These two ideas are the foundation of a central processing unit
(CPU), which is at the heart of computers loday. Starting with von Neumann,
there were a series of key advances thal led to computers powerful enough to
1.2 & The Origins of Digital Image Processing 27be used for digital image processing. Briefly, these advances may be summarized as follows: (1) the invention of the transistor at Bell Laboratories in 1948;
(2) the development in the 1950s and 1960s of the high-level programming languages COBOL (Common Business-Oriented Language) and FORTRAN
(Formula Translator); (3) the invention of the integrated circuit (FC) at Texas
Instruments in 1958; (4) the development of operating systems in the early
1960s; (S) the development of the microprocessor {a single chip consisting of
the central processing unN, memory, and mput and output controls) by Intel in
the early 1970s; (6) introduction by IBM of the personal computer in 1981; and
(7) progressive miniaturization of components, starting with large scale integration (LI) in the late 1970s, then very large scale integration (VLSI) in the 1980s,
to the present use of ultra Jarge scale integration (ULSI). Concurrent with
these advances were developmenis in the areas of mass storage and display systems, both of which are fundamental requirements for digital image processing.The first computers powerful enough to carry out meaningful image processing tasks appeared in the early 1960s. ‘The birth of what we call digital
image processing today can be traced to the availability of those machines and
to the onset of the space program during that period. It took the combination
of those two developments to bring into focus the potential of digital image
processing concepts. Work on using computer techniques for improving images from a space probe began at the Jet Propulsion Laboratory (Pasadena,
California) in 1964 when pictures of the moon transmitted by Ranger 7 were
processed by a computer to correct various types of image distortion inherent
in the on-board television camera. Figure 1.4 shows the first image of the
moon taken by Ranger 7 on July 31, 1964 at 9:09 a.m. Eastern Daylight Time
(EDT), about 17 minutes before impacting the lunar surface (the markers,
called reseau marks, are used for geometric corrections, as discussed in
Chapter 2). This also is the first image of the moon taken by a U.S. spacecraft.
The imaging lessons learned with Ranger 7 served as the basis for improved
methods used to enhance and restore images from the Surveyor missions to
the moon, the Mariner series of flyby missions to Mars, ihe Apollo manned
flights to the moon, and others, FIGURE 1.4 The
first picture of the
noon by a US,
spacecraft. Ranger
7 ook this image
on July 31, 1964 al
9:09 a.m, EDT,
about 17 minutes
before impacting
the lunar surface.
(Courtesy of
NASA.)
1.3 & Examples’ of Fields that Use Digital Image Processing 29military recognizance, automatic processing of fingerprints. screening of X-rays
and blood samples, and machine processing of aerial and satellile imagery for
weather prediction and environmental assessment. The continuing decline in the
ratio of computer price to performance and the expansion of networking and
communication bandwidth via the World Wide Web and the Internet have created unprecedented opportunitics for continued growth of digital image processing. Some of these application areas are illustrated in the following section.~| 1.34 Examples of Fields that Use Digital Image ProcessingToday, there is almost no area of technical endeavor that is not impacted in
some way by digital image processing. We can cover only a few of these applications in the context and space of the current discussion. However, limited as
it is, the material presented in this section will Jeave no doubt in your mind regarding the breadth and importance of digital image processing. We show in
this section numerous areas of application, each of which routinely utilizes the
digita] image processing techniques developed in the following chapters, Many
of the images shown in this section are used later in one or more of the examples given in the book. All images shown are digital.The areas of application of digital image processing are so varied that some
form of organization is desirable in attempting to capture the breadth of this
field. One of the simplest ways to develop a basic understanding of the extent of
image processing applications is to categorize images according 10 their source
(e.g., visual, X-ray, and so on). The principal energy source for images in use today
is the electromagnetic energy spectrum. Other important sources of energy include acoustic, ultrasonic, and electronic (in the form of electron beams used in
electron microscopy). Synthetic images. used for modeling and visualization, are
generated by computer. In this section we discuss briefly how images are generated in these various categories and the areas in which they are applied. Methods
for converting images into digital form are discussed in the next chapter.Images based on radiation from the EM spectrum are the most familiar,
especially images in the X-ray and visual bands of the specirum. Electromagnetic waves can be conceptualized as propagating sinusoidal waves of varying
wavelengths, or they can be thought of as a stream of massless particles, each
traveling in a wavelike pattern and moving at the speed of light. Each massless particle contains a certain amount (or bundle) of energy. Each bundle of
energy is called a phoron, If spectral bands are grouped according to energy
per photon, we obtain the spectrum shown in Fig, 1.5, ranging from gamma
rays (highest energy) at one end to radio waves (lowest energy) at the other.Energy of one photon (clectran volts)
1° 610° oe aaa to it io Fiat ter Sto7® ona-?ta-8 079
AGamma rays X-rays Ultraviolet Visible Infrared Microwaves Radio wavesFIGURE 1.5 The ciectromagnetic spectrum arranged according to energy per photon,
1.3 # Examples of Fields that Use Digital Image Processing 31or tumors. Figure 1.6(b} shows another major modality of nuclear imaging
called positron emission tomography (PET). The principle is the same as with
X-ray tomography, mentioned briefly in Section 1.2. However, instead of using
an external source of X-ray energy, the patient is given a radioactive isotope
that emits positrons as it decays. When a positron meets an electron, both are
annihilated and two gamma rays are given off. These are detected and a tomographic image is created using the basic principles of tomography. The image
shown in Fig, 1.6(b) is one sample of a sequence that constitutes a 3-D rendition
of the patient. This image shows a tumor in the brain and one in the lung, easily
visible as small white masses.A star in the constellation of Cygnus exploded about 15,000 years ago, generating a superheated stationary gas cloud (known as the Cygnus Loop) that glows
in a spectacular array of colors, Figure 1.6(c) shows an image of the Cygnus Loop
in the gamma-ray band. Unlike the (wo examples in Figs. 1.6(a) and (b), this
image was obtained using the natural radiation of the object being imaged. Finally,
Fig. 1.6(d) shows an image of gamma radiation from a valve in a nuclear reactor.
An area of strong radiation is seen in the lower left side of the image.1.4.2 X-Ray ImagingX-rays are among the oldest sources of EM radiation used for imaging. The
best known use of X-rays is medica] diagnostics, but they also are used extensively in industry and other areas, like astronomy. X-rays for medical and industrial imaging are generated using an X-ray tube, which is a vacuum tube
with a cathode and anode. The cathode is heated, causing free electrons to be
released. These electrons flow at high speed to the positively charged anode.
When the electrons strike a nucleus, energy is released in the form of X-ray
radiation. The energy (penetrating power) of X-rays is controlled by a voltage
applied across the anode, and by a current applied to the filament in the
cathode. Figure 1.7(a) shows a familiar chest X-ray generated simply by placing the patient between an X-ray source and a film sensitive to X-ray energy.
The intensity of the X-rays is modified by absorption as they pass through the
patient, and the resulting energy falling on the film develops it, much in the
same way that light develops photographic film. In digital radiography, digital
images are obtained by one of two methods: (1) by digitizing X-ray films; or
(2) by having the X-rays that pass through the patient fall directly onto devices
(such as a phosphor screen) that convert X-rays to light. The light signal in
turn is captured by a light-sensitive digitizing system. We discuss digitization
in more detail in Chapters 2 and 4.Angiography is another major application in an area called contrastenhancement radiography. This procedure is used to obtain images (called
angiograms) of blood vessels. A catheter (a small, flexible, hollow tube) is inserted, for example, into an artery or vein in the groin. The catheter is threaded
into the blood vessel and guided to the area to be studied. When the catheter
reaches the site under investigation, an X-ray contras{ medium is injected
through the tube. This enhances contrast of the blood vessels and enables the
radiologist to see any irregularities or blockages. Figure 1.7(b) shows an example of an aortic angiogram. The catheter can be seen being inserted into the
30 = Chapter 1 IntroductionThe bands are shown shaded to convey the fact that bands of the EM specetrum are not distinct but rather transition smoothly from one to the other,i.3.) Gamma-Ray ImagingMajor uses of imaging based on gamma tays include nuclear medicine and astronomical observations. [n nuclear medicine. the approach is to inject a patient with a radioactive isotope that emits gamma rays as it decays. images are
produced [rom the emissions collected by gamma ray detectors, Figure 1.6(a)
shows an image of acomplete bone scan obtained by using gamma-ray imaging,
Images of this sort are used to locate sites of hone pathology, such as infectionsahedFIGURE 1.6
Examples of
gamma-ray
imaging. (a) Bone
scan. (b) PET
image. (c) Cygnus
Loop. (d) Gamma
radiation (bright
spot) from a
reactor valve.
{Images courtesy
of (a) G.E.
Medical Systems.
(b} Dr. Michael
E. Casey,CTY
PET Systems,(c} NASA,(d) Professors
Zhong He and
David K. Wehe,
University of
Michigan.)
28 «Chapter 1 IntroductionIn parallel with space applications, digital image processing techniques
began in the late 1960s and early 1970s to be used in medical imaging, remote
Earth resources observations, and astronomy, The invention in the early 1970s
of computerized axial tomography (CAT), also called computerized tomography (CT) for short, is one of the most important events in the application of
image processing in medical diagnosis. Computerized axial tomography is a
process in which a ring of detectors encircles an object (or patient) and an
X-ray source, concentric with the detector ring, rotates about the object. The
X-rays pass through the object and are collected at the opposite end by the
corresponding detectors in the ring. As the source rotates, this procedure is repeated. Tomography consists of algorithms that use the sensed data to construct an image that represents a “slice” through the object. Motion of the
object in a direction perpendicular to the ring of detectors produces a set of
such slices, which constitute a three-dimensional (3-D) rendition of the inside
of the object. Tomography was invented independently by Sir Godfrey
N. Hounsfield and Professor Allan M. Cormack, who shared the 1979 Nobel
Prize in Medicine for their invention. It is interesting to note that X-rays were
discovered in 1895 by Wilhelm Conrad Roentgen, for which he received the
1901 Nobel Prize for Physics. These two inventions, nearly 100 years apart, led
to some of the most important applications of image processing today.From the 1960s until the present, the field of image processing has grown
vigorously. In addition to applications in medicine and the space program, digital image processing techniques now are used in a broad range of applications. Computer procedures are used to enhance the contrast or code the
intensity levels into color for easier interpretation of X-rays and other images
used in industry, medicine, and the biological sciences. Geographers use the
same or similar techniques to study pollution patterns [rom aerial] and satellite
imagery. Image enhancement and restoration procedures are used to process
degraded images of unrecoverable objects or experimental results too expensive to duplicate. In archeology, image processing methods have successfully
restored blurred pictures that were the only available records of rare artifacts
lost or damaged after being photographed. In physics and related fields, computer techniques routinely enhance images of experiments in areas such as
high-energy plasmas and electron microscopy. Similarly successful applications of image processing concepts can be found in astronomy, biology, nuclear
medicine, law enforcement, defense, and industry.These examples illustrate processing results intended for human interpretation. The second major area of application of digital image processing techniques mentioned at the beginning of this chapter is in solving problems dealing
with machine perception. In this case, interest is On procedures for extracting
from an image information in a form suitable for computer processing. Often.
this information bears little resemblance to visual features that humans use in
interpreting the content of an image. Examples of the type of information used
in machine perception are statistical moments, Fourier transform coefficients.
and muitidimensionai distance measures. Typical problems in machine perception that routinely utilize image processing techniques are automatic character
recognition. industrial machine vision for product assembly and inspection,
32 Chapter } B® Introduction FIGURE 1.7 Examples of X-ray iineging. fa) Chest X-ray. (4) Aortic angiogram. (c} Head
CT. (a) Cirewit beards. te) Cyenus Loop. Uosmes courtesy of fa} and (ci Ov. Dawid
R. Pickens, Dept. of Rais toy & Ridiolouicei Serenees. Vanderbilt Universily Medical
Center; (b) Dr. Thorna ort OD TES ves. University of Milehivar
Medical School: (Gd) Mi dsseptr to 1. w.rtis ied fo NOAAam pz
o fo
1.3 Examples of Fields that Use Digital Image Processing — 35visible and infrared bands in this section for the purpose of illustration. We
consider in the following discussion applications in light microscopy, astronomy, remote sensing, industry, and law enforcement.Figure 1.9 shows several exampies of images obtained with a light microscope.
The examples range from pharmaceuticals and micruinspection to materials
characterization. Even in microscopy alone, the apphealion arcas are too numerous to detail here. I is not difficuli to conceptualize the types of processes one
might apply to these images, ranging from enhancement to measurements. abedefFIGURE 1.9 Examples of light) inicroscay.
magnified 250%. (hb) Chalesterai His. fb 4   thin film—600x. {v) Surduen ef gale: € 01 ar 0
450x. (Images courtesy of 1. Michael AW) Piecedsoe. Fherkia State & texucrsrey 3
1.3 % Examples of Fields that Use Digital Image Processing 33large biood vessel on the lower left of the picture. Note the high contrast of the
large vessel] as the contrast medium flows up in the direction of the kidneys,
which are also visible in the image. As discussed in Chapter 2, angiography is a
major area of digital image processing, where tmage subtraction is used to enhance further the blood vessels being studied.Another important use of X-rays in medical imaging is computerized axial tomography (CAT). Due to their resolution and 3-D capabilities, CAT scans revolutionized medicine from the moment they first became available in the early
1970s. As noted in Section 1.2, each CAT image is a “slice” taken perpendicularly
through the patient. Numerous slices are generated as the patient is moved in a
longitudinal direction. The ensemble of such images constitutes a 3-D rendition of
the inside of the body, with the iongitudinal resolution being proportional to the
number of slice images taken. Figure 1.7(c) shows a typical head CAT slice image.Techniques similar to the ones just discussed, but generally involving higherenergy X-rays, are applicable in industrial processes. Figure 1.7(d) shows an X-ray
image of an electronic circuit board. Such images, representative of literally hundreds of industrial applications of X-rays, are used to examine circuit boards for
flaws in manufacturing, such as missing components or broken traces. Industrial
CAT scans are useful when the parts can be penetrated by X-rays, such as in
plastic assemblies, and even large bodies, like solid-propellant rocket motors.
Figure 1.7(¢) shows an example of X-ray imaging in astronomy. This image is the
Cygnus Loop of Fig. 1.6(c), but imaged this time in the X-ray band.1.3.32 Imaging in the Ultraviolet BandApplications of ultraviolet “light” are varied. They include Inhography, industrial
inspection, microscopy, lasers, biological imaging, and astronomical observations.
We illustrate imaging in this band with examples from microscopy and astronomy.
Ultraviolet light is used in fluorescence microscopy, one of the fastest growing areas of microscopy. Fluorescence is a phenomenon discovered in the middle of the nineteenth century, when it was first observed that the mineral
fluorspar fluoresces when ultraviolet light is directed upon it. The ultraviolet
light itself is not visible, but when a photon of ultraviolet radiation collides with
an electron in an atom of a fluorescent material, it elevates the electron to a higher
energy level. Subsequently, the excited electron relaxes to a lower level and emits
light in the form of a lower-energy photon in the visible (red) light region. The
basic task of the fluorescence microscope is to use an excitation light to irradiate
a prepared specimen and then to separate the much weaker radiating fluorescent light from the brighter excitation light. Thus, only the emission light reaches
the eye or other detector. The resulting fluorescing areas shine against a dark
background with sufficient contrast to permit detection. The darker the background of the nonfluorescing materia}, the more efficient the instrument.
Fluorescence microscopy is an excellent method for studying materials that
can be made to fluoresce, either in their natural form (primary fluorescence) or
when treated with chemicals capable of fMuorescing (secondary fluorescence).
Figures 1.8(a) and (b) show results typical of the capability of fluorescence
microscopy. Figure 1.8(a) shows a fluorescence microscope image of normal
corn, and Fig. 1.8(b) shows corn infected by “smut.” a disease of cereals. corn,
1,3 @ Examples of Fields that Use Digital Image Processing 37FIGURE 1.11
Sateilite image
of Hurricane
Katrina taken on
August 29, 2005,
(Courtesy of
NOAA, the spectra] bands in Table 1.1. The area imaged is Washington 1D.C., which includes features such as buildings, roads, vegetation, and a major river (the Potomac) going though the city. Images of population centers are used routinely
(over time) to assess population growth and shift patterns, pollution. and other
factors harmful to the environment. The differences between visual and infrared image features are quite noticeable in these images. Observe, for example, how well defined the river is from its surroundings in Bands 4 and S.Weather observation and prediction also are major applications of multispectral imaging from satellites. For example. Fig. 1.11 is an image of Hurricane
Katrina one of the most devastating storms in recent memory in the Western
Hemisphere. This image was taken by a Nalional Oceanographic and Atmospheric Administration (NOAA) satellite using sensors in the visible and infrared bands, The eye of the hurricane is clearly visible in this image.Figures 1.12 and 1.13 show an application of infrared imaging. These images
are part of the Nighttime Lights of the World data se\. which provides a global
inventory of human settlements. The images were gencrated by the infrared
imaging system mounted on a NOAA DMSP (Defense Metcorological Satellite Program) satellite. The infrared imaging system operates in the band 10.0
to 13.4 ym, and has the unique capability to obscrve faini sources of visiblenear infrared emissions present on the Earth’s surface. including cities. towns,
villages, gas flares, and fires. Even without formal training in image processing, it
is not difficult to imagine writing a computer program that would usc these images to estimate the percent of total electrical energy used by various regions of
the world.A major area of imaging in the visual spectrum is in automated visual inspection of manufactured goods. Figure 1.14 shows some examples. Figure 1.T4¢a)}
is a controller board for a CD-ROM drive. A typical imape processing task
with products like this is to inspect thea for msinsing pits (ie black syuarc on
the top, right quadrant of the fmiage ts ain cxample of a tissing component).
36 = Chapter 1 @ IntroductionTABLE 1.1
Thematic bands
in NASA's
LANDSAT
satellite.    Band No. Name Wavelength (pm) Characteristics and Uses1 Visible blue 0.45--0.52 Maximum water
penetration2 Visible green (1.52--0.60 Good for measuring plant
vigor3 Visible red 0,63-0.69 Vegetation discrimination4 Near infrared 0.76-0.90 Biomass and shoreline
mapping5 Middle infrared 155-175 Moisture content of soil
and vegetation6 Thermal infrared 10.4-12.5 Soil moisture; thermal
mapping,7 Middle infrared 2.08-2.35 Mineral mapping   Another major area of visual processing is remote sensing, which usually includes several bands in the visual and infrared regions of the spectrum. Table 1.1
shows the so-called thematic bands in NASA’s LANDSAT satellite. The primary
function of LANDSAT is to obtain and transmit images of the Earth from space
for purposes of monitoring environmental conditions on the planet. The bands
are expressed in terms of wavelength, with | um being equal to 107° m (we discuss the wavelength regions of the electromagnetic spectrum in more detail in
Chapter 2). Note the characteristics and uses of each band in Table 1.1.In order to develop a basic appreciation for the power of this type of
multispectral imaging, consider Fig. 1.10, which shows one image for each of1 2 3 FIGURE 1.10 LANDSAT satellite mines of the Wastineton. D.C. apa. Phe numbers rater fo ihe thematic
bands in Table 1.1. (images courtesy of NASAL?
34 = Chapter 1 @ Introductionab
aFIGURE 1,8Examples ofultravioletimaging,(a) Normal corn.(b) Smut corn.(c) Cygnus Loop.(Images courtesyof {a} and(b) Dr. MichaelW. Davidson,Florida StateUniversity,(c) NASA.) grasses, onions, and sorghum thal can be caused by any of more than /00 species
of parasitic fungi. Corn smut is particularly harmful because corn is one of thprincipal food sources in the world. As another illistration. Fig. L8{c) shows thCygnus Loop imaged in the high-enersy region of the ultraviolet bandImaging in the Visible and infrared BandsConsidering that the visuat bed of the cieelromagnetic soeetruin is the mes
familiar in all our actividies, IL is nol surprisiag that imdving in this bard out
. Podaes ey G4in Yo aeage hho. ot epeegat ta: Hoo pth yt giblot. ore  
  weighs by far all ils otliers
band often ts used in compumetiens wl:
38  Chepter } m IntroductionFIGURE 1.12
Infrared satellite
images of the
Americas. The
small gray map is
provided for
reference.
(Courtesy of
NOAA.)  Figure 1.34(b) is an imaged pill container. The objective here ws to have # maPldfci shows an application ins dich bmaec chine ]uok for nisstig pulls. fives
processing is used (a look for betes That are nol Mica up to an acceptibly:
level. Figure |.f4(d) shows u clear plesite part with ae unaecepiable number of
air pockets 10 it. Delecting anomahes tike these is a major theaie of ititistrial1Inspectton Ural inchides ollier produ such asweedanccharb Pigidie Pf dfe
lao Examples of Fields that Use Digital Image Processing 39 shows a batch of cereal during inspection for color and the presence of anomalies such as burned flakes. Finally, Fig. 1./4(f) shows an image of an intraocular
implant (replacement lens for the human eye). A “structured light’ dlumination technique was used to highlight for casier detection flat lens deformations
toward the center of the Jens, The markings ai | o'clock and S o'clock are
tweezer damage. Most of the other small speckde detail ts debris. The objective
in this type of inspection is to find damaged of incorrectly manufactured iinplants automatically. prior to packaging.As a final illustration of imape processing in the visual spectra, consider
Fig, 1.15. Figure 1.15(a) shows a thumb print. Imaves of fingerprints are routinely processed by computer, either to enhance them or to find features that
aid in the automated scarch of a database for pelertial matches. Ploure 115{}
shows an image of paper curreney, Applicauions of digital image processing inthis area include automated counting and. in faw cnforeemeni. the reading of the serial number for the purpose of @acking and identifying bis. The two vc
hicle images shown in Figs. jf ics and Gi are exrtnples of automated femnse
plate reading. The light rectangles indice Ua. aca in which the pape systemFIGURE 7.13
Snfrared satellite
images of the
remaining
populated part of
the world. The
smai] gray rap is
pravided for
Teference.
(Courtesy of
NOAA}
1.3 2 Examples of Fields that Use Digital image Processing 41ParreR. eK CeAESTIL8129 4a)AESTIUSLZ9A radar waves can penctrate clouds, and under certain conditions can also sec
through vegetation, icc, and dry sand. [In many cases, radar is the only way ta
explore inaccessible regions of the Earth’s surface. An imaging racar works
like a flash camera in that it provides ils own Hlumination (oucrawave pulses)
to illuminate an area on the ground and take a snapshot image. Instead of u
camera lens, a radar uses an antenna and digital computer processing to record
its images. In a radar image. one can sec only the microwave cnergy that was
reflected back toward the radar antenna.Figure 1.16 shows a spaceborne radar image covering a rugged mrountainous area of southeast Tibet. about 90 km east of the city of Lhasa. In the lower
right corner is a wide valley of the Lhasa River. which is popaiated by Tibctan
farmers and yak herders and includes the vilage of Mesba. Mountalus mm this
area reach about 5800 m (19.000 11) above sea devel while the valley floors lic
about 4300 m (14,000 Mt) above sen leved. Neste ihe clayily atel deiaif ot dis
image, unencumbered by clouds or olher siposghene conditos dat persally
interfere with images in the vistt basse.FIGURE }.35
Some additional
examples of
Hnaging in the
visual spectrum,
(a} Thumb print,
(6) Paper
currency, (¢) and
(d} Automated
license plate
reading.(Figure (4)
courtesy of the
National Institute
of Standurds and
Technology.
Figures (e) and
(d} courtesy of
Tor. Juan Herrera,
Perce plies
Corporation.)
1.3 & Examples of Fields that Use Digital Image Processing 43 adFIGURE 1.17 MRI images of a human (a) knee, and (b) spine. (Image (a) courtesy of
Dr. Thomas R. Gest, Division of Anatomical Sciences, University of Michigan
Medical School. and (b) courtesy of Dr. David R. Pickens, Department of Radiology
and Radiological Sciences, Vanderbill University Medical Contr.)returning sound waves are determined by the composition of the Earth below
the surface. These are analyzed by computer, and images are generated from
the resulting analysis.For marine acquisition, the energy source consists usually of two air guns
towed behind a ship. Returning sound waves are detected by hydraphones
placed in cables thal are vither lowed behind the ship, laid on the bottom of
the ocean, or hung ‘from buovs (vertical cables). Lhe two air guns are alternately pressurized to ~ 2000 psi and then set off. The constant motion of the
ship provides a transversal direchion of motion thal. tagether with the return
ing sound waves, is used to penerute a 3-D map of the composition of the
Earth below the boliom of the ocean,Figure 1.19 shows a cross-scefional image of a well-known 3-D model
against which the performance of seismic imaging alyoritims is tested. The
arrow points to a hydrocarbon (oil andor eas) trap. This target is bnghter than
the surrounding layers because the change in density in the larget region isGamma X-ray Optical Infrared Radio     FIGURE 1.18 fmages of The Crib Pulser iin the center of cach ineine} covering the cloetromagnetic spectrom,
(Courtesy of NASA.}
40 = Chapter 1 # Introductiona b  o BAAS ESESAR TEAS
c Q fate MR: as.
e _ mere LE sipkindicn |FIGURE 1.14
Some examples
of manufactured
goods often
checked using
digital image
processing.{a} A circuit
board controller.
(b) Packaged pills.
(c) Bottles.(d) Air bubbles
in a Clear-plastic
product.(¢) Cereal.(f) Image of
intraocular
implant.(Fig. (f) courtesy
of Mr. Pete Sites,
Perceptics
Corporation.) detected the plate. The biack rectangles show (hevresults of automated reading
of the plate content by the system, License plate and other applications of character recognition are used extepsively for traffic momtoring and surveillance,i. Imaging in the Microwave Band
The dominant application of imaging in the microwave band is radar. The
unique feature of iuagiag rackir is its ebihty to collect dati over virtually any
region atiuny time. regardless of weathes or ambient Vehting conditions, Some
42 Chapter 1 @ IntroductionFIGURE 1.16
Spaceborne radar
image of
mountains in
southeast Tibet.
(Courtesy of
NASA.) 1.3.6 Imaging in the Radio BandAs in the case of imaging ai the other end of the spectrum (gamma rays), the
major applications of imaging in the radio band are in medicine and astronomy.
In medicine, radio waves are used in magnetic resonance imaging (MR}). This
technique places a patient in a powerful magnet and passes radio waves through
his or her body in short pulses, Each pulse causes a responding pulse of radio
waves to be emitted by the patient's tissues. The location from which these signals originate and their strength are determined by a computer, which produces
a two-dimensional picture of a section of the patient. MRI can praduce pictures
in any plane. Figure [.17 shows MRE images of a human Knee and spine.The last image to tie right in Fig. 1.18 shows an image of the Crab Pulsar in
the radio band. Aiso shown for an interesting comparison are images of the
same region but taken in most of the bands discussed earlier, Note that each
image gives a totally different “view” of the Pulsar.1.3.7 Exampies in which Other Imaging Modalities Are UsedAlthough imaging in the electromagnetic spectrum is dominant by fir. there
are a number of other imaging modalities thal also are important, Specifically.
we discuss in this section acoustic imaging, clectran microscopy. and syithe tic
(computer-generated) imaging.Imaging using “sound” finds application in ecological exploration, industry.
and medicine. Geological applications use sGund in the low ond of the sound
spectrum (hundreds of Hz) while imaging in other areas use ultrasound {naillions of Hz). The most importimt commercial applications af image processiny
in geology are In mineral aad oil eNplorauen. bor mage vcquisrtion over liad.
one of the main approaches is i tise a hurge Uuek wo a darge flat sive! plinte.
The plate is pressed on the ground by the truck. and the truek tf vibvated
through a frequency spectrian um ie POG 1iy Vie sirengel: aid speed of Gre
13 8 Examples of Fields that Use Digital Image Processing 45 is confined and focused using metal apertures and magnetic lenses into a thin,
monochromatic beam. This beam is focused onto the sample using a magnelic
lens. Interactions occur inside the irracdialed sample, affecting the electron
beam. These interactions and effects arc detected and transformed into an
image, much in the same way that light is reflected from. or absorbed by, abjects in a scene. These basic steps are carried out in all electron microscopes.A transmission electron microscope (TEM) works much like a slide projector. A projector shines (transmits) a beam of light through a slide: as the light
passes through the slide, it is modulated by (he contents of the slide. This transmitted beam is then projected onto the viewing screen, forming an enlarged
image of the slide. TEMs work the same way, except thal they shine a bean of
electrons through a specimen {analogous io the side). The fracuion of the
beam transmitted through the specimen is projected onto a phosphor screen,
The interaction of the electrons with the phosphor produces light and, therefore, a viewable image. A scanning electron microscope (SEM), on the other
hand, actually scans the electron bean and records the interaction of beam
and sample at each location. This produces one dot on a phosphor screen. A
complete image is formed by a raster scan of the heam ihrough the sample.
much like a TV camera. The electrons mleract with a phosphor screen unct
produce light. SEMs are suitable for “hilky” samples. while TiMs require
very thin samples,Electron microscopes are capalile :
microscopy is limited to magnifications on the order [AGM ofobovory ineh min ab
edFIGURE 1.20
Examples of
ultrasound
imaging. (a) Baby.
(b) Another
view of bahy.{c} Thyroids,(d) Muscle lavers
showing lesion.
(Courtesy of
Siemens Medicui
Systems, Inc.,
Ultrasound
Group.)
44 Chapter 1m IntroductionFIGURE 1.19
Cross-sectional
image of a seismic
model. The arrow
points to a
hydrocarbon (oil
and/or gas) trap.
{Courtesy ofDr. Curtis Ober,
Sandia National
Laboratories.) larger. Seismic interpreters look for these “bright spots” to find oi! and gas, The
layers above also are bright, but their brightness dacs not vary as strongly
across the layers. Many seismic reconstruction algorithms have difficulty imaging this target because of the faults above it.Although ultrasound imaging is used routinely in manufacturing, the best
known applications of this technique are in medicine, especially in obstetrics,
where unborn babies are imaged to determine the health of their development. A byproduct of this examination is determining the sex of the baby. U]trasound images are generated using the following basic procedure:1. The ultrasound system (a computer, ultrasound probe consisting of a
source and receiver. and a display) transmits high-frequency (1 to 5 MHz)
sound pulses into the bady.2. The sound waves travel into (he body and hit a boundary belween tissues
(e.g.. between fluid and soft tissue, soft tissue and bone). Some of the
sound waves are retlected back to the probe, while some travel on further
unti] they reach another boundary and yet reflected.3. The reflected waves are picked up by the probe and relayed to the computer.4, The machine calculates the distance from the probe to the tissue ar organ
boundaries using the speed of sound in tissue (1540 m/s) and the time of
each echo’s return,5. The system disphiys the distances and jntensities of the echucs on the
screen, forming + two-dimensional image.In a typical ultrasound image, millions of pulses and echoes are sent and received each second. The probe can be moved along the surface of the body and
angled to obtain various views. Figure 1.20 shows several examples.We continue the discussion on imaging modalities with some examples of
electron microscopy. Electron microscopes function as their optical counterparis, except that they use a focused beam of electrons omtead of Jight 1
image a specimen. The operation of electron microscopes involves the follow
ing basic steps: A stream of electrons is pvoduved bv an electron source and ac
celerated toward fhe apecimen using a postlive elect eical poteritiabh Phis street
1.4 2 Fundamental Steps in Digital Image Processing 47 BEY Fundamental Steps in Digital Image ProcessingIt is helpful to divide the maicrial covered in the following chapters into the
two broad categories defined in Section 1.4: methods whose input and output
are images, and methods whose inputs may be images bul whose oulputs are
attributes extracted from those imagcs. This organization is summarized mn
Fig. 1.23. The diagram does not imply that every process is applied fo an image.
Rather, the intention is to convey an idea of all the methodologies that can be
applied to images for different purposes itnd possibly with different abjectives.
The discussion in this section may be viewed as a brief overview of the material
in the remainder of the book,image acquisition is the first process in Pip. £23. The discussion in Section §.3
gave some hints regarding the origin of digital images. This Lopie is considercd
in much more detail in Chapter 2, where we also introduce a number of basic
digital image concepts that are used throughout the book. Note that acquisition could be as simple as being given an mmuge that js already in digital form.
Generally, the image acquisition stape tvelves preprocessing, such as scaling.image enhancement is ihe process of masipulauing an imipe so that the resu]t is more suitable than the original for a# specifi application. The word
specific is important here, because il establishes ai the outset that enbancement
techniques are problem or sented. Thus. lor esa Ve ea ntethod Ura ils (quite i
ful for enhancing X-ray imeses tii net be ihe ben
satellite images taken ni the inliared Band of the ab
cdFIGURE 1.22(a) and (b) Fractal
images. (c) and
(d) Images
peneraled from
3-D computer
models of the
objects shawn.
(Figures (a) and
{b) courtesy of
Ms. Melissa{), Binds,
Swarthmore
College: (c) and
{d) courtesy of
NASA.)
46 Chapter} mf Introduction 2FIGURE 1.21 (a) 250x SEM image of a tungsten filament following thermal failure
(note the shattered pieces on the lower left). (b) 2500* SEM image of damaged
integrated circuit. The white fibers are oxides resulting fram thermal destruction.
{Figure (a) courtesy of Mr. Michael Shaffer, Department of Geological Sciences,
University of Oregon, Eugene; (b) courtesy of Dr. J. M. Hudak, McMaster University,
Hamilton, Ontario, Canada.}can achieve magnification of 10,000 or more. Figure 1.21 shows two SEM images of specimen failures due lo thermal overload.We conclude the discussion of imaging modalities by looking briefly at images that are not obtained from physical objects. Instead, they are generaled
by computer. Fractals are striking examples of computer-generated images
{Lu (1997]}. Basically. a fractal is nothing more than an iterative reproduction
of a basic pattern according to some matheniatical rules. For instance, dling is
one of the simplest ways to generate a fractal umage. A square can be subdivided into four square subregions. cach of which can be further subdivided
into four smaller square regions, and so on. Depending on the complexity of
the rules for filling each subsquare. some beautiful “le mages can be generated
using this method. Of course. the geometry can be arbilrary. For instance, the
fractal image could be grown radially ot of a center point. Figure 1.22(a)
shows a fractal grown in this way. Pigies 1.22(b) shows another fractal (a
“moonscape”) that provides an interesting analogy to the images of space
used as illustrations in some of the preceding sections,Fractal images tend toward artistry. mathematical formulations of “growth”
of subimage elements according to a set of rules. They are useful sometimes as
random textures. A more stracttrrect approach to inaage generation by computer
lies in 3-D modeling. This is an arca that provides an important intersection
between image processing and compiler graphics and is the basis for many
3-D visualization systems (ue. flight simulators), Figures 1.22(c} and (cd) shaw
examples of computer-generuted images. Since the original object is created in
3-D, images can be venerated ip any perspective fram plane projections of the
3-D volume. Images of this type cau be used for nredical traning and fer a host
of other applications. suelo ae crim forcrsies and special effeuts.
1.4 % Fundamental] Steps in Digital Image Processing 49Color image processing is an area that has been gaining in importance because of the significant increase in the use of digital images over the Internet.
Chapter 6 covers a number of fundamental concepts in color models and basic
color processing in a digital domain. Color is used also in later chapters as the
basis for extracting features of interest in an image.Wavelets are the foundation for representing images in various degrees of
resolution. In particular, this material is used in this book for image data compression and for pyramidal representation, in which images are subdivided
successively into smafler regions.Compression, as the name implies, deals with techniques for reducing the
storage required to save an image, or the bandwidth required to iransmit it. Although storage technology has improved significantly over the past decade, the
same cannot be said for transmission capacity. This is true particulary in uses of
the Internet, which are characterized by significant pictorial content. Image
compression ts familiar (perhaps inadvertently) to most users of computers in
the form of image file extensions, such as the jpg file extension used in the
JPEG (Joint Photographic Experts Group) image compression standard.Morphological processing deals with tools for extracting image components
that are useful in the representation and description of shape. The material in
this chapter begins a transition from processes that output images to processes
that output image attributes, as indicated in Section 1.1.Segmentation procedures partition an image into its constituent parts or
objects. In general, autonomous segmentation is one of the most difficult
tasks in digital image processing. A rugged segmentation procedure brings
the process a Jong way toward successful solution of imaging problems that
require objects to be identified individually. On the other hand, weak or erratic segmentation algorithms almost always guarantee eventual failure. In
general, the more accurate the segmentation, the more likely recognition is
to succeed.Representation and description almost always follow the output of a segmentation stage, which usually is raw pixe] data, constituting either the boundary of
a region (i.e., the set of pixels separating one image region from another) or all
the points in the region itself. In either case. converting the data to a form suitable for computer processing is necessary. The first decision that must be made
is whether the data should be represented as a boundary or as a complete region.
Boundary representation is appropriate when the focus is on external shape
characteristics, such as corners and inflections. Regional representation is appropriate when the focus is on internal properties, such as texture or skeletal
shape. In some applications, these representations complement each other.
Choosing a representation is only part of the solution for transforming raw
data into a form suitable for subsequent computer processing. A method musi
also be specified for describing the data so that features of interest are highlighted. Description, also called feature selection, deals with extracting attributes
that result in some quantitative information of interest or are basic for differentiating one class of objects from another.Recognition is the process that assigns a label (e.g., “vehicle") to an object
based on its descriptors. As detailed in Section 1.1, we conciude our coverage of
48 Chapter I @ IntroductionFIGURE 1.23
Fundamental
steps in digital
image processing.
The chapter(s)}
indicated in the
boxes is where the
" material
described in the
box is discussed.Outputs of these processes generally are images=CHAPTER F (UARIERK s HALE Rs
Wavelets and
multiresolution
processing        
    CHAPTERS   
 —_l  MorphotogicalColor image- Compression
processingProcessing       
     tah dtbr Robe   CTE aS  
     
 Imagerestoration Segmentation    
 
      
   
    
 Pdbabatae:
L1bsvii ct   EUESPIE RS SA  
  Knowledge base   Representation
& description   Image
fiktering and
enhancement         
   
     
  TELAVER HtSUA HEE FLoOutputs of these processes generally are image attributes  
   
 
  
 Object
recogmilion  
 ImageProblem (> acquisitiondomain 
  There is no general “theory” of image enhancement. When an image is
processed for visual interpretation, the viewer is the ultimate judge of how
well a particular method works, Enhancement techniques are so varied, and
use so many different image processing approaches, that it is difficult to assemble a meaningful body of techniques suitable for enhancement in one
chapter without extensive background development. For this reason. and also
because beginners in the field of image processing generally find enhancement applications visually appealing, interesting, and relatively simple to understand, we use image enhancement as examples when introducing new
concepts in parts of Chapter 2 and in Chapters 3 and 4. The material in the
latter two chapters span many of the methods used traditionally for image enhancement, Therefore, using examples from image enhancement to introduce
new image processing methods developed in these early chapters not only
saves having an extra chapter in the book dealing with image enhancement
but, more importantly, is an effective approach for introducing newcomers ta
the details of processing techniques early in the book. However, as you will
see in progressing through the rest of the book, the material developed in
these chapters is applicable to a much broader elass of problems than just
image enhancement.image restoration is an area that also deals with improving the appearance
of an image. However, unlike enhancement, which is subjective, image restoration is objective, in the sense that restoration techniques tend to be based on
mathematical or probabilistic models of image degradation. Enhancement, on
the other hand, is based on human subjective preferences regarding what constitutes a “pood™ enhancement result.
1.5 #& Components of an Image Processing System 51   Network FIGURE 1.24
~ aa Components of a
is general-purpose
image processing
sysiem, 
    Image displays Computer Mass storage    
 
      
  
  
 Specialized
image processing
hardware   Image processing
softwareHardcopy      Image sensorsProblem
domainthe output of the physical sensing device into digital form. For instance, in a
digital video camera, the sensors produce an electrical oulput proportional to
light intensity. The digitizer converts these outputs to digital data. These topics
are covered in Chapter 2.Specialized image processing hardware usually consists of the digitizer just
mentioned, plus hardware that performs other primitive operations, such as an
arithmetic logic unit (ALU), that performs arithmetic and logical operations
in parallel on entire images. One example of how an ALU is used is in averaging images as quickly as they are digitized. for the purpose of noise reduction.
This type of hardware sometimes is called a front-end subsystem, and its most
distinguishing characteristic is speed. In other words, this unit performs functions that require fast data throughputs (e.g., digitizing and averaging video
images at 30 frames/s) that the typical main computer cannot handle.The computer in an image processing system is a general-purpose computer
and can range from a PC to a supercomputer. In dedicated applications, sometimes custom computers are used te achieve a required leve] of performance,
but our interest here is on general-purpose image processing systems. In these
systems, almost any well-equipped PC-type machine is suitable for off-line
image processing tasks.Software for mage processing consists of specialized modules that perform
specific tasks. A well-designed package also includes the capability for the user
50 Chapter 1m Introductiondigital image processing with the development of methods for recognition of
individual objects.So far we have said nothing about the need for prior knowledge or about the
interaction between the knowledge base and the processing modules in Fig. 1.23.
Knowledge about a problem domain is coded into an image processing system
in the form of a knowledge database. This knowledge may be as simple as detailing regions of an image where the information of interest is known to be
located, thus limiting the search that has to be conducted in seeking that information. The knowledge base also can be quite complex, such as an interrelated
list of all major possible defects in a materials inspection problem or an image
database containing high-resolution satellite images of a region in connection
with change-detection applications. In addition to guiding the operation of each
processing module, the knowledge base also controls the interaction between
modules. This distinction is made in Fig. 1.23 by the use of double-headed arrows
between the processing modules and the knowledge base, as opposed to singleheaded arrows linking the processing modules.Although we do not discuss image display explicitly at this point, it is important to keep in mind that viewing the results of image processing can take place
at the output of any stage in Fig. 1.23. We also note that not all image processing applications require the complexity of interactions implied by Fig. 1.23. In
fact, not even all those modules are needed in many cases. For example, image
enhancement for human visual interpretation seldom requires use of any of the
other stages in Fig. 1.23. In general, however, as the complexity of an image processing task increases, so does the number of processes required to solve theprobiem.1a Components of an Image Processing SystemAs recently as the mid-1980s, numerous models of image processing systems
being sold throughout the world were rather substantial peripheral devices
that attached to equally substantial host computers. Late in the 1980s and
early in the 1990s, the market shifted to image processing hardware in the
form of singie boards designed to be compatible with industry standard buses
and to fit into engineering workstation cabinets and personal computers. In
addition to lowering costs, this market shift also served as a catalyst for a significant number of new companies specializing in the development of software
written specifically for image processing.Although large-scale image processing systems still are being sold for massive imaging applications, such as processing of satellite images, the trend continues toward miniaturizing and blending of general-purpose small computers
with specialized image processing hardware, Figure 1.24 shows the basic components comprising a typical general-purpose system used for digital image
processing. The function of each component is discussed in the following paragraphs, starting with image sensing.With reference to sensing, two elements are required to acquire digital images. The first is a physical device that is sensitive to the energy radiated by the
object we wish to image. The second, called a digitizer, is a device for converting
@ References and Further ReadingSummaryThe main purpose of the materia! presented in this chapter is to provide a sense of perspective about the origins of digital image processing and, more important, about current and future areas of application of this technology. Although the coverage of these
topics in this chapter was necessarily incomplete due to space limitations, it should
have left you with a clear impression of the breadth and practical scope of digital image
processing. As we proceed in the following chapters with the development of image
processing theory and applications, numerous examples are provided to keep a clear
focus on the utility and promise of these techniques. Upon concluding the study of the
final chapter, a reader of this book will have arrived at a level of understanding that is
the foundation for most of the work currently underway in this field.References and Further ReadingReferences at the end of later chapters address specific topics discussed in those
chapters, and are keyed to the Bibliography at the end of the book. However, in this
chapter we follow a different format in order to summarize in one place a body of
journals that publish materia! on image processing and related topics. We also provide a list of books from which the reader can readily develop a historical and current
perspective of activities in this field. Thus, the reference material cited in this chapter
is intended as a general-purpose, easily accessible guide to the published literature on
image. processing.Major refereed journals that publish articles on image processing and related topics
include: [EEE Transactions on Image Processing; IEEE Transactions on Pattern Analysis
and Machine Intelligence; Computer Vision, Graphics, and Image Pracessing (prior to
1991); Computer Vision and Image Understanding; IEEE Transactions on Systems, Man
and Cybernetics; Artificial Intelligence; Pattern Recognition; Pattern Recognition Letiers;
Journal of the Optical Society of America (prior to 1984); Journal of the Optical Society
of America —A: Optics, Image Science and Vision; Optical Engineering; Applied Optics —
information Processing; LEEE Transactions on Medical Imaging; Journal of Electronic
Imaging; IEEE Transactions on Information Theory; LEEE Transactions on Communtcations; [EEE Transactions on Acoustics, Speech and Signal Processing; Proceedings of
the IEEE; and issues of the EEE Transactions on Computers prior to 1980. Publications
of the International Society for Optical Engineering (SPIE) also are of interest.The following books, listed in reverse chronological order (with the number of
books being biased toward more recent publications), contain material that complements our treatment of digital image processing. These books represent an easily accessible overview of the area for the past 30-plus years and were selected to provide a
variety of treatments, They range from textbooks, which cover foundation material; to
handbooks, which give an overview of techniques; and finally to edited books, which
contain materia] representative of current research in the field.Prince, J. L. and Links, J. M. [2006]. Medical Imaging, Signals, and Systems, Prentice
Hall, Upper Saddle River, NJ.Bezdek, J. C. et al. [2005]. Fuzzy Models and Algorithms for Pattern Recognition and
Image Processing, Springer, New York.Davies, E. R. [2005]. Machine Vision: Theory, Algorithms, Practicalities, Morgan Kaufmann, San Francisco, CA.Rangayyan, R. M. [2005]. Biomedical image Analysis, CRC Press, Boca Raton, FL.53
52  Chopter 1 a Introductionto write code that, as a minimum, utilizes the specialized modules. More sophisticated software packages allow the integration of those modules and
general-purpose software commands from at least one computer language.Mass storage capability is a must in image processing applications. An
image of size 1024 X 1024 pixels, in which the intensity of each pixel is an 8-bit
quantity, requires one megabyte of storage space if the image is not compressed. When dealing with thousands, or even millions, of images, providing
adequate storage in an image processing system can be a challenge. Digital
storage for image processing applications falls into three principal categories:
{1) short-term storage for use during processing, (2) on-line storage for relatively fast recall, and (3) archival storage, characterized by infrequent access.
Storage is measured in bytes (eight bits), Kbytes (one thousand bytes), Mbytes
(one million bytes), Gbytes (meaning giga, or one billion, bytes), and Tbytes
(meaning tera, or one trillion, bytes).One method of providing short-term storage is computer memory. Another is by specialized boards, called frame buffers, that store one or more
images and can be accessed rapidly, usually at video rates (e.g., at 30 complete images per second). The latter method allows virtually instantaneous
image zoom, as well as scroif (vertical shifts) and pan (horizontal shifts).
Frame buffers usually are housed in the specialized image processing hardware unit in Fig. 1.24. On-line storage generally takes the form of magnetic
disks or optical-media storage. The key factor characterizing on-line storage
is frequent access to the stored data. Finally, archival storage is characterized
by massive storage requirements but infrequent need for access. Magnetic
tapes and optical disks housed in “jukeboxes” are the usual media for archival
applications.Image displays in use today are mainly color (preferably flat screen) TV
monitors. Monitors are driven by the outputs of image and graphics display
cards that are an integral part of the computer system. Seldom are there requirements for image display applications that cannot be met by display cards
available commercially as part of the computer system. In some cases, it is necessary to have stereo displays, and these are implemented in the form of headgear containing two small displays embedded in goggles worn by the user.Hardcopy devices for recording images include laser printers, film cameras,
heat-sensitive devices, inkjet units, and digital units, such as optical and CDROM disks, Film provides the highest possible resolution, but paper is the obvious medium of choice for written material. For presentations, images are
displayed on film transparencies or in a digital medium if image projection
equipment is used. The latter approach is gaining acceptance as the standard
for image presentations. ~Networking is almost a default function in any computer system in use today.
Because of the large amount of data inherent in image processing applications,
the key consideration in image transmission is bandwidth. In dedicated networks, this typically is not a problem, but communications with remote sites via
the Internet are not always as efficient. Fortunately, this situation is improving
quickly as a result of optical fiber and other broadband technologies.
56  Chopter ] @ IntroductionAndrews, H.C. and Hunt, B. R. [1977}. Digital Image Restoration, Prentice Hail, Upper
Saddle River, NJ.Pavlidis, T. [1977]. Structural Pattern Recognition, Springer-Verlag, New York.Tou, J. T. and Gonzalez, R. C. [1974]. Pattern Recognition Principles, Addison-Wesley,
Reading, MA.Andrews, H. C. [1970]. Computer Techniques in image Processing, Academic Press,
New York.
Digital Image
FundamentalsThose who wish to succeed must ask the right
preliminary questions. AristotlePreviewThe purpose of this chapter ts to introduce you to a number of basic concepts
in digital image processing thal are used throughout the book. Section 2.1
summarizes the mechanics of the human visual system, including image formation in the eye and its capabilities for brightness adaptation and discrimtnation. Section 2.2 discusses light. other components of the electromagnetic
spectrum, and their imaging characteristics, Section 2.3 discusses imaging
sensors and how they are used {o generate digital images. Section 2.4 introduces the concepts of uniform image sampling and intensity quantization.
Additional topics discussed in that section include digtial image representation, the effects of varying the number of samples and intensity levels in an
image, the concepts of spatial] and intensity resolution, and the principles of
image interpolation. Section 2.5 deals with a variety of basic relationships
between pixels. Finally, Section 2.6 is an introduction to the principal mathematical tools we use throughout the book. A second objective of that section is to help you begin developing a “feel” for how these tools are used in
a variety of basic image processing tasks. The scope of these tools and their
application are expanded as needed in the remainder of the book.57
& References and Further Reading 55Smirnov, A. [1999], Processing of Multidimensional Signals, Springer-Verlag, New York.Sonka, M., Hlavac, V., and Boyle, R. [1999]. fmage Processing, Analysis, and Computer
Vision, PWS Publishing, New York.Haskell, B, G. and Netravali, A. N. [1997]. Digital Pictures: Representation, Compression,
and Standards, Perseus Publishing, New York.Jahne, B, [1997]. Digital {mage Processing: Concepts, Algorithms, and Scientific Applications, Springes- Verlag, New York.Castleman, K. R. [1996]. Digital Image Processing, 2nd ed., Prentice Hall, Upper Saddle
River, NJ.Geladi, P. and Grahn, H. [1996]. Multivariate Jmage Analysis, John Wiley & Sons, New York.Bracewell, R.N. [1995]. Two-Dimensional Imaging, Prentice Hall, Upper Saddle River, NJ.Sid-Ahmed, M. A. [1995]. mage Processing: Theory, Algorithms, and Architectures,
McGraw-Hill, New York.Jain, R., Rangachar, K., and Schunk, B, {1995]. Computer Vision, McGraw-Hill, New York.Mitiche, A. [1994], Computational Analysis of Visual Motion, Perseus Publishing, New York.Baxes, G. A. [1994]. Digital Image Processing: Principles and Applications, John Wiley
& Sons, New York.Gonzalez, R. C. and Woods, R. E. [1992]. Digital Image Processing, Addison-Wesley,
Reading, MA.Haralick, R. M. and Shapiro, L. G. [1992]. Computer and Robot Vision, vols. 1 & 2,
Addison-Wesley, Reading, MA.Pratt, W. K. [1991] Digital Image Processing, 2nd ed., Wiley-Interscience, New York.Lim, J. S. [1990]. Tivo-Dimensional Signal and Image Processing, Prentice Hall. Upper
Saddle River, NJ.Jain, A. K. [1989]. Fundamentals of Digital Image Processing, Prentice Halt, Upper
Saddle River, NJ.Schalkoff, R. J. [1989]. Digital image Processing and Computer Vision, John Wiley &
Sons, New York.Giardina, C. R. and Dougherty, E. R. [1988]. Morphological Methods in Image and Signal
Processing, Prentice Hall, Upper Saddle River, NJ.Levine, M. D. [1985]. Vision in Man and Machine, McGraw-Hill, New York.Serra, J. [1982]. /mage Analysis and Mathematical Morphology, Academic Press, New
York.Ballard, D. H. and Brown, C. M. [1982]. Computer Vision, Prentice Hall, Upper Saddle
River, NJ.Fu, K. 8. [1982]. Syntactic Pattern Recognition and Applications, Prentice Hall, Upper
Saddie River, NJ.Nevatia, R. [1982]. Machine Perception, Prentice Hall, Upper Saddle River, NJ.Pavlidis, T. [1982]. Algorithms for Graphics and Image Processing, Computer Science
Press, Rockville, MD.Rosenfeld, A. and Kak, A. C. [1982]. Digital Picture Processing, 2nd ed., vols. 1 & 2,
Academic Press, New York.Hall, E. L. [1979]. Computer mage Processing and Recognition, Academic Press, New York.Gonzalez, R. C. and Thomason, M. G. (1978]. Syntactic Pattern Recognition: An Introduction, Addison-Wesley, Reading, MA.
54 = Ceepter 1 @ IntroductionUmbaugh, S. E. [2005}. Computer imaging: Digital Image Analysis and Processing, CRC
Press, Boca Raton, FL.Gonzalez, R. C., Woods, R. E., and Eddins, S. L. (2004). Digital Image Processing Using
MATLAB, Prentice Hall, Upper Saddle River, NJ.Snyder, W. E. and Qi, Hairong [2004]. Machine Vision, Cambridge University Press,
New York.Klette, R. and Rosenfeld, A. [2004]. Digital Geometry — Geometric Methods for Digital
Picture Analysis, Morgan Kaufmann, San Francisco, CA.Won, C. S. and Gray, R. M. [2004]. Stochastic Image Processing, Kluwer Academic/Plenum
Publishers, New York.Soitle, P. [2003]. Morphological image Analysis: Principles and Applications, 2nd ed.,
Springer-Verlag, New York.Dougherty, E. R. and Lotufo, R. A. [2003). Hands-on Morphological image Processing,
SPIE— The Internationa} Society for Optica] Engineering, Bellingham, WA.Gonzalez, R. C. and Woods, R. E. (2002). Digital Image Processing, 2nd ed., Prentice
Hall, Upper Saddle River, NJ.Forsyth, D. F. and Ponce, J. [2002]. Computer Vision—A Modern Approach, Prentice
Hall, Upper Saddle River, NJ. ;Duda, R. O., Hart, P E., and Stork, D. G. [2001]. Paitern Classification, 2nd ed., John
Wiley & Sons, New York.Pratt, W. K. [2001). Digital Image Processing, 3rd ed., John Wiley & Sons, New York.Ritter, G. X. and Wilson, J. N. (2001), Handbook of Computer Vision Algorithms in
Image Algebra, CRC Press, Boca Raton, FL.Shapiro, L. G. and Stockman, G. C. [2001]. Comzputer Vision, Prentice Halt, Upper Saddle
River, NJ.Dougherty, E. R. (ed.) [2000]. Random Processes for Image and Signal Processing,
IEEE Press, New York.Etienne, E. K. and Nachtegael, M. (eds,). [2000]. Fuzzy Techniques in Image Processing,
Springer-Verlag, New York.Goutsias, }., Vincent, L., and Bloomberg, D.S. (eds.). [2000]. Mathematical Morphology
and Its Applications to Image and Signal Processing, Kluwer Academic Publishers,
Boston, MA,Mallot, A. H. [2000]. Computational Vision, The MIT Press, Cambridge, MA.Marchand-Maillet. S. and Sharaiha, Y. M. [2000]. Binary Digital Image Processing: A
Discrete Approach, Academic Press, New York.Mitra, §. K. and Sicuranza, G. L. (eds.} [2000]. Nonlineur Image Processing, Academic
Press, New York.Edelman, S. [1999]. Representation and Recognition in Vision, Tne MIT Press, Cambridge, MA. «Lillesand, T. M. and Kiefer, R. W. [1999]. Remote Sensing and lmage Interpretation, John
Wiley & Sons, New York.Mather, P. M. [1999]. Compurer Processing of Remotely Sensed images: An Introduction.
Joha Wiley & Sons, New York.Petrou, M. and Bosdogianni, P. [1999]. nage Processing: The Fundamentals, John Wiley
& Sons, UK.Russ, J.C. [1999]. The {maze Processing Handbook, 3rd ed., CRC Press, Boca Raton, FL
58 Chapter 2 m Digital Image FundamentalsFIGURE 2.1
Simplified
diagram of a cross
section of the
human eye.BAW Elements of Visual PerceptionAlthough the field of digital image processing is built on a foundation of mathematical and probabilistic formulations, human intuition and analysis play a
central role in the choice of one technique versus another, and this choice
often is made based on subjective, visual judgments, Hence, developing a basic
understanding of human visual perception as a first step in our journey
through this book is appropriate. Given the complexity and breadth of this
topic, we can only aspire to cover the most rudimentary aspects of human vision. In particular, our interest is in the mechanics and parameters related to
how images are formed and perceived by humans, We are interested in learning the physical limitations of human vision in terms of factors that also are
used in our work with digital images. Thus, factors such as how human and
electronic imaging devices compare in terms of resolution and ability to adapt
to changes in illumination are not only interesting, they also are important
from a practical point of view.2.1.1 Structure of the Human EyeFigure 2.1 shows a simplified horizontal cross section of the human eye. The
eye is nearly a sphere, with an average diameter of approximately 20 mm.
Three membranes enclose the eye: the cornea and sclera outer cover, the
choroid; and the retina. The cornea is a tough, transparent tissue that coversCornea. kris
2.1 # Elements of Visual Perceptionthe anterior surface of the eye. Continuous with the cornea, the sclera is an
opaque membrane that encloses the remainder of the optic globe.The choroid lies directly below the sclera. This membrane contains a network of blood vessels that serve as the major source of nutrition to the eye.
Even superficial injury to the choroid, often not deemed serious, can lead to
severe eye damage as a result of inflammation that restricts blood flow. The
choroid coat is heavily pigmented and hence helps to reduce the amount of extraneous light entering thé eye and the backscatter within the optic globe. At
its anterior extreme, the choroid is divided into the ciliary body and the iris.
The latter contracts or expands to control the amount of light that enters the
eye. The central opening of the iris (the pupil) varies in diameter from approximately 2 to 8 mm. The front of the iris contains the visible pigment of the eye,
whereas the back contains a black pigment.The Jens is made up of concentric layers of fibrous cells and is suspended by
fibers that attach to the ciliary body. It contains 60 to 70% water, about 6% fat,
and more protein than any other tissue in the eye. The lens is colored by a
slightly yellow pigmentation that increases with age. In extreme cases, excessive clouding of the lens, caused by the affliction commonly referred to as
cataracis, can lead to poor color discrimination and loss of clear vision. The
lens absorbs approximately 8% of the visible light spectrum, with relatively
higher absorption at shorter wavelengths. Both infrared and ultraviolet light
are absorbed appreciably by proteins within the lens structure and, in excessive amounts, can damage the eye.The innermost membrane of the eye is the retina, which lines the inside of
the wall’s entire posterior portion. When the eye is properly focused, light
from an object outside the eye is imaged on the retina. Pattern vision is afforded by the distribution of discrete light receptors over the surface of the retina.
There are two classes of receptors: cones and rods. The cones in each eye number between 6 and 7 million. They are located primarily in the central portion
of the retina, called the fovea, and are highly sensitive to color. Humans can resolve fine details with these cones largely because each one is connected to its
own nerve end, Muscles controlling the eye rotate the eyeball until the image
of an object of interest fails on the fovea. Cone vision is called photopic or
bright-light vision.The number of rods is much larger: Some 75 to 150 million are distributed
over the retinal surface. The larger area of distribution and the fact that several rods are connected to a single nerve end reduce the amount of detail discernible by these receptors. Rods serve to give a general, overall picture of the
field of view. They are not involved in color vision and are sensitive to low levels of illumination. For example, objects that appear brightly colored in daylight when seen by moonlight appear as colorless forms because only the rods
are stimulated. This phenomenon is known as scotopic or dim-light vision,Figure 2.2 shows the density of rods and cones for a cross section of the
right eye passing through the region of emergence of the optic nerve from the
eye. The absence of receptors in this area results in the so-called blind spot (see
Fig. 2.1). Except for this region, the distribution of receptors is radially symmetric about the fovea, Receptor density is measured in degrees from the59
2.1 m Elements of Visual Perception 61 lens for distant or near objects, respectively. The distance between the center
of the jens and the retina along the visual axis is approximately 17 mm. The
range of focal lengths is approximately 14 mm to 17 mm, the latter taking
place when the eye is relaxed and focused at distances greater than about 3 m.The geometry in Fig, 2.3 illustrates how to obtain the dimensions of an
image formed on the retina. For example, suppose that a person is looking at a
tree 15 m high at a distance of 100 m. Letting # denote the height of that object
in the retinal image, the geometry of Fig. 2.3 yields 15/100 = h/17 or
h = 2.55 mm. As indicated in Section 2.1.1, the retinal image is focused primarily on the region of the fovea. Perception then takes place by the relative
excitation of light receptors, which transform radiant energy into electrical impulses that ultimately are decoded by the brain.2.1.3 Brightness Adaptation and DiscriminationBecause digital images are displayed as a discrete set of intensities, the eye’s
ability to discriminate between different intensity levels is an important consideration in presenting image processing results. The range of light intensity levels
to which the human visual system can adapt is enormous— on the order of 107°—
from the scotopic threshold to the glare limit. Experimental evidence indicates
that subjective brightness (intensity as perceived by the human visual system) is a
logarithmic function of the light intensity incident on the eye. Figure 2.4, a plotGlare Jimit —|   
   g
zl
&\8 Be
Els B
o 1a
uo is
218
a l<
wv
Scotopic
Scotopic __ |. “> Photopic
threshold dot i ta ta iat
6 ~4-2 6 2 4Log of intensity (mL)FIGURE 2.3
Graphical
representation of
the eye looking at
a palm tree. Point
C is the optical
center of the lens.FIGURE 2.4
Range of
subjective
brightness
sensations
showing a
particular
adaptation jevel.
60 Chapter 2 @ Digital Image FundamentalsFIGURE 2.2
Distribution of
rods and cones in
the retina.180,600Blind spot
aan. [ pons135,000 .90,000No. of rods or cones per mm?\{\
I
1
b
‘
l
|
!
I
}
I
1
|
‘
i
i
i
‘
1 80° 60° 40° 20° ” 20° 40° 40° 80°
Degrees from visual axis (center of fovea)fovea (that is, in degrees off axis, as measured by the angle formed by the visual axis and a line passing through the center of the lens and intersecting the
retina). Note in Fig. 2.2 that cones are most dense in the center of the retina (in
the center area of the fovea), Note also that rods increase in density from the
center out to approximately 20° off axis and then decrease in density out to the
extreme periphery of the retina.The fovea itself is a circular indentation in the retina of about 1.5 mm in diameter, However, in terms of future discussions, talking about square or rectangular arrays of sensing elements is more useful. Thus, by taking some
liberty in interpretation, we can view the fovea as a square sensor array of size
1.5mm X 1.5 mm. The density of cones in that area of the retina is approximately 150,000 elements per mm’. Based on these approximations, the number
of cones in the region of highest acuity in the eye is about 337,000 elements.
Just in terms of raw resolving power, a charge-coupled device (CCD) imaging
chip of medium resolution can have this number of elements in a receptor
array no larger than 5mm X 5 mm. While the ability of humans to integrate
intelligence and experience with vision makes these types of number comparisons somewhat superficial, keep in mind for future discussions that the basic
ability of the eye to resolve detail certainly is comparable to current electronic
imaging sensors.2.1.2 Image Formation in the EyeIn an ordinary photographic camera, the lens has a fixed focal length, and focusing at various distances is achieved by varying the distance between the
lens and the imaging plane, where the film (or imaging chip in the case of a
digital camera) is located. In the human eye, the converse is true; the distance
between the lens and the imaging region (the retina) is fixed, and the focal
length needed to achieve proper focus is obtained by varying the shape of the
lens, The fibers in the ciliary body accomplish this, flattening or thickening the
62  Chopter 2 w Digital Image FundamentalsFIGURE 2.5 Basic
experimental
setup used to
characterize
brightness
discrimination,of light intensity versus subjective brightness, illustrates this characteristic. The
long solid curve represents the range of intensities to which the visual system
can adapt. In photopic vision alone, the range is about 10°. The transition from
scotopic to photopic vision is gradual over the approximate range from 0.001
to 0.1 millilambert (~3 to —1 mL in the log scale), as the double branches of
the adaptation curve in this range show.The essentiai point in interpreting the impressive dynamic range depicted in
Fig. 2.4 is that the visual system cannot operate over such a range simultaneously,
Rather, it accomplishes this large variation by changing its overall sensitivity, a
phenomenon known as brightness adaptation. The total range of distinct intensity levels the eye can discriminate simultaneously is rather small when compared with the total adaptation range. For any given set of conditions, the
current sensitivity level of the visual system is called the brightness adaptation
level, which may correspond, for example, to brightness B, in Fig, 2.4. The
short intersecting curve represents the range of subjective brightness that the
eye can perceive when adapted to this level. This range is rather restricted,
having a level 8, at and below which all stimuli are perceived as indistinguishable blacks. The upper portion of the curve is not actually restricted but, if extended too far, loses its meaning because much higher intensities would simply
raise the adaptation level higher than B,,.The ability of the eye tu discriminate between changes in light intensity at
any specific adaptation level is also of considerable interest. A classic experiment used to determine the capability of the human visual system for brightness discrimination consists of having a subject look at a flat, uniformly
illuminated area large enough to occupy the entire field of view. This area typically is a diffuser, such as opaque glass, that is illuminated from behind by a
light source whose intensity, 7, can be varied. To this field is added an increment of illumination, A/, in the form of a short-duration flash that appears as
a circle in the center of the uniformly illuminated field, as Fig. 2.5 shows.If AZ is not bright enough, the subject says “no,” indicating no perceivable
change. As A/ gets stronger, the subject may give a positive response of “yes,” indicating a perceived change. Finally, when A/ is strong enough, the subject will
give a response of “yes” all the time. The quantity 4/.//7, where A/, is the increment of illumination discriminable 50% of the time with background i!lumination
Jis called the Weber ratio. A small value of A/,/7 means that a small percentage
change in intensity ts discriminable. This represents “good” brightness discrimination. Conversely, a large value of A/.// means that a large percentage change
in intensity is required. This represents “poor” brightness discrimination.anteoe
64 Chapter 2 m Digital Image FundamentalsabcFIGURE 2.7
i}lustration of the
Mach band effect.
Perceived
intensity is not a
simple function of
actual intensity,    
  Actual mtensity   
 
 . bo ce ben Perceived intensity 
     ne However, they appear to the cye to bec. ne darker as the background gets
lighter, A more familiar example is a piece of paper that seems white when
lying on a desk, but can appear lotally black when used to shield the eyes while
looking directly at a bright sky.Other examples of human perception phenomena are optical ilfusions, in
which the eye fills in nonexisting information or wrongly perceives geometrieal properties of objects. Figure 2.9 shows some examples. In Fig. 2.9(a), the
outline of a square is seen clearly, despite the fact (hat no lines defining such a
figure are part of the image. The same effect, this time with a circle, can be seen
in Fig. 2.9(b); note how just a few fines are sufficient to give the illusion of a   abeFIGURE 2.8 Exampies of simuliancors cuatrast. All the inner squares have the same
intensity, but they appen: progressive !s darker as the back ground becores lighter.
2.2 = Light and the Electromagnetic Spectrum 65           complete circle. The two horizontal line segments in Fig. 2.9(c) are of the same
iength, but one appears shorter than the other. Finally, all Jines in Fig. 2.9(d) that
are ortented at 45° are equidistant and paralicl. Yet the crosshatching creates the
illusion that those lines are far from being parallel. Optical ilusions are a characteristic of the human visual system that is not fully understood.eRe Light and the Electromagnetic SpectrumThe electromagnetic spectrum was introduced in Section 1.3. We now consider
this topic in more detail. In 1666, Sir Isaac Newton discovered that when a beam
of sunlight is passed through a giass prism, the emerging beam of light is not
white but consists instead of a continuous spectrum of colors ranging from violet
at one end to red at the other. As Fig. 2.10 shows. the range of colors we perceive
in visible light represents a very small portion of the electromagnetic spectrum.
On one end of the spectrum are radio waves with wavelengths billions of times
longer than those of visible light. On the other end of the spectrum are gamma
rays with wavelengths millions of times smaller than those of visible light. The
electromagnetic spectrum can be expressed in terms of wavelength, frequency,
or energy. Wavelength (A) and frequency (7) are related by the expression
‘
Ae (2.2-1)icab
‘cd
FIGURE 2.9 Somewell-known
Optica} illusions.
2.1 @ Elements of Visual Perception 63 10 FTFa
wn
—!___ -2gl—1 td
-4 -3 -2 -! @ 1 2 3 4log } A plot of log A/,// as a function of log 7 has the general shape shown in
Fig. 2.6, This curve shows that brightness discrimination is poor (the Weber
ratio is large) at low levels of illumination, and it improves significantly (the
Weber ratio decreases) as background illumination increases. The two branches in the curve reflect the fact that at low levels of ithumination vision is carried
out by the rods, whereas at high levels (showing better discrimination) vision is
the function of cones.If the background illumination is held constant and the intensity of the
other source, instead of flashing, is now allowed to vary incrementally from
never being perceived to always being perceived, the typical observer can discern a total of one to two dozen different intensity changes. Roughly, this result is related to the number of different intensities a person can see at any one
point in a monochrome image. This result does not mean that an image can be
represented by such a small number of intensify values because, as the eye
roams about the image, the average background changes, thus allowing a
different set of incremental changes to be detected at each new adaptation
level. The net consequence is that the eye is capable of a much broader range
of overall intensity discrimination. In fact, we show in Section 2.4.3 that the eye
is capable of detecting objectionable contouring effects in monochrome images whose overall intensity is represented by fewer than approximately two
dozen levels.Two phenomena clearly demonstrate that perceived brightness is not a
simple function of intensity. The first is based on the fact that the visual system tends to undershoot or overshoot around the boundary of regions of different intensities, Figure 2.7(a) shows a striking example of this phenomenon.
Although the tensity of the stripes is constant, we actually perceive a brightness pattern that is strongly scalloped near ihe boundaries (Fig. 2.7(c)]. These
seemingly scalloped bands are called Mach bands after Ernst Mach, who first
described the phenomenon in 1865.The second phenomenon, called simultaneous contrast, is related to the fact
that a region’s perceived brightness does not depend simply on tis intensity, as
Fig. 2.8 demonstrates. All the center squares have exactly the same intensily.FIGURE 2.6
Typical Weber
ratio as a function
of intensity,
66 Chapter 2 @ Digital Image FundamentalsEnergy of one photon (electron volts)
10° 6108) (id ttt wo! to? ge toto 8 tav® a? o-B to?
—__ jd | L. | nae lL i 1 L... oh Se
Frequency (Hz)17" «10 19! ya ig? 9 ag'S jo 0 to? oo"! ta oto? ie® to? to®
doo id L Lott 1!    Wavclength (meters)7'2 107! 40" 40°" 10-8 10°? 18 to las os to? tort Te ee CO
wwe td
—_ aa a a (a   
  Gamma rays X-rays Ultraviolet Infrared Microwaves Radio waves   Visible spectrum 0.4 x 1076 0.5 x 10% 06x10% 0.7.x 1078
Uhtraviolet Violet Biue Green Yellow Orange Red InfraredFIGURE 2.10 The electromagnetic spectrum. The visible spectrum is shown zoomed to facilitate explanation,
but note that the visible spectrum is a rather narrow portion of the EM spectrum.where c is the speed of light (2.998 x 10° m/s). The cnergy of the various components of the electromagnetic spectrum is given by the expressionE = hp (2.2-2)where # is Planck's constant. The units of wavelength are meters, with the terms
microns (denoted pm and equal to 10°° m) and nanometers (denoted nm and
equal to 10°” m) being used just as frequently. Frequency is measured in Hertz
(Hz), with one Hertz being equal to one cycle of a sinusoidal wave per second.
A commonly used unit of energy is the electron-volt.Electromagnetic waves can be visualized as propagating sinusoidal waves
with wavelength A (Fig, 2.11). or they can be thought of as a stream of massless
particles, each traveling in a wavelike pattern and moving at the speed of light.
Each masstess particle contains a certain amount (or bundle) of energy. Each~FIGURE 2.11ate ee A ee el
Graphicai
representation of
one wavelength. \
NL AS L/S
2.3 m Lmage Sensing and Acquisitionreflected from a planar surface. An example in the second category is when
X-rays pass through a patient's body for the purpose of generating a diagnostic X-ray film. In some applications, the reflected or transmitted energy is focused onto a photoconverter (e.g, a phosphor screen), which converts the
energy into visible light. Electron microscopy and some applications of gamma
imaging use this approach.Figure 2.12 shows the three principal sensor arrangements used to transform illumination energy into digital images. The idea is simple: Incoming energy is transformed into a voltage by the combination of input electrical power
and sensor materia] that is responsive to the particular type of energy being
detected. The output voltage waveform is the response of the sensor(s), and a
digital quantity is obtained from each sensor by digitizing its response. In this
section, we look at the principal modalities for image sensing and generation.
Image digitizing is discussed in Section 2.4.EnergyLd |Filter   Power in Sensing material      69(a} Single imaging(b) Line sensor.
(c) Array sensor.
2.2 @ Light and the Electromagnetic Spectrum 67bundle of energy is called a photon. We see from Eq, (2.2-2) that energy is
proportional to frequency, so the higher-frequency (shorter wavelength) electromagnetic phenomena carry more energy per photon. Thus, radio waves
have photons with low energies, microwaves have more energy than radio
waves, infrared still more, then visible, ultraviolet, X-rays, and finally gamma
rays, the mosi energetic of all. This is the reason why gamma rays are so dangerous to living organisms.Light is a particular typé of electromagnetic radiation that can be sensed by
the human eye. The visible (color) spectrum is shown expanded in Fig. 2.10 for
the purpose of discussion (we consider color in much more detail in Chapter 6).
The visible band of the electromagnetic spectrum spans the range from approximately 0.43 ym (violet) to about 0.79 jm (red). For convenience, the color spectrum is divided into six broad regions: violet, blue, green, yellow, orange, and red.
No color (or other component of the electromagnetic spectrum) ends abruptly,
but rather each range blends smoothly mto the next, as shown in Fig, 2.10.The colors that humans perceive in an object are determined by the nature
of the light reflected from the object. A body that reflects light relatively balanced in all visible wavelengths appears white to the observer. However, a
body that favors reflectance in a limited range of the visible spectrum exhibits
some shades of color. For example, green objects reflect light with wavelengths
primarily in the 500 to 570 nm range while absorbing most of the energy at
other wavelengths.Light that is void of color is called monochromatic (or achromatic) light.
The only attribute of monochromatic light is its intensity or amount. Because
the intensity of monochromatic light is perceived to vary from black to grays
and finally to white, the term gray /eve/ is used commonly to denote monochromatic intensity. We use the terms intensity and gray level interchangeably
in subsequent discussions. The range of measured values of monochromatic
light from black to white is usually cailed the gray scale, and monochromatic
images are frequently referred to as gray-scale images.Chromatic (color) light spans the electromagnetic energy spectrum from
approximately 0.43 to 0.79 jzm, as noted previously. In addition to frequency,
three basic quantities are used to describe the quality of a chromatic light
source: radiance, luminance, and brightness. Radiance is the total amount of
energy that flows from the light source, and it is usually measured in watts
(W). Luminance, measured in lumens (Im), gives a measure of the amount of
energy an observer perceives from a light source. For example, light emitted
from a source operating in the far infrared region of the spectrum could have
significant energy (radiance), but an observer would hardly perceive it; its luminance would be almost zero. Finally, as discussed in Section 2.1, brightness is
a subjective descriptor of light perception that is practically impossible to
measure. It embodies the achromatic notion of intensity and is one of the key
factors in describing color sensation.Continuing with the discussion of Fig. 2.10, we note that at the shortwavelength end of the electromagnetic spectrum, we have gamma rays and
X-rays. As discussed in Section 1.3.1, gamma radiation ts important for medical
and astronomical imaging, and for imaging radiation in nuclear environments,
68 Chapter 2 m Digital Image FundamentalsHard (high-energy) X-rays are used in industrial applications. Chest and
dental X-rays are in the lower energy (soft) end of the X-ray band. The soft
X-ray band transitions into the far ultraviolet light region, which in turn
blends with the visible spectrum at longer wavelengths. Moving still higher in
wavelength, we encounter the infrared band, which radiates heat, a fact that
makes it useful in imaging applications that rely on “heat signatures.” The part
of the infrared band close to the visible spectrum is called the near-infrared region. The opposite end of this band is called the far-infrared region. This latter
region blends with the microwave band. This band is weil known as the source
of energy in microwave ovens, but it has many other uses, including communication and radar. Finally, the radio wave band encompasses television as well
as AM and FM radio. In the higher energies, radio signals emanating from certain stellar bodies are useful in astronomical observations. Examples of images
in most of the bands just discussed are given in Section 1.3.In principle, if a sensor can be developed that is capable of detecting energy
radiated by a band of the electromagnetic spectrum, we can image events of
interest in that band. It is important to note, however, that the wavelength of
an electromagnetic wave required to “see” an object must be of the same size
as or smaller than the object. For example, a water molecule has a diameter on
the order of 107! m. Thus, to study molecules, we would need a source capable
of emitting in the far ultraviolet or soft X-ray region. This limitation, along
with the physical properties of the sensor material, establishes the fundamental limits on the capability of imaging sensors, such as visible, infrared, and
other sensors in use today.Although imaging is based predominantly on energy radiated by electromagnetic waves, this is not the only method for image generation. For example, as discussed in Section 1.3.7, sound reflected from objects can be
used to form ultrasonic images. Other major sources of digital images are
electron beams for electron microscopy and synthetic images used in graphics
and visualization.| 2.3 | Image Sensing and AcquisitionMost of the images in which we are interested are generated by the combination of an “illumination” source and the reflection or absorption of energy
from that source by the elements of the “scene” being imaged. We enclose
illumination and scene in quotes to emphasize the fact that they are considerably more general than the familiar situation in which a visible light source illuminates a common everyday 3-D (three-dimensional) scene, For example,
the illumination may originate from a source of electromagnetic energy such
as radar, infrared, or X-ray system. But, as noted earlier, it could originate
from less traditional sources, such as ultrasound or even a computer-penerated
illumination pattern. Similarly, the scene elements could be familiar objects,
but they can just as easily be molecules, buried rock formations, or a human
brain. Depending on the nature of the source, illumination energy is reflected
from, or transmitted through, objects. An example in the first category is light
70 Chapter 2 m Digital image FundamentalsFIGURE 2.13
Combining a
single sensor with
motion to
generate a 2-D
image. 
 
   Rotation   needs dl
Linear motionOne image line outper increment of rotation
and full linear disptacement
of sensor from left to right2.3.1 Image Acquisition Using a Single SensorFigure 2.12(a) shows the components of a single sensor. Perhaps the most familiar sensor of this type is the photodiode, which is constructed of silicon materials and whose output voltage waveform is proportional to light. The use of
a filter in front of a sensor improves selectivity. For example, a green (pass) filter in front of a light sensor favors light in the green band of the color spectrum. As a consequence, the sensor output will be stronger for green light than
for other components in the visible spectrum.In order to generate a 2-D image using a single sensor, there has to be relative displacements in both the x- and y-directions between the sensor and the
area to be imaged. Figure 2.13 shows an arrangement used in high-precisior
scanning, where a film negative is mounted onto a drum whose mechanical rotation provides displacement in one dimension. The single sensor is mountec
on a lead screw that provides motion in the perpendicular direction. Because
mechanical motion can be controlled with high precision, this method is an in
expensive (but slow) way to obtain high-resolution images. Other similar me
chanical arrangements use a flat bed, with the sensor moving in two linea
directions. These types of mechanical digitizers sometimes are referred to a.
microdensitometers.Another example of imaging with a single sensor places a laser source coin
cident with the sensor. Moving mirrors are used to control the outgoing bean
in a scanning pattern and to direct the reflected laser signal onto the senso)
This arrangement can be used also to acquire images using strip and array sen
sors, which are discussed in the following two sections.2.3.2 Image Acquisition Using Sensor StripsA geometry that is used much more frequently than single sensors consists of a
in-line arrangement of sensors in the form of a sensor strip, as Fig. 2.12(b) show
The strip provides imaging elements in one direction. Motion perpendicular to th
strip provides imaging in the other direction, as shown in Fig. 2.14(a). This is th
type of arrangement used in most flat bed scanners. Sensing devices with 4000 «
more in-line sensors are possible. In-line sensors are used routinely in airborn
imaging applications, in which the imaging system is mounted on an aircraft thi
2.3 # Image Sensing and Acquisition 71One image line out per
increment of linear molion 
     
 Image
reconstruction  
 
  3-D objectGs"Sensor ringabCross-sectional jmages
of 3-D object   
 
   FIGURE 2.14 (a) Image acquisition using a linear sensor strip. (b) Image acquisition using a circular sensor strip.flies at a constant altitude and speed over the geographical area to be imaged,
One-dimensional imaging sensor strips that respond to various bands of the
electromagnetic spectrum are mounted perpendicular to the direction of
flight. The imaging strip gives one line of an image at a time, and the motion of
the strip completes the other dimension of a two-dimensional image. Lenses
or other focusing schemes are used to project the area to be scanned onto the
sensors.Sensor strips mounted in a ring configuration are used in medical and industria] imaging to obtain cross-sectional (“slice”) images of 3-D objects, as
Fig. 2.14(b) shows. A rotating X-ray source provides illumination and the sensors opposite the source collect the X-ray energy that passes through the object (the sensors obviously have to be sensitive to X-ray energy). This is the
basis for medical and industrial computerized axial tomography (CAT) imaging as indicated in Sections 1.2 and 1.3.2. It is important to note that the output
of the sensors must be processed by reconstruction algorithms whose objective
is to transform the sensed data into meaningful cross-sectional images (see
Section 5.11). In other words, images are not obtained directly from the sensors by motion alone; they require extensive processing. A 3-D digital volume
consisting of stacked images is generated as the object is moved in a direction
2.3 i Image Sensing and Acquisition 73Nhinination (energy}       Output (digitized) imagea Imaging system =(Internal) image planeSeene elementa
b
FIGURE 2.25 An example of the digital image acquisition process. (a) Energy (“illumination”) source. (>) Ar
element of a scene. (c) [maging system. (d) Projection of the scene onto the image plane. (e) Digitized image.cdeand finite; that is.
0 < f(xy) < co (2.3-1)The function f(x, ¥) may be characterized by {wo components: (1) the amount
of source iumination ticident on the scene being viewed, and (2) the amount of illumination reflected by the objects in the scene. Appropriately, these are called the
ilumination and reflectance components and are denoted by f(x, ¥) and r(x, ¥),
respectively. The two functions combine as a product to form f(x, 4°):foxy) = fOy vr (ey) (2.3-2)
where
0 < i(x. ¥) < 90 (2.3-3)
and
O<r(ny) <1 (2.3-4)Equation (2.3-4) indicates that reflectance is bounded by 0 (total absorption)
and 1 (total reflectance). The nature of (x, y) is determined by the ilumination source, and r(x, ¥) is determined by the characteristics of the imaged objeets. Jt is noted that these expressions also are applicable to images formed
via transmission of the illumination through a medium, such as a chest X-ray.
2.4 ® Image Sampling and Quantization 75agente ren  Quantization
c
c
9oo[os oe Oe Ont De Bl Os Os ee ee oO Che Pi ed
Samplingfunction in both coordinates and in amplitude. Digitizing the coordinate values
is called sampling. Digitizing the amplitude vatues is called quantization.The one-dimensional function in Fig. 2.16(b) is a plot of amplitude (intensity
level) values of the continuous image along the line segment AB in Fig. 2.16(a).
The random variations are due to image noise. To sample this function, we take
equally spaced samples along line AB, as shown in Fig. 2.16(c), The spatial location of each sample is indicated by a vertical tick mark in the bottom part of the
figure. The samples are shown as smai] white squares superimposed on the function. The set of these discrete locations gives the sampled function. However, the
values of the samples stil] span (vertically) a continuous range of intensity values, In order to form a digital function, the intensity values also must be converted (quantized) into discrete quantities. The right side of Fig. 2.16(c) shows
the intensity scale divided into eight discrete intervals, ranging from black to
white. The vertical tick marks indicate the specific value assigned to each of the
eight intensity intervals. The continuous intensity levels are quantized by assigning one of the eight values to each sample. The assignment is made depending on
the vertical proximity of a sample to a vertical tick mark, The digital samples
resulting from both sampling and quantization are shown in Fig. 2.16(d), Starting at the top of the image and carrying out this procedure line by line produces
a two-dimensional digital image. It is implied in Fig, 2.16 that, in addition to the
number of discrete levels used, the accuracy achieved in quantization is highly
dependent on the noise content of the sampled signal.Sampling in the manner jus! described assumes that we have a continuous
image in both coordinate directions as wel] as in amplitude. In practice, theab
edFIGURE 2.16
Generating a
digital image.(a) Continuous
image. (b) A scan
line from A to B
in the continuous
image, used to
illustrate ihe
concepts of
sampting and
quantization.(c} Sampling and
quantization.(d) Digitalscan line.
74  Guapter 2 m Digital Image FundamentalsEXAMPLE 2.1:
Some typical
values of
illumination and
reflectance,The discussion of
sampling in this section is
of an intuitive nature, We
consider this topic in
depth in Chapter 4In this case, we would deal with a transmissivity instead of a reflectivity function, but the limits would be the same as in Eq. (2.3-4), and the image function
formed would be modeled as the product in Eq. (2.3-2).@ The values given in Eqs. (2.3-3) and (2.3-4) are theoretical bounds. The following average numerical figures illustrate some typical ranges of i(x, y) for
visible light. On a clear day, the sun may produce in excess of 90,000 Im/m?
of illumination on the surface of the Earth. This figure decreases to less than
10,000 Im/m? on a cloudy day. On a clear evening, a full moon yields about
0.1 Im/m? of illumination. The typical illumination level in a commercial office
is about 1000 Im/m*. Similarly, the following are typical values of r(x, y): 6.01
for black velvet, 0.65 for stainless steel, 0.80 for flat-white wall paint, 0.90 for
silver-plated metal, and 0.93 for snow. aLet the intensity (gray level) of a monochrome image at any coordinates
(Xp, Yo) be denoted by€ = f(x. Yo) (2.3-5)
From Eqs. (2-3-2) through (2.3-4), it is evident that ¢ lies in the range
Loin = € 5 Linax (2.3-6)Ia theory, the only requirement on Lyin is that it be positive, and on Lyx that
it be finite. In practice, Lyin = Emin ?min 200 Lax = émax “max Using the preceding average Office illumination and range of reflectance values as guidelines, we may expect Lyin ¥ 10 and L,,,, * 1000 to be typicat limits for indoor
values in the absence of additional i!{umination.The interval [Lins Lmax] is called the gray (or intensity) scale. Common
practice is to shift this interval numerically to the interval [0, L — 1], where
£ = Ois considered black and ¢ = L — 1is considered white on the gray scale.
All intermediate values are shades of gray varying from black to white.| 2.4 | Image Sampling and QuantizationFrom the discussion in the preceding section, we see that there are numerous
ways to acquire images, but our objective in ali is the same: to generate digital
images from sensed data. The output of most sensors is a continuous voltage
waveform whose amplitude and spatial behavior are related to the physical
phenomenon being sensed. To create a digital image, we need to convert the
continuous sensed data into digital form. This involves two processes: sampling
and quantization.2.4.) Basic Concepts in Sampling and QuantizationThe basic idea behind sampling and quantization is illustrated in Fig. 2.16.
Figure 2.16(a) shows a continuous image f that we want to convert to digital
form. An image may be continuous with respect to the a- and y-coordinates,
and also in amplitude. To convert il to digital form, we have to sample the
72 Chapter 2 % Digital Image Fundamentals1p save cases, We image
ihe soures directly. as in
objaining images of the
sun.Image intensities can   
 Processing oF as a result
oF interpretation, For
example, in cadat images
objects maving toward a
radac system often are
interpreted as having
negative velacities while
objecis moving away are
reted as having     values, When storing anddisplaying imag:
narmally scale (he inten
 perpendicular to the sensor ring. Other modalitics of imaging based on the
CAT principle include magnetic resonance imaging (MRI) and positron emission tomography (PET). The illumination sources. sensors, and types of images
are different. but conceptually they are very similar lo the basic imaging approach shown in Fig, 2.!4(b). «.3.? Image Acquisition Using Sensor ArraysFigure 2.12(c) shows individual sensors arranged in the form of a 2-D array.
Numerous electromagnetic and some ultrasonic sensing devices frequently
are arranged in an array format. This is also the predominant arrangement
found in digital cameras. A typical sensor for these cameras is a CCD array.
which can be manufactured with a broad range of sensing properties and can
be packaged in rugged arrays of 4000 x 4000 clements or more. CCD sensors are used widely in digital cameras and other light sensing instruments.
The response of each sensor is proportional to the integral of the light energy projected onto the surface of the sensor, a property that is used in astronomical and other applications requiring low noise images. Noise reduction
is achieved by Jetting the sensor integrate the input fight signal over minutes
or even hours, Because the sensor array in Fig. 2.12(c} is two-dimensional, its
key advantage is that a complete image can be obtained by focusing the energy pattern onto the surface of the array. Motion obviously is not necessary.
as is the case with the sensor arrangements discussed in the preceding two
sections.The principal manner in which array sensors are used is shown in Fig. 2.15,
This figure shows the energy from an illumination source being reflected
from a scene element (as mentioned at the beginning of this section, thy energy also could be transmitted through the scene elements). The first function
performed by the imaging system in Fig. 2.15(c) is to colicct the incoming
energy and focus it onto an image plane. If the illumination is light. the front
end of the imaging system is an optical lens that projects the viewed scene
onto the lens focal plane, as Fig. 2.15(d) shows. The sensor array, which is
coincident with the focal plane. produces outputs proportional Lo the integral
of the light recetved at cach sensor. Digital anc analog circuitry sweep these
outputs and convert them to an analog signal. which is then digitized by another section of the imaging system. The output is a digital image. as shown
diagrammatically in Fig, 2.1S(e). Conversion of an image into digital form is
the topic of Section 2.4,2.°.. A Simple Image Formation ModelAs introduced in Section 1.1. we denote images by two-dimensional func:
tions of the form f(x,y). The value or amplitude of f at spatial coordinates
(x, y) is a positive scalar quantity whose physical meaning is determined by
the source of the image. When an image is generated from a physical process
its intensity values are proportional to energy radiated by a physical source
(e.g., electromagnctic waves). As a consequence, f(x, y) must be nonzerc
76 Ceapter 2 @ Digital Image Fundamentalsmethod of sampling is determined by the sensor arrangement used to generate
the image. When an image is generated by a single sensing clement combined
with mechanical motion, as in Fig. 2.13, the output of the sensor is quantized in
the manner described above. However, spatial sampling is accomplished by selecting the number of individua] mechanical increments at which we activate
the sensor to collect data. Mechanical motion can be made very exact so, in
principle, there is almost no limit as to how fine we can sample an image using
this approach. In practice, limits on sampling accuracy are determined by
other factors, such as the quality of the optical components of the system.When a sensing strip is used for image acquisition, the number of sensors in
the strip establishes the sampling limitations in one image direction. Mechanical motion in the other direction can be controiled more accurately, but it
makes little sense to try to achieve sampling density in one direction that exceeds the sampling limits established by the number of sensors in the other,
Quantization of the sensor outputs completes the process of generating a digital image.‘When a sensing array is used for image acquisition, there is no motion and
the number of sensors in the array establishes the limits of sampling in both directions. Quantization of the sensor outputs is as before. Figure 2.17 illustrates
this concept. Figure 2.17(a)} shows a continuous image projected onto the
plane of an array sensor. Figure 2.17(b) shows the image after sampling and
quantization. Clearty, the quality of a digital image is determined to a large degree by the number of samples and discrete intensity levels used in sampling
and quantization. However, as we show in Section 2.4.3, image content is also
an important consideration in choosing these parameters.        a whlabFIGURE 2.17 (2) Continuous image projected onto a sensor array, (b) Result of image
sampling and quantization.
2,4 @ Image Sampling and Quantization24.2 Representing Digital ImagesLet f(s, f} represent a continuous image function of two continuous variables,
s and ¢, We convert this function into a digital image by sampling and quantization, as explained in the previous section. Suppose that we sample the
continuous image into a 2-D array, f(x, y}, containing M@ rows and N
columns, where (x, y) are discrete coordinates. For notational clarity and
convenience, we use ipteger values for these discrete coordinates:
x =0,1,2,...,M—-1 and y =0,1,2,...,.N— 1. Thus, for example, the
value of the digital image at the origin is f(0,0), and the next coordinate
value along the first row is f(0, 1). Here, the notation (0, 1) is used to signify
the second sample along the first row. It does not mean that these are the values of the physical coordinates when the image was sampled. In general, the
value of the image at any coordinates (x, y} is denoted f(x, y), where x and y
are integers. The section of the real plane spanned by the coordinates of an
image is called the spatial domain, with x and y being referred to as spatial
variables or spatial coordinates.As Fig. 2.18 shows, there are three basic ways to represent f(x,y).
Figure 2.18(a) is a plot of the function, with two axes determining spatial locationfy)      Ki ;
i atl nt Hit i att
‘< ui i     Origin Origin
it 300 oo0
900 000
coo 000
o00 000
000 ao0
O00 000
vO0 . 000
O00 : 000
ao0d 0000o0d08 oogon
eoReoO aco good
Coe eOOds--1OR0006|
'a
beFIGURE 2.1877(a) Image plottedas a surface.
(b) Imagedisplayed as a
visual intensityarray.(c) Image shownas a2-Dnumerical array(0..5, and 1represent black,gray, and white,
respectively).
78Chapter 2 w Digital image Fundamentalsand the third axis being the values of f (intensities) as a function of the two spatial variables x and y. Although we can infer the structure of the image in this
example by looking at the plot, complex images generally are too detailed and
difficult to interpret from such plots. This representation is useful when working with gray-scale sets whose elements are expressed as triplets of the form
(x, y, z), where x and y are spatial coordinates and z is the value of f at coordinates (x, y). We work with this representation in Section 2.6.4,The representation in Fig. 2.18(b) is much more common. It shows f(x, y)
as it would appear on a monitor or photograph. Here, the intensity of each
point is proportional to the value of f at that point. In this figure, there are only
three equally spaced intensity values. If the intensity is normalized to the interval [0, 1), then each point in the image has the value 0, 0.5, or 1. A monitor
or printer simply converts these three values to black, gray, or white, respectively, as Fig. 2.18(b) shows. The third representation is simply to display the
numerical values of f(x, y) as an array (matrix). In this example, f is of size
600 X 600 elements, or 360,000 numbers. Clearly, printing the complete array
would be cumbersome and convey little information. When developing algotithms, however, this representation is quite useful when only parts of the
image are printed and analyzed as numerical values. Figure 2.18(c) conveys
this concept graphically.We conclude from the previous paragraph that the representations in
Figs. 2.18(b) and (c) are the most useful. [mage displays allow us to view results ata glance, Numerical arrays are used for processing and algorithm development. In equation form, we write the representation of an M x N numerical
vray asF(0,0) F(0, 1) ve fO,.N-1
f(x, y) = iC » i yo Fn ~) | aay
f(M 1,0) f(M-11) - f(M-1N
Both sides of this equation are equivalent ways of expressing a digital image
quantitatively. The right side is a matrix of real numbers, Each element of this
matrix is called an image element, picture element, pixel, or pel, The terms
image and pixel are used throughout the book to denote a digital image and
its elements.In some discussions it is advantageous to use a more traditional matrix notation to denote a digital image and its elements: ,4.0 4.1 cee fo, N~141,0 ayy oo @N-1A= (2.4-2)L4@M-10  @M-1b oe) Mai NH)
24 m Image Sampling and Quantization 79Clearly, a, = f(x = i, y = j) = f(é, j), 80 Egs. (2.4-1) and (2.4-2) are identical
matrices. We can even represent an image as a vector, v. For example, a column
vector of size MN X 1 is formed by letting the first M elements of v be the first
column of A, the next M elements be the second column, and so on. Alternatively, we can use the rows instead of the columns of A to form such a vector.
Either representation is valid, as long as we are consistent.Returning briefly to Fig.2.18, note that the origin of a digital image is at the
top left, with the positive x-axis extending downward and the positive y-axis
extending to the right. This is a conventional representation based on the fact
that many image displays (e.g., TV monitors) sweep an image starting at the
top left and moving to the right one row at a time. More important is the fact
that the first element of a matrix is by convention at the top left of the array, so
choosing the origin of f(x, y) at that point makes sense mathematicaily. Keep
in mind that this representation is the standard right-handed Cartesian coordinate system with which you are familiar.* We simply show the axes pointing
downward and to the right, instead of to the right and up.Expressing sampling and quantization in more formal mathematical terms
can be useful at times. Let Z and R denote the set of integers and the set of
real numbers, respectively. The sampling process may be viewed as partitioning the xy-plane into a grid, with the coordinates of the center of each cell in
the grid being a pair of elements from the Cartesian product Z”, which is the
set of all ordered pairs of elements (z;, z;), with z; and z, being integers from
Z. Hence, f(x, y) is a digital image if (x, y) are integers from Z? and fis a
function that assigns an inlensity value (that is, a real number from the set
of real numbers, R) to each distinct pair of coordinates (x, y). This functional
assignment is the quantization process described earlier. If the intensity levels also are integers (as usually is the case in this and subsequent chapters),
Z replaces R, and a digital image then becomes a 2-D function whose coordinates and amplitude values are integers.This digitization process requires that decisions be made regarding the values for M, N, and for the number, L, of discrete intensity levels. There are no
restrictions placed on M and N, other than they have to be positive integers.
However, due to storage and quantizing hardware considerations, the number
of intensity levels typically is an integer power of 2:L = 2 (2.4-3)We assume that the discrete levels are equally spaced and that they are integers in the interval (0, 2 — 1]. Sometimes, the range of values spanned by the
gray scale is referred to informally as the dynamic range. This is a term used in
different ways in different fields. Here, we define the dynamic range of an imaging system to be the ratio of the maximum measurable intensity to the minimum ‘Recall that a right-handed coordinate system is such that, when the index of the right hand points in the direction of the positive x-axis and the middle finger points in the (perpendicular) direction of the positive
y-axis, the thumb points up. As Fig. 2.18(4) shows, this indeed is the case in our image coordinate system.Often, it is useful for
compuiation ot for
algorithm development
Purposes to scale the £
intensity values to the
range [9, 1],in which case
they cease to be integers.
However, in most cases
these values are scaled
back to the integer range
[0, L ~ 1] for image
storage and display.
80 — Chepter 2 3 Digital Image FundamentalsFIGURE 2.19 An
image exhibiting
saturation and
noise. Saturation is
the highest value
beyond which all
intensity levels are
clipped (note how
the entire
saturated area has
a high, constant
intensity level).
Noise in this case
appears as a grainy
texture pattern.
Noise, especially in
the darker regions
of an image {e.g.,
the stem of the
rose) masks the
lowest detectable
true intensity level.Saturation ——.   detectable intensity level in the system. As a rule, the upper limit is determined
by saturation and the lower limit by #oise (sec Fig. 2.19). Basically, dynamic
range establishes the lowest and highest intensily levels thal a system can represent and, consequently, that an image can have. Closely associated with this concept is image contrast, which we define as the difference in intensity between
the highest and iowest intensily levels in an image. When an appreciable number of pixels in an image have a high dynamic range, we can expect the image
to have high contrast. Conversely, an image with low dynamic range typically
has a dull, washed-out gray look. We discuss these concepts in more detail in
Chapter 3.
The number, b, of bits required to stare a divitized tmage isb= MXN SK (24-4)When M = N, this equation becomes
boo Nek (2.4-5)Table 2.1 shows the number of bils required to store square images with various values of N and k. The number of imensity levels corresponding to each
value of & is shown in parentheses, When it sean have 2° intensity levels,
itis common practice to refer Lo ine riwe as br nage” for example, an
image with 256 possible discrete ttensily values is called un S-bil image, Note
that storage requirements for S-bit images of size 124 1024 and higher are
not insignificant    
      K
2.4 @# Image Sampling and Quantization 81TABLE 2.1
Number of storage bits for variouy values of N and &. L is the number of intensity jevels.N/k WE =2) 2b =4 3(E = 8)         S(L = 32) 6(L = 64) 7(E = 128) 8(L = 256)     4(L = 16)                          
     
 1024 2,048 3.072 4.086 $120 6,144 7,168 8,19264 4,096 8.192 12,288 16,384 20,480 24,576 28,672 32,768
128 16,384 32,768 49,152 65.536 81,920 98,304 114,688 131,072
256 65,536 13107F 196,608 262.144 327,680 393,216 458,752 524,288 
    S12 262,144 524.288 786,432 1,048,576 1,310,720 1,572,864 1,835,008 2,097,152
1024 1.048576 2.097.152 3,145,728 4,194 304 5,242 880 6,291,456 7,340,032 8,388,608
2048 4.194.304 = 8,388,608 = 12,582,912 16,777,216 = 20,971,520 25,165,824 29,369,128 33,554,432
4096 16.777,.216 —33,554.432 50,331,648 67,108,864 83,886,080 100,663,296 117,440,512 134,217,728
8192 67,308,864 134,217,728 201,326,592 268,435,456 335,544,320 402,653,184 469,762,048 — $36,870.912       
           
        
                 2.4.3 Spatial and Intensity ResolutionIntuitively, spatial resolution is a measure of the smallest discernible detail in
an image. Quantitatively, spatial resolution can be stated in a number of ways,
with line pairs per unit distance, and dots (pixels) per unit distance being
among the most common measures. Suppose that we construct a chart with
alternating black and white vertical lines, each of width W units (W can be
less than 1). The width of a dine pair is thus 2W, and there are 1/2W line pairs
per unit distance. For example, if the width of a line is 0.1 mm, there are 5 line
pairs per unit distance (mm). A widely used definition of image resolution is
the largest number of discernibie line pairs per unit distance (e.g., 100 line
pairs per mm). Dots per unit distance is a measure of image resolution used
commonly in the printing and publishing industry, In the U.S., this measure
usually is expressed as dots per inch (dpi). To give you an idea of quality,
newspapers are printed with a resolution of 75 dpi, magazines at 133 dpi,
glossy brochures at 175 dpi, and the book page at which you are presently
looking is printed at 2400 dpi.The key point in the preceding paragraph is that, to be meaningful, measures of spatial resolution must be stated with respect to spatial units. Image
size by itself does not tell the complete story. To say that an image has, say, a
resolution 1024 = 1024 pixels is not a meaningful statement without stating
the spatial dimensions encompassed by the image. Size by itself is helpful only
in making comparisons between imaging capabilities. For example, a digital
camera with a 20-megapixel CCD imaging chip can be expected to have a
higher capability to resolve detail than an 8-megapixel camera, assuming that
both cameras are equipped with comparable lenses and the comparison images are laken at the same distance.Intensity resolution similarly refers to the smallest discernible change in intensity level. We have considerable discretion regarding the number of samples used to generate a digital image, but this is not true regarding the number
2.4 % Image Sampling and Quantization 83         FIGURE 2.20 Typical effects of reducing spatia) resolution. Images shown at: (a} 1250
dpi, (b) 300 dpi, (c) 150 dpi, and (d) 72 dpi. The thin black borders were added for
clarity. They are not part of the data,
82 Chapter 2 m Digital Image FundamentalsEXAMPLE 2.2:
Tlustration of the
effects of reducing
image spatial
resolution.of intensity levels. Based on hardware considerations, the number of intensity
levels usually is an integer power of two, as mentioned in the previous section.
The most common number is 8 bits, with 16 bits being used in some applications in which enhancement of specific intensity ranges is necessary. Intensity
quantization using 32 bits is rare. Sometimes one finds systems that can digitize the intensity levels of an image using 10 or 12 bits, but these are the exception, rather than the rule. Unlike spatial resolution, which must be based on a
per unit of distance basis to be meaningful, it is common practice to refer to
the number of bits used to quantize intensity as the intensity resolution. For example, it is common to say that an image whose intensity is quantized into 256
jievels has 8 bits of intensity resolution. Because true discernible changes in intensity are influenced not only by noise and saturation values but also by the
capabilities of human perception (see Section 2.1}, saying than an image has 8
bits of intensity resolution is nothing more than a statement regarding the
ability of an 8-bit system to quantize intensity in fixed increments of 1/256
units of intensity amplitude.The following two examples illustrate individually the comparative effects
of image size and intensity resolution on discernable detail. Later in this section, we discuss how these two parameters interact in determining perceived
image quality.® Figure 2.20 shows the effects of reducing spatial resolution in an image.
The images in Figs. 2.20(a) through (d) are shown in 1250, 300, 150, and 72
dpi, respectively. Naturally, the lower resolution images are smaller than the
original. For example, the original image is of size 3692 2812 pixels, but the
72 dpi image is an array of size 213 X 162. In order to facilitate comparisons,
all the smaller images were zoomed back to the original size (the method
used for zooming is discussed in Section 2.4.4). This is somewhat equivalent to
“getting closer” to the smaller images so that we can make comparable statements about visible details.There are some small visual differences between Figs, 2.20(a) and (b), the
most notable being a slight distortion in the large black needle. For the most
part, however, Fig. 2.20(b) is quite acceptable. In fact, 300 dpi is the typical
minimum image spatial resolution used for book publishing, so one would
not expect to see much difference here. Figure 2.20(c) begins to show visible
degradation (see, for example, the round edges of the chronometer and the
small needle pointing to 60 on the right side). Figure 2.20(d) shows degradation that is visible in most features of the image. As we discuss in Section
4.5.4, when printing at such low resolutions, the printing and publishing industry uses a number of “tricks” (such as locally varying the pixel size} to
produce much better results than those in Fig. 2.20(d). Also, as we show in
Section 2.4.4, it is possible to improve on the results of Fig. 2.20 by the choice
of interpolation method used. "
84 Chapter 2 m Digital Image FundamentalsEXAMPLE 2.3;
Typical effects of
varying the
number of
intensity leveis in
a digital image.uxaeFIGURE 2.21{a) 452 x 374,
256-level image.
{b)~(d) Image
displayed in 128,
64, and 32
intensity levels,
while keeping the
image size
constant.W@ In this example, we keep the number of samples constant and reduce the
number of intensity levels from 256 to 2, in integer powers of 2. Fgure 2.21 (a)
is a452 X 374 CT projection image, displayed with k = § (256 intensity levels).
Images ‘such as this are obtained by fixing the X-ray source in one position,
thus producing a 2-D image in any desired direction. Projection images are
used as guides to set up the parameters for a CT scanner, including tilt, number
of slices, and range.Figures 2.21(b) through (h) were obtained by reducing the number of bits
from & = 7tak = 1 while keeping the image size constant at 452 X 374 pixels.
The 256-, 128-, and 64-level images are visually identical for all practical purposes, The 32-level image in Fig. 2.21(d), however, has an imperceptible set ol
2.4 &% Image Sampling and Quantization 85very fine ridge-like structures in areas of constant or nearly constant intensity
(particularly in the skull). This effect, caused by the use of an insufficient number of intensity levels in smooth areas of a digital image, is called false contouring, so called because the ridges resemble topographic contours in a map.
False contouring generally is quite visible in images displayed using 16 or jess
uniformly spaced intensity levels, as the images in Figs. 2.21(e) through (h) show.As a very rough rule of thumb, and assuming integer powers of 2 for convenience, images of size 256 * 256 pixels with 64 intensity levels and printed on a
size format on the order of 5 X 5 cm are about the lowest spatial and intensity
resolution images that can be expected to be reasonably free of objectionable
sampling checkerboards and false contouring. r |ef
BhFIGURE 2.21
(Continued)
(e)-(h) Image
displayed in 16,8,
4, and 2 intensity
levels. (Original
courtesy ofDr. David R.
Pickens,
Department of
Radiology &
Radiological
Sciences,
Vanderbilt
University
Medical Center}
86  Chepter 2 m Digital Image Fundamentals RR
FIGURE 2.22 (a) Image with a low level of detail. (b} Image with a medium level of detail. (c) Image with a
relatively large amount of detail. (Lmage (b) courtesy of the Massachusetts Institute of Technology.)The results in Examples 2.2 and 2.3 illustrate the effects produced on image
quality by varying N and & independently. However, these results only partially
answer the question of how varying N and & affects images because we have not
considered yet any relationships that might exist between these two parameters. An early study by Huang [1965] attempted to quantify experimentally the
eftects on image quality produced by varying N and & simullancously, The experiment consisted of a set of subjective tests. Images similar to those shows in
Fig. 2.22 were used.'The woman’s Jace is representative of an image with relatively little detail; the picture of the cameraman contains an intermediatc
amount of detail: and the crowd picture contains, by comparison, a large amount
of detail.Sets of these three Lypes of images were generated by varying V and k.and
observers were then asked to rank them according to their subjective quality.
Results were summarized in the form of so-called isopreference curves in the
Nk-plane, (Figure 2.23 shows average isopreference curves representative of
curves corre sponding to (he images in Mig. 2.22.) Each point in the Nk-plane
represents an image having values af NV and & equal to the coordinates of that
point. Points lying on an isupreference curve correspond to images of cqual
subjective quality, LOwas found in the course of the experiments that the isopreference ct:rves tencted to shift night and upward, but their shapes in each of
the three image categories were similar to those in Fig. 2.23. This is not unexpected, because a shift up and right in the curves.simply means larger values
for N and &. which implies better picture quaitlyThe key point of interest in the context of the present discussion is that isopreference vuryes teed to become more vertical as the detail in the image increases, This result s sts chat for images with a darge amount of ditail
only a few intensity tavels may be needed. For example. the lsopreference
curve in Fig. 2.23 corresponding to the crowd is nearly vertical. This indicatesthat. for a fixed Vales ae A the perceived quality Por this        
 Pf image ts
24 ws Image Sampling and Quantization   32 64 128 256nearly independent of the number of intensity levels used (for the range of intensity levels shown in Fig. 2.23). It is of interest also to note that perceived
quality in the other two image categories remained the same in some intervals
in which the number of samples was increased, but the number of intensity
levels actually decreased. The most likely reason for this result is that a decrease in k tends to increase the apparent contrast, a visual effect that humans
often perceive as improved quality in an image. °2.4.4 Image InterpolationInterpolation is a basic tool used extensively in tasks such as zooming, shrinking, rotating, and geometric corrections. Our principal objective in this section
is to introduce interpolation and apply it to image resizing (shrinking and
zooming), which are basically image resampling methods. Uses of interpolation in applications such as rotation and geometric corrections are discussed in
Section 2.6.5. We also retuin to this topic in Chapter 4, where we discuss image
resampling in more detail.Fundamentally, interpolation is the process of using known data to estimate
values at unknown locations. We begin the discussion of this topic with a simple example. Suppose that an image of size 500 x 500 pixels has to be enlarged 1.5 times to 750 X 750 pixels. A simple way to visualize zooming is to
create an imaginary 750 x 750 grid with the same pixel spacing as the original,
and then shrink it so that it fits exactly over the original image. Obviously, the
pixel spacing in the shrunken 750 750 grid will be less than the pixel spacing
in the original image. To perform intensity-level assignment for any point in
the overlay, we look for its closest pixel in the original image and assign the intensity of that pixel to the new pixel in the 750 x 750 grid. When we are finished assigning intensities to all the points in the overlay grid, we expand it to
the original specified size to obtain the zoomed image.FIGURE 2.23
Typical
isopreference
curves for the
three types of
images inFig. 2.22.87
88 Chapter 2. % Digital Image FundamentalsContrary to what the
name suggests, note Wat
bilinear interpolation is
nop linear because of the
ayterm. EXAMPLE 2.4:
Comparison of
interpolation
approaches for
image shrinking
and zooming.The method just discussed is called acarest neighbor interpolation because it
assigns to each new location the intensity of its nearest neighbor in the original
image (pixel neighborhoods are discussed formally in Section 2.5). This approach is simple but, as we show later in this section, it has the tendency to
produce undesirable artifacts, such as severe distortion of straight edges. For
this reason, it is used infrequently in practice. A more suitable approach is
bilinear interpolation, in which we use the four nearest neighbors to estimate
the intensity at a given location. Let (x, v) denote the coordinates of the tocation to which we want to assign an intensity value (think of it as a point of the
grid described previously), and let v(x, y) denote that intensity value. For bilinear interpolation, the assigned valuc is obtained using the equationwt, vy) = ax + by + cxy ti (2.4-6)where the four coefficients are determined from the four equations in four unknowns that can be written using the four nearest neighbors of point (4. ¥). As
you will see shortly, bilinear interpolation gives much better results than nearest neighbor interpolation, with a modest increase in compulalional burden.
The next level of complexity is bicubic interpolation, which involves the sixteen nearest neighbors of a point. The intensity value assigned to point (x, y) is
obtained using the equation
a3
waxy = SY Naps! (24-7)
620 j=0
where the sixteen coefficients are determined from the sixteen equations in
sixteen unknowns that can be wrilten using the sixteen nearest neighbors of
point (x, y). Observe that Eq. (2.4-7) reduces in form to Ey. (2.4-6) if the limits of both summations in the lormer equation are Ovo i. Generally, bicubic interpolation does a better job of preserving fine detail than its bilinear
counterpart. Bicubic interpolation is the standard used in commercial image
editing programs, such as Adobe Photoshop and Corel Photopaint. & Figure 2.24(a) is the same imuge as Fig. 2.20(d). which was oblained by reducing the resolution of the 1250 dpi image in Fig, 2.20(a) to 72 dpi (the size
shrank from the original size of 3692 x 2812 lo 213 * 162 pixels} and then
zooming the reduced image back to its original size. "fo generate Fig. 2.20(d)
we used nearest neighbor interpolation both to shrink and zoom the image. As
we commented before, the result in Fig. 2.24(a) is rather noor. Figures 2.24(b}
and (c) are the results of repeating the same procedure but using, respectively.
bilinear and bicubic interpolation for both shrinking and zooming. The result
obtained bv using bilinear interpolation is a significant improvement over nearest neighbor interpolation. The bicubic result is sightly sharper than the bilinear image. Figure 2.24(d) is the same as Fig. 2,2U(c). which was obtained using
nearest neighbor interpolation for both shrinking and zooming, We comment:
ed in discussing that figure that reducing the resolution to 150 dpi began showing degradation in the image. Figures 2.24(c) and (f) show the results of using
24 Image Sampling and Quantization 89[             abedefFIGURE 2.24 (a) image reduced to 72 dpi and zoomed back to is original size (3692 X 2812 pixels) using
nearest neighbor tnterpolation. This figure is the same as Fig. 2.20(d). (b) Image shrunk and zoomed using
biJinear interpolation. (c) Same as (b) but using bicubic interpolation. (d)~(f) Same sequence, but shrinking
down to 150 dpi iastend of 72 dpi (Fig. 2.24(d) is the same as Fig, 2.20(c)]. Compare Figs. 2.24(e) and (f),
especially the Jatter, with the original image in Fig. 2.20(a).bilinear and bicubic interpolation, respectively, to shrink and zoom the image.
In spite of a reduction in resolution from 1250 to 150, these last two images
compare reasonably favorably with the original, showing once again the
power of these two interpolation methods. As hefore, bicubic interpolation
yielded slightly sharper results.
2.5 ® Some Basic Relationships between PixelsMixed adjacency is a modification of 8-adjacency. It is introduced to eliminate the
ambiguities that often arise when 8-adjacency is used. For example, consider the
pixel arrangement shown in Fig, 2.25(a) for V = {1}. The three pixels at the top
of Fig. 2.25(b) show multiple (ambiguous) 8-adjacency, as indicated by the dashed
lines. This ambiguity is removed by using m-adjacency, as shown in Fig. 2.25(c).A (digital) path (or curve) from pixel p with coordinates (x, y) to pixel ¢
with coordinates (s, f) is a sequence of distinct pixels with coordinates(Xo, ¥o)s (X15 Ys es tn Pn)where (Xp, Yo) = (+, ¥).(%n: Yn) = (8,4), and pixels (x, y;) and (x;-1, yj-1) are
adjacent for 1 =i n. In this case, n is the length of the path. If
(Xo. Yo) = (Xn: ¥n), the path is a closed path. We can define 4-, 8-, or m-paths
depending on the type of adjacency specified. For example, the paths shown in
Fig. 2.25(b) between the top right and bottom right points are 8-paths, and the
path in Fig. 2.25(c) is an m-path.Let S represent a subset of pixels in an image. Two pixels p and q are said to
be connected in S if there exists a path between them consisting entirely of pixels in S. For any pixel p in S, the ser of pixels that are connected to it in S$ is
called a connected component of S. If it only has one connected component,
then set $ is called a connected set.Let R be a subset of pixels in an image. We call R a region of the image if R
is a connected set. Two regions, R; and R; are said to be adjacent if their union
forms a connected set. Regions that are not adjacent are said to be disjoint. We
consider 4- and 8-adjacency when referring to regions. For our definition to
make sense, the type of adjacency used must be specified. For example, the twa
regions (of 1s) in Fig, 2.25(d) are adjacent only if 8-adjacency is used (according
to the definition in the previous paragraph, a 4-path between the two regions
does not exist, so their union is not a connected set).dots GF a boa onabo
defFIGURE 2.25 (a) An arrangement of pixels. (b) Pixels that are 8-adjacent (adjacency is
shown by dashed Jines; note the ambiguity). {c) »-adjacency. (d) Two regions (of 1s) that
are adjacent if 8-adjecency is used. (e) The circled point is part of the boundary of the
1-valued pixels only if 8-adjacency between the region and background is used. (f) The
inner boundary of the 1-valued region does not form a closed path, but its outer
boundary does.91
90 Chapter 2 @ Digital Image FundamentalsWe use the symbols
and U to denote set
intersection and union,
tespeclively, Given seis
A and B. recail that their
intersection is the sei of
elements that are members of both A and &.
‘The union of these 1wo
sets is the set of elements
that are members of A,
of Bor of doth. We
discuss sets in more
detail in Section 2.6.4.It is possible to use more neighbors in interpolation, and there are more
complex techniques, such as using splines and wavelets, that in some instances
can yield better results than the methods just discussed. While preserving fine
detail is an exceptionally important consideration in image generation for 3-D
graphics (Watt [1993], Shirley [2002]) and in medical image processing
{Lehmann et al. [1999]), the extra computational burden seldom is justifiable
for general-purpose digital image processing, where bilinear or bicubic interpolation typically are the methods of choice.BEER Some Basic Relationships between PixelsIn this section, we consider several important relationships between pixels in a
digital image. As mentioned before, an image is denoted by f(x, y). When referring in this section to a particular pixel, we use lowercase letters, such as p and g.2.5.1 Neighbors of a Pixel
A pixel p at coordinates (x, y) has four horizontal and vertical neighbors whose
coordinates are given by(x+ ly), —Ly). Gy + DGny - YDThis set of pixels, called the 4-neighbors of p, is denoted by N4(p). Each pixel
is a unit distance from (x, y), and some of the neighbor locations of p lie outside the digital image if (x, y) is on the border of the image. We deal with this
issue in Chapter 3.The four diagonal neighbors of p have coordinates(xt+iyt+1).Q¢+1y- 1.x - hy 4+1)¢-Ly
and are denoted by N p(p), These points, together with the 4-neighbors, are called
the 8-neighbors of p, denoted by Ng(p). As before, some of the neighbor locations
in Np(p) and Ng(p) fall outside the image if (x, y) is on the border of the image.2.5.2 Adjacency, Connectivity, Regions, and BoundariesLet V be the set of intensity values used 10 define adjacency. In a binary image,
V = {1} if we are referring to adjacency of pixels with value 1. In a gray-scale
image, the idea is the same, but set V typically contains more elements. For example, in the adjacency of pixels with a range of possible intensity values 0 to 255, set
V could be any subset of these 256 values. We consider three types of adjacency:{a) 4-adjacency. Two pixels p and g with values from V are 4-adjacent if q is in
the set N4(p).
(b) 8-adjacency. Two pixels p and g with values from V are 8-adjacent if q is in
the set Vy(p).
{c) m-adjacency (mixed adjacency). Two pixels p and q with values from V are
m-adjacent if
(i) gis in Np), or
{il) gis in Naf p) and the set Na(p}M N4(q) has no pixels whose values
are from V.
2.5 m Some Basic Relationships between PixelsThis is intuitive. Conceptually, until we arrive at Chapter 10, it is helpful to
think of edges as intensity discontinuities and boundaries as closed paths.2.5.3 Distance Measures
For pixels p, g, and z, with coordinates (x, y), (s, 9, and (v, w), respectively, D
is a distance function or metric if
(a) D(p,q)=0 (D(p,gh=0 iff p= 4),
{b) D(p, q) = D(q, p), and
() D(p,z) = D(p,q) + D(q.2.
The Euclidean distance between p and gq is defined as ;
DAp.q) = [x - s+ — HY (2.5-1)
For this distance measure, the pixels having a distance less than or equal to
some value r from (x, y) are the points contained in a disk of radius r centered
at (x, y).
The D, distance (called the city-block distance) between p and q is defined as
Dp.) = |x — s| + ly >t (2.5-2)In this case, the pixels having a D, distance from (x, y) less than or equal to
some value r form a diamond centered at (x, y). For example, the pixels with
D, distance = 2 from (x, y) (the center point) form the following contours of
constant distance:Ne
NeW
HForRN
Ne hy
Ne2The pixels with D, = 1 are the 4-neighbors of (x, y).
The Ds distance (called the chessboard distance) between p and q is defined asDe(p, 4) = max(}x ~ s|, ly — #[) (2.5-3)In this case, the pixels with Dg distance from (x, y) less than or equal to some
value r form a square centered at (x,y). For example, the pixels with
D, distance < 2 from (x, y) (the center point) form the following contours of
constant distance:222 2 2
211 1 2
2 t 0 1 2
2111 2
222 2 2The pixels with Dy = 1 are the 8-neighbors of (x, y).93
92 Chapter 2 @ Digital Image FundamentalsSuppose that an image contains K disjoint regions, Ry, k = 1,2,...,K,
none of which touches the image border.’ Let R, denote the union of all the K
regions, and let (R,,)° denote its complement (recall that the complement of a
set S is the set of points that are not in S). We call al! the points in R, the
foreground, and ail the points in (R,)° the background of the image.The boundary (also called the border or contour) of a region R is the set of
points that are adjacent to points in the complement of R. Said another way,
the border of a region is the set of pixels in the region that have at least one
background neighbor. Here again, we must specify the connectivity being
used to define adjacency. For example, the point circled in Fig. 2.25(e) is not a
member of the border of the 1-valued region if 4-connectivity is used between
the region and its background. As a rule, adjacency between points in a region
and its background is defined in terms of 8-connectivity to handle situations
like this.The preceding definition sometimes is referred to as the inner border of
the region to distinguish it from its outer border, which is the corresponding
border in the background. This distinction is important in the development of
border-following algorithms. Such algorithms usually are formulated to follow the outer boundary in order to guarantee that the result will form a
closed path. For instance, the inner border of the 1-valued region in Fig,
2.25(4) is the region itself. This border does not satisfy the definition of a
closed path given earlier. On the other hand, the outer border of the region
does form a closed path around the region.If R happens to be an entire image (which we recall is a rectangular set of
pixels), then its boundary is defined as the set of pixels in the first and last rows
and columns of the image. This extra definition is required because an image
has no neighbors beyond its border. Normally, when we refer to a region, we
are referring to a subset of an image, and any pixels in the boundary of the
region that happen to coincide with the border of the image are included implicitly as part of the region boundary.The concept of an edge is found frequently in discussions dealing with regions and boundaries. There is a key difference between these concepts, however. The boundary of a finite region forms a closed path and is thus a
“global” concept. As discussed in detail in Chapter 10, edges are formed from
pixels with derivative values that exceed a preset threshold. Thus, the idea of
an edge is a “local” concept that is based on a measure of intensity-level discontinuity at a point. It is possible to link edge points into edge segments, and
sometimes these segments are linked in such a way that they correspond to
boundaries, but this is not always the case. The one exception in which edges
and boundaries correspond is in binary images. Depending on the type ol
connectivity and edge operators used (we discuss these in Chapter 10), the
edge extracted from a binary region will be the same as the region boundary ‘We make this assumption to avoid having to deal with special cases. This is Jone without loss of gener
alily because if one ot more regions touch the border of an image, we can simply pad the image with :
T-pixel-wide border of background values.
2.6 @ An Introduction to the Mathematical Tools Used in Digital Image Processing 95On the other hand, the mairix product is given byay a2\ by be| _ | andr + ay2ba and + al4, 422 || by bx Andy + Ayby)  Agyby2 + aypbr2 |
We assume array operations throughout the book, unless stated otherwise.
For example, when we refer to raising an image to a power, we mean that
each individual pixel is raised to that power; when we refer to dividing an
image by another, we mean that the division is between corresponding pixel
Pairs, and so on.2.6.2 Linear versus Nonlinear OperationsOne of the most important classifications of an image-processing method is
whether it is linear or nonlinear. Consider a general operator, H, that produces
an output image, g(x, y), for a given input image, f(x, y):H[ PQ, y)] = ex ») (2.6-1)
H is said to be a linear operator ifHlaifi(x. y) + aff je. | = GAL fis y)) + a HLF /Cx, »)|
(2.6-2)
= agit, y) + aj8)(x y)where a;,a;. f;(x, y), and f;(x, ») are arbitrary constants and images (of the
same size), respectively. Equation (2.6-2) indicates that the output of a linear
operation due to the sum of two inputs is the same as performing the operation on the inputs individually and then summing the results. In addition, the
output of a tinear operation to a constant times an input is the same as the output of the operation due to the original input multiplied by that constant. The
first property is called the property of additivity and the second is called the
property of Homogeneity.As a simple example, suppose that H is the sum operator, 2; that is, the
function of this operator is simply to sum its inputs. To test for linearity, we
start with the left side of Eq. (2.6-2) and attempt to prove that it is equal to the
right side:il. These are array summa
Daisy) + Daj fey) tions, not the sums of all
the elements of the
images As such, the suma; DF i(xy) + aj DF (x y) of a single image i theimage itself.Slash + a f(x »)]= a 8)(t, ¥} + 058; (%. ¥)where the first step follows from the fact that summation is distributive. So, an
-xpansion of the left side is equal to the right side of Eq. (2.6-2), and we conslude that the sum operator is linear.
94   
 Proceeding, you
find it helpful to
pload and study the
review material available
im the Tutorials section of
the book Web site. The
review covers iniroductory material on matrices
and vectors, linear systems, set theory, and
probability. 
 
     Chopter 2M Digital Image FundamentalsNote that the D4 and Dy distances between p and q are independent of any
paths that might exist between the points because these distances involve only
the coordinates of the points. If we elect to consider m-adjacency, however, the
D,, distance between two points is defined as the shortest m-path between the
points. In this case, the distance between two pixels will depend on the values
of the pixels along the path, as well as the values of their neighbors. For instance, consider the following arrangement of pixels and assume that p, p2, and
pa, have value 1 and that p, and p; can have a value of 0 or 1:P3 Ps
Pi P2
PSuppose that we consider adjacency of pixels valued 1 (i.c., V = {1}). If p;
and p; are 0, the length of the shortest m-path (the D,, distance) between p
and pg is 2. If p, is 1,then po and p will no longer be m-adjacent (see the definition of m-adjacency) and the length of the shortest m-path becomes 3 (the
path goes through the points pp,p2p4). Similar comments apply if p3 is 1 (and
P, is 0); in this case, the tength of the shortest m-path also is 3. Finally, if both
p, and p;are 1, the length of the shortest m-path between p and p, is 4. In this
case, the path goes through the sequence of points pp; p2psPa
2.6. An Introduction to the Mathematical Tools Used
in Digital Image ProcessingThis section has two principal objectives: (1) to introduce you to the various
mathematical tools we use throughout the book; and (2) to help you begin developing a “feel” for how these tools are used by applying them toa variety of
basic image-processing tasks, some of which will be used numerous times in
subsequent discussions, We expaud the scope of the tools and their application
as necessary in the following chapters.2.6.1 Array versus Matrix OperationsAn array operation involving one or more images is carried out on a pixel-bypixel basis. We mentioned earlier in this chapter that images can be viewed
equivalently as matrices. In fact, there are many situations in which operations between images are carried out using matrix theory (see Section 2.6.6),
It is for this reason that a clear distinction must be made between array and
matrix operations. For example, consider the following 2 X 2 images:ay aya d by br |
an
a, day by bn |
Yhe array product ot these lwo images is
ay2by2
@22by)[2 ay || by by] _ ab)
tay |) by, by | 4021im
96 Chapter 2 m Digital Image FundamentalsOn the other hand, consider the max operation, whose function is to find
the maximum value of the pixels in an image. For our purposes here, the simplest way to prove that this operator is nonlinear, is to find an example that
fails the test in Eq. (2.6-2). Consider the following two imagesn=l 4 and n=(§ ;|and suppose that we let a; = 1 and a, = ~1. To test for linearity, we again
start with the left side of Eq. (2.6-2):ooe{ofs 2] cols J} mds 3]Working next with the right side, we obtain(ymax {f? | + (-1)max{|& a =3+(-1)7=-4The left and right sides of Eq. (2.6-2) are not equal in this case, so we have
proved thal in general the max operator is nonlinear.As you will see in the next three chapters, especially in Chapters 4 and 5, linear Operations are exceptionally important because they are based on a large
body of theoretical and practical results that are applicable to image processing. Nonlinear systems are not nearly as well understood, so their scope of application is more limited. However, you will encounter in the following
chapters several nonlinear image processing operations whose performance
far exceeds what is achievable by their linear counterparts.2.6.3 Arithmetic OperationsArithmetic operations between images are array operations which, as discussed
in Section 2.6.1, means that arithmetic operations are carried out between corresponding pixel pairs. The four arithmetic operations are denoted ass(yy) = F(x. y) + gO ¥)
d(x. y) = fO% ¥) — gory)
ple. y) = fix, y) X BOx ¥)
vy) = fix y= a(n y)(2.6-3)It is understood that the operations are performed between corresponding
pixel pairs in f and g for x = 0.1,2..... M-~-l andy =0,1.2.....N -1
2.6 An Introduction to the Mathematical Tools Used in Digital Image Processing 97where, as usual, M and N are the row and column sizes of the images. Clearly,
s,d,p, and v are images of size M X N also. Note that image arithmetic in the
manner just defined involves images of the same size. The following examples
are indicative of the important role played by arithmetic operations in digital
image processing.@ Let g(x, y) denote a corrupted image formed by the addition of noise,
n(x, y), to a noiseless image f(x, y); that is,a(x, y) = fx. y) + n(x, y) (2.6-4)where the assumption is that at every pair of coordinates (x, y) the noise is uncorrelated* and has zero average value. The objective of the following procedure is to reduce the noise content by adding a set of noisy images, {g;(x, y)}.
This is a technique used frequently for image enhancement.If the noise satisfies the constraints just stated, it can be shown (Problem 2.20)
that if an image 2(x, y) is formed by averaging K different noisy images,B(x, y) = EDM y) (2.6-5)
then it follows that
Efex, y)} = fy) . (2.6-6)
and1Cit) = K Zw) (2.6-7)
where E{(x, y)} is the expected value of %, and 02, and o3,,y are the
variances of % and », respectively, all at coordinates (x, y). The standard deviation (square root of the variance) at any point in the average image ises (2.6-8)ford = fen ,
BUY) ates)
VKAs K increases, Eqs. (2.6-7) and (2.6-8) indicate that the variability (as measured
by the variance or the standard deviation) of the pixel values at cach location
(x, y) decreases. Because E{ g(x, y)} = f(x, y), this means that (x, y) approaches f(x, y) as the number of noisy images used in the averaging process
increases. In practice, the images g;(x, y) must be registered (aligned) in order to
avoid the introduction of blurring and other artifacts in the output image. *Recall that the variance of a random variable < with mean a is defined as EG > my. where EY: fis
the expected value of the argument. The covariance of two random variables 2, and z, is defined as
Elz, — Xz, — 7,))- If the variables are uncorrelated, their covariance is 0.EXAMPLE 2.5:
Addition
(averaging) of
noisy images for
noise reduction.
98 Chapter? w= Digital Image Fundamentals FIGURE 2.26 (a) Image of Galaxy Pair NGC 3314 corrupted by additive Gaussian noise. (b)}-(f) Results of
averaging 5, 10,20, 50, and 100 noisy images. respectively, (Original image courtesy of NASA.)The images shawn in this
example are from a
galaxy pair called NGC
3314, taken by NASA's
Hubble Space Telescope.
NGC 3314 lies about 140
miltion light-years [rom
Earth, in the direction of
the southemn-hemisphere
constellation Hydra. The
bright stars forming a
pinwheel shape near the
center of the front galaxy
were [ormed [rom inter
stellar gas and dustAn important application of image averaging is in the field of astronomy.
where imaging under very low light levels frequently causes sensor noise to
render single images virtually uscless for analysis, Figure 2.26(a} shows an 8-bit
image in which corruption was simulated by adding to if Gaussian noise with
zero mean and a standard deviation of 64 inicusity fevels. This image, bypical of
noisy images taken under low Heht conditions, is useless for all practical purposes, Figures 2.26(b) through (1) show the resulls of averaging 5.10.20, 50, and
100 images, respectively. We see thal the result in Fig. 2.26(e), obtained with
K = SO, is reasonably clean, The image Fis Off). vesulting from averaging
100 noisy images, is only 2 slight improvement over the image in Fig. 2. ?OCe)Addition is a diseiete version of confintivus iulegration. In astionemical          observations, a procuss cquivealenGio the picthort just descriical is to ise the iilegrating capabilities of COD (see Sechon 23.3) ar suman sensers tor neseMU SCH OFeT lane pyrtouds of tanic. € poli also reduction by Ohscryiig £
is used to reduce serisur teise, Phe net ctfeet. haweccr. is arialogeus to averauingabset of noisy diestol ariapees
2.6 # An Introduction to the Mathematical Tools Used in Digital Image Processing 99WA frequent application of image subtraction is in the enhancement of EXAMPLE 2.6:
differences between images. For example, the image in Fig. 2.27(b) was obtained Image subtraction
by setting to zero the least-significant bit of every pixel in Fig. 2.27(a). Visually, '01 enhancing
. bgt : A . differences.these images are indistinguishable. However, as Fig. 2.27(c) shows, subtracting
one image from the other clearly shows their differences. Black (0) values in
this difference image indicaie locations where there is no difference between
the images in Figs. 2.27(a) and (b).As another illustration,we discuss briefly an area of medical imaging called
mask mode radiography, a commercially successful and highly beneficial use
of image subtraction. Consider image differences of the form, a(x, y) = f(x,y) — hx, y) (2.6-9) change deccetion via
uae ‘action is used
In this case h(x, y), the mask, is an X-ray image of a region of a patient’s body jon. whit'a meron ot
captured by an intensified TV camera (instead of traditional X-ray film) locat-— “h#pter 10.
ed opposite an X-ray source. The procedure consists of injecting an X-ray contrast medium into the patient’s bloodstream, taking a series of images called
live images [samples of which are denoted as f(x, y)] of the same anatomical
region as h(x, y), and subtracting the mask from the series of incoming live images after injection of the contrast medium. The net effect of subtracting the
mask from each sample live image is that the areas that are different between
f(x, y) and A(x, y) appear in the output image. g(x, y), as enhanced detail.
Because images can be captured at TV rates, this procedure in essence gives
a movie showing how the contrast medium propagates through the various
arteries in the area being observed.
Figure 2.28(a) shows a mask X-ray image of the top of a patient’s head prior
to injection of an iodine medium into the bloodstream, and Fig. 2,28(b) is a
sample of a live image taken after the medium was injected. Figure 2.28(c) is 
   abeFIGURE 2.27 (a) Infrared image of the Washington, D.C. area. (b} Image obtained by setting to vero the least
significant bit of every pixel in (a). (c) Difference of the two images, scaled to the range [0.255] for chartiy.
100 Chapter 2 @ Digital Image Fundamentalsab
cdFIGURE 2.28
Digital
subtraction
angiography.(a) Mask image.
(b) A live image.
{c) Difference
between (a) and
(b), (d) Enhanced
difference image.
(Figures (a) and
(b) courtesy of
The Image
Sciences Institute,
University
Medical Center,
Utrecht, The
Netherlands.)EXAMPLE 2.7:
Using image
multiplication and
division for
shading
correction. the difference between (a) and (b}. Some fine blood vessel] structures are visible in this image. The difference is clear in Fig. 2.28(d), which was obtained by
enhancing the contrast in (c) (we discuss contrast enhancement in the next
chapter). Figure 2.28(d) is a clear “map” of how the medium is propagating
through the blood vessels in the subject's brain. q%§ An important application of image multiplication (and division) is séading
correction. Suppose that an imaging sensor produces images that can be modeled as the product of a “perfect image,” denoted by /{x. v), times a shading
function, A(x, y); that is, e(x, y) = f(x, yx, y). HAC, y) is known, we can
obtain f(x, y) by multiplying the sensed image by the inverse of A(x, y) (i.e., dividing g by 2). IE A(x, y) is not known, but access to the imaging system is possible, we can obtain an approximation to the shading function by imaging a
target of constant intensity. When the sensor is not available, we often can estimate the shading pattern directly from the image. as we discuss in Section
9.6. Figure 2.29 shows an example of shading correction.Another common use of image multiplication is in masking, also called
region of interest (ROI), operations. The process, illustrated in Fig. 2.30, consists simply of multiplying a given image by a mask image that has Is in the
ROT and 0s elsewhere. There can be more than one ROI in the mask image,
and the shape of the ROI can be arbitrary, although rectangular shapes are
used frequently for ease of implementation. EgA few comments about implementing image arithmetic operations are im
order before we leave this section. In practice, most images are displayed
using 8 bits (even 24-bit color images consist of three separite 8-bit channels).
Thus, we expect image valucs to be in the range [ro o 258. When images
2.6 @ An Introduction to the Mathematical Tools Used in Digital Image Processing 101 FIGURE 2.29 Shading correction. (a) Shaded SEM image of a tungsten filament and support, magnified
approximately 130 times. (b) The shading pattern. (c) Product of {a} by the reciprocal of (b), (Original image
courtesy of Michael Shaffer, Department of Geological Sciences, University of Oregon, Eugenc.)are saved in a standard format, such as TIFF or JPEG, conversion to this
range is automatic, However, the approach used for the conversion depends
on the system used. For example, the values in the difference of two 8-bit images can range from a minimum of —255 to a maximum of 25S, and the values
of a sum image can range from 0 to 510. Many software packages simply set
all negative values to 0 and set to 255 all values that exceed this limit when
converting images to 8 bits, Given an image f, an approach that guarantees
that the full range of an arithmetic operation between images is “captured”
into a fixed number of bits is as follows. First, we perform the operationfm = f > min(f) {2.6-10) eeeFIGURE 2.30 (a) Digital dental X-ray image. (hE R
1 and black corresponds to 0). (c} Product ol ¢ 
  vask for isolaliag iecth wiih flings (while corvespa mids 1Gb) :
102 Chapter 2 @ Digital Image Fundamentalswhich creates an image whose minimum value is 0. Then, we perform the
operationfe = K[ fa/max( Fn] (26-11)
which creates a scaled image, f,, whose values are in the range (0, K]. When
working with 8-bit images, setting K = 255 gives us a scaled image whose intensities span the full 8-bit scale from 0 to 255, Similar comments apply to 16-bit
images or higher. This approach can be used for all arithmetic operations.
When performing division, we have the extra requirement that a small number
should be added to the pixels of the divisor image to avoid division by 0.2.6.4 Set and Logical OperationsIn this section, we introduce briefly some important set and logical operations.
We also introduce the concept of a fuzzy set.Basic set operations
Let A be a set composed of ordered pairs of real numbers. If a = (a), a2) is an
element of A, then we writeacd (2.6-12)
Similarly, if a is not an element of A, we write
aga (2.6-13}
The set with no elements is called the nuff or empty set and is denoted by the
symbol @.
Aset is specified by the contents of two braces: { - }. For example, when we
write an expression of the form C = {wlw = —d,de D}, we mean that set €is the set of elements, w, such that w is formed by multiplying each of the ele
ments of set D by — 1, One way in which sets are used in image processing is tc
let the elements of sets be the coordinates of pixels (ordered pairs of integers.
representing regions (objects) in an image.If every element of aset A is also an element of a set B, then A is said to be
a subset of B, denoted asACB (2.6-14
The union of two sets A and B, denoted by
C=AUB (2.6-15is the set of elements belonging to either A, B, or both. Similarly, th
intersection of two sets A and B, denoted byD=ANB (2.6-16is the set of elements belonging to both A and B. Two sets A and B are said to b:
disjoint or mutually exclusive if they have no common elements, in which caseANB=2 (2.6-17
104 Chopter 2 Digital Image Fundamentalsabe
deFIGURE 2.31(a) Two sets of
coordinates, A and B,
in 2-D space. (b) The
union of A and B.
(c) The intersection
of A and &.(d) The
complement of A.
(e) The difference
between A and B, In
(b)-(e) the shaded
areas represent the
members of the set
operation indicated.EXAMPLE 28:
Set operations
involving image
intensities,abe
FIGURE 2.32 Setoperations
involving grayscale images.{a) Original
image. (b) Image
negative obtained
using set
complementation.
(c} The union of
(a) and a constant
image.(Original image
courtesy of G.E.
Medical Systems.)              @ Let the elements of a gray-scale image be represented by a set A whose
elements are triplets of the form (x, y, z}, where x and y are spatial coordinates and z denotes intensity, as mentioned in Section 2.4.2. We can define
the complement of A as the sct Ao = {(x,y, K ~ 2l€x,y, z)e A$, which
simply denotes the set of pixels of A whose intensities have been subtracted
from a constant K. ‘This constant is equal to 2k 1, where & is the number of
intensity bits used to represent z. Let A denote the 8-bil gray-scale image in
Fig. 2.32(a), and suppose that we want to form the negative of A using set
2.6 ® An Introduction to the Mathematical Tools Used in Digital Image ProcessingThe set universe, U, is the set of all elements in a given application. By definition, all set elements in a given application are members of the universe defined for that application. For example, if you are working with the set of real
numbers, then the set universe is the real line, which contains ali the real numbers, In image processing, we typically define the universe to be the rectangle
containing all the pixels in an image.The complement of a set A is the set of elements that are not in A:.A‘ = {wlw¢ A} (2.6-18)
The difference of two sets A and B, denoted A ~ B, is defined as
A- B= {wlwe A,w¢e B} = ANB (2.6-19)We see that this is the set of elements that belong to A, but not to B. We could,
for example, define A° in terms of U and the set difference operation:
AS=U-A.Figure 2.31 illustrates the preceding concepts, where the universe is the set
of coordinates contained within the rectangle shown, and sets A and B are the
sets of coordinates contained within the boundaries shown. The result of the
set operation indicated in each figure is shown in gray.In the preceding discussion, set membership is based on position (coordinates). An implicit assumption when working with images is that the intensity
of all pixels in the sets is the same, as we have not defined set operations involving intensity values (e.g., we have not specified what the intensities in the
intersection of two sets is), The only way that the operations illustrated in Fig.
2.31 can make sense is if the images containing the sets are binary, in which case
we can talk about set membership based on coordinates, the assumption being
that all member of the sets have the same intensity. We discuss this in more detail in the following subsection.When dealing with gray-scale images, the preceding concepts are not applicable, because we have to specify the intensities of all the pixels resulting
from a set operation. In fact, as you will see in Sections 3.8 and 9.6, the union
and intersection operations for gray-scale values usually are defined as the
max and min of corresponding pixel pairs, respectively, while the complement
is defined as the pairwise differences between a constant and the intensity of
every pixel in an image. The fact that we deal with corresponding pixel pairs
tells us that gray-scale set operations are array operations, as defined in
Section 2.6.1. The following example is a brief illustration of set operations involving gray-scale images. We discuss these concepts further in the two sections mentioned above. ‘The operations in Eqs. (2.6-12)-(2.6-19) are the basis for the algebra of sets, which starts with properties
such as the commurative laws: AUB = BUA and AO B= BMA, and from these develops a broad
theory based on set operations. A treatment of the algebra of sets is beyond the scope of the present discussion, but you should be aware of its existence.103
2.6 ® An Introduction to the Mathematical Tools Used in Digital Image Processingoperations. We simply form the set A, = AC = {(x, y, 255 — z)l(x. y,z) eA}.
Note that the coordinates are carried over, so A,, is an image of the same size
as A. Figure 2.32(b) shows the result.The union of two gray-scale sets A and B may be defined as the setAUB= {max(a. jaca, bes}
zThat is, the union of two gtay-scale sets (images) is an array formed from the
maximum intensity between pairs of spatially corresponding elements. Again,
note that coordinates carry over, so the union of A and B is an image of the
same size as these two images. As an illustration, suppose that A again represents the image in Fig. 2.32(a), and let B denote a rectangular array of the
same size as A, but in which all values of z are equal to 3 times the mean intensity, 77, of the elements of A. Figure 2.32(c) shows the result of performing
the set union, in which al] values exceeding 372 appear as values from A and all
other pixels have value 3m, which is a mid-gray value. *Logical operationsWhen dealing with binary images, we can think of foreground (1-valued) and
background (0-valued) sets of pixels. Then, if we define regions (objects) as
being composed of foreground pixels, the set operations illustrated in Fig. 2.31
become operations between the coordinates of objects in a binary image.
When dealing with binary images, it is common practice to refer to union, intersection, and complement as the OR, AND, and NQT J/ogical operations,
where “logical” arises from logic theory in which 1 and 0 denote true and false,
respectively.Consider two regions (sets) A and B composed of foreground pixels. The
OR of these two sets is the set of elements (coordinates) belonging either to A
or B or to both. The AND operation is the set of elements that are common to
A and B. The NOT operation of a set A is the set of elements not in A. Because we are dealing with images, if A is a given set of foreground pixels,
NOT(A) is the set of all pixels in the image that are not in A, these pixels
being background pixels and possibly other foreground pixels, We can think
of this operation as turning all elements in A to 6 (black) and all the elements
not in A to | (white). Figure 2.33 illustrates these operations. Note in the
fourth row that the result of the operation shown is the set of foreground pixels that belong to A but not to B, which is the definition of set difference in
Eq. (2.6-19). The last row in the figure is the XOR (exclusive OR) operation,
which is the set of foreground pixels belonging to A or B, but not both. Observe that the preceding operations are between regions, which clearly can be
irregular and of different sizes, This is as opposed to the gray-scale operations
discussed earlier, which are array operations and thus require sets whose spatial dimensions are the same. That is, gray-scale set operations involve complete images, as opposed to regions of images.We need be concerned in theory only with the cability to implement the AND,
OR, and NOT logic operators because these three operators are functionally105
106 Chapter 2 mt Digital Image FundamentalsFIGURE 2.33 NOTA)
Mlustration oflogical operations a
involvingforeground (A) AND (B)
(white) pixels.
Black represents
binary Os and
white binary Is.
The dashed lines
are shown for
reference only.
They are not part
of the result.Zz Eo]ANfess  (A) OR (8). {A) AND [NOT (8}]   (4) XOR (8)KOR2 complete. In other words, any other logic operator can be implemented by using
only these three basic functions, as in the fourth row of Fig. 2.33, where we implemented the set difference operation using AND and NOT. Logic operations
are used extensively in image morphology, the topic of Chapter 9.Fuzzy setsThe preceding set and logical results are crisp concepts, in the sense that elements either are or are not members of a set. This presents a serious limitation
in some applications. Consider a simple example. Suppose that we wish to categorize al] people in the world as being young or not young. Using crisp sets,
let U denote the set of all people and Set A be a subset of U, which we call the
set of young people. In order to form set A, we necd a membership function
that assigns a vajue of 1 or i to every clement (person) in U. Hf the value assigned to an element of U is 1, then that ciement is a member of A; otherwise
it is not. Because we are dealing with a bi-valucd logic, the membership function simply defines a threshold at or below which a,person is considered young,
and above which a person is considered not young. Suppose that we define as
young any person of age 20 or younge. We sev an immediate difficulty. A person whose age is 20 years and | sec would noi be a meniber of the set of young
people. This limitation arises regardless of the ave threshold we use 10 classify a
person as being young. What we need is moe flexibility in what we mean by
“young,” that is, we need a graduaf transition from young to not young, The theory of fuzzy sets implements this concept fy uuhiving iembershiys functions
2.6 ® An Introduction to the Mathematical Tools Used in Digital Image Processing 107that are graduai between the limit values of 1 (definitely young) to 0 (definitely not young). Using fuzzy sets, we can make a statement such as a person being
50% young (in the middle of the transition between young and not young). In
other words, age is an imprecise concept, and fuzzy logic provides the tools to
deal with such concepts. We explore fuzzy sets in detail in Section 3.8.2.6.4 Spatial Operations
Spatial operations are performed directly on the pixels of a given image. Weclassify spatial operations into three broad categories: (1) single-pixel operations, (2) neighborhood operations, and (3) geometric spatial transformations.Single-pixel operationsThe simplest operation we perform on a digital image is to alter the values of
its individual pixels based on their intensity. This type of process may be expressed as a transformation function, 7, of the form:s = T(z) (2.6-20)where z is the intensity of a pixel in the original image and s is the (mapped)
intensity of the corresponding pixel in the processed image. For example,
Fig. 2.34 shows the transformation used to obtain the negative of an 8-bit
image, such as the image in Fig. 2.32(b), which we obtained using set operations.
We discuss in Chapter 3 a number of techniques for specifying intensity transformation functions.Neighborhood operationsLet S,, denote the set of coordinates of a neighborhood centered on an arbitrary point (x, y) in an image, f. Neighborhood processing generates a corresponding pixel at the same coordinates in an output (processed) image, g, such
that the value of that pixel is determined by a specified operation involving the
pixels in the input image with coordinates in S,,. For example, suppose that
the specified operation is to compute the average value of the pixels in a recangular neighborhood of size m X n centered on (x, y). The locations of pixelss= T(z)
255 a 2 255FIGURE 2.34 Intensity
transformation
function used to
obtain the negative of
an 8-bit image. The
dashed arrows show
transformation of an
arbitrary input
intensity value zp into
its corresponding
output value sp.
108 Chapter 2 @ Digital Image FundamentalsabedFIGURE 2.35
Local averaging
using
neighborhood
processing, The
procedure is
illustrated in(a) and (b) for a
rectangular
neighborhood.
(c) The aortic
angiogram
discussed in
Section 1.3.2.
(a) The result of
using Eq. (2.6-21)with m =n = 41.The images are of
size 790 X 686
pixels.      The value of this pixel
is the average value of the
pixels in 3,,        Image f Image g  in this region constitute the set §,,. Figures 2.35(a) and (b) illustrate the
process, We can express this operation in equation farm asD> frerele Sey1FLEET
iey) (2.6-24)where r and ¢ are the row and column coordinates, of the pixels whose coardinates are members ol the set S,.. image g is created by varying the coordinates (x, ))) so that the center of the neighborhood moves irom pixel to pixel in
image f. and repeating the neighborhood operation at cach new tocatian. bor
instance, the image in Fig. 2.39(4) was created in this manner using a neigh
borhood of size 4! < 41. The net effect is lo pertorm local blerrin
inal image. This tvpe of process is used, for example, to elinunate
and thus render * blobs” correspon  mall de   Santo (Ne larges: replicas of ue iia
110 = Ghapter 2 w Digital Image FundamentalsTABLE 2,2
Affine transformations based on Eq. (2.6-23). Identity 0 0 x=v
o 1 4 yee »
00 1] i
x
Scaling ¢ 0 0 x= CU
oO vs 0 y = cyto
0 oO t
Rotation cosé@ sind x= vcosé ~ wsind
-sin@ cos@ 0 y = vsin @ + wcosé
0 o 1
Translation 1 0 0] x=0th —21 0 yowts, lr
ty}  Shear (vertical) 1 0 0] x=0ts,w
te 1 0 yaw aa
i 0 01
J
Shear (horizontal) Ls, 07] xu
010 yruytw
001  each location, (v, w), computing the spatial location, (x, y), of the corresponding pixel in the output image using Eq. (2.6-23) directly. A problem with the
forward mapping approach is that two or more pixels in the input image can
be transformed to the same location in the output image, raising the question
of how to combine multiple output values into a single output pixel. In addition, it is possible that some output locations may not be assigned a pixel at all.
The second approach, called inverse mapping, scans the output pixel locations
and, at each location, (x, y), computes the corresponding location in the input
image using (v, w) = T(x, y). It then interpolates (using one of the techniques discussed in Section 2.4.4) among the nearest input pixels to determine
the intensity of the output pixel value. Inverse mappings are more efficient to
implement than forward mappings and are used in numerous commercial implemeniations of spatial transformations (for example, MATLAB uses this
approach).
2.6 ® An Introduction to the Mathematical Tools Used in Digital Image Processing 109discuss neighborhood processing in Chapters 3 and 5, and in several other
places in the book.Geometric spatial transformations and image registrationGeometric transformations modify the spatial relationship between pixels in an
image. These transformations often are called rubber-sheez transformations because they may be viewed as analogous to “printing” an image on a sheet of
rubber and then stretching the sheet according to a predefined set of rules, In
terms of digital image processing, a geometric transformation consists of two
basic operations: (1) a spatial transformation of coordinates and (2) intensity
interpolation that assigns intensity values to the spatially transformed pixels.
The transformation of coordinates may be expressed as(x, y) = T{(v, w)} (2.6-22)where (v, w) are pixel coordinates in the original image and (x, y) are the corresponding pixel coordinates in the transformed image. For example, the
transformation (x, y} = T{(v, w)} = (v/2, w/2) shrinks the original image to
half its size in both spatial directions. One of the most commonly used spatial
coordinate transformations is the affine transform (Wolberg [1990]), which has
the general formfh fz 0
[x yI]=[vwlT=[ew 1] i wt 0 (2.6-23)
ty, ty 1This transformation can scale, rotate, translate, or sheer a set of coordinate
points, depending on the value chosen for the elements of matrix T. Table 2.2
illustrates the matrix values used to implement these transformations. The real
power of the matrix representation in Eq. (2.6-23) is that it provides the framework for concatenating together a sequence of operations. For example, if we
want to resize an image, rotate it, and move the result to some location, we
simply form a 3 x 3 matrix equal to the product of the scaling, rotation, and
translation matrices from Table 2.2.The preceding transformations relocate pixels on an image to new locations. To complete the process, we have to assign intensity values to those locations. This task is accomplished using intensity interpolation. We already
discussed this topic in Section 2.4.4. We began that section with an example of
zooming an image and discussed the issue of intensity assignment to new pixel
locations. Zooming is simply scaling, as detailed in the second row of Table 2.2,
and an analysis similar to the one we developed for zooming is applicable to
the problem of assigning intensity values to the relocated pixels resulting from
the other transformations in Table 2.2. As in Section 2.4.4, we consider nearest
neighbor, bilinear, and bicubic interpolation techniques when working with
these transformations.In practice, we can use Eq, (2,6-23) in two basic ways, The first, called a
forward mapping, consists of scanning the pixels of the input image and, at
2.6 & An Introduction to the Mathematical Tools Used in Digital Image Processing 111a BiedFIGURE 2.36 (a) A 300 dpi image of the letter T. (b) Image rotated 21° using nearest neighbor interpolation
to assign intensity values to the spatially transformed pixels. {c) Image rotated 21° using bilinear
interpolation. (d) Image rotated 21° using bicubic interpolation. The cnlarged sections show edge detail for
the three interpolation approaches.B The objective of this example is to illustrate image rotation using an affine EXAMPLE 2.9:
transform. Figure 2.36(a) shows a 300 dpi image and Figs. 2.36(b)-(d) are the re-_ Image rotation
sults of rotating the original image by 21°, using nearest neighbor, bilinear, and and avinied
bicubic interpolation, respectively. Rotation is one of the most demanding geo- erporanen
metric transformations in terms of preserving straight-line features. As we see inthe figure, nearest neighbor interpolation produced the most jagged edges and, asin Section 2.4.4, bilincar interpolation yielded significantly improved results. Asbefore, using bicubic interpolation produced slightly sharper results. In fact if youcompare the enlarged detail in Figs. 2.36(c) and (d), you will notice in the middleof the subimages that the number of vertical gray “blocks” that provide the in
tensity transition from light to dark in Fig. 2.36{c) is larger than the correspond
ing number of blocks in (d). indicting that the latter is a sharper edge. Similarresults would be obtained with the other spatial transformations in Table 2.2 thatrequire interpotation (the identily transformation does not. and neither does thetranslation transformation if the increments are an integer number of pixels).This example was implemented using the Inverse mapping approach discussed inthe preceding paragraph. ®Image registration is an important application of digital image processing
used to align two or more images of the same scene. In the preceding discussion, the form of the transformation function required to achieve a desired
geometric transformation was known, [n image registration, we have available
the input and output images. bul the specific tansfarmation that produced the
output image from the inpul generals is unknown. The problem. then, is to estimate the transformation function and then use it to register the two images.
To clarify terminology. the input iniage is the image that we wish to transform,
and what we cal] the referenee unege is Ue image agaist which we want to
register the input.
112 Chapter 2. @ Digital Image FundamentalsFor example, it may be of interest to align (register) two or more images
‘taken at approximately the same time, but using different imaging systems,
such as an MRI (magnetic resonance imaging) scanner and a PET (positron
emission tomography) scanner. Or, perhaps the images were taken at different
times using the same instrument, such as satellite images of a given location
taken several days, months, or even years apart. In either case, combining the
images or performing quantitative analysis and comparisons between them requires compensating for geometric distortions caused by differences in viewing angle, distance, and orientation; sensor resolution; shift in object positions;
and other factors.One of the principal approaches for solving the prob{em just discussed is to
use tie points (also called control points), which are corresponding points
whose locations are known precisely in the input and reference images. There
are numerous ways to select tie points, ranging from interactively selecting
them to applying algorithms that attempt to detect these points automatically,
In some applications, imaging systems have physical artifacts (such as small
metallic objects} embedded in the imaging sensors. These produce a set of
known points (called reseat marks) directly on all images captured by the system, which can be used as guides for establishing tie points.The problem of estimating the transformation function is one of modeling.
For example, suppose that we have a set of four tie points each in an input and a
reference image. A simple model based on a bilinear approximation is given byx= cv + cow + cyvw + cy (2.6-24)and
y = csv + cw + cpuw t cy (2.6-25)where, during the estimation phase, (v, w) and (x, y) are the coordinates of tie
points in the input and reference images, respectively. If we have four pairs of
corresponding tie points in both images, we can write eight equations using
Egs. (2.6-24) and (2.6-25) and use them to solve for the eight unknown coefficients, C;, Cz,..., Cy. These coefficients constitute the model that transforms
the pixels of one image into the locations of the pixels of the other to achieve
registration.Once we have the coefficients, Eqs. (2.6-24) and (2,.6-25) become our vehicle for transforming all the pixels in the input image to generate the desired
new image, which, if the tie points were selected correctly, should be registered
with the reference image. In situations where four tie points are insufficient to
obtain satisfactory registration, an approach used frequently is to select a larger
number of tie points and then treat the quadrilaterals formed by groups of
four tie points as subimages. The subimages are processed as above, with all
the pixels within a quadrilateral being transformed using the coefficients determined from those tie points. Then we move to another set of four tie points
and repeat the procedure until all quadrilateral regions have been processed.
Of course. it is possible to use regions that are more complex than quadrilaterals and employ more complex models, such as polynomials fitted by Jeast
2.6 « An Introduction to the Mathematical Tools Used in Digital Image Processing — 113squares algorithms. Jn general, the number of control points and sophistication
of the model required to solve a problem is dependent on the severity of the
geometric distortion. Finally. keep in mind that the transtormation defined by
Eqs. (2.6-24) and (2.6-25). or any other model for that matter, simply maps the
spatial coordinates of the pixels in the inpul image. We still need (o perform intensity interpolation using any of the methods discussed previously to assign
intensity values to those pixels.
5
& Figure 2.37(a) shows a reference image and Fig. 2.37(b) shows the same EXAMPLE 2.10:
image, but distorted geometrically by vertical and horizontal shear. Qut objec: Image
tive is to use the reference image to obtain tic points and then use the tic Tststtatlon
points to register the images. The tie points we selected (manually) are shown
as small white squares near the corners of the images (we needed only four tie ab
cdFIGURE 2.37
Imige
repistration.(a) Reference
image. (b} Input
(peometrically
distorted image).
Corresponding tic
points are shown
as small while
squares near the
cuimers,{c} Registered
image (note the
errors inthe
border}(d} Difference
between (a) and
fc). showing pore
POPAM TAnLOUerrors.&
eeeHesaaaaadadad Eo
ee0eaaaaaaadad
114 Chapter 2 m Digital Image FundamentalsConsult the Tutorials section in the book Web sitefor a brief tutorial on vectors and matrices.FIGURE 2,38
Formation of a
vector from
corresponding
pixel values in
three RGB
component
images.points because the distortion is linear shear in both directions). Figure 2.37(c)
shows the result of using these tie points in the procedure discussed in the preceding paragraphs to achieve registration. We note that registration was not
perfect, as is evident by the black edges in Fig. 2.37(c). The difference image in
Fig. 2.37(d) shows more clearly the slight lack of registration between the reference and corrected images. The reason for the discrepancies is error in the manual selection of the tie points, It is difficult to achieve perfect matches for tie
points when distortion is so severe. td]2.6.6 Vector and Matrix OperationsMultispectral image processing is a typical area in which vector and matrix operations are used routinely. For example, you will learn in Chapter 6 that color
images are formed in RGB color space by using red, green, and blue component
images, as Fig. 2.38 illustrates. Here we see that each pixel of an RGB image has
three components, which can be organized in the form of a column vectorz=| 2% (2.6-26)where z, is the intensity of the pixel in the red image, and the other two elements are the corresponding pixel intensities in the green and blue images,
respectively. Thus an RGB color image of size M < N can be represented by
three component images of this size, or by a total of MN 3-D vectors, A general
multispectral case involving 1 component images (e.g., see Fig. 1.10) will result
in n-dimensional vectors. We use this type of vector representation in parts of
Chapters 6, 10, 11, and 12.Once pixels have been represented as vectors we have at our disposal the
tools of vector-matrix theory. For example, the Euclidean distance, D, between
a pixel vector z and an arbitrary point a in n-dimensional space is defined as
the vector productD(a) = [@- "a ~ af (2.6-27)= [Ca — a)? + (ee — a)? + + (ey > cat
22
23| Component image 3 (Blue)
Component image 2 (Green)   Component image | (Red)
2.6 @ An Introduction to the Mathematical Tools Used in Digital Image ProcessingWe see that this is a generalization of the 2-D Euclidean distance defined in
Eq. (2.5-1), Equation (2.6-27) sometimes is referred to as a vector norm, denoted by ||z ~ all. We will use distance computations numerous times in later
chapters.Another important advantage of pixel vectors is in linear transformations,
represented as. w= A(z~—a) (2.6-28)where A is a matrix of size m X x” and z and a are column vectors of size
n X 1. As you will learn later, transformations of this type have a number of
useful applications in image processing,As noted in Eq. (2.4-2), entire images can be treated as matrices (or, equivalently, as vectors), a fact that has important implication in the solution of numerous image processing problems. For example, we can express an image of
size M X N as a vector of dimension MN X 1 by letting the first row of the
image be the first N elements of the vector, the second row the next N elements, and so on, With images formed in this manner, we can express a broad
range of linear processes applied to an image by using the notationg=Hf+a_ (2.6-29)where fis an MN X 1 vector representing an input image,n is an MN X 1 vector representing an M X N noise pattern, g is an MN X 1 vector representing
a processed image, and H isan MN X MN matrix representing a linear process
applied to the input image (see Section 2.6.2 regarding linear processes). It is
possible, for example, to develop an entire body of generalized techniques for
image restoration starting with Eq. (2.6-29), as we discuss in Section 5.9. We
touch on the topic of using matrices again in the following section, and show
other uses of matrices for image processing in Chapters 5, 8, 11, and 12.2.6.7 Image TransformsAll the image processing approaches discussed thus far operate directly on the
pixels of the input image; that is, they work directly in the spatial domain. In
some cases, image processing tasks are best formulated by transforming the
input images, carrying the specified task in a transform domain, and applying
the inverse transform to return to the spatial domain. You will encounter a
number of different transforms as you proceed through the book. A particularly important class of 2-D linear transforms, denoted T(u, v), can be expressed in the general formM-1 N=1
Tv) = >} DS fey yr(ny, wv) (2.6-30)x=0 y=0
where f(x, y) is the input image, r(x, y, u, v) is called the forward transformation kernel, and Eq. (2.6-30) is evaluated for u = 0,1,2,...,M@ — 1 and
v = 0,1,2,..., M — 1. As before, x and y are spatial variables, while M and N115
116 Chopter 2 @ Digital Image FundamentalsFIGURE 2.39
General approach
for operating in
the linear
transform
domain.EXAMPLE 2.11;
Image processing
in the transform
domain.abedFIGURE 2.40(a) image corrupted
by sinusoidal
interference. (b)
Magnitude of the
Fourier transform
showing the bursts
of energy responsible
for the interference.
(c) Mask used to
eliminate the energy
bursts. (d) Result of
computing the
inverse of the
modified Fourier
transform, (Original
image courtesy of
NASA.)   T(x, [ 6, 5 R(T, v)| Pp
. peration | Te Inverse Lg
fle. y) 7 Transform R Incr gy)
—— IN neue siorm fe
Spatial Spatial
domain Transform domain doniatinare the row and column dimensions of f. Variables # and v are called the
transform variables. T(u, ¢) is called the forward transform of f(x, y), Given
T(u, v), we can recover f(x, y) using the inverse transform of T(r, 0},M~\ N~1f(x,y) = > Ss Tu, v)s€x, yu, vyv=0 v=0(26-31)for x = 0,1,2,...,4f — 12 and y= 0,1,2,...,M ~ 1. where s(x yt. v) is
called the inverse transformation kernel. Together, Eqs. (2.6-30) and (2.6-31)
are called a transform pair.Figure 2.39 shows the basic steps for performing image processing in the
linear transform domain. First, the input image is transformed, the transform is
then modified by a predefined operation, and, finally, the outpul image is obtained by computing the inverse of the modified transform. Thus, we see that
the process goes from the spatial domain to the transform domain and then
back to the spatial domain.@ Figure 2.40 shows an example of the steps in Fig, 2.39, in this case the transform used was the Fourier transform, which we mention briefly later in this
section and discuss in detail in Chapter 4. Figure 2.4(){a) is an image corrupted
118 — Chapiers 2. @ Digital image Fundamentals Consult the Tutorials section in the book Web site
for a brief overview of
probability theory.forward and inverse kernels of a transform pair satisfy these two conditions,
and f(x, y) is a square image of size M x M, Egs. (2.6-30) and (2.6-31) can be
expressed in matrix form:T= AFA (2.6-38)where Fis an M X M matrix containing the elements of f(x, y) [see Eq. (2.4-2)],
A is an M X M matrix with elements a, = r,(i,j), and T is the resulting
M X M transform, with values T(u, v) for u,v = 0,1,2,...,M@ —- 1.To obtain the inverse transform, we pre- and post-multiply Eq. (2.6-38) by
an inverse transformation matrix B:BYB = BAFAB (2.6-39)
IfB= A,
F = BTB (2.6-40)indicating that F [whose elements are equal to image f(x, y)] can be recovered completely from its forward transform. If B is not equal to A“, then use
of Eq. (2.6-40) yields an approximation:F = BAFAB (2.6-41)In addition to the Fourier transform, a number of important transforms, including the Walsh, Hadamard, discrete cosine, Haar, and slant transforms, can
be expressed in the form of Eqs. (2.6-30) and (2.6-31) or, equivalently, in the
form of Eqs. (2.6-38) and (2.6-40). We discuss several of these and some other
types of image transforms in later chapters.2.6.8 Probabilistic Methods
Probability finds its way into image processing work in a number of ways. The
simplest is when we treat intensity values as random quantities. For example,
let z;,é = 0,1,2,...,£ — 1, denote the values of all possible intensities in an
M X N digital image. The probability, p(z,), of intensity level z, occurring in a
given image is estimated asn,Plea) = 555 (2.6-42)
where 7, is the number of times that intensity z, occurs in the image and MN
is the total number of pixels, Clearly,Le}D la) = 1 (2.6-43)k=0
Once we have p(z,). we can determine a number of important image characteristics. For example, the mean (average) intensity is given by~L-1
m= SD xeplz) (2.6-44)
kao
2.6 An Introduction to the Mathematical Tools Used in Digital Image Processingby sinusoidal interference, and Fig. 2.40(b) is the magnitude of its Fourier
transform, which is the output of the first stage in Fig. 2.39. As you will learn in
Chapter 4, sinusoidal interference in the spatial domain appears as bright
bursts of intensity in the transform domain. In this case, the bursts are in a circular pattern that can be seen in Fig. 2.40(b). Figure 2.40(c) shows a mask
image (called a filter) with white and black representing 1 and 0, respectively.
For this example, the operation in the second box of Fig.2.39 is to multiply the
mask by the transform, thus eliminating the bursts responsible for the interference. Figure 2.40(d) shows the final result, obtained by computing the inverse
of the modified transform. The interference is no longer visible, and important
detail is quite clear. In fact, you can even see the fiducial marks (faint crosses)
that are used for image alignment. sThe forward transformation kernel is said to be separable if
r(x, y, 4, v) = 14(x, @)ro(y, v) (2.6-32)
In addition, the kernel is said to be symmetric if r;(x, y) is functionally equal to
r(x, y), 50 that
r(x, yu, 0) = 1y(x, wry, v) (2.6-33)
Identical comments apply to the inverse kernel by replacing r with s in the preceding equations.
The 2-D Fourier transform discussed in Example 2.11 has the following forward and inverse kernels:P(X, yy, V) = eT FA HX/M +ey/N) , (2.6-34)
and
1
=~ plen(ux/Mt+vy/N) 2.6-35
s(x 1.0) = aaaze (2635)
respectively, where j = \/-1, so these kernels are complex. Substituting thesekernels into the general transform formulations in Eqs. (2.6-30) and (2.6-31)
gives us the discrete Fourier transform pair:M-\ N-}
Tine) = DD fla pe Prarie) (2.6-36)
x50 y=0
and
] M~-) N-1 ;
F069) = ay DS DS Ty, vp emer wiN) (2.6-37)
u=G v=0These equations are of fundamental importance in digital image processing,
and we devote most of Chapter 4 to deriving them starting from basic principles and then using them in a broad range of applications.It is not difficult to show that the Fourier kernels are separable and symmetric (Problem 2.25), and that separable and symmetric kernels allow 2-D
transforms to be computed using 1-D transforms (Problem 2.26). When the117
2.6 & An Introduction to the Mathematical Tools Used in Digital Image ProcessingSimilarly, the variance of the intensities ished
o = Sz -— my pla)k=(2.6-45)The variance is a measure of the spread of the values of z about the mean, so it
is a useful measure of image contrast. In general, the nth moment of random
variable z about the mean is defined as£-]
rl) = (ze — mi)" (za) (2.6-46)k=0
We see that jo(z) = 1, 4(z) = 0, and jez) = o?. Whereas the mean and
variance have an immediately obvious relationship fo visua) properties of an
image, higher-order moments are mote subile. For example, a positive third
moment indicates that the intensities are biased to values higher than the
mean, a negative third moment would indicate the opposite condition, and a
zero third moment would tell us that the intensities are distributed approximately equally on both sides of the mean. These features are useful for computational purposes, but they do not tell us much about the appearance of an
image in general.@ Figure 2.41 shows three 8-bit images exhibiting low, medium, and high contrast, respectively. The standard deviations of the pixel intensities in the three
images are 14.3, 31.6, and 49.2 intensity levels, respectively. The corresponding
variance values are 204.3, 997.8, and 2424.9, respectively. Both sets of values
tell the same story but, given that the range of possible intensity values in
these images is (0, 255], the standard deviation values relate to this range much
more intuitively than the variance. @As you will see in progressing through the book, concepts from probability
play a central role in the development of image processing algorithms, For example, in Chapter 3 we use the probability measure in Eq. (2.6-42) to derive intensity transformation algorithms. In Chaper 5, we use probability and matrix
formulations to develop image restoration algorithms. In Chapter 10, probability is used for image segmentation, and in Chapter 11 we use it for texture description. In Chapter 12, we derive optimum object recognition techniques
based on a probabilistic formulation.  119“the units of the variance
are in intensity values
squared. When compar
ing contrast values, we
usually use the standard
deviation, o (square fool
of the variance), instead
because iis dimensions
aye dicecily in terms of
itensity values. EXAMPLE 2.12:
Comparison of
standard
deviation values
as measures of
image intensity
contrast,abeFIGURE 7.41
Images cxhibiting
{a) low contrast.
(b) medium
conisasi, anc{c) high contrast.
120 Gayter 2 1 Digital Image FundamentalsThus far, we have addressed the issue of applying probability to a single random variable (intensity) over a single 2-D image. If we consider sequences of
images, we may interpret the third variable as time. The tools needed to handle
this added complexity are stochastic image processing techniques (the word
stochastic is derived from a Greek word meaning roughly “to aim at a target,”
implying randomness in the outcome of the process), We can go a step further
and consider an entire image (as opposed to a point) to be a spatial random
event. The tools needed to handle formulations based on this concept are techniques from random fields. We give one example in Section 5.8 of how to treat
entire images as random events, but further discussion of stochastic processes
and random fields is beyond the scope of this book. The references at the end of
this chapter provide a starting point for reading about these topics.SummaryThe material in this chapter is primarily background for subsequent discussions. Our treatment of the human visual system, although brief, provides a basic idea of the capabilities of
the eye in perceiving pictorial information. The discussion on light and the electromagnetic
spectrum is fundamental in understanding the origin of the many images we use in this
book. Similarly, the image model developed in Section 2.3.4 is used in the Chapter 4 as the
basis for an image enhancement technique called homomeorphic filtering.The sampling and interpolation ideas introduced in Section 2.4 are the foundation
for many of the digitizing phenomena you are likely to encounter in practice, We will
return to the issue of sampling and many of its ramifications in Chapter 4, after you
have mastered the Fourier transform and the frequency domain.The concepts introduced in Section 2.5 are the basic building blocks for processing
techniques based on pixel neighborhoods. For example, as we show in the following
chapter, and in Chapter 5, neighborhood processing methods are at the core of many
image enhancement and restoration procedures. In Chapter 9, we use neighborhood
operations for image morphology; in Chapter 10, we use them for image segmentation;
and in Chapter 11 for image description. When applicable, neighborhood processing is
favored in commercial applications of image processing because of their operational
speed and simplicity of implementation in hardware and/or firmware.The material in Section 2.6 will serve you well in your journey through the book. Although the level of the discussion was strictly introductory, you are now in a position to
conceptualize what it means to process a digital image. As we mentioned in that section,
the tools introduced there are expanded as necessary in the following chapters. Rather
than dedicate an entire chapter or appendix to develop a comprehensive treatment of
mathematical concepts in one place, you will find it considerably more meaningful to
learn the necessary extensions of the mathematical tools from Section 2.6 in later chapters,in the context of how they are applied to solve problems in image processing..References and Further ReadingAdditional reading tor the material in Section 2.1 regarding the structure of the human
eye may be found in Atchison and Smith [2000] and Oyster [1999]. For additional reading
on visual perception, see Regan (2000] and Gordon [1997}. The book by Hubel [1988] and
the classic book by Cornsweet []970] also are of interest. Born and Wolf [1999] is a basic
reference that discusses light in terms of electromagnetic theory. Electromagnetic energy
propagation is covered in some detail by Felsen and Marcuvitz [1994].
122Geepter 2 @ Digital Image Fundamentals*232.42.5w262.72.8
429modeled as a circular array of diameter 1.5 mm and that the cones and spacesbetween the cones are distributed uniformly throughout this array.When you enter a dark theater on a bright day, it takes an appreciable intervalof time before you can see well enough to find an empty seat. Which of the visualprocesses explained in Section 2.1 is at play in this situation?Although it is not shown in Fig. 2.10, alternating current certainly is part of theelectromagnetic spectrum. Commercial alternating current in the United Stateshas a frequency of 77 Hz. What is the wavelength in meters of this component of
the spectrum?You are hired to design the front end of an imaging system for studying theboundary shapes of cells, bacteria, viruses, and protein. The front end consists, inthis case, of the illumination source(s) and corresponding imaging camera(s).The diameters of circles required to enclose individual specimens in each ofthese categories are 25, 0.5, 0.05, and 0.005 zm, respectively.(a) Can you solve the imaging aspects of this problem with a single sensor and
camera? If your answer is yes, specify the illumination wavelength band and
the type of camera needed. By “type,” we mean the band of the electromagnetic spectrum to which the camera is most sensitive (e.g., infrared).(b) If your answer in (a) is no, what type of illumination sources and corresponding imaging sensors would you recommend? Specify the light sources
and cameras as requested in part (a). Use the minimum number of ilhumination sources and cameras needed to solve the problem.By “solving the problem,” we mean being able to detect circular details of diameter 25, 0.5, 0.05, and 0.005 ym, respectively.
A CCD camera chip of dimensions 14 x 14 mm, and having 2048 x 2048 elements, is focused on a square, flat area, located 0,5 m away. How many line
pairs per mm will this camera be able to resolve? The camera is equipped with
a 35-mm lens, (Hint: Model the imaging process as in Fig. 2.3, with the focal
length of the camera lens substituting for the focal length of the eye.)
An automobile manufacturer is automating the placement of certain components on the bumpers of a limited-edition line of sports cars. The components
are color coordinated, so the robots need to know the color of each car in order
to select the appropriate bumper component. Models come in only four colors:
blue, green, red, and white. You are hired to propose a solution based on imaging. How would you solve the problem of automatically determining the color of
each car, keeping in mind that cos: is the most important consideration in your
choice of components?Suppose that a flat area with center at (Xp, yy) is illuminated by a light sourcewith intensity distributionix, y) = KetelAssume for simplicity that the reflectance of the area is constant and equal to1.0, and let K = 255. If the resulting image is digitized with & bits of intensityresolution, and the eye can detect an abrupt change of four shades of intensitybetween adjacent pixels, what value of & will cause visible false contouring?Sketch the image in Problem 2.7 for & = 4.A common measure of transmission for digital data is the baud rate, defined as
the number of bits transmitted per second. Generally, transmission is accomplished
The area of image sensing is quite broad and very fast moving. An excellent source
of information on optical and other imaging sensors is the Society for Optical Engrneering (SPIE). The following are representative publications by the SPIE in this area:
Blouke et ai. [2001], Hoover and Doty [1996], and Freeman [1987].The image model presented in Section 2.3.4 is from Oppenheim, Schafer, and
Stockham [1968]. A reference for the illumination and reflectance values used in that
section is the LESNA Lighting Handbook [2000]. For additional reading on image
sampling and some of its effects, such as aliasing, see Bracewell [1995]. We discuss this
topic in more detail in Chapter 4. The early experiments mentioned in Section 2.4.3
on perceived image quality as a function of sampling and quatization were reported
by Huang [1965]. The issve of reducing the number of samples and intensity tevels in
an image while minimizing the ensuing degradation is still of current interest, as exemplified by Papamarkos and Atsalakis ]2000], For further reading on image shrinking and zooming, see Sid-Abmed [1995], Unser et al. [1995}, Umbaugh [2005], and
Lehmann et al. [1999]. For further reading on the topics covered in Section 2.5, see
Rosenfeld and Kak {1982}, Marchand-Maillet and Sharaiha {2000}, and Ritter and
Wilson [2001].Additional reading on linear systems in the context of image processing (Section 2.6.2)
may be found in Castleman [1996]. The method of noise reduction by image averaging
(Section 2.6,3) was first proposed by Kohler and Howell [1963]. See Peebles [1993] regarding the expected value of the mean and variance of a sum of random variables.
Image subtraction (Section 2.6.3) is a generic image processing tool used widely for
change detection. For image subtraction to make sense, it is necessary that the images
being subtracted be registered or, alternatively, that any artifacts due to motios be
identified. Two papers by Meijering et al, [1999, 2001] are illustrative of the types of
techniques used to achieve these objectives.A basic reference for the materia) in Section 2.6.4 is Cameron [2005]. For more advanced reading on this topic, see Tourlakis [2003]. For an introduction to fuzzy sets. see
Section 3.8 and the corresponding references in Chapter 3. For further details on singlepoint and neighborhood processing (Section 2.6.5), see Sections 3.2 through 3.4 and the
references on these topics in Chapter 3. For geometric spatial transformations.see Wolberg [1990].Noble and Daniel {1988] is a basic reference for matrix and vector operations
(Section 2.6.6), See Chapter 4 for a detailed discussion on the Fourier transform
(Section 2.6.7), and Chapters 7, 8, and 11 for examples of other types of transforms
used in digital image processing. Peebles {1993} is a basic introduction to probability
and random variables (Section 2.6.8) and Papoulis [1991] is a more advanced treatment of this topic. For foundation material on the use of stochastic and random
fields for image processing, see Rosenfeld and Kak [1982), Jaéhne [2002], and Won
and Gray [2004].For details of software implementation of many of the techniques illustrated in this
chapter, see Gonzalez, Woods, and Eddins {2004}.Problems2.1 Using the background information provided in Section 2.1, and thinking purely
in geometric terms, estimate the diameter of the smallest printed dot that the
eye can discern if the page on which the doi is printed is 0.3 m away from the
eyes. Assume for simplicity that the visual system ceases to detect the dot when
the image of the dot on the fovea becomes smaller than the diameter of one receptor (cone) in that area of the retina. Assume further that the fovea can be@ Problems 121 Deisiled sohitions to the
problems marked with a
slar can be found in the
book Web site. The site
also contuins suggested
projects based on the material in this chapter.
2.10w2H#212
243
2.142.15in packets consisting of a start bi, a byte (8 bits) of information, and a stop bit.Using these facts, answer the following:{a) How many minutes would it take to transmit a 2048 x 2048 image with 256
intensity levels using a 33.6K baud modem?(b} What would the time be at 3000K baud.a representative medium speed of a
phone DSL (Digital Subscriber Line) connection?High-definition television (HDTY) generates images with 1080 horizontal TV
lines interlaced (where every other line is painted on the tube face in each of two
fields, each field being 1/60th of a second in duration). The width-to-heigh! aspect ratio of the images is 16:9. The fact that the number of horizontal lines is
fixed determines the vertical resolution of the images, A company has designed
an image capture system that generates digital images from HDTV images. The
resolution of each TV (horizontal) line in their system is in proportion to vertical
resolution, with the proportion being the width-to-height ratio of the images.
Each pixel in the color image has 24 bits of intensity resolution, 8 bits each for a
ted, a green, and a blue image. These three “primary” images form a color image.
How many bits would it take to store a 90-minute HDTV movie?Consider the two image subsets, S$; and $2, shown in the following figure. For
V = {1}, determine whether these two subsets are (a) 4-adjacent, (b) 8-adjacent,
or (c) m-adjacent.5 5,
0 fe 6 0 Ofo oO 4 Tt 0
1 10 0 1 @!o 1 0 oF
1f0 0 1 O11 1 0 Oto
OLO A dg 8 0 0 op 0
00113 006012 1Develop an algorithm for converting a one-pixel-thick 8-path to a 4-path.
Develop an algorithm for converting a one-pixel-thick m-path to a 4-path.
Refer to the discussion at the end of Section 2.5.2, where we defined the background as (R,}°, the complement of the union of all the regions in an image. In
some applications, it is advantageous to define the background as the subset of
pixels (&,,)‘ that are not region hole pixels (informally, think of holes as sets of
background pixels surrounded by region pixels). How would you modify the definition to exclude hole pixels from (R,,)°? An answer such as “the background is
the subset of pixels of (R,)° that are not hole pixels” is not acceptable. (Hint:
Use the concept of connectivity.}Consider the image segment shown.x(a) Let V = {0,1,2} and compute the lengths of the shortest 4-, 8-,and m-path
between p and q. If a particular path does not exist between these two
points, explain why.(b) Repeat for V = {2,3, 4}.223 14
(p30 4 2:1
1203 4@ Problems123
124 Chapter 2 mt Digital Image Fundamentals2.16 &(a) Give the condition(s) under which the D, distance between two points p2.17
«2.182.19* 2.20221*2.22and q is equal to the shortest 4-path between these points.
(b) Is this path unique?
Repeat Problem 2.16 for the Dg distance.
In the next chapter, we will dea! with operators whose function is to compute
the sum of pixel values in a small subimage area, S. Show that these are linear
operators,
The median, , of a set of numbers is such that half the values in the set are
below ¢ and the other half are above it. For example, the median of the set of
values {2,3, 8,20, 21, 25,31} is 20. Show that an operator that computes the
median of a subimage area, 5, is nonlinear.
Prove the validity of Eqs. (2.6-6) and (2.6-7). [ Hinz: Start with Eq. (2.6-4) and use
the fact that the expected value of a sum is the sum of the expected values.]
Consider two 8-bit images whose intensity levels span the full range from 0 to 255.{a} Discuss the limiting effect of repeatedly subtracting image (2) from image
(1). Assume that the result is represented also in eight bits.{b) Would reversing the order of the images yield a different result?Image subtraction is used often in industrial applications for detecting missing
components in product assembly. The approach is to store a “golden” image that
corresponds to a correct assembly; this image is then subtracted from incoming
images of the same product. Ideally, the differences would be zero if the new products are assembled correctly, Difference images for products with missing components would be nonzero in the area where they differ from the golden image.
What conditions do you think have to be met in practice for this method to work?2.23 %(a) With reference to Fig. 2.31, sketch the set (A‘—B) U (B-A)2.242.25* 2.26(b) Give expressions for the sets shown shaded in the following figure in terms
of sets A, B, and C. The shaded areas in each figure constitute one set, so
give one expression for each of the three figures.ABWhat would be the equations analogous to Eqs, §2.6-24} and (2.6-25) that would
result from using triangular instead of quadrilateral regions?Prove that the Fourier kernels in Eqs. (2.6-34) and (2.6-35) are separable and svmmetric, What are the advantages of using separable transformations on images?
Show that 2-D transforms with separable, symmetric kernels can be computed
by (1) computing 1-D transforms along the individual rows (columns) of the
input, followed by (2} computing 1-D transforms along the columns (rows) of
the result from step (1).
@ Problems 1252.27 A plant produces a line of translucent miniature polymer squares. Stringent quality requirements dictate 100% visual inspection, and the plant manager finds the
use of human inspectors increasingly expensive. Inspection is semiautomated. At
each inspection station, a robotic mechanism places each polymer square over a
light focated under an optical system ihat produces a magnified image of the
square. The image completely fills a viewing screen measuring 80 X 80 mm. Defects appear as dark circular blobs, and the inspector’s job is to look at the screen
and reject any sample that has one or more such dark biobs with a diameter of
0.8 mm or larger, as measured on the scale of the screen. The manager believes
that if she can find a way to automate the process completely, she will increase
profits by 50%. She also believes that success in this project will aid her climb up
the corporate ladder. After much investigation, the manager decides that the way
to solve the problem is to view each inspection screen with a CCD TV camera
and feed the output of the camera into an image processing system capable of detecting the blobs, measuring their diameter, and activating the accept/reject buttons previously operated by an inspector. She is able to find a system that can do
the job, as long as the smatiest defect occupies an area of at least 2 X 2 pixels in
the digital image. The manager hires you to help her specify the camera and Jens
system, but requires that you use off-the-shelf components. For the lenses, assume that this constraint means any integer multiple of 25 mm or 35 mm, up to
200 mm, For the cameras, it means resolutions of 512 x 512, 1024 x 1024, or
2048 X 2048 pixels. The individual imaging elements in these cameras are
squares measuring 8 X 8 wm, and the spaces between imaging elements are
2 wm. For this application, the cameras cost much more than the lenses, so the
problem should be solved with the lowest-resolution camera possible, based on
the choice of lenses, As a consultant, you are to provide a written recommendation, showing in reasonable detail the analysis that led to your conclusion. Use
the same imaging geometry suggested in Problem 2.5.
3.1 @ Background 127EAR Background
3.1.1 The Basics of Intensity Transformations and Spatial FilteringAll the image processing techniques discussed in this section are implernented
in the spatial domain, which we know from the discussion in Section 2.4.2 is
simply the plane containing the pixels of an image. As noted in Section 2.6.7,
spatial domain techniques operate directly on the pixels of an image as opposed, for example, to the frequency domain (the topic of Chapter 4) in which
operations are performed on the Fourier transform of an image, rather than on
the image itself. As you will learn in progressing through the book, some image
processing tasks are easier or more meaningful te implement in the spatial domain while others are best suited for other approaches. Generally, spatial domain techniques are more efficient computationally and require less processing
resources to implement.The spatial domain processes we discuss in this chapter can be denoted by
the expressiona(x, ») = THF. (3.1-1)where f(x, y) is the input image, g(x, y) is the output image, and 7 is an operator on f defined over a neighborhood of point (x, y). The operator can apply
to a single image (our principal focus in this chapter) or to a set of images, such
as performing the pixel-by-pixel sum of a sequence of images for noise reduclion, as discussed in Section 2.6.3. Figure 3.1 shows the basic implementation
of Eq. (3.1-1) on a single image. The point (x, y) shown is an arbitrary location
in the image, and the smal! region shown containing the point is a neighborhood of (x, y), as explained in Section 2.6.5. Typically, the neighborhood is rectangular, centered on (x, y), and much smaller in size than the image.Origin ~ (x, »)3 X 3 neighborhood of (x. +) Image f  | Spatial domainOther neighborhood
shapes, such as digital
approximations to ciscles, are used Sometimes,
bul rectangular shapes
are by [ar the most
prevalent because they
are much easier to implement computationally,FIGURE 3.1
A3x3
neighborhood
about a point
(x, y) in an image
in the spatial
domain. The
neighborhood is
moved from pixel
to pixel in the
image to generate
an output image.
130 Geapter 3 @ Intensity Transformations and Spatial FilteringFIGURE 3.3 Some
basic intensity
transformation
functions. Ali
curves were
scaled to fit in the
range shown.by an expression of the forms = 7(r), where 7 is a transformation that maps a
pixel value r into a pixel value s. Because we are dealing with digital quantities,
values of a transformation function typically are stored in a one-dimensional
array and the mappings from r to s are implemented via table lookups. For an
8-bit environment, a lookup table containing the values of T will have 256 entries.As an introduction to intensity transformations, consider Fig. 3.3, which
shows three basic types of functions used frequently for image enhancement: linear (negative and identity transformations), logarithmic (log and
inverse-log transformations}, and power-law (nth power and nth root transformations). The identity function is the trivial case in which output intensities are identical to input intensities, It is included in the graph only for
completeness.3.2.1 Image NegativesThe negative of an image with intensity levels in the range [0, L ~ 1] is obtained by using the negative transformation shown in Fig. 3.3, which is given by
the expressions=L-l1-r (3.2-1)
Reversing the intensity levels of an image in this manner produces theequivalent of a photographic negative. This type of processing is particularly
suited for enhancing white or gray detail embedded in dark regions of an L-1 
 
  Negative3L/4Output intensity level, s
8LA Inverse log 
 
 !
0 Lfa L2 ah ja L-iInput intensity tevel, r
128  Ghpter3 @ Intensity Transformations and Spatial FilteringabFIGURE 3.2
Intensity
transformation
functions.(a} Contraststretching
function.(b) Thresholding
function.The process that Fig. 3.1 itlustrates consists of moving the origin of the neighborhood trom pixel to pixel and applying the operator T to the pixels in the
neighborhood to yield the output at that location. Thus, for any specific location
(x, y), the value of the output image g at those coordinates is equal to the result
of applying T to the neighborhood with origin at (x, y) in f. For example, suppose that the neighborhood is a square of size 3 X 3, and that operator T is defined as “compute the average intensity of the neighborhood.” Consider an
arbitrary location in an image, say (100, 150). Assuming that the origin of the
neighborhood is at its center, the result, ¢(100, 150), at that location is computed as the sum of f (100, 150) and its 8-neighbors, divided by 9 (i.e., the average
intensity of the pixels encompassed by the neighborhood). The origin of the
neighborhood is then moved to the next location and the procedure is repeated
to generate the next value of the output image g. Typically, the process starts at
the top left of the input image and proceeds pixel by pixel in a horizontal scan,
one row at a time. When the origin of the neighborhood is at the border of the
image, part of the neighborhood wili reside outside the image. The procedure is
either to ignore the outside neighbors in the computations specified by 7, or to
pad the image with a border of 0s or some other specified intensity values. The
thickness of the padded border depends on the size of the neighborhood. We
will return to this issue in Section 3.4.1,As we discuss in detail in Section 3.4, the procedure just described is called
spatial filtering, in which the neighborhood, along with a predefined operation,
is called a spatial filter (als. referred to as a spatial mask, kernel, template, or
window). The type of operation performed in the neighborhood determines
the nature of the filtering process,The smailest possible neighborhood is of size 1 X 1. In this case, g depends
only on the value of f at a single point (x, y) and T in Eq. (3.1-t) becomes an
intensity (also called gray-level or mapping) transformation function of the forms=T() (3.1-2)where, for simplicity in notation, s and r are variables denoting, respectively,
the intensity of g and fat any point (x, y). For example, if T(r) has the form
in Fig. 3.2(a), the effect of applying the transformation to every pixel of f to
generate the corresponding pixets in g would be to produce an image of     sy = Tn) fee eoPor) =m.  Dark  Dark +--+ Liviitelbow fkM 4
Dark +—-~ Light Dark +--— Light
3.2 & Some Basic Intensity Transformation Functionshigher contrast than the original by darkening the intensity levels below &
and brightening the levels above k. In this technique, sometimes called
contrast stretching (see Section 3.2.4). values of r lower than & are compressed by the transformation function into a narrow range of 5, toward
black. The opposite is true for values of r higher than &, Observe how an intensity value 7, is mapped to obtain the corresponding value sy. In the limiting case shown in Fig. 3.2(b), T(r) produces a two-level (binary) image. A
mapping of this form is calted a thresholding function. Some fairly simple, yet
powerful, processing approaches can be formulated with intensity transformation functions. In this chapter, we use intensity transformations principally
for image enhancement. In Chapter 10, we use them for image segmentation.
Approaches whose results depend only on the intensity at a point sometimes
are called point processing techniques, as opposed to the neighborhood processing techniques discussed earlier in this section.3.4.2 About the Examples in This ChapterAlthough intensity transformations and spatial filtering span a broad range of
applications, most of the examples in this chapter are applications to image
enhancement. Enhancement is the process of manipulating an image so that
the result is more suitable than the original for a specific application. The
word specific is important here because it establishes at the outset that enhancement techniques are problem oriented. Thus, for example, a method
that is quite useful for enhancing X-ray images may not be the best approach
for enhancing satellite images taken in the infrared band of the electromagnetic spectrum. There is no general “theory” of image enhancement. When an
image is processed for visual interpretation, the viewer is the ultimate judge
of how weil a particular method works. When dealing with machine perception, a given technique is easier to quantify. For example, in an automated
character-recognition system, the most appropriate enhancement method is
the one that results in the best recognition rate, leaving aside other considerations such as computational requirements of one method over another.
Regardiess of the application or method used. however, image enhancement
is one of the most visually appealing areas of image processing. By its very nature, beginners in image processing generally find enhancement applications snteresting and relatively simple to understand. Therefore. using examples from
image enhancement to illustrate the spatial processing methods developed in
this chapter not only saves having an extra chapter in the book dealing with
image enhancement but, more importantly, is an effective approach for introducing newcomers to the details of processing techniques in the spatial domain.
As you will see as you progress through the book, the basic material developed in
this chapter is applicable to a much broader scope than just image enhancement.EES Some Basic Intensity Transformation FunctionsIntensity transformations are among the simplest of ali image processing techniques. The values of pixels, before and after processing. will be denoted by r
and s, respectively. As indicated in the previous section. these values are related129
3.2 % Some Basic Intensity Transformation Functions 131 image, especially when the black areas are dominant in size. Figure 3.4
shows an example. The original image is a digital mammogram showing a
small lesion. In spite of the fact that the visual content is the same in both
images, note how much easier it is to analyze the breast tissue in the negative image in this particular case.3.2.2 Log Transformations
The general form of the log transformation in Fi
y= ctlog(] +r) (3.2-2)where c is a constant, and it is assumed that r = 0. The shape of the log curve
in Fig. 3.3 shows that this transformation maps 4 narrow range of low intensity
values in the input into a wider range of output levels. The opposite is true of
higher values of input levels. We use a transformation of this type to expand
the values of dark pixcts in an image while compressing the higher-level values. The opposite is true of the inverse log trans{ormation,Any curve having the general shape of the Jog functions shown im Fig. 3.3
would accomplish this spreading/compressing of mtensity levels in an image,
but the power-law transformations discussed in the next section are much
more versatile for this purpose. The log function has the important characteristic that it compresses the dynamic range of images with large variations in
pixel values. A classic Justration of an application in which pixel values have
a large dynamic range is the Fourier spectrum. which will be discussed in
Chapter 4. At the moment. we are concerned only with the image characteristics of spectra. It is not unusual to encounter spectrum values that range from 0
to 10° or higher. While processing uembers such as these presents no problems
for a computer. image display svsicims generally will nol be able to reproduce   abFIGURE 3.4(a) Original digital
mammogram.{b) Negative
image obtained
using the negative
transformationin Eq. (3.2-1).
(Courtesy of G.E.
Medicat Systems.)
132 Chapter 3 s§ Intensity Transformations and Spatial FilteringabFIGURE 3.5(a) Fourier
spectrum.(b) Result of
applying the tog
transformation in
Eq. (32-2) with
c=l1 faithfully such a wide range of intensity values. The net effect is that a significant degree of intensity detail can be lost in the display of a typical Fourier
spectrum.As an illustration of log transformations, Fig. 3.5(a) shows a Fourier spectrum with values in the range 0 to 1.5 x 10°. When these values are scaled linearly for display in an &-bit system. the brightest pixels will dominale the
display. at the expense of lower (and just as important} values of the spectrum. The effect of this dominance is illustrated vividly by the relatively smal!
area of the image in Fig. 3.4/a) that is not perceived as black. If instead of displaying the values in (his manner. we first apply Eq. (3.2-2) Gwith ¢ = | in this
case) to the spectrum values then the range of values of the result becomes 0
to 6.2, which is more manageable, Figure 3.3(45) shows the result of scaling this
new range linearly and displaying the spectrum in the same 8-bit display. The
wealth of detail visible in this image as compared to an unmodified display of
the spectrum is evident from these pictures. Most of the Fourier spectra seen
in image processing publications have been scaled in just this manner.3.2.3 Power-Law (Gamma) Transformations
Power-Jaw transformations have the hasic formyser (22-3)
where c and y are posilive constants. Sometimes Eq. (3.2-3) is written ass=c(r + #)* to account for aa offset (that isa measurably autput when the
ay ally are an issue of display calibration input is zero}. Howse
and asaresull thes ve normals iguered in Ry. €3.2-3), Plots of s versus r for
13.6. Ay tithe case ui che log ransformia various valucs oi y ure shown If
tion, power-law ci ves 8 10D Pracd
input values ini a wi
for higher valucs ¢ 
  
 
     yf
Obs tell Valites Of y aap a marrow
dULpFUL ¥ tees, with (the opposite heing truefineiton, hows ée
3.2 &@ Some Basic Intensity Transformation Functions 133 L-1 C75 =
y= 0.04
f
y= 0.50 |
3L/4 y= 0.20
%
o 7 = 0.40
2 s
> =
= y = 0.67
2 L/2 y=1
Zz / y=15
3
fo) y= 25
L/4 y= 50
y = 10.0
y = 25.0
0 aT
0 Lf Lj2. 3b L-1Input intensity tevel, 7here a family of possible transformation curves obtained simply by varying y.
As expected, we see in Fig. 3.6 that curves generated with values of y > 1
have exactly the opposite effect as those generated with values of y < 1.
Finally, we note that Eq. (3.2-3) reduces to the identity transformation when
ec=y=l.A variety of devices used for image capture, printing, and display respond
according to a power law. By convention, the exponent in the power-law equation is referred to as gamma {hence our use of this symbol in Eq. (3.2-3)].
The process used to correct these power-law response phenomena is called
gamma correction, For example, cathode ray tube (CRT) devices have an
intensity-to-voltage response that is a power function, with exponents varying from approximately 1.8 to 2.5. With reference to the curve for y = 2.5 in
Fig. 3.6, we see that such display systems would tend to produce images that
are darker than intended. This effect is illustrated in Fig, 3.7. Figure 3.7(a)
shows a simple intensity-ramp image input into a monitor. As expected, the
output of the monitor appears darker than the input, as Fig. 3.7(b) shows.
Gamma correction in this case is straightforward, All we need to do is preprocess the input image before inputting it into the monitor by performing
the transformation s = r¥25 = /°4. The result is shown in Fig. 3.7(c). When
input into the same monitor, this gamma-corrected input produces an output that is close in appearance to the original image, as Fig. 3.7(d) shows. A
similar analysis would apply to other imaging devices such as scanners and
printers. The only difference would be the device-dependent value of
gamma (Poynton [1996]}.FIGURE 3.6 Plots
of the equations = cr’ for
various values of
y (¢ = linall
cases). All curves
were scaled to fit
in the range
shown.
134 — Chopter 3 # Intensity Transformations and Spatial Filtering,ab
edFIGURE 3.7{a} Intensity ramp
image. (b) Image
as viewed on a
simulated monitor
with a gamma of
2.5.(c) Gammacorrected image.
(d) Corrected
image as viewed
on the same
monitor, Compare(d) and (a).     
 Gamma Original image as viewedOriginal image
correction on monitorGamma-eorrected image Gamma-corrected image as
viewed on the same monitorGamma correction is important if displaying an image accurately on ¢
computer screen is of concern. Images that are nol corrected properly car
look either bleached out. or. whal is more likely, too dark. Trying to reproduce
colors accurately also requires some knowledge of gamma correction because
varying the value of gamma changes not only the intensity, but also the ratios
of red to green to blue in a color image. Gamma correction has become in
creasingly important in the past few years. as the use of cigital images for
commercial purposes over the Internet has increased. TL is not unusual tha
images created for a popular Web site will be viewed by sillions of people
the majority of whom will have different monitors and/or monitor setlings
Some computer sys!ems cven have partial gamma correction buill in. Also
current image standards do not contain the value of gamma with which ar
image was created, thus complicating the issue further. Given these cori
straints, a reasonable approach when storing image Weh site is to pre
process the images will a gamma that repre }
monitors and competer systems thal one expeel: i:
given point in Une  Peary rouverage” of tliethe   Msnake at ane
3.2 * Some Basic Intensity Transformation Functions 135@ In addition to gamma correction, power-law transformations are useful for
general-purpose contrast manipulation. Figure 3.8(a} shows a magnetic resonance image (MRI) of an upper thoracic human spine with a fracture dislocation and spinal cord impingement. The fracture is visible near the vertical
center of the spine, approximately one-fourth of the way down from the top of
the picture. Because the given image is predominantly dark. an expansion of
intensity levels is desirable. This can be accomplished with a power-law transformation with a fractionat exponent. The other images shown in the figure
were obtained by processing Fig. 3.8(a) with the power-law transformation EXAMPLE 3,4:
Contrast
enhancement
using power-law
transformations.ab
edFIGURE 3.8(a) Magnetic
resonanceimage (MRI) of a
fractured human
spine.(b)-(d) Results of
applying the
transformation in
(32,.2-3) with1 and1.6, 0.4, and
spectively.
inal image
courtesy of Dr,
David R. Pickens,
Department of
Radiology and
Radiological
Science
Ve
\niversity
Medical Center.)
136 Chapter3 @ Intensity Transformations and Spatial FilteringEXAMPLE 3.2:
Another
illustration of
power-law
transformations.ab
edFIGURE 3.9(a) Aerial image.
(b}-(d) Results of
applying the
transformation in
Eg. (3.2-3) withc= 1 and
y = 3.0,4.0, and
5.0, respectively.(Original image
for this example
courtesy of
NASA.)function of Eq. (3.2-3). The values of gamma corresponding to images (b)
through (d) are 0.6, 0.4, and 0.3, respectively (the value of ¢ was 1 in all cases).
We note that, as gamma decreased from 0.6 to 0.4. more detail became visible.
A further decrease of gamma to 0.3 enhanced a little more detail in the background, but began to reduce contrast to the point where the image started to
have a very slight “washed-out” appearance, especially in the background. By
comparing all results, we sce that the best enhancement in terms of contrast
and discernable detail was obtained with y = 0.4. A value of y = 0.3 is an approximate limit below which contrast in this particular image would be
reduced to an unacceptable level.8& Figure 3.9(a) shows the opposite problem of Fig. 3.8(a). The image to be
processed now has a washed-out appearance, indicaling that a compression
of intensity levels is desirable. This can be accomplished with Eq. (3.2-3}
using values of y greater than 1. The results of processing Fig. 3.9(a) with
y = 3.0, 4.0, and 5.0 are shown in Figs. 3.9(b) through (d). Suitable results
were obtained with gamma values of 3.0 and 4.0, the iatter having a slightly
138 Chapter 3 Intensity Transformations and Spatial FilteringabcdFIGURE 3.10
Contrast stretching.
(a) Form of
transformation
function. (b) A
low-contrast image.
(c) Result of
contrast stretching.
(d) Result of
thresholding.
(Original image
courtesy of Dr.
Roger Heady,
Research School of
Biological Sciences,
Australian National
University,
Canberra,
Australia.)a.bFIGURE 3.13 (a) This
transformation
highlights intensity
range [A, B] and
reduces all other
intensities to a lower
level. (b) This
transformation
highlights range[A, B] and preserves
all other intensity
levels.  Output intensity level, s 0 LfA L/2 3L/4 L-|
Input intensity level, r  slicing, can be implemented in several ways, but most are variations of two basic
themes. One approach is to display in one value (say, white) all the values in the
range of interest and in another (say, black) all other intensities. This transformation, shown in Fig. 3.{](a), produces a binary image. The second approach,
based on the transformation in Fig, 3.11(b), brightens (or darkens) the desired
range of intensities but leaves al] olher intensity levels in the image unchanped.L =D pe nen nce een en |5 oo y
~ fied t
f
3.2 & Some Basic Intensity Transformation Functionsmore appealing appearance because it has higher contrast. The result obtained
with y = 5.0 has areas that are too dark, in which same detail is lost. The dark
region to the left of the main road in the upper Jeft quadrant is an example of
such an area. a3.2.4 Piecewise-Linear Transformation FunctionsA complementary approach to the methods discussed in the previous three sections is to use piecewise linear functions. The principal advantage of piecewise
linear functions over the types of functions we have discussed thus far is that
the form of piecewise functions can be arbitrarily complex. In fact, as you will
see shortly, a practical implementation of some important transformations can
be formulated only as piecewise functions. The principal disadvantage of piecewise functions is that their specification requires considerably more user input.Contrast stretchingOne of the simplest piecewise linear functions is a contrast-stretching transformation. Low-contrast images can result from poor illumination, lack of dynamic range in the imaging sensor, or even the wrong setting of a lens aperture
during image acquisition. Contrast stretching is a process that expands the
range of intensity levels in an image so that it spans the full intensity range of
the recording medium or display device.Figure 3.10(a) shows a typical transformation used for contrast stretching. The
locations of points (r,, 5) and (72, 82) control the shape of the transformation function. If; = sand rz = s2, the transformation is a linear function that produces no
changes in intensity levels. If r,; = r:, 5, = Oand s, = L — 1, the transformation
becomes a thresholding function that creates a binary image, as illustrated in
Fig. 3.2(b). Intermediate values of (r), s;) and (rp, 8) produce various degrees of
spread in the intensity levels of the output image, thus affecting its contrast. In general,r) = r2 ands; = 3, is assumed so that the function is single valued and monotonically increasing. This condition preserves the order of intensity levels, thus
preventing the creation of intensity artifacts in the processed image.Figure 3.10(b) shows an 8-bit image with low contrast. Figure 3.10(c) shows
the result of contrast stretching, obtained by setting (r;, 51) = (*min. 0) and
(725 $2) = Cmax, b — 1), where fain aNd Pg, denote the minimum and maximum intensity levels in the image. respectively. Thus, the transformation function stretched the levels linearly from their original range to the full range
[0, L — 1]. Finally, Fig. 3.10(d)} shows the result of using the thresholding function defined previously, with (7,5,) = (m,0) and (rz, s2) = On, L — 1),
where m is the mean intensity level in the image. The original image on which
these results are based is a scanning electron microscope image of pollen, magnified approximately 700 times.Intensity-level slicingHighlighting a specific range of intensities in an image often is of interest. Applications include enhancing features such as masses of water in sateilile imagery
and enhancing flaws in X-ray images. The process, often called intensity-level137
3.2 © Some Basic Intensity Translormation Functions 139@ Figure 3.12(a) is an aortic angiopram near the kidney area (sce Section EXAMPLE 3.3:
intensity-level
sheme.1.3.2 for a more detailed explanation of this image). The objective of this example is to use intensity-level slicing to highlight the major blood vessels that
appear brighter as a result of an injected conuast medium, Figure 3.12(b)
shows the result of using a transformation of the form in Fig. 3.1 1(a), with the
selected band near the top of the scale, because the range of interest is brighter
than the background. The net result of this transformation is that the blood
vesse] and parts of the kidneys appear white. while all other intensities are
black, This type of enhancement produces a binary image and is useful for
studying the shape of the flow of the contrast medium (to detect blockages, for
example).If, on the other hand, interest lies in the actual intensity valucs of the region
of interest, we can use the transformation in Fig. 3.11(b). Figure 3,12(c) shows
the result of using such a transformation in which a band of intensities in the
mid-gray region around the mean intensily was set to black, while all other intensities were left unchanged. Here, we see that the pray-level tonality of the
major blood vessels and part of the kidney area were Jefl intact. Such a result
might be useful when interest lies in measuring the actual flow of the contrast
medium as a function of time in a series of images.Bit-plane slicingPixels are digital numbers composed of bits, For example, the intensily of each
pixel in a 256-level gray-scale image is composed of 8 bits (.¢.. one byte). Instead of highlighting intensity-level ranges, we could highlight the contribution  abcFIGURE 3.12 (a) Aortic angiogram. (b) Resull of usi sticing transformation of the iype iflustraled in Fig
3.11(a), with the range of intensities of Mlerent selected in Ue upper ene of the gray seaic. te) Result ot
using the transformation in Fig. 3.1) (9). with the selected area set ta black. so that grays m the arca of the
blood vessels and Kidneys were preserved, (Orininal image courtesy ef 13. PRemax Ro Gest Line
Michigan Medical Schoo!.)  
         eeYSTy of
140 — Gpter 3 Intensity Transformations and Spatial FilteringFIGURE 3,13 One 8-bit byte — Bit plane 8Bit-plane
representation of
an 8-bit image.{most significant)Bit plane |
(least significant} made to total image appearance by specific bits. As Fig. 3.13 illustrates, an 8-bit
image may be considered as being composed of eight i-bit planes, with plane |
containing the lowest-order bit of all pixels in the image and plane & ail the
highest-order bits.Figure 3,14(a) shows an 8-bit gray-scale image and Figs. 3.14(b) through (i)
are its eight 1-bit planes, with Fig.3.14(b) corresponding to the lowest-order bit.
Observe that the four higher-order bit planes, especially the last two, contain a
significant amount of the visually significant data. The lower-order planes contribute to more subtle intensity details in the image. The original image has a
gray border whose intensity is 194. Notice that the corresponding borders of some
of the bit planes are black (0), while others are white (1). To see why, consider a   BD. ¢
Bet
FhFIGURE 3.14 (a) An 8-bit gray-scale image of size {00 * 1192 pixels. (b) through (i) Bit planes | dough 8
with bit plane 1 corresponding to the east significant bil. Each bir plane is a binary mage.
3.3 # Histogram Processing 143  T
Histogram of dark image    Histogram of light image |         
 Histogram of low-contrast imageHistogram of high-contrast image  ae iFIGURE 3.16 Four basic image types: dark. light, low contrast, high
contrast, and their corresponding histograms.
3.2 ™ Some Basic Intensity Transformation Furictions 141pixel in, say, the middle of the lower border of Fig. 3.14{a). The corresponding
pixels in the bit planes, starting with the bighest-order plane, have values 1 100
0010, which is the binary representation of decimal 194. The value of any pixel
in the original image can be similarly reconstructed from its corresponding
binary-valued pixels in the bit planes.In terms of intensity transformation functions, it is not difficult to show that
the binary image for the 8th bit plane of an 8-bit image can be obtained by
processing the input image with a thresholding intensity transformation function that maps al! intensities between 0 and 127 to and maps all levels between 128 and 255 to 1, The binary image in Fig. 3.14(i) was obtained in just
this manner. It is left as an exercise (Problem 3.4) to obtain the intensity transformation functions for generating the other bit planes.Decomposing an image into its bit planes is useful for analyzing the relative importance of each bit in the image, a process that aids in determining
the adequacy of the number of bits used to quantize the image, Also, this type
of decomposition is useful for image compression (the topic of Chapter 8), in
which fewer than all planes are used in reconstructing an image. For example,
Fig. 3.15(a) shows an image reconstructed using bit planes 8 and 7. The reconstruction is done by multiplying the pixels of the nth plane by the constant
2°"! This is nothing more than converting the nth significant binary bit to
decima). Each plane used is multiplied by the corresponding constant, and all
planes used are added to obtain the gray scale image. Thus, to obtain
Fig. 3.15(a), we multiplied bit plane & by 128, bit plane 7 by 64, and added the
two planes. Although the main features of the original image were restored,
the reconstructed image appears flat, especially in the background. This is not
surprising because two planes can produce only four distinct intensity levels.
Adding plane 6 to the reconstruction helped the situation, as Fig. 3.15(b)
shows, Note that the background of this image has perceptible false contouring. This effect is reduced significantly by adding the 5th plane to the reconstruction, as Fig. 3.15{c) illustrates, Using more planes in the reconstruction
would not contribute significantly to the appearance of this image. Thus, we
conclude that storing the four highest-order bit planes would allow us to reconstruct the original image in acceptable detail. Storing these four planes instead of the original image requires 50% less storage (ignoring memory
architecture issues). Be
FIGURE 3.15 Images reconstructed using (a) bit planes 8 and 7, (b) bit planes 8.7, and 6: and {c) bit planes &.
7,6, and 5. Compare (c} with Pig. 3.14{a}
142  Geapter 3 m Intensity Transformations and Spatial FilteringConsult the book Web
site for a review of basic
probability theory.Histogram ProcessingThe histogram of a digital image with intensity levels in the range [0, L — 1]
is a discrete function A(r,} = my, where r, is the &th intensity value and ny is
the number of pixels in the image with intensity r;. It is common practice to
normalize a histogram by dividing each of its components by the total number of pixels in the image, denoted by the product MN, where, as usual, M
and N are the row and column dimensions of the image. Thus, a normalized
histogram is given by p(r,) = n,/MN, for k = 0,1,2,,..,L — 1. Loosely
speaking, p(r,) is an estimate of the probability of occurrence of intensity
level r; in an image. The sum of all components of a normalized histogram is
equal to 1.Histograms are the basis for numerous spatial domain processing techniques. Histogram manipulation can be used for image enhancement, as
shown in this section. In addition to providing useful image statistics, we shall
see in subsequent chapters that the information inherent in histograms also is
quite useful in other image processing applications, such as image compression
and segmentation. Histograms are simple to calculate in software and also
lend themselves to economic hardware implementations, thus making them a
popular too! for real-time image processing.As an introduction to histogram processing for intensity transformations,
consider Fig. 3.16, which is the pollen image of Fig. 3.10 shown in four basic intensity characteristics: dark, light, low contrast, and high contrast. The right
side of the figure shows the histograms corresponding to these images. The
horizontal axis of each histogram plot corresponds to intensily values, r,. The
vertical axis corresponds to values of h(r,) = ny or p(7,) = ny/ MN if the values are normalized. Thus, histograms may be viewed graphically simply as
plots of A(r;,) = ny versus ry or p(ry) = 14/MN versus ry.We note in the dark image that the components of the histogram are concentrated on the low (dark) side of the intensity scale. Similarly, the components of the histogram of the light image are biased toward the high side of
the scale. An image with low contrast has a narrow histogram located typi-~
cally toward the middle of the intensity scale. For a monochrome image this
implies a dull, washed-out gray took. Finally, we see that the components of
the histogram in the high-contrast image cover a wide range of the intensity
scale and, further, that the distribution of pixels is not too far from uniform,
with very few vertical lines being much higher than the others. Intuitively, it
is reasonable to conclude that an image whose pixels tend to occupy the entire
range of possible intensity levels and, in addition, tend to be distributed uniformly, will have an appearance of high contrast and will exhibit a large variety of gray tones. The net effect will be an image that shows a great deal of
gray-leve] detail and has high dynamic range. It will be shown shortly that is
is possible to develop a transformation function that can automatically
achieve this effect, based only on information available in the histogram o}
the input image.
144 Ghapter3 ™ Intensity Transformations and Spatial FilteringabFIGURE 3.17(a) Morotonically
increasing,
function, showing
how multiple
values can map to
# single value.(b) Strictly
monotonically
increasing
function. This is a
one-to-one
mapping, both
ways.3.3.1 Histogram EqualizationConsider for a moment continuous intensity values and let the variable r denote the intensities of an image to be processed. As usual, we assume that r is
in the range [0, L — 1], with r = 0 representing black and r = L — 1 representing white. For 7 satisfying these conditions, we focus attention on transformations (intensity mappings) of the forms=T(r) OsSrsL-—-1 (3.3-1)
that produce an output intensity tevel s for every pixel in the input image haying intensity r. We assume that:(a) T(r) is a monotonically’ increasing function in the interval0 < r = L — 1;and
(b) OF Tr) = L-1 for OsrsL—-t. ’
In some formulations to be discussed later, we use the inverser=T's) O<s5L-1 (3.3-2)
in which case we change condition (a) to(a) T(r) is a strictly monotonically increasing function in the interval
OsrsL-Il.,The requirement in condition (a) that T(r) be monotonically increasing
guarantees that output intensity values will never be less than corresponding
input values, thus preventing artifacts created by reversals of intensity. Condition (b) guarantees that the range of output intensities is the same as the
input. Finally, condition (a‘) guarantees that the mappings from s back to r
will be one-to-one, thus preventing ambiguities. Figure 3.17(a} shows a functionL-]
Single .
value. SySingle
value, %y    Multiple Single £— |
values value ‘Recall that a function 7(r) is monotonicatls increasing if T(r) 2 Try) lot ry > 17. T(r) isa strietty are
aentonically increasing function if TQ) > 7 (ry) for re > ry. Sitnilar definitions apply to monotonically
decreasing functions.
146 = Chapter 3 # Intensity Transformations and Spatial FilteringTo find the p,(s) corresponding to the transformation just discussed, we use
Eq. (3.3-3). We know from Leibniz’s rule in basic calculus that the derivative of
a definite integral with respect to its upper limit is the integrand evaluated at
the Jimit. That is, ds _ aT(r)dr dr
=(L- n4l f p(w) ao| (3.3-5)
=(L~ Ip)Substituting this result for dr/ds in Eq. (3.3-3), and keeping in mind that all
probability values are positive, yields a
PAs) = pr) #(3.3-6)1
= pr) “py | 1
- TT] OSssL-1We recognize the form of p,(s) in the fast tine of this equation as a uniform
probability density function. Simply stated, we have demonstrated that performing the intensity transformation in Eq. (3.3-4) yields a random variable, s,
characterized by a uniform PDF. Itis important to note from this equation that
T(r) depends on p,{r) but, as Eq. (3.3-6) shows, the resulting p,(s) always is
uniform, independently of the form of p,{r). Figure 3.18 illustrates these
concepts.pl) Ps)   -> Eg. (33-4) >~
i=abFIGURE 3.18 (a) An arbitrary PDF, (b) Result of applying the transformation in
Eq. (3.3-4) tv all intensity jevels, 7, The resulting intensities, s, have a uniform PDF,
independently of the form of the PDF of the r’s.
3.3 a Histogram Processingthal satisfies conditions (a) and (b). Here, we see that it is possible for multiple values to map to a single value and still satisfy these two conditions. That
is, a monotonic transformation function performs 4 one-to-one or many-toone mapping. This is perfectly fine when mapping from r to s. However,
Fig. 3.17(a} presents a problem if we wanted to recover the values of r uniquely from the mapped values (inverse mapping can be visualized by reversing
the direction of the arrows). This would be possible for the inverse mapping
of s, in Fig. 3.17(a), but the inverse mapping of s, is a range of values, which,
of course, prevents us in general from recovering the original value of r that
resulted in s,. As Fig. 3.17(b) shows, requiring that T(r) be strictly monotonic
guarantees thal the inverse mappings will be single valued (i.e. the mapping
is one-to-one in both directions). This is a theoretical requirement that allows
us to derive some important histogram processing techniques later in this
chapter. Because in practice we deal with integer intensity values, we are
forced to round all results to their nearest integer values. Therefore, when
strict monotonicity is not salisfied, we address the problem of a nonunique inverse transformation by Jooking for the closest integer matches. Example 3.8
gives an illustration of this.The intensity jevels in an image may be viewed as random variables in the
interval (0, 2 — 1]. A fundamental descriptor of a random variable is its probability density function (PDF). Let p,(r) and p,(s) denote the PDFs of rand s,
respectively, where the subscripts on p are used to indicate that p, and p, are
different functions in general. A fundamental result from basic probability
theory is that if p,(v) and T(r) are known, and T(r) is continuous and differentiable over the range of values of interest, then the PDF of the transformed
(mapped) variable s can be obtained using the simple formulapls) = pry (333)  Thus, we see that the PDF of the output intensity variable, s, is determined by
the PDF of the input intensities and the transformation function used [recall
that r and s are related by 7(r)).A transformation function of particular importance in image processing has
the formy= TV) =(L~1) i p(w) dw (3.3-4)
JOwhere w is a dummy variable of integration. The right side of this equation is
recognized as the cumulative distribution function (CDF) of random variable
r, Because PDFs always are positive, and recalling thai the integral of a function is the area under the function, it follows that the transformation function
of Eq. (3.3-4) satisfies condition (a) because the area under the function cannot decrease as r increases. When the upper limit in this equation is
r = (L — 1), the integral evaluates to 1 (the area under a PDF curve always
is 1),so the maximum value of s is (LZ — 1) and condition (b) is satisfied also.145
148 Chapter 3 @ Intensity Transformations and Spatial FilteringEXAMPLE 3.5:
A simple
illustration of
histogram
equalization.TABLE 3.1
Intensity
distribution and
histogram values
for a 3-bit,64 X 64 digital
image.The discrete form of the transformation in Eq. (3.3-4) isk
5 = Tr) =(L- DS ple)
=0
(3.3-8)L-‘N<
- EDS, k=0,1,2,...,L-1
1=0Thus, a processed (output) image is obtained by mapping each pixel in the
input image with intensity r, into a corresponding pixel with level s;, in the
output image, using Eq. (3.3-8). The transformation (mapping) 7(r,) in this
equation is called a histogram equalization or histogram linearization transformation. It is not difficult to show (Problem 3,10) that this transformation
satisfies conditions (a) and (b) stated previously in this section.@ Before continuing, it will be helpful to work through a simple example.
Suppose that a 3-bit image (L = 8) of size 64 X 64 pixels (MN = 4096) has
the intensity distribution shown in Table 3.1, where the intensity levels are integers in the range [0, L ~ 1] = [0, 7].The histogram of our hypothetical! image is sketched in Fig. 3.19(a). Values
of the histogram equalization transformation function are obtained using
Eq. (3.3-8). For instance,0
Sp = T(r) = 7D pAr;) = TpAro) = 1.33
j=0
Similarly,
1
sy = T(r) = 73) pry) = Tero) + 7p(r1) = 3.08
j=0and s, = 4.55, 33 = 5,67, 54 = 6.23, 85 = 6.65, 55 = 6.86, s7 = 7.00. This transformation function has the staircase shape shown in Fig. 3.19(b).  re ny P(r) = ny/MN
n= 0 790 0.19
n=l 1023 0.25
n= 2 850 0.21
n=3 656 0.16
nad 329 0.08
rs= 5 245 0.06
= 6 122 0.03
n=7 81 0.02
3.3 & Histogram Processing 147M To fix ideas, consider the following simple example. Suppose that the (con- EXAMPLE 3.4:tinuous) intensity values in an image have the PDF Hlustration of
Eqs. (3.3-4) andar (3.3-6).
pr) = @-1y forOsr=ZL 10 otherwiseFrom Eq. (3.3-4),‘
r >. 2 _?
s=rey= (b= f pwd = fp wdw= sySuppose next that we form a new image with intensities, s, obtained using
this transformation; that is, the s values are formed by squaring the corresponding intensity values of the input image and dividing them by (L ~ 1).
For example, consider an image in which L = 10, and suppose that a pixel
in an arbitrary location (x, y) in the input image has intensity 7 = 3. Then
the pixel in that location in the new image is s = T(r) = r2/9 = 1, We can
verify that the PDF of the intensities in the new image is uniform simply by
substituting p,(r) into Eq. (3.3-6) and using the fact that s = r7/(L — 1);        that is,
-1
dr 2r ds
ps) = p,(r) A “ip A
_% fa _P 7
(L-iy\[drb-1
_ ar (L-1)} 1
(L-1y | 2r L-1
where the last step follows from the fact that 7 is nonnegative and we assume
that L > 1. As expected, the result is a uniform PDF. *For discrete values, we deal with probabilities (histogram values) and summations instead of probability density functions and integrals.’ As mentioned
earlier, the probability of occurrence of intensity level r, in a digital image is
approximated bypAn) = MN k=0,1,2...,L-1 (3.3-7)where MN is the tota] number of pixels in the image, n, is the number of pixels that have intensity r,, and L is the number of possible intensity levels in the
image (e.g.,256 for an 8-bit image). As noted in the beginning of this section, a
plot of p,(r,) versus r, is commonly referred to as a histogram. The conditions of monotonicity stated earlier apply also in the discrete case. We simply restrict the values of ihe variabies to be discrete.
43 # Histogram ProcessingPn) Sk Psksq) 
 3+ ¢ 1)
20 , ¢ 564
15 ed 42 
 Ney  to+ 5 Le 28
Ot i bs a hy 14 :boro 1 @ AU4 pp tte ttt +t te
01234867 01234567 01234567
abe 149FIGURE 3.19 Illustration of histogram equalization of a 3-bit (8 intensity levels) image. (a) Originalhistogram. (b) Transformation function. (c) Equalized histogram.At this poini, the s values still have fractions because they were generated
by summing probability values, so we round them to the nearest integer:Sy = 1.33714 $4 = 6.23->6
$; = 3.08 3 Ss = 6.657
Sy = 455-5 56 = 6.86 ->7
$3 = 5676 87 = 7.00->7These are the values of the equalized histogram. Observe that there are only
five distinct intensity levels. Because ry, = 0 was mapped 10 sy) = 1, there are
790 pixels in the histogram equalized image with this value (see Table 3.1).
Also, there are in this image 1023 pixels with a value of s; = 3 and 850 pixels
with a value of s. = 5. However both r; and r, were mapped to the same
value, 6, so there are (656 +. 329) = 985 pixels in the equalized image with this
value. Similarly, there are (245 + 122 + 81) = 448 pixels with a value of 7 in
the histogram equalized image. Dividing these numbers by MN = 4096 yielded
the equalized histogram in Fig. 3.19(c).Because a histogram is an approximation to a PDF, and no new allowed intensity levels are created in the process. perfectly flat histograms are rare in
practical applications of histogram equalization. Thus, unlike its continuous
counterpart, it cannot be proved {in general) that discrete histogram equalization results in a uniform histogram. However, as you will see shortly, using Eq.
(3.3-8) has the general tendency to spread the histogram of the input image so
that the intensity levels of the equalized image span a wider range of the intensity scale. The net result is contrast enhancement. @We discussed earlier in this section the many advantages of having intensity
values that cover the entire gray scale, In addition to producing intensities that
have this tendency, the method just derived has the additional advantage that
it is fully “automatic.” In other words, given an image, the process of histogram
equalization consists simply of implementing Eq. (3.3-8), which is based on information that can be extracted directly from the given image, without the
150 © Chopter 3 @ Intensity Transformations and Spatial FilteringEXAMPLE 3.6:
Histogram
equalization.need for further parameter specifications. We note also the simplicity of the
computations required to implement the technique.
The inverse transformation from s back tor is denoted bym= Ts) k=012..,Lb-1 (3.3-9)It can be shown {Problem 3.10) that this inverse transformation satisfies conditions (a’} and (b) only if none of the levels, r,,k = 0,1,2,...,L — 1, are
missing from the input image, which in turn means that none of the components
of the image histogram are zero. Alihough the inverse transformation is not
used in histogram equalization, it plays a central role in the histogram-matching
scheme developed in the next section.% The left column in Fig. 3.20 shows the four images from Fig. 3.16, and the
center column shows the result of performing histogram equalization on each
of these images. The first three results from top to bottom show significant improvement. As expected, histogram equalization did not have much effect on
the fourth image because the intensities of this image already span the full intensity scale. Figure 3.21 shows the transformation functions used to generate the
equalized images in Fig. 3.20. These functions were generated using Eq. (3.3-8).
Observe that transformation (4) has a nearly linear shape, indicating that the
inputs were mapped to nearly equal outputs.‘The third column in Fig. 3.20 shows the histograms of the equalized images. It
is of interest to note that, while all these histograms are different, the histogramequalized images themselves are visually very similar. This is not unexpected because the basic difference between the images on the left column is one of
contrast, not content, In other words, because the images have the same content, the increase in contrast resulting from histogram equalization was
enough to render any intensity differences in the equalized images visually indistinguishable. Given the significant contrast differences between the original
images, this example illustrates the power of histogram equalization as an
adaptive contrast enhancement tool. # 4.2 Histogram Matching (Specification)As indicated in the preceding discussion, histogram equalization automatically determines a transformation function that seeks to produce an output
image thal has a uniform histogram. When automatic enhancement is desired, this is a good approach because the results from this technique are
predictable and the method is simple to implement. We show in this section
that there are applications in which attempting to base enhancement on a
uniform histogram is not the best approach. In particular, it is useful sometimes to be able to specify the shape of the histogram that we wish the
processed image to have. The method used to generate a processed image
that has a specified histogram is called Aistogram matching or histogram
specification.
152FIGURE 3.21
Transformation
functions for
histogram
equalization.
Transformations
(4) through (4)
were obtained from
the histograms of
the images (from
top to bottom) in
the left column of
Fig. 3.20 using
Eg. (3.3-8).Chepter 3m Intensity Transformations and Spatial Filtering25519264  255192128Let us return for a moment to continuous intensities r and z (considered continuous random variables), and let p,{r) and p,(z) denote their corresponding
continuous probability density functions. In this notation, r and z denote the intensity levels of the input and output (processed) images, respectively. We can
estimate p,(r) from the given input image, while p,(z) is the specified probability density function that we wish the output image to have.Let s be a random variable with the property,
s=T(r)=(L- vf P-(w) dw (3,3-10)
0
where, as before, w is a dummy variable of integration. We recognize this expression as the continuous version of histogram equalization given in Eq. (3.3-4).
Suppose next that we define a random variable z with the propertyZ
G(z) = (L - 1) [o dt=s (3.3-11)
where tis a dummy variable of integration. It then follows from these two
equations that G(z) = T(r) and, therefore, that z must satisfy the conditionz= GT) = G's) (3.3-12)The transformation T(r) can be obtained from Eq. (3.3-10) once p,(r) has
been estimated from the input image. Similarly, the transformation function
G(z) can be obtained using Eq. (3.3-11) because p,(z) is given.Equations (3.3-10) through (3.3-12) show that an image whose intensity
levels have a specified probability density function can be obtained from a
given image by using the following procedure:1. Obtain p,(r) from the input image and use Eq. (3.3-10) to obtain the val
ues of s.
2. Use the specified PDF in Eq. (3.3-11) to obtain the transformation functionG(x).
3.3 # Histogram Processing 1533. Obtain the inverse transformation z = G~'(s); because z is obtained from
$, this process is a mapping from s to z, the latter being the desired values,4. Obtain the output image by first equalizing the input image using Eq.
(3.3-10); the pixel values in this image are the s values. For each pixel with
value s in the equalized image, perform the inverse mapping z = G7}{s) to
obtain the corresponding pixel in the output image. When all pixels have
been thus processed, the PDF of the output image will be equal to the
specified PDF.%& Assuming continuous intensity values, suppose that an image has the intensity PDF p,(r) = 2r/(L — 1f for 0 =r = (ZL — 1) and p,(r) = 0 for other
values of r. Find the transformation function that will produce an image whose
intensity PDF is p,(z) = 327/(L - 1) for 0 = z = (L — 1) and p{z) = Ofor
other values of z.First, we find the histogram equalization transformation for the interval
{0, L - 1):—tm=w-nt- =~ 2 fy gw =
s=T(r)=(L- 1) [ pwyaw = 7 | wdw = 7By definition, this transformation is 0 for values outside the range [0, L — 1].
Squaring the values of the input intensities and dividing them by (L ~ 1)? will
produce an image whose intensities, s, have a uniform PDF because this is a
histogram-equalization transformation, as discussed earlier.We are interested in an image with a specified histogram, so we find next_ : Az 7 3 z 3 _ 2
G(z) (L vf PAW) dw (L _ wl w dw (L _ 1yover the interval (0, 2 — 1); this function is 0 elsewhere by definition. Finally,
we require that G(z) = s, but G(z) = 2/(L — 1); so H/(L — 1 = s, and
we have z= [i - 1s)”
So, if we multiply every histogram equalized pixel by (Z — 1)? and raise the
product to the power 1/3, the result will be an image whose intensities, z, have
the PDF p,{z} = 3z7/(L — 1)° in the interval [0, L — 1], as desired,Because s = 7’/(L — 1) we can generate the z's directly from the intensities, r, of the input image:V3
r Thus, squaring the value of each pixel in the original image, multiplying the result by (L — 1), and raising the product to the power 1/3 will yield an imageEXAMPLE 3.7:
Histogram
specification.
154 Chapter 3 m Intensity Transformations and Spatial Filteringwhose intensity levels, z, have the specified PDF. We see that the intermediate step of equalizing the input image can be skipped; all we need is to obtain
the transformation function T(r) that maps r to s. Then, the two steps can be
combined into a single transformation from r to z. 8As the preceding example shows, histogram specification is straightforward
in principle. In practice, a common difficulty is finding meaningful analytical
expressions for T(r) and G”'. Fortunately, the problem is simplified significantly when dealing with discrete quantities. The price paid is the same as for
histogram equalization, where only an approximation to the desired histogram
is achievable. In spite of this, however, some very useful results can be obtained, even with crude approximations.The discrete formulation of Eq. (3.3-10) is the histogram equalization transformation in Eq. (3.3-8), which we repeat here for convenience:k
5% = TU) = (L - 1) Dptr)
me (3.3-13) (L-D<
= n, =k =0,1,2..,L-1
MN Pywhere, as before, MN is the total number of pixels in the image, n; is the number of pixels that have intensity value r;, and L is the total number of possible
intensity levels in the image. Similarly, given a specific value of s,, the discrete
formulation of Eq. (3.3-11) involves computing the transformation functionq
Gay) = (L~ 1) Sez) (3.3-14)
i=0for a value of q,so that
Glig) = 5 (3.3-15)where p,(z;), is the ith value of the specified histogram. As before, we find the
desired value z, by obtaining the inverse transformation:2g = GM) (3.3-16)In other words, this operation gives a value of z for each value of s; thus, it performs a mapping from s to z.In practice, we do not need to compute the inverse of G. Because we deal
with intensity levels thal are integers (e.g., 0 to 255 for an 8-bit image), it is a
simple matter to compute all the possible values of G using Eq. (3.3-14) for
q = 0,1,2,...,L ~ 1. These values are scaled and rounded to their nearest
integer values spanning the range [0, L — 1]. The values are stored in a table.
Then, given a particular value of s,, we look for the closest match in the values
stored in the table. If, for example, the 64th entry in the table is the closest to
S;, then g = 63 {recall that we start counting at 0) and z 3 is the best solution
to Eq. (3.3-15)}, Thus, the given value s, would be associated with z¢3 (i.e., that
156  Cheprer 3 Intensity Transformations and Spatial Filteringa B PA) P-(24)
FIGURE 3.22(a) Histogram of a
3-bit image. (b)      
       Specified
histogram.
(c) Transformation
function obtained
from the specified
histogram.
(d) Result of
performing
histogram
specification.
Compare
(b) and (d).
In the next step, we compute all the values of the transformation function, G,
using Eq. (3,3-14):
0
Gz) = 71> plz) = 0.00
=U
Similarly,
1
(21) = 7D plz) = T plz) + plz:)] = 0.00
jn0and
G(z2) = 0.00 G(z4) = 2.45 G(x.) = 5.95
G(z3) = 1.05 G(z5) = 4.55 G(z7) = 7.00
TABLE 3.2 5
Specified Actual
Specified and
actual histograms fq Peleq) P(e)
(the values in the w= 0.00 4.00
third column are z= 0.00 0.00
from the 22=2 6.00 0.00
computations 23=3 0.15 6.19
performed in the z= 0.20 0.25
body of Example w= 5 0.30 0.24
3.8). mm = 6 0.20 0.24
z7=7 0.15 O11
a —
3.3 & Histogram Processing 155specific value of s, would map to z3). Because the zs are intensities used
as ihe basis for specifying the histogram p.(z), it follows that zy = 0,
z= 1,...,2,-) = L — 1, so %; would have the intensity value 63. By repeating this procedure, we would find the mapping of each value of s, to the
value of z, ihat is the closest solution to Eq, (3.3-15). These mappings are the
solution to the histogram-specification problem.Recalling that the s,s are the values of the histogram-equalized image, we
may summarize the histogram-specification procedure as follows:1. Compute the histogram p,(r) of the given image, and use it to find the histogram equalization transformation in Eq. (3.3-13), Round the resulting
values, s,, to the integer range [0, L — 1].Compute all values of the transformation function G using the Eq. (3.3-14)for g = 0,1,2,...,L ~ 1, where p,(z,) are the values of the specified his
togram. Round the values of G to integers in the range (0, L — 1]. Store
the values of G in a table,3. For every value of s,, k = 0,1,2,...,£ — 1, use the stored values of G
from step 2 to find the corresponding value of z, so that G(z,) is closest to
5; and store these mappings from s to z, When more than one value of z,
satisfies the given s, (i.e., the mapping is not unique), choose the smallest
value by convention.4. Form the histogram-specified image by first histogram-equalizing the
input image and then mapping every equalized pixel value, s,, of this
image to the corresponding value z, in the histogram-specified image
using the mappings found in step 3. As in the continuous case, the intermediate step of equalizing the input image is conceptual. It can be skipped
by combining the two transformation functions, T and G”!, as Example 3.8
shows.As mentioned earlier, for G"' to satisfy conditions (a’) and (b), G has to be
strictly monotonic, which, according to Eq. (3.3-14), means that none of the values p,(z;) of the specified histogram can be zero (Problem 3.10). When working
with discrete quantities, the fact that this condition may not be satisfied is not a
serious implementation issue, as step 3 above indicates. The following example
illustrates this numerically.We Consider again the 64 X 64 hypothetical image from Example 3.5, whose
histogram is repeated in Fig. 3.22(a). It is desired to transform this histogram
so that it will have the values specified in the second column of Table 3.2.
Figure 3.22(b) shows a sketch of this histogram.The first step in the procedure is to obtain the scaled histogram-equalized
values, which we did in Example 3.5:So=1 sy = 5 sg=7 He =78 =3 x3 =6 55 =7 8 =7EXAMPLE 3.8:
A simple exampte
of histogram
specification,
3.3 m Histogram Processing 157As in Example 3.5, these fractional values are converted to integers in our
valid range, (0, 7). The results are:Gz) = 0.00-+0 G(zs) = 2.452
G{z) = 0.000 G(zs) = 4.555
G(x) = 0.0030 Gz) = 5.956
G(z,) = 1.051 G(z1) = 7.00->7These results are summarized in Table 3.3, and the transformation function is
sketched in Fig. 3.22(c). Observe that G is not strictly monotonic, so condition
(a’) is violated. Therefore, we make use of the approach outlined in step 3 of
the algorithm to handle this situation.In the third step of the procedure, we find the smallest value of z, so that
the value G(z,) is the closest to s,. We do this for every value of s, to create
the required mappings from s to z. For example, sp = 1, and we see that
G(z3) = 1, which is a perfect match in this case, so we have the correspondence sy — 23. That is, every pixel whose value is 1 in the histogram equalized
image would map to a pixel valued 3 {in the corresponding location) in the
histogram-specified image. Continuing in this manner, we arrive at the mappings in Table 3.4.In the final step of the procedure, we use the mappings in Table 3.4 to map
every pixel in the histogram equalized image into a corresponding pixel in the
newly created histogram-specified image. The values.of the resulting histogram are listed in the third column of Table 3.2, and the histogram is
sketched in Fig. 3.22(d). The values of p,(z,) were obtained using the same
procedure as in Example 3.5. For instance, we see in Tabie 3.4 that s = 1 maps
to z = 3, and there are 790 pixels in the histogram-equalized image with a
value of 1. Therefore, p,(z3) = 790/4096 = 0.19.Although the final result shown in Fig. 3.22(d) does not match the specified
histogram exactly, the general trend of moving the intensities toward the high
end of the intensity scale defmitely was achieved. As mentioned earlier, obtaining the histogram-equalized image as an intermediate step is useful for explaining the procedure, but this is not necessary. Instead, we could list the
mappings from the rs to the ss and from the ss to the zs in a three-columnTABLE 3.3All possible0 values of the
transformation
function G scaled,
rounded, and
ordered with
respect to Z. 7% =O1 003 1
25 5
Z 6
a= 7Me&
no |2a
fy2
3
4
5
6
7
158 Chapter 3 m Intensity Transformations and Spatial FilteringTABLE 3.4
Mappings of all
the values of s,
into corresponding
values of Z,,EXAMPLE 3.9:
Comparison
between
histogram
equalization and
histogram
matching.abFIGURE 3.23(a) Image of the
Mars moon
Phobos taken by
NASA’s Mars
Global Surveyor.
(b) Histogram.
(Original image
courtesy of
NASA.)    table. Then, we would use those mappings to map the original pixels directly
into the pixels of the histogram-specified image. ES}wi Figure 3.23(a) shows an image of the Mars moon, Phobos, taken by NASA's
Mars Global Surveyor. Figure 3.23(b)} shows the histogram of Fig. 3.23(a). The
image is dominated by large, dark areas, resulting in a histogram characterized
by a large concentration of pixels in the dark end of the gray scale. At first
glance, one might conclude that histogram equalization would be a good approach to enhance this image, so that details in the dark areas become more
visible. It is demonstrated in the following discussion that this is not so.Figure 3.24(a) shows the histogram equalization transformation [Eq. (3.3-8)
or (3.3-13)] obtained Irom the histogram in Fig, 3.23(b). The most relevant
characteristic of this transformation function is how fast it rises from intensity
level 0 to a level near 190, This is caused by the large concentration of pixels in
the input histogram having levels near 0, When this transformation is applied
to the levels of the inpul image to obtain a histogram-equalized result, the net
effect is to map a very narrow interval of dark pixels into the upper end of the
gray scale of the output image. Because numerous pixels in the input image
have levels precisely in this interval. we would expect the result to be an image
with a light, washed-out appearance. As Fig, 3.24(b) shows, this is indeed the 
 
 of pixels (% 104)
3.3 W Histogram Processing 159   255 es ee
= 192
& 128
3
Z
Oo 64 .
% 64 128 192
Input intensity
7.00 T TT
2
xX 5.25
€ 3.50
s
8 1.75
E
5
2 wen
0 ne
0 64 128 192 255
Intensitycase. The histogram of this image is shown in Fig. 3.24(c). Note how all the intensity Jevels are biased toward the upper one-half of the gray scale.Because the problem with the transformation function in Fig. 3.24(a) was
caused by a large concentration of pixels in the original image with Jevels near
0, a reasonable approach is to modify the histogram of that image so that it
does not have this property. Figure 3.25(a) shows a manually specified function
that preserves the general shape of the original histogram, but has a smoother
transition of levels in the dark region of the gray scale. Sampling this function
into 256 equally spaced discrete values produced the desired specified histogram. The transformation function G(z) obtained from this histogram using
Eq. (3.3-14) is labeled transformation (1) in Fig. 3.25(b). Similarly, the inverse
transformation G7!(s) from Eq. (3.3-16) (obtained using the step-by-step procedure discussed earlier) is labeled transformation (2) in Fig. 3.25(b). The enhanced image in Fig. 3.25(c) was obtained by applying transformation (2) to
the pixels of the histogram-equalized image in Fig. 3.24(b). The improvement
of the histogram-specified image over the result obtained by histogram equalization is evident by comparing these two tmages. It is of interest to note that a
rather modest change in the original histogram was all that was required to
obtain a significant improvement in appearance. Figure 3.25(d) shows the histogram of Fig. 3.25(c). The most distinguishing feature of this histogram is
how its low end has shifted right toward the lighter region of the gray scale
{but not excessively so), as desired. %ab
¢FIGURE 3.24(a) Transformation
function for
histogram
equalization.(b) Histogramequalized image
(nate the washedout appearance),
(c) Histogramof (b).
160  Chupter 3 m Intensity Transformations and Spatial FilteringBe
b
dFIGURE 3.25(a) Specified
histogram.(bd) Transformations,
(c) Enhanced image
using mappings
from curve (2).(d) Histogram of (c).   
  
 
 
 
 
 
 
 
 
 
  7.00 ——t       2
x 3.25
3
& 3.50
S
g
€ 4.75
Ee}
z
0 —_1—
i) 64 128 192
Intensity
255
B12
2
2
= 128
J
a
3
Cc 64
7
0 eden |
0 64 128 192 255
Input intensity
70 J |
e
x 5,25
& 3.50
3
5
2 1.75
€
Zz
0 | nn Cnnnenene Da
0 64 128 192 255
IntensityAlthough it probably is obvious by now, we emphasize before leaving this
section that histogram specification is, for the most part. a uialand-crior
process. One can use guidelines learned from the problem at hand. just as we
did in the preceding example. At times, there may be cases in which it is possible to formulate what an “average” histogram should loak dike and use that as
the specified histogram. In cases such as these, histogram speciticstion by
comes a Straightforward process. In general, however. there are nc rules Tor
specifying histograms, and one must resort to analysis on a case -by-casi: basis
for any given enhancement task.
162 Chapter 3 w Intensity Transformations and Spatial Filteringabe FIGURE 3.26 (a) Original image. (b) Result of global histogram equalization. (c) Result of local
histogram equalization applicd to (a), using a neighborhaod of size 3 x 3.We follow convention in
using m for the mean
value. Bo not confuse it
with the same sym
used to denote the number of rows in an m X #
agighborkood, in which
we also follow actational
convention.component corresponding to value r;. As indicated previously, we may view
p(r,;) as an estimate of the probability that intensity 7; occurs in the image from
which the histogram was obtained,As we discussed in Section 2.6.8, the nth moment of r about its mean is defined asbot
Malt) = By ~ my" pr) (33-17)
f=.
where m is the mean (average intensity) value of 7 (i.¢., the average intensity
of the pixels in the image):
al
m= Srp) (3.3-18}
i=0
The second moment is particularly important:
L-t
polr) = Soy ~ mF ply) (3.3-19)
ied
We recognize this expression as the intensity variance, normally denoted by 0?
(recall that the standard deviation is the square root of the variance). Whereas
the mean is a measure of average intensity, the variance (or standard deviation) is a measure of contrast in an image. Observe that all moments are computed easily using the preceding expressions once the histogram has been
obtained from a given image. ‘Wher vorking with only the mean and variance, it is common practice to eslimaie them directly from the sample values, without computing the histogram.
Appropriately, these estimates are called the sample mean and sample variance.
They are given by the following familiar expressions fror basic statistics:p Moines ;
mnt us s 2 fv. yy (3.3-20)eOe
3.3 @ Histogram Processing 163and
M-~=n 2for x = 0,1,2,...,M — land y = Otd — 1. In other words, as we
know, the mean intensity of an image can be obtained simply by summing the
values of ali its pixels and dividing the sum by the total number of pixels in the
image. A similar interpretation applies to Eq. (3.3-21). As we illustrate in the following example, the results obtained using these two equations are identical to
the results obtained using Eqs. (3.3-18) and (3.3-19), provided that the histogram
used in these equations is computed from the same image used in Eqs. (3.3-20)
and (3.3-21),ime“ft, ») ~ m? (33-21)®@ Before proceeding, it will be useful to work through a simple numerical example to fix ideas. Consider the following 2-bit image of size 5 X S:ren wrod
—- WwW nN ©
We RW
MRONORF
NOS“ NThe pixels are represented by 2 bits; therefore, L = 4 and the intensity levels
are in the range [0,3]. The total number of pixels is 25, so the histogram has the
components(ra) = = 0.24; pln) = 3 = 0.28,p(n) = 3 = 028; pls) = 2 = 020where the numerator in nook is the number of pixets in the image with intensity
level r;. We can compute the average value of the intensities in the image using
Eq. (3.3-18):3m= Sinpln)i=0(0)(0.24) + (1)(0.28) + (2)(0.28) + (3)(0.20)
= 1.44
Letting f(x, y) denote the pens 5x5 array and using Eq. (3.3-20), we obtain
mas >) Sher)
xat) y=01.44fltt‘The denominator inEg. (3.3-21) iy written
sometimes as MN - I
instead of AN, This is
done to obtain aso
called unbiased estimate
of the variance. However, we are more interest
ed in Eqs. (3.3-21) and
(33-19) agreeing when
the histogram in the latter equation is computed
from the same image
used in Eq. (3.3-23). For
this we require the MN
term. The difference is
negligible for any image
of practical size.EXAMPLE 3.11:
Computing
histogram
Statistics.
3.3 % Histogram Processing 1613.3.3 Local Histogram ProcessingThe histogram processing methods discussed in the previous two sections are
global, in the sense that pixels are modified by a transformation function
based on the intensity distribution of an entire image. Although this global approach is suitable for overall enhancement, there are cases in which it is necessary to enhance details over small areas in an image. The number of pixels in
these areas may have negligible influence on the computation of a global
transformation whose shape does not necessarily guarantee the desired local
enhancement. The solution is to devise transformation functions based on the
intensity distribution in a neighborhood of every pixel in the image.The histogram processing techniques previously described are easily adapted
to jocal enhancement. The procedure is to define a neighborhood and move
its center from pixel to pixel. At each location, the histogram of the points in
the neighborhood is computed and either a histogram equalization or histogram specification transformation function is obtained. This function is
then used to map the intensity of the pixel centered in the neighborhood. The
center of the neighborhood region is then moved to an adjacent pixel location
and the procedure is repeated. Because only one row or column of the neighborhood changes during a pixel-to-pixel translation of the neighborhood, updating the histogram obtained in the previous location with the new data
introduced at each motion step is possible (Problem 3.12). This approach has
obvious advantages over repeatedly compuling the histogram of all pixels in
the neighborhood region each time the region is moved one pixel] location.
Another approach used sometimes to reduce computation is to utilize
nonoverlapping regions, but this method usually produces an undesirable
“blocky” effect.@ Figure 3.26(a) shows an 8-bit, 512 < 512 image that at first glance appears
to contain five black squares on a gray background. The image is slightly noisy,
but the noise is imperceptible. Figure 3.26(b) shows the result of global histogram equalization. As often is the case with histogram equalization of
smooth, noisy regions, this image shows significant cnhancement of the noise.
Aside from the noise, however, Fig. 3.26(b) does not reveal any new significant
details from the original, other than a very faint hint that the top left and bottom right squares contain an object. Figure 3.26(c) was obtained using local
histogram equalization with a neighborhood of size 3 x 3. Here, we see signilicant detail contained within the dark squares. The intensity values of these obiects were too close to the intensity of the large squares, and their sizes were
‘00 small, to influence global histogram equalization significantly enough tshow this detail. te3.3.4 Using Histogram Statistics for Image EnhancementStatistics obtained directly from an image histogram can be used for image enyancement. Let r denote a discrete random variable representing intensity valJes in the range [0,2 ~ 1], and let p(r;) denote the normalized histogramEXAMPLE 3.10:
Loeai histogram
equalization.
164  Ghopter 3 m Intensity Transformations and Spatial FilteringEXAMPLE 3.12;
Local enhancement using
histogram
Statistics.As expected, the results agree. Similarly, the result for the variance is the same
(1.1264) using either Eq. (3.3-19) or (3.3-21). aWe consider two uses of the mean and variance for enhancement purposes.
The global mean and variance are computed over an entire image and are useful for gross adjustments in overall intensity and contrast. A more powerful
use of these parameters is in local enhancement, where the focal mean and
variance are used as the basis for making changes that depend on image characteristics in a neighborhood about each pixel in an image.Let (x, y) denote the coordinates of any pixel in a given image, and let S,,
denote a neighborhood (subimage) of specified size, centered on (x, y). The
mean value of the pixels in this neighborhood is given by the expressionL-1
ms, = Dips, (n) (3.3-22)
i=0where. ps, is the histogram of the pixels in region S,,. This histogram has L
components, corresponding to the L possible intensity values in the input image.
However, many of the components are 0, depending on the size of S,,. For example, if the neighborhood is of size 3 x 3 and L = 256, only between 1 and 9
of the 256 components of the histogram of the neighborhood will be nonzero.
These non-zero values will correspond to the number of different intensities in
Sy (the maximum number of possible different intensities ina 3 x 3 region is 9,
and the minimum is 1).
The variance of the pixels in the neighborhood similarly is given byL-1
3, = Di — ms,)’ Ps, (n) (33-23)
i=0As before, the local mean is a measure of average intensity in neighborhood
S,,, and the local variance (or standard deviation) is a measure of intensity
contrast in that neighborhood. Expressions analogous to (3.3-20) and (3.3-21)
can be written for neighborhoods. We simply use the pixel values in the neighborhoods in the summations and the number of pixels in the neighborhood in
the denominator.As the following example illustrates, an important aspect of image processing using the local mean and variance is the flexibility they afford in developing
simple, yet powerful enhancement techniques based on statistical measures
that have a close, predictable correspondence with image appearance.W@ Figure 3.27(a) shows an SEM (scanning electron microscope) image of a
tungsten filament wrapped around a support. The filament in the center of
the image and its support are quite clear and easy to study. There is another
filament structure on the right, dark side of the image, but it is almost imperceptible, and its size and other characteristics certainly are not easily discernable. Local enhancement by contrast manipulation is an ideal approach to
problems such as this, in which parts of an image may contain hidden features.
3.3 m Histogram Processing abeFIGURE 3.27 (a) SEM image of a tungsten filament magnified approximately 130.
{b) Result of global histogram equalization. (c) hmage enhanced using local histogram
statistics. (Original image courtesy of Mr, Michae) Shaffer, Department of Geological
Sciences, University of Oregon, Eugene.)In this particular case, the problem is to enhance dark areas while leaving
the light area as unchanged as possible because it does not require enhancement, We can use the concepts presented in this section to formulate an enhancement method that can tell the difference between dark and light and, at
the same time, is capable of enhancing only the dark areas. A measure of
whether an area is relatively light or dark at a point (4, y) is to compare the average Jocal intensity, m5, lo the average image intensity, called the global
mean and denoted nig. This quantity is obtained with Eq, (3.3-18) or (3.3-20)
using the entire image. Thus, we have the first element of our enhancement
scheme: We will consider the pixel at a point (x, y) as a candidate for processing
if ms, = kgmg, where ky is a positive constant with value less than 10,Because we are interested in enbancing areas that have low contrast, we also
need a measure to determine whether the contrast of an area makes it a candidate for enhancement. We consider the pixel at a point (x, y) as a candidate for
enhancement if os 5 k20¢. where o¢; is the global standard deviation
obtained using Eqs, (3.3-19) or (3.3-21) and &, is a positive constant, The value
of this constant will be greater than 1.0 if we are interested in enhancing light
areas and Jess than 1.0 for dark areas.Finally, we need to restrict the lowest valucs of contrast we are willing to accept; otherwise the procedure would attempt to enhance constant areas, whose
standard deviation is zero. Thus, we also set a lower limit on the local standard
deviation by requiring that k,o;; = o's. with k; < ky. A pixel at (x, y) that
meets all the conditions for local enhancement is processed simply by multiplying it by a specified constant. £. to increase (or decrease) the value of its intensity level relative to the rest of the image. Pixels that do not meet the
enhancement conditions are not changed165
168 Ghopter 3 # Intensity Transformations and Spatial Fitteringve origin         n w(OD) (1)      wold Pow Mavis Othe h
i    
    Pixels of image   section under filter  FIGURE 3.28 The mechanics of linear spatial filtering using a 3 x 3 filter mask. The form chosen to denote
the coordinates of the filter mask coefficients simplifies writing expressions for lincar filtering.‘A. Spatial Correlation and ConvolutionThere are two closely related concepts that must be understood clearly when
performing Hinear spatial filtering. One is cerrefativn and the other is
convolution, Correlation is the process of moving a filter mask over the image
and computing the sum of products at each location, exactly as explained in
the previous section. The mechanics of convolution are the same, except that
the filter is first rotated by 180°. The best way lo explain the differences between the two concepts is by example. We begin with a 1-D illustration.
Figure 3.29(a) shows a [-D function, f. and a filter, 7. and Fig. 3.29(b) shows
the starting position to perform correlation. The first thing we note is that there
166Qupior 3m Intensity Transformations and Spatial FilteringWe summarize the preceding approach as follows, Let f(x, y) represent the
value of an image at any image coordinates (x, y), and let g(x, y) represent the
corresponding enhanced value at those coordinates. Then,E-f(x,y) ifms,, = kymg AND kigg = os, = k20G
(x, y) = (3.3-24)
(4, y) otherwisefor x = 0,1,2,...,.M —1 and y = 0,1,2,...,N — 1, where, as indicated
above, E, Xo, k;, and k, are specified parameters, mtg is the global mean of the
input image, and a¢ is its standard deviation. Parameters ms and o's, are the
local mean and standard deviation, respectively. As usual, M and N are the row
and column image dimensions,Choosing the parameters in Eq. (3.3-24) generally requires a bit of experimentation to gain familiarity with a given image or class of images. In this
case, the following values were selected: E = 4.0, ky = 0.4, k, = 0.02, and
kz = 0.4. The relatively low value of 4.0 for EF was chosen so that, when it was
multiplied by the levels in the areas being enhanced (which are dark), the result would still tend toward the dark end of the scale, and thus preserve the
general visual balance of the image. The value of ko was chosen as less than
half the global mean because we can see by looking at the image that the areas
that require enhancement definitely are dark enough to be below half the
global mean. A similar analysis led to the choice of values for k, and k.
Choosing these constants is not difficult in general, but their choice definitely
must be guided by a logical analysis of the enhancement problem at hand. Finally, the size of the focal area S,, should be as small as possible in order to
preserve detail and keep the computational burden as low as possible. We
chose a region of size 3 X 3.As a basis for comparison, we enhanced the image using global histogram
equalization. Figure 3.27(b) shows the result. The dark area was improved but
details still are difficult to discern, and the light areas were changed, something
we did not want to do. Figure 3.27(c) shows the result of using the local statistics method explained above. In comparing this image with the original in Fig.
3,27(a) or the histogram equalized result in Fig. 3.27(b), we note the obvious
detail that has been brought out on the right side of Fig. 3.27(c), Observe, for
example, the clarity of the ridges in the dark filaments. It is noteworthy that
the light-intensity areas on the left were left nearly intact, which was one of
our initial objectives. =EX Fundamentals of Spatial FilteringJn this section, we introduce several basic concepts underlying the use of spatial filters for image processing. Spatial filtering is one of the principal tools
used in this field for a broad spectrum of applications, so it is highly advisable
that you develop a solid understanding of these concepts. As mentioned at the
beginning of this chapter, the examples in this section deal mostly with the use
of spatial filters for image enhancement. Other applications of spatial filtering
are discussed in later chapters.
3.4 @ Fundamentals of Spatial Filtering 167The name filter is borrowed from frequency domain processing, which is
the topic of the next chapter, where “filtering” refers to accepting (passing) or
rejecting certain frequency components. For example, a filter that passes low
frequencies is called a lowpass filter, The net effect produced by a lowpass filter is to blur (smooth) an image. We can accomplish a similar smoothing directly on the image itself by using spatial filters (also called spatial masks,
kernels, templates, and windows). In fact. as we show in Chapter 4, there is a
one-to-one correspondencé between linear spatial filters and filters in the frequency domain. However, spatial filters offer considerably more versatility be~
cause, as you will see later, they can be used also for nonlinear filtering,
something we cannot do in the frequency domain.See Section 2.6.2
regarding linearity.3.4.1 The Mechanics of Spatial FilteringIn Fig. 3.1, we explained briefly that a spatial filter consists of (1) a
neighborhood, (typically a small rectangle), and (2) a predefined operation that
is performed on the image pixels encompassed by the neighborhood. Filtering
creates a new pixel with coordinates equal to the coordinates of the center of
the neighborhood, and whose value is the result of the filtering operation.’ A
processed (filtered) image is generated as the.center of the filter visits each
pixel in the input image. If the operation performed on the image pixels is linear, then the filter is called a linear spatial filter. Otherwise, the filter is
nonlinear, We focus attention first on linear filters and then illustrate some
simple nonlinear filters. Section 5.3 contains a more comprehensive list of nonlinear filters and their application. .Figure 3.28 illustrates the mechanics of linear spatial filtering using a3 x 3
neighborhood. At any point (x, y) in the image, the response, g(x, y), of the filter is the sum of products of the filter coefficients and the image pixels encompassed by the filter:g(x, y) = w(-1, -i)f(x - lyy¥ — 1) + wl(-1, f(x - ly) + |.
+ w(0, 0) f(x, y) +... + wO, fOr t+ Ly + I)Observe that the center coefficient of the filter, w(0, 0), aligns with the pixel at
location (x, y). For a mask of size m x v, we assume that m = 2a + 1 and
n = 2b + 1, where a and 6 are positive integers, This means that our focus in
the following discussion is on filters of odd size, with the smallest being of size it certainly is possible to
3 x 3. In general, linear spatial filtering of an image of size MX N witha fil- “or wth fiers of sventer of size m X ris given by the expression: odd sizes. However.
working with odd sizesa 6b simplifies indexing and= ws, + +f also is more intuitive
8% 9) >> =! (s. OfOr + sy ) because the filters have
centers falling on integervalues.where x and y are varied so that each pixel in w visits every pixel in f. ' The filtered pixel vatue typically is assigned to a corresponding location in a new image created to hold
the resuits of filtering. It is seldom the case that filtered pixels repiace the values of the corresponding
location in the original image, 3s this would change the content of the image while filtering still is being
performed.
3.4 Fundamentals of Spatial Filtering 169Correlation Convolution
/7 Ovigin I w a Origin f w rotated 140°
f 0001000 12328 god07T 0000 $2321 (i)
+
(b) oooT0C00 00010000 i)
12328 82321
L Starting position alignment
.
Zero padding
— =
fc) O00 CEHEIOXDODOHOO 0006000100600 006 00 (k)
12328 82321
@dOOooTOTO00000000 O800T00G1TCO0DBOOD
12328 82321£. Position after one shiftf@ODOVOD0D01 00000000 od000000FVOV00000 (mM12328 82321
£ Position after four shiftsMH) FO0000010090 00000 90G000010008000006 ()
12328 82321Final position _Full correlation result . Ful) convolution resuit
(g) 900823210000 oOoOvur722 28 0O40 {o)
Cropped correlation result Cropped convolution result
{h) 08232100 oOr23 2860 ro)FIGURE 3.29 illustration of |-D correlation and convolution of a filter with a discrete unit impulse. Note thatcorrelation and canvolution are functions of displacement.are parts of the functions that do not overlap. The solution to this problem is to
pad f with enough Os on either side to allow each pixel in w to visit every pixel in
f. If the filter is of size m, we need m — 1 Os on either side of f. Figure 3.29(c)
shows a properly padded function. The firsi value of correlation is the sum of
products of f and w for the initial position shown in Fig. 3.29(c) (the sum of
products is 0). This corresponds to a displacement x = 0. To obtain the second
value of correlation, we shifl 2 one pixel location to the right (a displacement of= 1) and compute the sum of products. The result again is 0. In fact, the first
nonzero resuli is when x = 3, in which case the 8 in t overlaps the } in fand the
result of correlation is 8. Proceeding in this manner, we obtain the full correlation
result in Fig, 3.29(g). Note that it took {2 values of x (ie, x = 0,1,2,....1)) lo
fully slide 2@ past f so that each pixcl in w visited every pixel inf. Often, we like
to work with correlation arrays that are the same size as f,in which case we crop
the full correlation to the size of the original function, as Fig. 3.29(h)} shows.Zeto padding ts not the
only option, For example.
we could duplicate the
value of the first and last
g¢lement ac 1 times on
cach side of fer mirror
the first and fast vr -- 4
elements and use the
miftored valucs for
paddingunseen
170 Chapter 3 m Intensity Transformations and Spatial FilteringNote that rotation by180° is equivalent (o flip
Ping the function forizontally.In 2-D, rotation by 180°
is equivalent to flipping
the mask along one axis
and then the other,There are two important points to note from the discussion in the preceding
paragraph. First, correlation is a function of displacement of the filter. In other
words, the first value of correlation corresponds to zero displacement of the
filter, the second corresponds to one unit displacement, and so on. The second
thing to notice is that correlating a filter w with a function that contains all 0s
anda single 1 yields a result that is a copy of w, but rotated by 180°. We call a
function that contains a single 1 with the rest being Os a discrete unit impulse.
So we conclude that correlation of a function with a discrete unit impulse
yields a rotated version of the function at the location of the impulse.The concept of convolution is a cornerstone of linear system theory. As you
will learn in Chapter 4, a fundamental property of convolution is that convolying a function with a unit impulse yields a copy of the function at the location
of the impulse. We saw in the previous paragraph that correlation yields a copy
of the function also, but rotated by 180°. Therefore, if we pre-rotate the filter
and perform the same sliding sum of products operation, we should be able to
obtain the desired result. As the right column in Fig. 3.29 shows, this indeed is .
the case. Thus, we see that to perform convolution all we do is rotate one function by 180° and perform the same operations as in correlation. As it turns out,
it makes no difference which of the two functions we rotate.The preceding concepts extend easily to images, as Fig. 3.30 shows. For a filter of size m X n, we pad the image with a minimum of m — 1 rows of 0s at
the top and bottom and 2 ~ 1 columns of 0s on the left and right. In this case,
m and n are equal to 3,so we pad f with two rows of Os above and below and
two columns of Os to the left and right, as Fig, 3.30(b) shows. Figure 3.30(c)
shows the initial position of the filter mask for performing correlation, and
Fig. 3.30(d) shows the full correlation result. Figure 3.30(e) shows the corresponding cropped result. Note again that the result is rotated by 180°. For convolution, we pre-rotate the mask as before and repeat the sliding sum of
products just explained. Figures 3.30(f) through (h) show the result. You see
again that convolution of a function with an impulse copies the function at the
location of the impulse. It should be clear that, if the filter mask is symmetric,
correlation and convolution yield the same result.Hf, instead of containing a single 1, image f in Fig. 3.30 had contained a region identically equal to w, the value of the correlation function (after normalization) would have been maximum when w was centered on that region
of f. Thus, as you will see in Chapter 12, correlation can be used also to find
matches between images.Summarizing the preceding discussion in equation form, we have that the
correlation of a filter w(x, y) of size m X n with an image f(x, y), denoted as
w(x, y) & fly, y), is given by the equation listed at the end of the last section,
which we repeat here for convenience:a b
wx ye f(y = YS Dw, ofeet sy +9 (3.4-1)
sena feb
This equation is evaluated for all values of the displacement variables x and y
so that al] elements of w visit every pixel in f, where we assume that f has been
padded appropriately. As explained earlier, a = (m — 1)/2,6 = (n — 1)/2,
and we assume for notational convenience that m and n are odd integers.
3.4 @ Fundamentals of Spatial Filtering 171Padded foedgovuo 8 08
gdg00 00 00 86
ooodo oad aon7 Ovigin fy) 906000 00090
oeao 00 o00 8109000
oo0008 wry) OOO KP OO Od
goriou0 123 Q0000 80800

00000 456,00 000060 0
ooo t 789 BUD BDO UEG
(a) (b)
Initial posttion for w Full correlation result Cropped correlation resuttT2PHHGeCKCHO OHOH HOH DO409
456000000 0U0FUOGHCD HF8BT7EB
7897000008 BHVvOLOKOe 06540
PO0GHODHOeH O0HU9BTHHOH 03210
000010000 000654000 CHOO0
oo0K8HROKHOHU 060321008
ge7oo0qcypou dg vaOvANOGCA OD
0OGD0000NL CBHOHHHO HYg9ovono8 BO ON OKA HHGA
©) (a) (e)
Rotated w Full convolution result - Cropped convolution result9 a7 HooGHK DHHHHK1OO BOOBe
654000000 DOT HOHO00 O12 3 6
B2H0080080 OF UHOR0G8 045 60
ood oogd 6 OHH 1T2Z9 00 287890
po0dgrtun OO BDHO4S 6000 VDHOGU
oo0oononv on 00 OH U07RB9HUO 4 .oo 0 ON OOO Uo UO Of Otpo0 00 80800 BUODOO oO 4vow Hou OORDOGHALD) {z) (n)In a similar manner, the convolution of w(x, y) and f(x, y), denoted by
w(x, y) & f(x, y),! is given by the expressiona &
w(x, y)* f(xy) = > PAG Df(x-—sy- oO (3.4-2)$26 I=
where the minus signs on the right flip f (i.c., rotate it by 180°). Flipping and
shifting f instead of w is done for notational simplicity and also to foliow
convention. The result is the same. As with correlation, this equation is evaluated for all values of the displacement variables x and y so that every element of w visits every pixel in f, which we assume has been padded
appropriately. You should expand Eq. (3.4-2) for a 3 x 3 mask and convince
yourself that the result using this equation is identical to the example in
Fig. 3.30. In practice, we frequently work with an algorithm that implements T Because correlation and convolution are commutative, we have that w(x, y) % f(x, ¥)
= f(xy) & w(x, y) and w(x, y) £05 ¥) = FCs y} te wo, y).FIGURE 3.30
Correlation
(middle row) and
convolution (last
row) of a2-D
filter with a 2-D
discrete, unit
impulse. The 0s
are shown in gray
to simplify visual
analysis.Often, when Uhe meaning is cléar,we denote
the result of correlation
or convohition by a function g(x. ¥), instead of
writing w(x, ¥) & fix. ¥)
or wha, y) & F(x. 9). For
example, see the equation al the end of the
Previous section, and
Eq. (3.5-1)
172 Chapter 3 @ Intensity Transformations and Spatial FilteringConsult the Tutorials section of ihe book Web sitefor a brief review of vectors and matrices.Eq. (3.4-1). If we want to perform correlation, we input w into the algorithm;
for convolution, we input w rotated by 180°. The reverse is true if an algorithm that implements Eq. (3.4-2) is available instead.As mentioned earlier, convolution is a cornerstone of linear system theory.
As you will learn in Chapter 4, the property that the convolution of a function
with a unit impulse copies the function at the location of the impulse plays a
central role in a number of important derivations. We will revisit convolution
in Chapter 4 in the context of the Fourier transform and the convolution theorem. Unlike Eg. (3.4-2), however, we will be dealing with convolution of
functions that are of the same size. The form of the equation is the same, but
the limits of summation are different.Using correlation or convolution to perform spatial filtering is a matter of
preference. In fact, because either Eq. (3.4-1) or (3.4-2) can be made to perform the function of the other by a simple rotation of the filter, what is important is that the filter mask used in a given filtering task be specified in a way
that corresponds to the intended operation, All the linear spatial filtering results in this chapter are based on Eq. (3.4-1).Finally, we point out that you are likely to encounter the terms,
convolution filter, convolution mask or convolution kernel in the image processing literature. As a rule, these terms are used to denote a spatial filter,
and not necessarily that the filter will be used for true convolution. Similarly,
“convolving a mask with an image” often is used to denote the sliding, sumof-products process we just explained, and does not necessarily differentiate
between correlation and convolution. Rather, it is used generically to denote
either of the two operations. This imprecise terminology is a frequent source
of confusion.3.4.3 Vector Representation of Linear FilteringWhen interest lies in the characteristic response, R, of a mask either for correlation or convolution, it is convenient sometimes to write the sum of
products asR= wiz + Wy% + 0. + Won2mn
mn
= Dupz (3.4-3)
k=l
=w7zwhere the ws are the coefficients of an m X a filter and the zs are the corresponding image intensities encompassed by the filter. If we are interested in
using Eq. (3.4-3) for correlation, we use the mask as given. To use the same
equation for convolution, we simply rotate the mask by 180°, as explained in
the last section. It is implied that Eq. (3.4-+) holds for a particular pair of coordinates (x, y). You will see in the next section why this notation is convenient
for explaining the characteristics of a given linear filter.
3.4 m@ Fundamentais of Spatial Filtering 173   As an example, Fig. 3.31 shows a general 3 X 3 mask with coefficients labeled as above. In this case, Eq. (3.4-3) becomesR= wyZ + WZ + ... + Wy2q9
= Dmx (3.4-4)
k=l=wy
where w and z are 9-dimensional vectors formed from the coefficients of the
mask and the image intensities encompassed by the mask, respectively.3.4.4 Generating Spatial Filter MasksGenerating an m X n linear spatial filter requires that we specify mn mask coefficients. In turn, these coefficients are selected based on what the filter is
supposed to do, keeping in mind that all we can do with linear filtering is to implement a sum of products. For example, suppose that we want to replace the
pixels in an image by the average intensity of a3 < 3 neighborhood centered
on those pixels. The average value at any location (x, y) in the image is the sum
of the nine intensity values in the 3 X 3 neighborhood centered on (x, y) divided by 9. Letting z;,,i = 1,2,...,9, denote these intensities, the average is12
R ==<Sz
95But this is the same as Eq. (3.4-4) with coefficient values w; = 1/9. In other
words, a linear filtering operation with a3 x 3 mask whose coefficients are 1/9
implements the desired averaging. As we discuss in the next section, this operation results in image smoothing. We discuss in the following sections a number of other filter masks based on this basic approach.In some applications, we have a continuous function of two variables, and
the objective is to obtain a spatial filter mask based on that function. For example, a Gaussian function of two variables has the basic formA(x, y) = eT RF
where a is the standard deviation and, as usual, we assume that coordinates x
and y are integers. To generate, say, a 3 X 3 filter mask from this function, weFIGURE 3.31
Another
representation of
a general 3 x 3
filter mask.
3.5 & Smoothing Spatial Filters 175ei  the coefficients of the filter are all 1s. The idea here is that it is computationally
more efficient to have coefficients valued 1. At the end of the filtering process
the entire image is divided by 9. An m X n mask would have a normalizing
constant equal to 1/mn. A spatial averaging filter in which all coefficients are
equal sometimes is called a box filter.The second mask in Fig. 3.32 is a little more interesting. This mask yields a socalled weighted average, terminology used to indicate that pixels are multiplied by
different coefficients, thus giving more importance (weight) to some pixels at the
expense of others. In the mask shown in Fig. 3.32(b) the pixel at the center of the
mask is multiplied by a higher value than any other, thus giving this pixel more
importance in the calculation of the average. The other pixels are inversely
weighted as a function of their distance from the center of the mask.The diagonal
terms are further away from the center than the orthogonal neighbors (by a factor of V2) and, thus, are weighed less than the immediate neighbors of the center
pixel. The basic strategy behind weighing the center point the highest and then
reducing the value of the coefficients as a function of increasing distance from the
origin is simply an attempt to reduce blurring in the smoothing process. We could
have chosen other weights to accomplish the same general objective. However,
the sum of all the coefficients in the mask of Fig. 3.32(b) is equal to 16, an attractive feature for computer implementation because it is an integer power of 2. In
practice, it is difficult in general to see differences between images smoothed by
using either of the masks in Fig. 3.32, or similar arrangements, because the area
spanned by these masks at any one location in an image is so small.With reference to Eq. (3.4-1), the general implementation for filtering an
M X N image with a weighted averaging filter of size m2 X_n (mm and 7 odd) is
given by the expressionS S ws ofx+ yt
S=-a t=-b - 7 ~—— (3.5-1)
SS Sws=—a tb gs y) =The parameters in this equation are as defined in Eq. (3-4-1). As before, it is understood that the complete fittered image is obtained by applying Eq. (3.5-1)
for x = 0,1,2...., M— i and y =0,1.2,....N — 1. The denominator inabFIGURE 3.32 Two
3 x 3 smoothing
(averaging) filter
masks. The
constant multiplier in front of each
mask is equal to 1
divided by the
sum of the values
of its coefficients,
as is required to
compute an
average.
174 Chapter 3. m Intensity Transformations and Spatial Filteringsample it about its center. Thus, w, = 4(-1,-1), w2 = A(—I,0),...,
Wy = A(1, 1). An m X n filter mask is generated in a similar manner. Recall
that a 2-D Gaussian function has a bell shape, and that the standard deviation
controls the “tightness” of the bell.Generating a nonlinear filter requires that we specify the size of a neighborhood and the operation(s) to be performed on the image pixels contained
in the neighborhood. For example, recalling that the max operation is nonlinear (see Section 2.6.2),a5 X 5 max filter centered at an arbitrary point (x, y)
of an image obtains the maximum intensity value of the 25 pixels and assigns
that value to location (x, y) in the processed image. Nonlinear filters are quite
powerful, and in some applications can perform functions that are beyond the
capabilities of linear filters, as we show ater in this chapter and in Chapter 5.EE Smoothing Spatial FiltersSmoothing filters are used for blurring and for noise reduction. Blurring is
used in preprocessing tasks, such as removal of small details from an image
prior to (large) object extraction, and bridging of small gaps in lines or curves,
Noise reduction can be accomplished by blurring with a linear filter and also
by nonlinear filtering.3.5.1 Smoothing Linear FiltersThe output (response) of a smoothing, linear spatial filter is simply the average
of the pixels contained in the neighborhood of the filter mask. These filters
sometimes are called averaging filters. As mentioned in the previous section,
they also are referred to a lowpass filters.The idea behind smoothing filters is straightforward. By replacing the value
of every pixel in an image by the average of the intensity levels in the neighborhood defined by the filter mask, this process results in an image with reduced “sharp” transitions in intensities. Because random noise typically
consists of sharp transitions in intensity levels, the most obvious application of
smoothing is noise reduction. However, edges (which almost always are desirable features of an image) also are characterized by sharp intensity transitions,
so averaging filters have the undesirable side effect that they blur edges. Another application of this type of process includes the smoothing of false contours that result from using an insufficient number of intensity levels, as
discussed in Section 2.4.3. A major use of averaging filters is in the reduction
of “irrelevant” detail in an image. By “irrelevant” we mean pixel regions that
are small with respect to the size of the filter mask. This latter application is illustrated later in this section.Figure 3.32 shows two 3 X 3 smoothing filters» Use of the first filter yields
the standard average of the pixels under the mask. This can best be seen by
substituting the coefficients of the mask into Eq, (3.4-4):12
R= dewhich is the average of the intensity levels of the pixels in the 3 X 3 neighborhood defined by the mask, as discussed earlier. Note that, instead of being 1/9,
176 Chapter 3 @ Intensity Transformations and Spatial FilteringEXAMPLE 3.13:
Image smoothing
with masks of
various sizes.Eq. (3.5-1) is simply the sum of the mask coefficients and, therefore, it is a constant that needs to be computed only once.#¢ The effects of smoothing as a function of filter size are illustrated in Fig. 3.33,
which shows an original image and the corresponding smoothed results obtained using square averaging filters of sizes m = 3, 5,9, 15, and 35 pixels, respectively. The principal features of these resuits are as follows: For mt = 3, we
note a general slight blurring throughout the entire image but, as expected, details that are of approximately the same size as the filter mask are affected considerably more. For example, the 3 x 3 and 5 x 5 black squares in the image,
the small letter “a,” and the fine grain noise show significant blurring when compared to the rest of the image. Note that the noise is less pronounced, and the
jagged borders of the characters were pleasingly smoothed.The result for m = 5 is somewhat similar, with a slight further increase in
blurring. For 7 = 9 we see considerably more blurring, and the 20% black circle is not nearly as distinct from the background as in the previous three images, illustrating the blending effect that blurring has on objects whose
intensities are close to that of its neighboring pixels. Note the significant further smoothing of the noisy rectangles. The results for m = 15 and 35 are extreme with respect to the sizes of the objects in the image. This type of
aggresive blurring generally is used to eliminate small objects from an image.
For instance, the three small squares, two of the circles, and most of the noisy
rectangle areas have been blended into the background of the image in
Fig. 3.33(f). Note also in this figure the pronounced black border. This is a result of padding the border of the original image with 0s (black) and then
trimming off the padded area after filtering. Some of the black was blended
into all filtered images, but became truly objectionable for the imagessmoothed with the larger filters. BdAs mentioned earlier, an important application of spatial averaging is to
biur an image for the purpose of getting a gross representation of objects of
interest, such that the intensity of smaller objects blends with the background and larger objects become “bloblike” and easy to detect. The size of
the mask establishes the relative size of the objects that will be blended with
the background. As an illustration, consider Fig. 3.34{a), which is an image
trom the Hubble telescope in orbit around the Earth. Figure 3.34(b) shows
the result of applying a 15 15 averaging mask to this image. We see that a
pumber of objects have either biended with the background or their intensity has diminished considerably. It is typical to follow an operation like this
with thresholding to eliminate objects based on their intensity, The result of
using the thresholding function of Fig. 3.2(b) with a threshold value equal te
25% of the highest intensity in the blurred image is shown in Fig. 3.34{c)
Comparing this result with the original image, we see that it is a reasonable
representation of what we would consider to be the largest, brightest objects in that image.
3.5 ©: Smoothing Spatial Filters 177& a ®
eee - “ eeeTTITTTT NINN =.aaaaaaad vanaaaaa  or |) eT2 fs
ee0 seeHUTCH HUIaaaaaaaa saaaaaaa    | --0ee0S88 | were
if i|see | ¢
fiig !
| HL Poreatd j
aa bo |
ete a ad add i se eeeFIGURE 3.33 (a) Original image. of size 500 500 pixels. (b)-{{) Results of smoothing
with square averaging filter masks of sizes mm = 3, 5. 9.15, and 35, respectively. The black
squares at the top are of size! 9,15,25, 35,4 spectively: their borders
are 25 pixels apart. The leters at the bottom range in size from 10 to 24 pois, in
increments of 2 points; the large Ictier at the top is 60 points. The vertical bars are 5 pixels
wide and 100 pixels high; their separation is 20 pixels. The diameter of the cireles is
pixels, and their borders are /5 pixels apart: their intensity levels range fram 0%. to 100%
black in increments of 20%. The background of the image ts 10% black. The rorsy
rectangles are of size 50. x 320 pixels.ean
seo
178 = Cuapter 3. Intensity Transformations and Spatial Filtering FIGURE 3.34 (a) Image of size 528 < 485 pixels from the Hubble Space Telescope. (b} Image filtered with a
15 x 15 averaging mask. (c) Result of thresholding (b). (Original image courtesy of NASA.)3.5.3 Order-Statistic (Nonlinear) FiltersOrder-siatistic fillers are nonlinear spatial filters whese response is based on ordering (ranking) the pixels contained in the image area encompassed by the til
ter, and then replacing the value of the center pixe! with the value determined
by the ranking result. The best-known filter in this category is the median filter,
which. as its name implies. replaces the value of a pixel by the median of the intensity values in the neighborhood of that pixcl (the original value of the pixel is
included in the computation of the median). Median filters are quite popular because, for certain bypes of random netse. they provide excellent noise-reduction
capabilities, with considerably less blurring (haa linear smoothing Mlers of similar size. Median filters are particularly effective in the presence of fnpulse noise,
also called salt-and-pepper noise because of its appearance as white and black
dots superimposed on an image.The median. £, of a set of valics is such (hat half the values in the set are
less than or equal to &, and half are greater than or equal to é. In order to perform median filtering at a point in an image. we list sort the values of the pixel
in the neighborhood, determing their median. aud assign that value to the cor
responding pixet in the filtered image. For example, ina 3X 3 neighborhood
the median is the Sth largest value. in a 5 & S neighborhood it is the 13ih
largest valuc. and so on. Whee several values in a neighborhood are the same,
all equal vatues are grouped. Pur example. suppose that a 3 * 3 neighbourhood
has values (1f 0,20, 15.20, 20, 25, 100). These values are sorted as (10. 15.
20, 20, 20; 20, 26, 25. 100). which results in a imedian of 20. Thus, the principal
pougls with distinct intensity levels to be          function of median filters is to force
more like their neighbors. In fact. isofated clusters of pixels that are light or dark with respect lo Une neighbors. aid whose arce is Jess thai er
half the filter dread coe clininidtedt hs ais NX ee ein Ger Tn Us cisePrttensily ef the neaghbors. barge    ~ehminuted™ means fornf hat the Weal
clusters are atlvctod) cans hi
3.6 ® Sharpening Spatial Filters 179 abe
FIGURE 3.35 (a) X-ray image of circuit board corrupted by salt-and-pepper noise. (b) Noise reduction with
a3 X 3 averaging mask. (c) Noise reduction with a 3 x 3 median filter. (Original image courtesy of Mr.
Joseph E. Pascente, Lixi, Inc.)Although the median filter is by far the most useful orderstatistic filter in
image processing, it is by no means the only one. The median represents the
50th percentile of a ranked set of numbers, but recall from basic statistics that
ranking lends itself io many other possibilities. For cxample, using the 100th
percentile resuits in the so-called max filter, which is useful for finding the
brightest points in an image. The response of a 3 X 3 max filter is given by
R= max{z,|k = 1,2,...,9}. The 0th percentile filter is the ntin fitter, used
for the opposite purpose. Median, max, min. and several other nonlinear filters
are considered in more detail in Section 5.3.@ Figure 3.35(a) shows an X-ray image of a circuit board heavily corrupted
by salt-and-pepper noise. To illustrate the point about the superiority of median filtering over average filtering in situations such as this, we show in Fig.
3.35(b) the result of processing the noisy image with a3 X 3 neighborhood ayeraging mask, and in Fig. 3.35(c) the result of using a 3 x 3 median filter. The
averaging filter blurred the image and ils noise reduction performance was
poor, The superiority in all respecis of median over average Sillering in this
case is quite evident. In general. median filtering is much better suited than averaging for the removal of salt-and-pepper noise. uiE@X Sharpening Spatial FiltersThe principal objective of sharpening is to highlight transitions in intensity.
Uses of image sharpening vary and include applications ranging trom electronic printing and medical imaging to indusirial inspection and autonomous guidance in military systems. In the Jast sectiun, we saw that image blurring could be
accomplished in the spatia] domain by pixel averaging in a neighborhood. Because averaging is analogous to tegration. it is logical lo conclude that sharpening can be accomplished by spatial differentiation. This. ip fact. is the case.See Seetion 10.3.5 reparding, pewenulesEXAMPLE 3.14:
Use of median
filtering for noise
reduction.
3.6 ® Sharpening Spatial Filters 181L Intensity transition 6 qe -@ @ 4-4-4
5 Constaat a /
intensity X Ramp i
= 4 <r Step ~/
g 3 '
= 2
1
0 x  fen (eTeToleTsTa ts T2 tlt Ds [iT see [eye]-* Istderivative @ O-1-1-1-1-1 0 0 0 6 a0
2ndderivativeO O-1 0000100 0 06
5
4
3
2
2 4 a
g 0}-o-@—p-0-0-0% “OOS SF BO B—*
S-1 “Be--e--e--w” — Zerocrossing
~2 .
“3 First derivative vi
~4 O Second derivative +
-5 Afirst- and second-order derivatives of a digital function, consider the example
in Fig. 3.36.Figure 3.36(b) (center of the figure) shows a section of a scan line (intensity profile), The values inside the small squares are the intensity values in
the scan line, which are plotted as black dots above it in Fig. 3.36(a). The
dashed line connecting the dots is included to aid visualization. As the figure shows, the scan Jine contains an intensity ramp, three sections of conStant intensity, and an intensity step. The circles indicate the onset or end of
intensity transitions. The first- and second-order derivatives computed
using the two preceding definitions are included below the scan line in Fig.
3.36(b), and are plotted in Fig. 3.36(c). When computing the first derivative
at a location x, we subtract the value of the function at that location from
the next point. So this is a “look-ahead” operation. Similarly, to compute the
second derivative at x, we use the previous and the next points in the computation. To avoid a situation in which the previous or next points are outside the range of the scan line, we show derivative computations in Fig. 3.36
from the second through the penultimate points in the sequence.Let us consider the properties of the first and second derivatives as we traverse the profile from lefi to right. First, we encounter an area of constant intensity and, as Figs. 3.36(b) and (c) show, both derivatives are zero there, so condition
(1) is satisfied for both. Next, we encounter an intensity ramp followed by a step,
and we note that the first-order derivative is nonzero at the onset of the ramp anda
b
cFIGURE 3.36
INustration of the
first and second
derivatives of a
1-D digital
function
representing a
section of a
horizontal
intensity profile
from an image. In
(a) and (c) data
points are joined
by dashed lines as
a visualization aid.
180 Chapter 3 @ Intensity Transformations and Spatial FilteringWe return to Eq. (3.6-1)
in Section 10.2.1 aud
show how it fotlows froma Taylor series expansion.For now, we accept it as a
definition.and the discussion jn this section deals with various ways of defining and implementing operators for sharpening by digital differentiation. Fundamentally, the
strength of the response of a derivative operator is proportional to the degree
of intensity discontinuity of the image at the point at which the operator is applied. Thus, image differentiation enhances edges and other discontinuities
(such as noise) and deemphasizes areas with slowly varying intensities.3.6.1 FoundationIn the two sections that follow, we consider in some detail sharpening filters
that are based on first- and second-order derivatives, respectively, Before proceeding with that discussion, however, we stop to look at some of the fundamental properties of these derivatives in a digital context. To simplify the
explanation, we focus attention initially on one-dimensional derivatives, In
particular, we are interested in the behavior of these derivatives in areas of
constant intensity, at the onset and end of discontinuities (step and ramp discontinuities), and along intensity ramps. As you will see in Chapter 10, these
types of discontinuities can be used to model noise points, lines, and edges in
an image. The behavior of derivatives during transitions into and out of these
image features also is of interest.The derivatives of a digital function are defined in terms of differences.
There are various ways to define these differences. However, we require that
any definition we use for a first derivative (1) must be zero in areas of constant
intensity; (2) must be nonzero at the onset of an intensity step or ramp; and
(3) must be nonzero along ramps. Similarly, any definition of a second derivative (1) must be zero in constant areas; (2) must be nonzero at the onset and
end of an intensity step or ramp; and (3) must be zero along ramps of constant
slope. Because we are dealing with digital quantities whose values are finite,
the maximum possible intensity change also is finite, and the shortest distance
over which that change can occur is between adjacent pixels.A basic definition of the first-order derivative of a one-dimensional function f(x) is the differenceFm yx +1) - fe) (64)We used a partial derivative here in order to keep the notation the same aswhen we consider an image function of two variables, f(x, y), at which time wewill be dealing with partial derivatives along the two spatial axes. Use of a par
tial derivative in the present discussion does not affect in any way the natureof what we are trying to accomplish. Clearly, df/ax = df/dx when there isonly one variable in the function; the same is true for the second derivative.
We define the second-order derivative of f(x) as the difference2of = f(x +1) + f(x - 1) - 2f(x) (3.6-2)It is easily verified that these two definitions satisfy the conditions stated
above. To illustrate this, and to examine the similarities and differences between
3.6 © Sharpening Spatial Filters 185 Letting f (4. y) denote the bined image. unsharp masking is expressed in
equation form as follows. First we obtain the mask:Limite) > for yy = Fore) (3.68) Then we add a weighted portion of the mask back 16 the original iat
adyry FON Yd koe ting hd vd (3.6-9)iwe have where we included a weight 4.68 0). Jor gentility, WE ‘
unsharp masking. as defined above. When é to the process ts referred teasa
be
deFIGURE 3.38(a) Blurred image
of the North Pole
of the moon.(6) Laplacian
without scaling,
(c) Laplacian with
scaling, (d) Image
sharpened using
the mask in Fig,
3.37(a). fe) Resuit
of using the mask
in Fig. 3.37(b).
(Original image
courtesy of
NASA.)
182Chapter 3. mt Intensity Transformations and Spatial Filteringthe step; similarly, the second derivative is nonzero at the onset and end of both
the ramp and the step; therefore, property (2) is satisfied for both derivatives. Finally, we see that property (3) is satisfied also for both derivatives because the
first derivative is nonzero and the second is zero along the ramp. Note that the
sign of the second derivative changes at the onset and end of a step or ramp. In
fact, we see in Fig, 3.36(c) that in a step transition a line joining these two values
crosses the horizontal axis midway between the two extremes. This zero crossing
property is quite useful for locating edges, as you will see in Chapter 10,Edges in digital images often are ramp-like transitions in intensity, in which
case the first derivative of the image would result in thick edges because the derivative is nonzero along a ramp. On the other hand, the second derivative would
produce a double edge one pixel thick, separated by zeros. From this, we conclude that the second derivative enhances fine detail much better than the first
derivative, a property that is ideally suited for sharpening images. Also, as you will
learn later in this section, second derivatives are much easier to implement than
first derivates, so we focus our attention initially on second derivatives,3.6.2 Using the Second Derivative for Image
Sharpening—The LaplacianIn this section we consider the implementation of 2-D, second-order derivatives and their use for image sharpening. We return to this derivative in
Chapter 10, where we use it extensively for image segmentation. The approach
basically consists of defining a discrete formulation of the second-order derivative and then constructing a filter mask based on that formulation. We are interested in isotropic filters, whose response is independent of the direction of
the discontinuities in the image to which the filter is applied. In other words,
isotropic filters are rotation invariant, in the sense that rotating the image and
then applying the filter gives the same result as applying the filter to the image
first and then rotating the result.It can be shown (Rosenfeld and Kak [1982]) that the simplest isotropic derivative operator is the Laplacian, which, for a function (image) f(x, y) of two
variables, is defined asef rf
Vpa—stes f ae + ay? (3.6-3)Because derivatives of any order are linear operations, the Lapiacian is a linear operator. To express this equation in discrete form, we use the definition in
Eq. (3.6-2), keeping in mind that we have to carry a second variable. In the
x-direction, we have >a
OF = fort Ly) + flr = Ly) ~ 2fte.9) (36-4)
and, similarly, in the y-direction we haveaf ;
aye = f(x,y t+ 1) + flay — 1) ~ 2fte¥) (3.6-5)
3.6 m Sharpening Spatial Filters 183Therefore, it follows from the preceding three equations that the discrete
Laplacian of two variables isVix, y) = fix t+ ly) + fa -dy) t+ fytl+feny-))
—4f (x, y) (3.6-6)This equation can be implemented using the filter mask in Fig. 3.37(a), which
gives an isotropic result for rotations in increments of 90°. The mechanics of
implementation are as in Section 3.5.1 for linear smoothing filters. We simply
are using different coefficients here.The diagonal directions can be incorporated in the definition of the digital
Laplacian by adding two more terms to Eq. (3.6-6), one for each of the two diagonal directions. The form of each new term is the same as either Eq. (3.6-4) or
(3.6-5), but the coordinates are along the diagonals. Because each diagonal term
also contains a —2f(x, y) term, the total subtracted from the difference terms
now would be —8f(x, y). Figure 3.37(b) shows the filter mask used to imple~
ment this new definition. This mask yields isotropic results in increments of 45°.
You are likely to see in practice the Laplacian masks in Figs. 3.37(c) and (d).
They are obtained from definitions of the second derivatives that are the negatives of the ones we used in Eqs. (3.6-4) and (3.6-5). As such, they yield equivaJent results, but the difference in sign must be kept in mind when combining (by
addition or subtraction) a Laplacian-filtered image with another image.Because the Laplacian is a derivative operator, its use highlights intensity
discontinuities in an image and deemphasizes regions with slowly varying intensity levels. This will tend to produce images that have grayish edge lines and
other discontinuities, all superimposed on a dark, featureless background.
Background features can be “recovered” while still preserving the sharpening              ab
cdFIGURE 3.37(a) Filter mask used
to implementEq. (3.6-6).(b) Mask used to
implement an
extension of this
equation that
includes the
diagonal] terms.{c) and (d) Two
other implementations of the
Laplacian found
frequently in
practice,
184 Chapter 3 s Intensity Transformations and Spatial FilteringEXAMPLE 3.15:
Image sharpening
using the
Laplacian.effect of the Laplacian simply by adding the Laplacian image to the original.
As noted in the previous paragraph, it is important to keep in mind which definition of the Laplacian is used. If the definition used has . negative center coefficient, then we subtract. rather than add. the Laplacian image to obtain a
sharpened result. Thus, the basic way in which we use the Laplacian for image
sharpening isgiv. y) = for) + oe [PfOx ¥)] (G.6-7)
where f(x, y) and g(x, y) are the inpul and sharpened images, respectively.
The constant is c = —] if the Laplacian filters in Fig. 3.37(a) or (b) are used,and c = 1if either of the other two [ilters is used.® Figure 3.38(a) shows a slightly blurred image of the North Pole of the
moon. Figure 3.38(b) shows the result of filtering this image with the Laplacian mask in Fig. 3.37(a). Large sections of this image are black because the
Laplacian contains both positive and negative values, and all negative values
are clipped at 0 by the display.A typical way to scale a Laplacian image is to add to it its minimum value to
bring the new minimum to zero and then scale the result to the full [0, £ - 1]
intensity range, as explained in Eqs, (2.6-10) and (2.6-11). The image in
Fig. 3.38(c) was scaled in this manner. Note thal the dominant features of the
image are edges and sharp intensity discontinuities. The background, previously
black, is now gray due to scaling. This grayish appearance is typical of Laplacian
images that have been sealed properly. Figure 3.38(d) shows the result obtained
using Eq. (3.6-7) with ¢ = -1. The detail in this image is unmistakably clearer
and sharper than in the original image. Addins the original image (o the Laplacian restored the overall intensity variations in the image, with the Laplacian increasing the contrast at the locations of intensity discontinuities. The net result is
an image in which smali details were enhanced and the background tonality was
reasonably preserved. Finally, Fig. 3.38(c) shows the result of repeating the preceding procedure with the filter in Fig. 3.37(b). Here. we note a significant improvement in sharpness over Fig. 3.38(d). This is not unexpected because using
the filter in Fig. 3.37(b) provides additional differentiation (sharpening) in the
diagonal directions, Results such as those in Figs. 3.38(d} and (e) have made the
Laplacian a tool of choice for sharpening digital images. 
  3.4 $ Unsharp Masking and Highboost FilteringA process that has been used [or many vears by the printing and publishing industry to sharpen images consists of subtracting’an unsharp (smoothed) version of an image from the original image. Phis process. called unsharp masking,
consists of the following steps:1. Blur the original image.2. Subtract the blurred image from the original (the resulting difference is
called the mask.)3. Add the mask to the origina
3.6 & Sharpening Spatial Filters masking [Eq. (3.6-9) with kK = 1). This image is a sight improvement over the
original, but we can do better. Figure 3.40(e) shows the result of using Eq, (3.6-9)
with k = 4.5, the largest possible value we could use and still keep positive all the
values in the final result. The improvement in this image over the original is
significant. Ee3.6.4 Using First-Order Derivatives for (Nonlinear) Image
Sharpening—The GradientFirst derivatives in image processing are implemented using the magnitude ofthe gradient. For a function f(x, y), the gradient of f al coordinates (x, y) is de
fined as the two-dimensional column vectoraf |gy | ax |al | af
layThis vector has the important geometrical property that it points in the direc
tion of the greatest rate of change of f at location (x, ).
The magnitude (length) of vector Vf, denoted as M(x. y'), whereVf = grad(f) = f (3.6-10)M{x, y) = mag( Vf) = Var + 8s (3.6-11)
is the value at (x, y) of the rate of change in the direction of the gradient vee
tor. Note that M(x, y) is an image of the same size as the original. created when
x and y are allowed to vary over all pixel Jucations in f. {1 is common practice
to refer to this image as the gradient? image (or simply as the gredivnt when the
meaning is clear).187anu opoFIGURE 3.40(a) Original
image.(b) Resuit of
blurring with a
Gaussian filter.
(c} Unsharp
mask. {d) Result
of using unsharp
masking.(e) Result of
using highboost
filtering.We discuss the gradient
in detai) in Section10.2.5. Were, we are interested only in using the
magnitude of the gradient for inmge sharpening
186 Chapter 3 m Intensity Transformations and Spatial FilteringaneFIGURE 3.39 1-D
illustration of the
mechanics of
unsharp masking.
(a) Original
signal. (b) Blurred
signal with
original shown
dashed for reference. (c) Unsharp
mask. (d) Sharpened signal,
obtained by
adding (c) to (a).EXAMPLE 3.16:
Image sharpening
using unsharp
masking.Original signatBBlurred signal
adUnsharp maskLaSharpened signal
Un  highboost filtering. Choosing k < 1 de-emphasizes the contribution of the unsharp mask.Figure 3,39 explains how unsharp masking works. The intensity profile in
Fig. 3.39(a) can be interpreted as a horizontal scan line through a vertical edge
that transitions from a dark to a light region in an image. Figure 3.39(b) shows
the result of smoothing, superimposed on the original signal (shown dashed)
for reference. Figure 3.39(c) is the unsharp mask, obtained by subtracting the
blurred signal from the original. By comparing this result with the section of
Fig. 3.36(c) corresponding to the ramp in Fig. 3.36(a), we note that the unsharp
mask in Fig. 3.39(c) is very similar to what we would obtain using a secondorder derivative. Figure 3.39(d) is the final sharpened result, obtained by
adding the mask to the original signal. The points at which a change of slope in
the intensity occurs in the signal are now emphasized (sharpened). Observe
that negative values were added to the original. Thus, it is possible for the final
result to have negative intensities if the original image has any zero values or
if the value of k is chosen large enough to emphasize the peaks of the mask to
a level larger than the minimum value in the original. Negative values would
cause a dark halo around edges, which, if k is large enough, can produce objectionable results.Figure 3.40(a) shows a slightly blurred image of white text on a dark gray
background. Figure 3.40(b) was obtained using a Gaussian smoothing filter
(see Section 3.4.4) of size 5 x 5 with o = 3. Figure 3.40(c) is the unsharp
mask, obtained using Eq. (3.6-8). Figure 3.40(d) was obtained using unsharp
188  Chopter 3 m Intensity Transformations and Spatial Filteringa
be
deBecause the components of the gradient vector are derivatives, they are linear operators. However, the magnitude of this vector is not because of the
squaring and square root operations. On the other hand, the partial derivatives
in Eq. (3.6-10) are not rotation invariant (isotropic), but the magnitude of the
gradient vector is. In some implementations, it is more suitable computationalty to approximate the squares and square root operations by absolute values:M(x, y) © [8x + 18) (3.6-12)This expression still preserves the relative changes in intensity, but the isotropic
property is lost in general. However, as in the case of the Laplacian, the isotropic properties of the discrete gradient defined in the following paragraph are preserved only for a limited number of rotational increments that depend on the
filter masks used to approximate the derivatives. As it turns out, the most popular masks used to approximate the gradient are isotropic at multiples of 90°.
These results are independent of whether we use Eq. (3.6-11) of (3.6-12), so
nothing of significance is lost in using the latter equation if we choose to do so,As in the case of the Laplacian, we now define discrete approximations to
the preceding equations and from there formulate the appropriate filter
masks. In order to simplify the discussion that follows, we will use the notation
in Fig. 3.41(a) to denote the intensities of image points in a 3 X 3 region. ForFIGURE 3.41A3 & 3 region of
an image (the zs
are intensity
values),(b)-(c) Roberts
cross gradient
operators,
(d)-(e) Sobel
operators. Ail the
mask coefficients
sum to zero, as
expected of a
derivative
operator.
3.6 @ Sharpening Spatial Filtersexample, the center point, z5, denotes f(x, y) at an arbitrary location, (x, y); Z1
denotes f(x — 1, y ~ 1); and so on, using the notation introduced in Fig. 3.28,
As indicated in Section 3.6.1, the simplest approximations to a first-order derivative that satisfy the conditions stated in that section are g, = (zg — zs) and
8y = (% — 25). Two other definitions proposed by Roberts [1965] in the early
development of digital image processing use cross differences:8x = (& — 25) and gy = {ze ~ 26) (3.6-13)
If we use Eqs. (3.6-11) and (3.6-13), we compute the gradient image as
Mix, y) = [zo — 25) + (a5 ~ 269°)” (36-14)
If we use Eqs. (3.6-12) and (3.6-13), then
M(x, y) © [29 ~ Z| + [%s — 26] (3.6-15)where it is understood that x and y vary over the dimensions of the image in
the manner described earlier. The partial derivative terms needed in equation
(3.6-13) can be implemenied using the two linear filter masks in Figs. 3.41(b)
and (c). These masks are referred to as the Roberts cross-gradient operators.Masks of even sizes are awkward to implement because they do not have a
center of symmetry. The smallest filter masks in which we are interested are of
size 3 x 3. Approximations to g, and g, using a3 X 3 neighborhood centered
on Z; are as follows:é .
Ry = of = (27 + 22g + 2) — (2) + 222 + 23) (3.6-16)
andc)
&y = o (23 + 2% + 29) ~ (2; + 2z4 + 25) (3.6-17)These equations can be implemented using the masks in Figs. 3.41(d) and (e).
The difference between the third and first rows of the 3 X 3 image region im
plemented by the mask in Fig. 3.41(d) approximates the partial derivative in’the x-direction, and the difference between the third and first columns in the
other mask approximates the derivative in the y-direction. After computing
the partial derivatives with these masks, we obtain the magnitude of the gradient as before. For example, substituting g, and g, into Eq. (3.6-12) yieldsM(x, y) © [(z7 + 2zg + 2%) — (a1 + 2z, + 23)]
+ [(z3 + 226 + 25) — (21 + za + z7)|  (3.6-18)The masks in Figs, 3.41(d) and (e) are called the Sebel operators. The idea behind using a weight value of 2 in the center coefficient is to achieve some
smoothing by giving more importance to the center point (we discuss this in
more detail in Chapter 10). Note that the coefficients in all the masks shown in
Fig. 3.41 sum to 0, indicating that they would give a response of 0 in an area of
constant intensity, as is expected of a derivative operator.189
192 Chapter 3 # Intensity Transformations and Spatial FilteringabedFIGURE 3.43(a) Image of
whole body bone
scan.(b) Laplacian of
(a). (c) Sharpened
image obtained by
adding {a) and (b).
(d) Sobel gradient
of (a).
190 Chapter 3m Intensity Transformations and Spatial FilteringEXAMPLE 3.17:
Use of the
gradient for edge
enhancement.abFIGURE 3.42(a) Optical image
of contact lens
(note defects on
the boundary at 4
and 5 o’clock).
(b) Sobel!
gradient.
(Original image
courtesy of Pete
Sites, Perceptics
Corporation.)As mentioned earlier, the computations of g, and g, are linear operations because they involve derivatives and, therefore. can be implemented
as a sum of products using the spatial masks in Fig. 3.41. The nonlinear aspect of sharpening with the gradient is the computation of M(x. y) involving
squaring and square roots, or the use of absolute values, all of which are
nonlinear operations. These operations are performed after the linear
process that yields g, and g,.® The gradient is used frequently in industrial inspection, either to aid humans in the detection of defects or, what is more common, as a preprocessing
step in automated inspection. We will have more to say about this in Chapters
10 and 11. However, it wil] be instructive at this point to consider a simple example to show how the gradient can be used to enhance defects and climinate
slowly changing background features. In this example, enhancement is used as
a preprocessing step for automated inspection, rather than for human analysis.Figure 3.42(a) shows an optical image of a contact Jens, illuminated by a
lighting arrangement designed to highlight imperfections, such as the two edge
defects in the lens boundary seen at 4 and 5 o’clock. Figure 3.42(b) shows the
gradient obtained using Eq. (3.6-12) with the two Sobe! masks in Figs. 3.41 (d)
and (e). The edge defects also are quite visible in this image, but with the
added advantage that constant or slowly varying shades of gray have been
eliminated, thus simplifying considerably the computational task required tor
automated inspection. The gradient can be used also to highlight small specs
that may not be readily visible in a gray-scale image (specs like these can be
foreign matter, air pockets in a supporting solution, or miniscule imperfections
in the lens). The ability to enhance smail discontinuities in an otherwise flat
gray field is another important feature of the gradient. he
3.7 Combining Spatial Enhancement Methods 193efghHGURE 3.43
(Continued)fe} Sobel image
smoothed with a
5 x 5 averaging
filter, (f) Mask
image formed by
the product of (c)
and (¢).(g) Sharpened
image obtained
by the sum of (a)
and (1). (h) Final
result obtained by
applying & powerlaw transformation
to {g), Compare
{g) and (h) with
(a). (Original  Systems}
3.7 & Combining Spatial Enhancement MethodsCombining Spatial Enhancement MethodsWith a few exceptions, like combining blurring with thresholding (Fig, 3.34),
we have focused attention thus far on individual approaches. Frequently, a
given task will require application of several complementary techniques in
order to achieve an acceptable result. In this section we illustrate by means of
an example how to combine several of the approaches developed thus far in
this chapter to address a dificult image enhancement task.The image in Fig. 3.43(a) is a nuclear whole body bone scan, used to detect
diseases such as bone infection and tumors. Our objective is to enhance this
image by sharpening it and by bringing out more of the skeletal detail. The
narrow dynamic range of the intensity levels and high noise content make this
image difficult to enhance. The strategy we will follow is to utilize the Laplacian to highlight fine detail, and the gradient to enhance prominent edges. For
reasons that will be explained shortly, a smoothed version of the gradient
image will be used to mask the Laplacian image (see Fig. 2.30 regarding masking). Finally, we will attempt to increase the dynamic range of the intensity levels by using an intensity transformation.Figure 3.43(b) shows the Laplacian of the original image, obtained using the
filter in Fig. 3.37(d). This image was scaled (for display only) using the same
technique as in Fig. 3.38(c). We can obtain a sharpened image at this point simply by adding Figs. 3.43(a) and (b), according to Eq. (3.6-7). Just by looking at
the noise level in Fig. 3.43(b), we would expect a rather noisy sharpened image
if we added Figs, 3.43(a) and (b), a fact that is confirmed by the result in
Fig. 3.43(c). One way that comes immediately to mind toreduce the noise is to
use a median filter. However, median filtering is a nonlinear process capable
of removing image features. This is unacceptable in medical image processing.An alternate approach ts to use a mask formed from a smoothed version of
the gradient of the original image. The motivation behind this is straightforward
and is based on the properties of first- and second-order derivatives explained in
Section 3.6.1. The Laplacian, being a second-order derivative operator, has the
definite advantage that it is superior in enhancing fine detail. However, this
causes it to produce noisier results than the gradient. This noise is most objectionable in smooth areas, where it tends to be more visible. The gradient has a
stronger average response in areas of significant intensity transitions (ramps and
steps) than does the Laplacian. The response of the gradient to noise and fine
detail is lower than the Laplacian’s and can be Jowered further by smoothing the
gradient with an averaging filter. The idea, then, is to smooth the gradient and
multiply it by the Laplacian image. In this context, we may view the smoothed
gradient as a mask image. The product will preserve details in the strong areas
while reducing noise in the relatively flat areas. This process can be interpreted
roughly as combining the best features of the Laplacian and the gradient. The
result is added to the original to obtain a final sharpened image.Figure 3.43(d) shows the Sobel gradient of the original image, computed
using Eq. (3.6-12). Components g, and g, were obtained using the masks in
Figs. 3.41(d) and (¢). respectively. As expected, edges are much more dominant191
194 Chapter 3 m Intensity Transformations and Spatial Filteringin this image than in the Laplacian image. The smoothed gradient image in
Fig. 3.43(e) was obtained by using an averaging filter of size 5 x 5. The two
gradient images were scaled for display in the same manner as the Laplacian
image. Because the smallest possible value of a gradient image is 0, the background is black in the scaled gradient images, rather than gray as in the scaled
Laplacian. The fact that Figs. 3.43(d) and (e) are much brighter than Fig, 3.43(b)
is again evidence that the gradient of an image with significant edge content
has values that are higher in general than in a Laplacian image.The product of the Laplacian and smoothed-gradient image is shown in
Fig. 3.43(f). Note the dominance of the strong edges and the relative lack of
visible noise, which is the key objective behind masking the Laplacian with a
smoothed gradient image. Adding the product image to the original resulted in
the sharpened image shown in Fig. 3.43(g). The significant increase in sharpness of detail in this image over the original is evident in most parts of the
image, including the ribs, spinal cord, pelvis, and skull. This type of improvement would not have been possible by using the Laplacian or the gradient
alone,The sharpening procedure just discussed does not affect in an appreciable
way the dynamic range of the intensity levels in an image. Thus, the final step
in our enhancement task is to increase the dynamic range of the sharpened
image. As we discussed in some detail in Sections 3.2 and 3.3, there are a number of intensity transformation functions that can accomplish this objective.
We do know from the results in Section 3.3.2 that histogram equalization is not
likely to work well on images that have dark intensity distributions like our
images have here. Histogram specification could be a solution, but the dark
characteristics of the images with which we are dealing lend themselves much
better to a power-law transformation. Since we wish to spread the intensity
levels, the value of y in Eq. (3.2-3) has to be jess than 1. After a few trials with
this equation, we arrived at the result in Fig. 3.43(h), obtained with y = 0.5
and c = 1. Comparing this image with Fig. 3.43(g), we see that significant new
detail is visible in Fig. 3.43(h). The areas around the wrists, hands, ankles, and
feet are good examples of this. The skeletal bone structure also is much more
pronounced, including the arm and leg bones. Note also the faint definition of
the outline of the body, and of body tissue. Bringing out detail of this nature by
expanding the dynamic range of the intensity levels also enhanced noise, but
Fig. 3.43(h) represents a significant visual improvement over the original image.The approach just discussed is representative of the types of processes that
can be linked in order to achieve results that are not possible with a single technique. The way in which the results are used depends on the application. The
final user of the type of images shown in this example is likely to be a radiologist.
For a number of reasons that are beyond the scope of our discussion, physicians
are unlikely to rely on enhanced results to atrive at a diagnosis. However, enhanced images are quite useful in highlighting details that can serve as clues for
further analysis in the original image or sequence of images, Jn other areas, the
enhanced result may indeed be the final product. Examples are found in the
printing industry, in image-based product inspection, in forensics, in microscopy.
3.8 m Using Fuzzy Techniques for Intensity 195in surveillance, and in a host of other areas where the principal objective of enhancement is to obtain an image with a higher content of visual detail.Ee Using Fuzzy Techniques for Intensity
Transformations and Spatial FilteringWe conclude this chapter with an introduction to fuzzy sets and their application to intensity transformations and spatial filtering, which are the main topics of discussion in the preceding sections. As it turns out, these two
applications are among the most frequent areas in which fuzzy techniques for
image processing are applied. The references at the end of this chapter provide
an entry point to the literature on fuzzy sets and to other applications of fuzzy
techniques in image processing. As you will see in the following discussion,
fuzzy sets provide a framework for incorporating human knowledge in the solution of problems whose formulation is based on imprecise concepts.3.8.1 IntroductionAs noted in Section 2.6.4, a set is a collection of objects (elements) and set theory is the set of tools that deals with operations on and among sets. Set theory,
along with mathematical logic, is one of the axiomatic foundations of classical
mathematics. Central to set theory is the notion of set membership. We are
used to dealing with so-called “crisp” sets, whose membership only can be true
or false in the traditional sense of bi-valued Boolean Jogic, with 1 typically indicating true and 0 indicating false. For example, let Z denote the set of all
people, and suppose that we want to define a subset, A, of Z, called the “set of
young people.” In order to form this subset, we need to define a membership
function that assigns a value of 1 or 0 to every element, z,of Z. Because we are
dealing with a bi-valued logic, the membership function simply defines a
threshold at or below which a person is considered young, and above which a
person is considered not young. Figure 3.44(a) summarizes this concept using
an age threshold of 20 years and letting 4.4(z) denote the membership function just discussed.We see an immediate difficulty with this formulation: A person 20 years of
age is considered young, but a person whose age is 20 years and 1 second is not
a member of the set of young people. This is a fundamentai problem with crisp
sets that limits the use of classical set theory in many practical applications.ra Hatz)Degree of membership
S
BSa Age (z)2 i0 20 30 40 50
2Membership functions
also are called
characwristic functions.abFIGURE 3.44
Membership
functions used (o
generate (a) a
crisp set. and (b) a
fuzzy set.
196 Chapter 3 # Intensity Transformations and Spatial FilteringWe follow conventional
fuzzy set notation in
using Z, instead of the
more traditional set
notation £/, to denote the
set universe in a given
application.What we need is more flexibility in what we mean by “young,” that is, a gradual
transition from young to not young. Figure 3.44(b) shows one possibility. The
key feature of this function is that it is infinite valued, thus allowing a continuous transition between young and not young. This makes it possible to have
degrees of “youngness.” We can make statements now such as a person being
young (upper flat end of the curve), relatively young (toward the beginning of
the ramp), 50% young (in the middle of the ramp), not so young (toward the
end of the ramp), and so on (note that decreasing the slope of the curve in Fig.
3.44(b) introduces more vagueness in what we mean by “young.”) These types
of vague (fuzzy) statements are more in line with what humans use when talking imprecisely about age. Thus, we may interpret infinite-valued membership
functions as being the foundation of a fizzy logic, and the sets generated using
them may be viewed as fuzzy sets. These ideas are formalized in the following
section.3.8.2 Principles of Fuzzy Set TheoryFuzzy set theory was introduced by L. A. Zadeh in a paper more than four
decades ago (Zadeh [1965]). As the following discussion shows, fuzzy sets provide a formalism for dealing with imprecise information.DefinitionsLet Z be a set of elements (objects), with a generic element of Z denoted by z;
that is, Z = {z}. This set is called the universe of discourse. A fuzzy set' Ain Z
is characterized by a membership function, ys 4(z), that associates with each element of Z a real number in the interval [0, 1]. The value of j.4(z) at z represents the grade of membership of z in A. The nearer the value of jz,(z) is to
unity, the higher the membership grade of z in A, and conversely when the
value of yz 4(z) is closer to zero. The concept of “belongs to,” so familiar in ordinary sets, does not have the same meaning in fuzzy set theory. With ordinary
sets, we say that an element either belongs or does not belong to a set. With
fuzzy sets, we say that all zs for which 4 4(z) = 1 are full members of the set,
all zs for which x 4(z) = 0 are not members of the set, and all zs for which
#-4(z) is between 0 and 1 have partial membership in the set. Therefore, a fuzzy
set is an ordered pair consisting of values of z and a corresponding membership function that assigns a grade of membership to each z. That is,A= {z, pa(z)lzeZ} (3.8-1)When the variables are continuous, the set A in this equation can have an infinite number of elements. When the values of z are discrete, we can show the elemeuts of A explicitly. For instance, if age increments in Fig. 3.44 were limited
to integer years, then we would haveA = {G1 (2.1) (3, DD, (20.1), (21, 0.9), (22, 0.8)... (25, 0.5)(24.0.4)...., (29, 0.1} "The term furzy subset is also used in the literature. indicating that 4 is as subset of Z. However. fuzzy set
is used more frequently.
3.8 & Using Fuzzy Techniques for Intensity 197where, for example, the element (22, 0.8) denotes that age 22 has a 0.8 degree
of membership in the set. All elements with ages 20 and under are full members of the set and those with ages 30 and higher are not members of the set.
Note that a plot of this set would simply be discrete points lying on the curve
of Fig. 3.44(b), so x ,(z) completely defines A, Viewed another way, we see that
a (discrete) fuzzy set is nothing more than the set of points of a function that
maps each element of the problem domain (universe of discourse) into a number greater than 0 and less*than or equal to 1. Thus, one often sees the terms
fuzzy set and membership function used interchangeably.When j4(z) can have only two values, say 0 and 1, the membership function
reduces {0 the familiar characteristic function of an ordinary (crisp) set A.
Thus, ordinary sets are a special case of fuzzy sets. Next, we consider several
definitions involving fuzzy sets that are extensions of the corresponding definitions from ordinary sets.Empty set: A fuzzy set is empty if and only if its membership function is identically zero in Z.Equality: Two fuzzy sets A and B are equal, written A = B, if and only if
wa(Z) = pea(z) for all ze Z.Complement: The complement (NOT) of a fuzzy set A, denoted by ‘A, or
NOT(A), is defined as the set whose membership function isHaz) = 1 — pala) . (3.8-2)
for all ze Z,Subset: A fuzzy set A is a subset of a fuzzy set 8 if and only if
Hal2) S mylz) (3.8-3)for all zeZ.Union: The union (OR) of two fuzzy sets A and B, denoted A U B, or AOR B,
is a fuzzy set U with membership function
Hu(z) = max[aa(z), 4a(2)] (3.8-4)for all ze Z.Intersection: The intersection (AND) of two fuzzy sets A and B, denoted
ANB, or A AND B.is a fuzzy set J with membership functionHr(z) = min(w4(z), Hn(2)] (3.8-5)for all ze Z.Note that the familiar terms NOT, OR, and AND are used interchangeably
when working with fuzzy sets to denote complementation, union, and intersecion, respectively.The nowwtion “for all
ze Z” reads: “for all <
belonging to 2,”
Some common membership functionsTypes of membership functions used in practice include the following.3.8 @ Using Fuzzy Techniques for Intensity 199  Triangular:
1-(a-2)/b a-bs2z2<a
wQ)=41-(-ajfe aXzSate (3.8-6)
ro otherwise
Trapezoidal:
l1-—(a-2j/e a-~cSz<a
_ jl asz<b
HV (g-byd bscebtd (38-7)
0 otherwise
Sigma:
1-(a-z/b av>bszea
az) = 91 z>a (3.8-8)
0 otherwise
S-shape:
0 zZ< a
z-ay
2 asz<b
c-a@
S(z 4, b,c) = > {3.8-9)
1- 2(2=4) b<zsc
1 Z2>¢
Bell-shape: .
(2) = S(z;¢ — b,c — b/2,¢) Zee (3.8-10) ceisetince i etovred to
Me i- S(2; cot b/2,¢ + 5) z>e “re as the 1 (or a) function,
Trancated Gaussian:
i~ ar
a -ceSzf£ate
H(2) = ew» a ce
0 otherwise G81)Typically, only the independent variable, z, is included when writing 4(z) in
order to simplify equations. We made an exception in Eq. (3.8-9) in order to
use its form in Eq. (3.8-10). Figure 3.46 shows examples of the membership
198 Chapter 3 ™ Intensity Transformations and Spatial Filteringab
cdFIGURE 3.45(a) Membership
functions of two
sets, A and B.(b)
Membership
function of the
complement of A.
{c} and (d)
Membership
functions of the
union and
intersection of the
two sets,EXAMPLE 3.18:
lilustration of
fuzzy set
definitions.   
        o
‘S ry
£1 1
E  |ma(z). Malz)
2 “ws eee L- pa(z) = 1 - walz)
te
o
3
= Comptement
a 9 Zz 9 - Zz
+
1 1 a
i} \
= max[ saz), Aa(2}] H ul \
f . ,
t pe alz) = min{14(z), A9(2)]
Union Intersection
0 z 0 : 2& Figure 3.45 illustrates some of the preceding definitions. Figure 3.45(a)
shows the membership functions of two sets, A and B, and Fig. 3.45(b) shows
the membership function of the complement of A. Figure 3.45(c) shows the
membership function of the union of A and B, and Fig. 3.45(d) shows the
corresponding result for the intersection of these two sets. Note that these
figures are consistent with our familiar notion of complement, union, and
intersection of crisp sets.’ ®Although fuzzy logic and probability operate over the same [0, 1] interval,
there is a significant distinction to be made between the two. Consider the
example from Fig. 3.44. A probabilistic statement might read: “There is a
50% chance that a person is young,” while a fuzzy statement would read
“A person’s degree of membership within the set of young people is 0.5.”
The difference between these two statements is important. In the first
statement, a person is considered to be either in the set of young or the set
of not young people; we simply have only a 50% chance of knowing to
which set the person belongs. The second statement presupposes that a
person is young to some degree, with that degree being in this case 0.5.
Another interpretation is to say that this is an “average” young person:
not really young, but not too near being not young. In other words, fuzzy
logic is not probabilistic at all; it just deals with degrees of membership in
a set. In this sense, we see that fuzzy logic concepts find application in situations characterized by vagueness and imprevision, rather than by randomness. “You are tikely to encounter examples in the Hterature in which the area under the curve of the membership function of. say. the intersection of two fuzzy sets. is shaded to indicate the result of the operation. This is a carryover from ordinary set operations and is incorrect. Only the points along the
membership function itself are applicable when dealing with fuzzy sets.
200 = Chopter 3 w Intensity Transformations and Spatial FilteringPE
e.@etFIGURE 3.46
Membership
functions corresponding to Eqs.
(3.8-6)-(3.8-11).w     vl------—Bell-shape a~c a ate functions just discussed. The first three functions are piecewise linear, the next
two functions are smooth, and the last function is a truncated Gaussian function. Equation (3.8-9) describes an important S-shape function that it used frequently when working with fuzzy sets. The value of z = 6 at which $ = 0.5 in
this equation is called the crossover point. As Fig. 3.46(d) shows, this is the
point at which the curve changes inflection. It is not difficult to show (Problem
3.31) that b = (a + c)/2. In the bell-shape curve of Fig. 3.46(e), the value of b
defines the bandwidth of the curve.3.8.3 Using Fuzzy SetsIn this section, we lay the foundation for using fuzzy sets and iflustrate the re~
sulting concepts with examples from simple, familiar situations. We then apply
the results to image processing in Sections 3.8.4 and 3.8.5. Approaching the
presentation in this way makes the material much easier to understand, especially for readers new to this area. .Suppose that we are interested in using color to categorize a given type of
fruit into three groups: verdant, half-mature, and mature, Assume that observations of fruit at various stages of maturity have led to the conclusion that
verdant fruit is green, half-mature fruit is yellow, and mature fruit is red. The
labels green, yellow, and red are vague descriptions of color sensation. As a
starting point, these labels have to be expressed in a fuzzy format. That is, they
have to be fuzzified. This is achieved by defining membership as a function of
3.8 @ Using Fuzzy Techniques for Intensity 201  
   
  lz)
e Pirin 2
oO
£2
&
o
E
pa
°
»
i
»
a
Color (wavelength)
u(2)
a4 Hyetiow(2)
F; 10 7 Hyveen (2) AY 7 Pred(2)
&
E
3 0.5
vy
3 X
z /\ 7
a 9 z zy
Color (wavelength)color (wavelength of light), as Fig. 3.47(a) shows. In this context, color is a
linguistic variable, and a particular color (e.g., red at a fixed wavelength) is a
linguistic value. A linguistic value, zo, is fuzzified by using a membership functions to map it to the interval [0, 1}, as Fig. 3.47(b) shows.The problem-specific knowledge just explained can be formalized in the
form of the following fuzzy IF-THEN rules:R;: IF the color is green, THEN the fruit is verdant.
ORR;: IF the color is yellow, THEN the fruit is Aalf-mature.
ORR,. IF the color is red, THEN the fruit is mature.These rules represent the sum total of our knowledge about this problem; they
are really nothing more than a formalism for a thought process.The next step of the procedure js to find a way to use inputs (color) and the
knowledge base represented by the IF-THEN rules to create the output of the
fuzzy system. This process is known as implication or inference. However, before implication can be applied, the antecedent of each rule has to be
processed to yield a single value, As we show at the end of this section, multiple parts of an antecedent are linked by ANDs and ORs. Based on the definitions from Section 3.8.2, this means performing min and max operations. To
simplify the explanation, we deal initially with rules whose antecedents contain only one part.Because we are dealing with fuzzy inputs, the outputs themselves are fuzzy.
so membership functions have to be defined for the outputs as well. Figure 3.48a
bFIGURE 3.47(a) Membership
functions used to
fuzzify color.(b) Fuzzifying a
specific color zp.
(Curves describing
color sensation are
bell shaped; see
Section 6.1 for an
example. However, using triangular shapes as an
approximation is
common practice
when working
with fuzzy sets.)The part of an [F-THEN
rule to the left of THEN
olten is referred to as the
antecedent (or premise).
The part to the right is
called the consequent (ar
conclusion.)
202 Chapter 3 m Intensity Transformations and Spatial FilteringFIGURE 3.48
Membership
functions
characterizing the
outputs verdant,
half-mature, and
mature.ab
G4FIGURE 3.49(a) Shape of the
membership function
associated with the
color red, and(b) corresponding
output membership
function. These two
functions are
associated by rule 23.
(c) Combined
representation of the
two functions. The
representation is 2-D
because the
independent
variables in (a) and
(b) are different.(d) The AND of (a)
and (b), as defined in
Eq. (3.8-5).           
  #
&
B10} ye Brera) + Bray?)
Z
2 ~ Mmat (0)
S os
&
oa
vv
a>Ww 20 30 40 S50 60 70 86 90 160
Maturity (%)shows the membership functions of the fuzzy outputs we are going to use in this
example. Note that the independent variable of the outputs is maturity, which is
different from the independent variable of the inputs.Figures 3.47 and 3.48, together with the rule base, contain all the information required to relate inputs and outputs. For example, we note that the expression red AND mature is nothing more than the intersection (AND)
operation defined earlier. In the present case, the independent variables of the
membership functions of inputs and outputs are different, so the result will be
two-dimensional. For instance, Figs, 3.49(a) and (b) show the membership
functions of red and mature, and Fig. 3.49(c) shows how they relate in two dimensions. To find the result of the AND operation between these two functions, recall from Eq. (3.8-5) that AND is defined as the minimum of the twomembership functions; that is,B3(Z, v) = min {itred (z), Monat (v)} (3.8-12)Hv)H(z)  % Umat(e) 
 
  te
MaturityM
3.8 # Using Fuzzy Techniques for Intensity 203where 3 in the subscript denotes that this is the result of rule R; in the knowledge base. Figure 3.49(d) shows the result of the AND operation.’Equation (3.8-12) is a general result involving two membership functions.
In practice, we are interested in the output resulting from a specific input. Let
Zo denote a specific value of red. The degree of membership of the red color
component in response to this input is simply a scalar value, 12,.¢(2p). We find
the output corresponding to rule Ry and this specific input by performing the
AND operation between p,,(Z)) and the general result, y3(z, v), evaluated
also at Zo. As noted before, the AND operation is implemented using the minimum operation:Q3(v) = min{ Hyea{Zo), U3 (Zo, V}} (3.8-13)where Q;(v) denotes the fuzzy output due to rule R3 and a specific input. The
only variable in Q, is the output variable, v, as expected.To interpret Eq. (3.8-13) graphically, consider Fig. 3.49(d) again, which
shows the general function y23(z, v). Performing the minimum operation of a
positive constant, c, and this function would clip ail values of 3(z, v) above
that constant, as Fig. 3.50(a) shows. However, we are interested only in one
value (29) along the color axis, so the relevant result is a cross section of the
truncated function along the maturity axis, with the cross section placed at 2 ,
as Fig. 3.50(b) shows [because Fig. 3.50(a) corresponds to rule R3, it follows
that c = p,.4{ze)]. Equation (3.8-13) is the expression for this cross section.Using the same line of reasoning, we obtain the fuzzy responses due to the
other two rules and the specific input zo, as follows:Ov) = min {PL yetigwlZa), M2(Zo. Y)} (3.8-14)  1 Note that Eq, (3.8-12) is formed from ordered pairs of values {p,e(Z)s HoearY)}, and recall that a set of
ordered pairs is commonly called a Cartesian product, denoted by X x V, where X is a set of values
{itredlZ1)s HrealZ2)s- ++ Hrev(Za)} generated from u,,¢(2) by varying z.and V is a similar set of » values
generated from pyagY) bY varying uv. Thus, XX V = ((peyeal25)s Mmed(Uid>-+- + (thread Za)s ane Un))}s
and we see from Fig, 3.49(d) that the AND operation involving two variables can be expresses as a
mapping from X¥ x V to the range {0.1], denoted as ¥ x V — {0,1}. Although we do nat use this notation in the present discussion, we mention it here because you are likely lo encounter it in the literature on fuzzy sets.abFIGURE 3.50(a) Result of
computing the
minimum of an
arbitrary
constant, c, and
function js3(z, v)
from Eq. (3,8-12).
The minimum is
equivalent to an
AND operation.
(b) Cross section
{dark line) ata
specific color, zy.agape
3.8 w Using Fuzzy Techniques for Intensity 205   u(2)
& Hyatton (2)
£
gM 1 Begreen (2)
3
Ee
z
s 0.5
; /\
oO
ao
Aa Oo z Zo 1
Color (wavelength) Degree of membership10 20 30 40 50 6 70 80 % 10
Maturity (%)u(r)i)Degree of membership
o
La10 20 30 40 50 60 70 8 90 10
Maturity (%)Up te this point, we have considered IF-THEN rules whose antecedents
have only one part, such as “IF the color is red.” Rules containing more than
one part must be combined to yield a single number that represents the entire antecedent for that rule, For example, suppose that we have the rule: IF
the color is red OR the consistency is soft, THEN the fruit is mature. A
membership function would have to be defined for the linguistic variable
soft. Then, to obtain a single number for this rule that takes into account
both parts of the antecedent, we first evaluate a given input color value of
red using the red membership function and a given value of consistency using
the soft membership function. Because the two parts are linked by OR, we
use the maximum of the two resulting values.! This value is then used in the
implication process to “clip” the mature output membership function, which
is the function associated with this rule. The rest of the procedure is as before, as the following summary illustrates.  ‘Antecedents whose parts are connected by ANDs are similarly evaluated using the min operation.a
b
¢FIGURE 3.51(a) Membership
functions with a
specific color, 2).
selected.(b) Individual fuzzy
sets obtained from
Eqs. (3.8-13)(3.8-15). (c) Final
fuzzy set obtained
by using Eq. (3.816) or (3.8-17).cores
204 = Ghopter 3 @ Intensity Transformations and Spatial Filteringand
Or(v) = min{Mereen(Za)s Hi(Zy V)} (3.8-15)Each of these equations is the output associated with a particular rule and a
specific input. That is, they represent the result of the implication process mentioned a few paragraphs back. Keep in mind that each of these three responses
is a fuzzy set, even though the input is a scalar value.To obtain the overall response, we aggregate the individual responses. In the
tule base given at the beginning of this section the three rules are associated
by the OR operation. Thus, the complete (aggregated) fuzzy output is given byQ = Q, OR Q, ORQ; (3.8-16)and we see that the overall response is the union of three individual fuzzy sets.
Because OR is defined as a max operation, we can write this result asQ(v) = max{min{ys(20), He(Zo ¥)}} (3.8-17)forr = {1,2,3} and s = {green, yellow, red}. Although it was developed in
the context of an example, this expression is perfectly general; to extend it ton
tules, we simply let r = {1, 2,...,/2}; similarly, we can expand s to include any
finite number of membership functions. Equations (3.8-16) and (3.8-17) say
the same thing: The response, Q, of our fuzzy system is the union of the individual fuzzy sets resulting from each rule by the implication process.Figure 3.51 summarizes graphically the discussion up to this point. Figure
3.51{a) shows the three input membership functions evaluated at zy, and Fig.
3.51{b) shows the outputs in response to input zp. These fuzzy sets are the
clipped cross sections discussed in connection with Fig. 3.50(b). Note that, numerically, Q, consists of all Os because fyreen(2y) = 0; that is, Q, is empty, as defined in Section 3.8.2, Figure 3.51(c) shows the final result, Q, itself a fuzzy set
formed from the union of Q), Q>, and Q3.We have successfully obtained the complete output corresponding to a specific input, but we are still dealing with a fuzzy set. The last step is to obtain a
crisp Output, vp, from fuzzy set Q using a process appropriately called
defuzzification. There are a number of ways to defuzzify Q to obtain a crisp
output. One of the approaches used most frequently is to compute the center
of gravity of this set (the references cited at the end of this chapter discuss
others). Thus, if Q(v) from Eq. (3.8-17) can have K possibie values,
Q(1), O(2),...QUK), its center of gravity is given bySK vQ(v): (3.8-18
D2) ~Up =Evaluating this equation with the (discrete)! values of Q in Fig. 3.51(c) yields
uv = 72.3, indicating that the given color z, implies a fruit maturity of approximately 72%. ' Fuzzy set Q in Fig. 3.51 (c) is shown as a solid curve for clarity, but keep in mind that we are dealing with
digital quantities in this book. so Q is a digital function
206 Chapter 3m Intensity Transformations and Spatial FilteringFigure 3.52 shows the fruit example using two inputs: color and consistency.
We can use this figure and the preceding material to summarize the principal
steps followed in the application of rule-based fuzzy logic:1, Fuzzify the inputs: For each scalar input, find the corresponding fuzzy values by mapping that input to the interval [0, 1], using the applicable membership functions in each rule, as the first two columns of Fig. 3.52 show.2. Perform any required fuzzy logical operations: The outputs of all parts of
an antecedent must be combined to yield a single value using the max or
min operation, depending on whether the parts are connected by ORs or
by ANDs. In Fig. 3.52, all the parts of the antecedents are connected by-- 1, Fuzzify inputs. - - 2. Apply fuzzy logical 3. Apply implication
operation(s) (OR = max). method (min).green hard verdantJE color is green consistency is hard THEN fruit is verdagu
1 :half mat IF color is yellow OR consistency is meditun THEN  fruitis Aalf-mature {
d ti
4
t
+     
    red matere
4. Apply
IF color is red OR consistency is soft THEN fruit is mature aggregation
| | - method (max).
Input 1 Input 2 §. Defuzzify
Color (zy) Consistency (cy) (centes of
gravity). Output oo eine
Maturity (vy)FIGURE 3.52 Example illustrating the five basic steps used typically to implement a fuzzy, rule-based system:
(1) fuzzification, (2) logical operations {only OR was used in this example), (3) implication,
(4) aggregation, and (5) defuzzification.
3.8 m Using Fuzzy Techniques for Intensity 207ORs, so the max operation is used throughout. The number of parts of an
antecedent and the type of logic operator used to connect them can be different from rule to rule.3. Apply an implication method: The single output of the antecedent of each
rule is used to provide the output corresponding to that rule. We use
AND for implication, which is defined as the min operation. This clips the
corresponding output membership function at the value provided by the
antecedent, as the third and fourth columns in Fig. 3.52 show.4, Apply an aggregation method to the fuzzy sets from step 3: As the last col
umn in Fig. 3.52 shows, the output of each rule is a fuzzy set. These must becombined to yield a single output fuzzy set. The approach used here is toOR the individual outputs, so the max operation is employed.Defuzzify the final output fuzzy set: In this final step, we obtain a crisp,scalar output. This is achieved by computing the center of gravity of theaggregated fuzzy set from step 4.5.When the number of variables is large, it is common practice to use the shorthand notation (variable, fuzzy set) to pair a variable with its corresponding
membership function. For example, the rule IF the color is green THEN the
fruit is verdant would be written as IF (z, green) THEN (v, verdant) where, as
before, variables z and v represent color and degree of maturity, respectively,
while green and verdant are the two fuzzy sets defined by the membership
functions pyreen(Z) ANd Hyerq(v), Tespectively.In general, when dealing with M IF-THEN rules, N input variables,
21, 22,+--Zy, and one output variable, v, the type of fuzzy rule formulation
used most frequently in image processing has the form ~IF (z;, Ay;) AND (2), Aja) AND... AND (zy, Aiy) THEN (v, By)
IF (z;, A21} AND (2, An) AND... AND (2y, Aw) THEN (v, Bz)
bees (3.8-19)IF (z;, Aaa) AND (22, Ayy2) AND... AND (zy, Ay) THEN (0, By)
ELSE (2, Bg)where A,; is the fuzzy set associated with the ith rule and the jth input variable, B;
is the fuzzy set associated with the output of the ith rule,and we have assumed that
the components of the rule antecedents are linked by ANDs. Note that we have
introduced an ELSE rule, with associated fuzzy set By. This rule is executed when
none of the preceding rules is completely satisfied; its output is explained below.
As indicated earlier, all the elements of the antecedent of each rule must be 1. sor or and
evaluated to yield a single scalar value. In Fig. 3.52, we used the max Operation in the rule set depends
because the rules were based on fuzzy ORs. The formulation in Eq. (3.8-19) See nichesuses ANDs, so we have to use the min operator, Evaluating the antecedents of — depends on the problem
at hand. We used ORs inthe ith rule in Eq. (3.8-19) produces a scalar output, A,, given by Fie 330 end ANDs
. ; Eq, (3.8-19) 10 give you
A; = min{i,, (2): f= 1,2,..., M} (3.8-20) — fannliarity with both
, formulations
208  Chepter3 @ intensity Transformations and Spatial Filteringfori = 1,2,...,M, where y4,(z;) is the membership function of fuzzy set Aj
evaluated at the value of the jth input. Often, A; is called the strength level (or
firing level) of the ith rule. With reference to the preceding discussion, A, is simply the value used to clip the output function of the ith rule.The ELSE rule is executed when the conditions of the THEN rules are
weakly satisfied (we give a detailed example of how ELSE rules are used in
Section 3,8.5). Its response should be strong when ail the others are weak. In a
sense, one can view an ELSE rule as performing a NOT operation on the
results of the other rules. We know from Section 3.8.2 that pnotia) =
w3a(z) = 1 — yeg(z). Then, using this idea in combining (ANDing) all the levels of the THEN rules gives the following strength level for the ELSE rule:Ag =min{1- A; F= 1,2,....M} (3.8-21)We see that if all the THEN rules fire at “full strength” (all their responses
are 1), then the response of the ELSE rule is 0, as expected. As the responses
of the THEN rules weaken, the strength of the ELSE rule increases. This is
the fuzzy counterpart of the familiar IF-THEN-ELSE rules used in software programming.When dealing with ORs in the antecedents, we simply replace the ANDs in
Eq. (3.8-19) by ORs and the min in Eq. (3.8-20} by a max; Eq. (3.8-21) does not
change. AJthough one could formulate more complex antecedents and consequents than the ones discussed here, the formulations we have developed using
only ANDs or ORs are quite general and are used in a broad spectrum of
image processing applications. The references at the end of this chapter contain
additional (but Jess used) definitions of fuzzy logical operators, and
discuss other methods for implication {including multiple outputs) and defuzzification. The introduction presented in this section is fundamental and serves as
a solid base for more advanced reading on this topic. In the next two sections,
we show how to apply fuzzy concepts to image processing.3.8.4 Using Fuzzy Sets for Intensity TransformationsConsider the general problem of contrast enhancement, one of the principal
applications of intensity transformations. We can state the process of enhancing the contrast of a gray-scale image using the following rules:IF a pixel is dark, THEN make it darker.
IF a pixel is gray, THEN make it gray.
IF a pixel is bright, THEN make it brighter.Keeping in mind that these are fuzzy terms, we can express the concepts of
dark, gray, and bright by the membership functions in Fig. 3.53(a).In terms of the output, we can consider darker as being degrees of a dark intensity value (100% black being the limiting shade of dark), brighter, as being
degrees of a bright shade (100% white being the limiting value), and gray as
being degrees of an intensity in the middle of the gray scale. What we mean by
210 = Ghepter 3 @ Intensity Transformations and Spatial Filtering eReFIGURE 3.54 (a) Low-contrast image. {b) Result of histogram equalization. {c) Resull of using
fuzzy, rule-based contrast enhancement,         FIGURE 3.55 (2) and (b) Histograms of Figs. 3.54(a) und (b). ic) Inputonembershir
functions superimposed on (a). {cl) Histogram of Fig. 3.5400)
38 «© Using Fuzzy Techniques for Intensity 209a bFIGURE 3.53(a) Input and
(b) output
membership
functions for
fuzzy, rule-based
contrast
enhancement.  Mbvighter() 
  Uy) Ve % vy“degrees” here is the amount of one specific intensity. For example, 80% black
is a very dark gray. When interpreted as constant intensities whose strength is
modified, the output membership functions are singletons (membership functions that are constant), as Fig. 3.53(b) shows. The various degrees of an intensity
in the range {0, 1] occur when the singletons are clipped by the strength of the response from their corresponding rules, as in the fourth column of Fig. 3.52 (but
keep in mind that we are working here with only one input, not two, as in the figure), Because we are dealing with constants in the output membership functions, it follows from Eq. (3.8-18) that the output, vo, to any input, Zo, is given by_ Petark (Zo} X Vg + pray (Zn) X Vp + pibrign (Zo) X Vpm= 3.8-22
0 Haart (20) + BgrayZa) * Morigns(2o) (3-622) The summations in the numerator and denominator in this expressions are
simpler than in Eq. (3.8-18) because the output membership functions are constants modified (clipped) by the fuzzified values.Fuzzy image processing is computationally intensive because the entire
process of fuzzification, processing the antecedents of alt rules, implication, aggregation, and defuzzification must be applied to every pixel in the input
image. Thus, using singletons as in Eq. (3.8-22) significantly reduces computational requirements by simplifying implication, aggregation, and defuzzification. These savings can be significant in applications where processing speed is
an important requirement.@ Figure 3.54(a) shows an image whose intensities span a narrow range of the EXAMPLE 3.19:
gray scale [see the image histogram in Fig, 3.55(a)], thus giving the image an _ Illustration of
appearance of low contrast. As a basis for comparison, Fig. 3.54(b) is the result. 148°: . : : enhancement
of histogram equalization. As the histogram of this result shows [Fig. 3.55(b)], using fuzzy, ruleexpanding the entire gray scale does increase contrast, but introduces intensi- based contrast
ties in the high and Jow end that give the image an “overexposed” appearance. modification.
For example, the details in Professor Einstein’s forehead and hair are mostly
lost. Figure 3.54(c) shows the result of using the rule-based contrast modification approach discussed in the preceding paragraphs, Figure 3.55(c) shows the
input membership functions used, superimposed on the histogram of the original image. The output singletons were selecied at vg = 0 (black), vy, = 127
(mid gray), and v, = 255 (white}.
3.8 & Using Fuzzy Techniques for Intensity 213                 IF IF FIGURE 3.58
—T 7 ee ee | Fuzzy rules for
ZE boundary
Z| _ Le ; detection.
THEN THEN
zs ZE eal vi | as ZE [rae vw |
oe LU
| ZE
ee ee ae L
Rule | Rule 2
Fr , a
ze |
THEN | i ; THEN
ZE | 2s r—| WH “E | bs b—| wH
ZE
Rule 3 Rule 4oegray scale, For example, Fig. 3.59(c} was obtained by performing the mtensity
scaling defined in Eqs. (2.6-10) and (2.6-11), with K = ZL — 1, The net result is
that intensity values in Fig, 3.59(c) span the full gray scale fromOto({ZL -— 1). &     FIGURE 3.59 (a) CT scan of a human head. (b) Result of fuzzy spatial filtering using the membership
functions in Fig. 3.87 and the rules in Fig. 2.58. (¢) Result alter intensity scaling. The ibin black picture
borders in (b) and (c) were added for clarity; they are not part of the data. (Original image courtesy of
Dr. David R. Pickens, Vanderbilt University.)
212 Chapter 3 m Intensity Transformations and Spatial FilteringEXAMPLE 3,20:
Illustration of
boundary
enhancement
using fuzzy, rulebased spatial
filtering.abFIGURE 3.57(a) Membership
function of the
fuzzy set zero.
{b) Membership
functions of the
fuzzy sets black
and white.    Pixel neighborhood Intensity differencesa b.FIGURE 3.56 (a) A 3 x 3 pixel neighborhood, and (b) corresponding intensity differences
between the center pixels and its neighbors. Only 4, dy, dg, and dg were used in the
present application to simplify the discussion.where zero is a fuzzy set also. The consequent of each rule defines the values to
which the intensity of the center pixel (zs) is mapped. That is, the statement
“THEN 2; is white” means that the intensity of the pixel located at the center
of the mask is mapped to white. These rules simply state that the center pixel is
considered to be part of a uniform region if the intensity differences just mentioned are zero (in a fuzzy sense); otherwise it is considered a boundary pixel.Figure 3.57 shows possible membership functions for the fuzzy sets zero, black,
and white, respectively, where we used ZE, BL, and WH to simplify notation. Note
that the range of the independent variable of the fuzzy set ZE for an image with L
possible intensity levels is [~L + 1,1 ~ 1] because intensity differences can
range between —(Z — 1) and (Z — 1). On the other hand, the range of the output
intensities is [0, L — 1], as in the original image. Figure 3.58 shows graphically the
rules stated above, where the box labeled zs indicates that the intensity of the center pixel is mapped to the output value WH or BL.@ Figure 3.59(a) shows a 512 x 512 CT scan of a human head, and Fig. 3.59(b) is
the result of using the fuzzy spatial filtering approach just discussed. Note the effectiveness of the method in extracting the boundaries between regions, including
the contour of the brain (inner gray region). The constant regions in the image appear as gray because when the intensity differences discussed earlier are near
zero, the THEN rules have a strong response. These responses in turn clip function
WH. The output (the center of gravity of the clipped triangular regions) is a constant between (L ~ 1)/2 and (Z ~ 1), thus producing the grayish tone seen in the
image. The contrast of this image can be improved significantly by expanding the5
1| \ - ~
0
“Ltt 0 L-1 0 L-1Intensity differences Intensity
3.8 ® Using Fuzzy Techniques for Intensity 211Comparing Figs. 3.54(b) and 3.54(c), we see in the latter a considerable improvement in tonality. Note, for example, the level of detail in the forehead
and hair, as compared to the same regions in Fig. 3.54(b). The reason for the
improvement can be explained easily by studying the histogram of Fig. 3.54(c),
shown in Fig. 3.55(d). Unlike the histogram of the equalized image, this histogram has kept the same basic characteristics of the histogram of the original
image. However, it is quite evident that the dark levels (talk peaks in the low
end of the histogram) were moved left, thus darkening the levels. The opposite
was true for bright levels, The mid grays were spread slightly, but much less
than in histogram equalization.The price of this improvement in performance is considerably more processing complexity. A practical approach to follow when processing speed and
image throughput are important considerations is to use fuzzy techniques to
determine what the histograms of well-balanced images should look like.
Then, faster techniques, such as histogram specification, can be used to achieve
similar results by mapping the histograms of the input images to one or more
of the “ideal” histograms determined using a fuzzy approach. n3.8.5 Using Fuzzy Sets for Spatial FilteringWhen applying fuzzy sets to spatial filtering, the basic approach is to define
neighborhood properties that “capture” the essence of what the filters are supposed to detect, For example, consider the problem of detecting boundaries
between regions in an image. This is important in numerous applications of
image processing, such as sharpening, as discussed earlier in this section, and in
image segmentation, as discussed in Chapter 10.We can develop a boundary extraction algorithm based on a simple fuzzy
concept: /f @ pixel belongs to a uniform region, then make it white; else make it
black, where, black and white are fuzzy sets. To express the concept of a “uniform region” in fuzzy terms, we can consider the intensity differences between
the pixel at the center of a neighborhood and its neighbors. For the 3 x 3
neighborhood in Fig. 3.56(a), the differences between the center pixel (labeled
Zs) and each of the neighbors forms the subimage of size 3 X 3 in Fig. 3.56(b),
where d; denotes the intensity difference between the ith neighbor and the
center point (ie.,d; = z; ~ 25, where the zs are intensity values), A simple set
of four IF-THEN rules and one ELSE rule implements the essence of the
fuzzy concept mentioned at the beginning of this paragraph:IF dz is zero AND dg is zero THEN 2s is white ewig only theIF dg is zero AND dg is zero THEN 2s is white between theIF dg is zero AND d, is zero THEN 2; ts white caste. ™IF dg is zero AND dy is zero THEN 2; is white Lage hacipiton
ELSE 2; is black ceenton ate opproach shown here.
214  Guepter 3m intensity Transformations and Spatial FilteringSummaryThe material you have just learned is representative of current techniques used for intensity transformations and spatial filtering. The topics included in this chapter were selected for their value as fundamental material that would serve as a foundation in an
evolving field. Although most of the examples used in this chapter were related to
image enhancement, the techniques presented are perfectly general, and you will encounter them again throughout the remaining chapters in contexts totally unrelated to
enhancement. In the following chapter, we look again at fillering, but using concepts
from the frequency domain. As you will see, there is a one-to-one correspondence between the linear spatial filters studied here and frequency domain filters.References and Further ReadingThe material in Section 3.1 is from Gonzalez [1986]. Additional reading for the material in Section 3.2 may be found in Schowengerdt [1983], Poyton [1996], and Russ
[1999].-See also the paper by Tsujii et al. [1998] regarding the optimization of image
displays. Early references on histogram processing are Hummel (1974], Gonzalez and
Fittes [1977], and Woods and Gonzalez [1981]. Stark [2000] gives some interesting generalizations of histogram equalization for adaptive contrast enhancement. Other approaches for comrast enhancement are exemplified by Centeno and Haertel [1997]
and Cheng and Xu [2000]. For further reading on exact histogram specification see
Coltuc, Bolon, and Chassery [2006]. For extensions of the local histogram equalization
method, see Caselles et al. [1999], and Zhu et al. [1999]. See Narendra and Fitch [1981]
on the use and implementation of local statistics for image processing. Kim et al.
[1997] present an interesting approach combining the gradient with local statistics for
image enhancement.For additional reading on linear spatial fitters and their implementation, see Umbaugh [2005]. Jain [1989], and Rosenfeld and Kak (1982]. Rank-order filters are discussed in these references as well. Wilburn [1998] discusses generalizations of
rank-order filters. The book by Pitas and Venetsanopoulos [1990} also deals with
median and other nonlinear spatial filters. A special issue of the /EEE Transactions
in Image Processing [1996] is dedicated to the topic of nonlinear image processing,
The material on high boost filtering is from Schowengerdt [1983]. We will encounter
again many of the spatial filters introduced in this chapter in discussions dealing
with image restoration (Chapter 5) and edge detection (Chapter 10).Fundamental references for Section 3.8 are three papers on fuzzy logic by
L. A. Zadeh (Zadeh (1965, 1973, 1976]). These papers are well written and worth
reading in detail, as they established the foundation for fuzzy logic and some of its
applications. An overview of a broad range of applications of fuzzy logic in image
processing can be found in the book by Kerre and Nachtegael [2000]. The example
in Section 3.8.4 is based on a similar application described by Tizhoosh [2000]. The
example in Section 3.8.5 is basically from Russo and Ramponi [1994]. For additional
examples of applications of fuzzy sets to intensity transformations and image filtesing, see Patrascu [2004] and Nie and Barner [2006], respectively. The preceding
range of references from 1965 through 2006 is a good starting point for more detailed study of the many ways in which fuzzy sets can be used in image processing.
Software implementation of most of the methods discussed in this chapter can be
found in Gonzalez, Woods, and Eddins [2004].
Problems
*3.1 Give a single intensity transformation function for spreading the intensities of
an image so the lowest intensity is C and the highest is L — 1.3.2. Exponentials of the form ee” with aa positive constant, are useful for constructing smooth intensity transformation functions. Start with this basic function and construct transformation functions having the general shapes shown in
the following figures. The constants shown are input parameters, and your proposed iransformationssmust include them in their specification. (For simplicity
in your answers, Lo is not a required parameter in the third curve.)s= T(r) $= T(r) $= T(r)AAf3by
(ay (b) (©3.3 %(a) Give a continuous function for implementing the contrast stretching transformation shown in Fig. 3.2(a). In addition to m, your function must include
a parameter, £, for controlling the slope of the function as it transitions
from low to high intensity values. Your function should be normalized so
that its minimum and maximum values are 0 and 1, respectively.(b) Sketch a family of transformations as a function of parameter E, for a fixed
value m = L/4, where L is the numsber of intensity levels in the image.{c) What is the smallesi value of E that will make your function effectively perform as the function in Fig. 3.2(b)? In other words, your function does not
have to be identical to Fig. 3.2(b). It just has to yield the same result of producing a binary image. Assume that you are working with 8-bit images, and
let m = 128. Let C denote the smallest positive number representable in
the computer you are using.3.4 Propose a set of intensity-slicing transformations capable of producing all the
individual bit planes of an 4-bit monochrome image. (For example, a transformation function with the property T(r) = 0 for r in the range [0, 7], and
T(r) = 15 for r in the range [8, 15] produces an image of the 4th bit plane in an
8-bit mage.)3.5 *(a) What effect would setting to zero the half of lower-order bit planes have onthe histogram of an image in general?(b} What would be the effect on the histogram if we set to zero the half of higher-order bit planes instead?‘3.6 Explain why the discrete histogram equalization technique does not. in general,
yield a flat histogram. m Problems 215Detailed solutions to the
problems marked with a
star can be found in the
bouk Web site. The sile
also contains suggested
projects based on the material in this chapter.
216 Goepter 3 @ Intensity Transformations and Spatial Filtering3.7*393.103.11#3123.133.14Suppose that a digital image is subjected to histogram equalization. Show that a
second pass of histogram equalization (on the histogram-equalized image) will
produce exactly the same result as the first pass.In some applications it is useful to model the histogram of input images as
Gaussian probability density functions of the formi tem?
P(r) = Vin e
Tewhere m and o are the mean and standard deviation of the Gaussian PDF. The
approach is to let m and o be measures of average intensity and contrast of a
given image. What is the transformation function you would use for histogram
equalization?Assuming continuous values, show by example that it is possible to have a case
in which the transformation function given in Eq. (3.3-4) satisfies conditions (a)
and (b) in Section 3.3.1, but its inverse may fail to be single valued.(a) Show that the discrete transformation function given in Eq, (3.3-8) for his
togram equalization satisfies conditions (a) and (b) in Section 3.3.1.*(b) Show that the inverse discrete transformation in Eq. (3.3-9) satisfies conditions (a’) and (b) in Section 3.3.1 only if none of the intensity levels
ry, k = 0,1,...,L — 1, are missing.An image with intensities in the range [0, 1] has the PDF p,(r) shown in the following diagram. It is desired to transform the intensily levels of this image so
that they will have the specified p,(z) shown. Assume continuous quantities and
find the transformation (in terms of r and z) that wili accomplish this.  pdr) pdz)2 2lL r z
1 O05 1Propose a method for updating the local histogram for use in the local enhancement technique discussed in Section 3.3.3.
Two images, f(x, y) and g(x, y), have histograms h, and f,. Give the conditions under which you can determine the histograms of
w(a) f(xy) + ee yd
(b) f(x, »} ~ a(x, ¥}
(©) f(t, y) X aly} .
(d) f(x, y) + g(x, y)
fe) f(x y)* ay)
in terms of hy and A,. Explain how to obtain the histogram in each case.The images shown on the next page are quite different, but their histograms are
the same, Suppose that each image is blurred with a 3 X 3 averaging mask.(a) Would the histograms of the blurred images still be equal? Explain.
{b) If your answer is no, sketch the two histograms.
3.15 The implementation of Jinear spatial fillers requires moving the center of a
mask throughout an image and, at each location, computing the sum of products
of the mask coefficients with the corresponding pixels at that location (see
Section 3.4), A lowpass filter can be implemented by setting all cuefficients to 1,
allowing use of a so-called box-filter or moving-average algorithm, which consisis of updating only the part of the computation that changes from one Jocation to the next.% (a) Formulate such an algorithm for an 7 X 4 filter, showing the nature of the
computations involved and the scanning sequence used for moving the
mask around the image.(b) The ratio of the number of computations performed by a brute-force implementation to the number of computations performed by the box-filter algorithm is called the computational advantage. Obtain the computational
advantage in this case and plot it as a function of # for nm > 1, The 1/n? scaling factor is common to both approaches, so you need not consider it in obtaining the computational advantage. Assume thatthe image has an outer
border of zeros that is wide enough to allow you to ignore border effects in
your analysis.3.16 %(a} Suppose that you filter an image, f(x, y), with a spatial filter mask, w(x, y),
using convolution, as defined in Eq. (3.4-2), where the mask is smailer than
the image in both spatial directions. Show the important property that, if the
coefficients of the mask sum to zero, then the sum of all the elements in the
resulting convolution artay (filtered image) will be zero also (you may ignore computational inaccuracies). Also, you may assume that the border of
the image has béen padded with the appropriate number of zeros.(b) Would the result to (a) be the same if the filtering is implemented using carrelation, as defined in Eq. (3.4-1)?3.17 Discuss the limiting effect of repeatedly applying a 3 x 3 lowpass spatial filler
to a digital image. You may ignore border effects. Is this effect different frorn applying a5 x 5 filter?3.18 & (a) It was stated in Section 3.5.2 that isolated clusters of dark or light (with respect to the background) pixels whose area is less than one-half the area of
a median filter are eliminated (forced to the median value of the neighbors}
by the filter, Assume a filter of size 7 x #, with # odd, and explain why this
is $0.(b) Consider an image having various scis of pixel clusters. Assume that all
points in a cluster are lighter or darker than the background (but not both
simultaneously in the same cluster), and that the area oj cach cluster is tess
than or equal to n7/2. In terms of », under what condition would one or
more of these clusters cease to be isolated in the sense described in part (a)?m@ Problems217
218 Chapter 33 m Intensity Transformations and Spatiat FilteringW3193.2043.213.223.23{a) Develop a procedure for computing the median of an n X nm neighborhood,(b) Propose a technique for updating the median as the center of the neighborhood is moved from pixel to pixel.(a) In a character recognition application, text pages are reduced to binary
form using a thresholding transformation function of the form shown in Fig.
3.2(b). This is followed by a procedure that thins the characters until they
become strings of binary 1s on a background of 0s. Due to noise, the binarization and thinning processes result in broken strings of characters with
gaps ranging from 1 to 3 pixels. One way to “repair” the gaps is to run an averaging mask over the binary image to blur it, and thus create bridges of
nonzero pixels between gaps. Give the (odd) size of the smallest averaging
mask capable of performing this task.{b) After bridging the gaps, it is desired to threshold the image in order to convert it back to binary form. For your answer in (a), what is the minimum
value of the threshold required to accomplish this, without causing the segments to break up again?The three images shown were blurred using square averaging masks of sizesn = 23, 25, and 45, respectively. The vertical bars on the left lower part of (a)and (c) are blurred, but a clear separation exists between them. However, thebars have merged in image (b), in spite of the fact that the mask that produced
this image is significantly smaller than the mask that produced image (c). Explain the reason for this.eee ead
(a)Consider an application such as the one shown in Fig, 3.34, in which it is desired
to eliminate objects smaller than those enclosed by a square of size q X q pixels.
Suppose that we want to reduce the average intensity of those objects to onetenth of their original average value. In this way, those objects will be closer to
the intensity of the background and they can then be eliminated by thresholding. Give the (odd) size of the smallest averaging mask that will accomplish the
desired reduction in average intensity in only pne pass of the mask over the
image.In a given application an averaging mask is applied to input images to reduce
noise, and then a Laplacian mask is applied to enhance small details, Would the
result be the same if the order of these operations were reversed?
220 Chapter3 w Intensity Transformations and Spatial FilteringW3333.34What would be the effect of increasing the neighborhood size in the fuzzy filtering approach discussed in Section 3.8.5? Explain the reasoning for your answer
(you may use an example to support your answer).Design a fuzzy, rule-based system for reducing the effects of impulse noise on a
noisy image with intensity values in the interval [0, L ~ 1]. As in Section 3.8.5,
use only the differences d, ds, d,, and dy in a 3 X 3 neighborhood in order to
simplify the problem. Let z; denote the intensity at the center of the neighborhood, anywhere in the image. The corresponding output intensity values should
be z§ = zs; + v, where v is the output of your fuzzy system. That js, the output of
your fuzzy system is a correction factor used to reduce the effect of a noise spike
that may be present at the center of the 3 x 3 neighborhood. Assume that the
noise spikes occur sufficiently apart so that you need not be concerned with
multiple noise spikes being present in the same neighborhood. The spikes can be
dark or light. Use triangular membership functions throughout.*(a) Give a fuzzy statement for this problem.
%&(b) Specify the IF-THEN and ELSE rules.
(ce) Specify the membership functions graphically, as in Fig. 3.57.
(d) Show a graphical representation of the rule set, as in Fig. 3.58.
(e) Give asummary diagram of your fuzzy system similar to the one in Fig. 3.52.
*3.2443,253.263.273.283.293.303.31
3.32@ Problems 219Show that the Laplacian defined in Eq. (3.6-3) is isotropic (invariant to rotation). You will need the following equations relating coordinates for axis rotation by an angle @:x= x’ cos@— y'sin@yux’ sin @ + y’ cos@where (x, y) are the unrotated and (x’, y') are the rotated coordinates.You saw in Fig. 3.38 that the Laplacian with a —8 in the center yields sharper results than the one with a —4 in the center. Explain the reason in detail.With reference to Problem 3.25,(a) Would using a larger “Lapiacian-like” mask, say, of size 5 X 5 with a ~24 in
the center, yield an even sharper result? Explain in detail.(b) What happens when the size of the mask becomes equal to the image size.Give a 5 x 5 mask for performing unsharp masking in a single pass through an
image. Assume that the average image is obtained using Gaussian filter.
Show that subtracting the Laplacian from an image is proportional to unsharp
masking, Use the definition for the Laplacian given in Eq. (3-6-6).
(a) Show that the magnitude of the gradient given in Eq. (3.6-11) is an isotropic
operation, (See Problem 3.24.) :
(b) Show that the isotropic property is lost in general if the gradient is computed
using Eq, (3.6-12).
A CCD TV camera is used to perform a long-term study by observing the
same area 24 hours a day, for 30 days. Digital images are captured and transmitted to a central location every 5 minutes. The illumination of the scene
changes from natural daylight to artificial lighting. At no time is the scene
without illumination, so it is always possible to obtain an image. Because the
range of illumination is such that it is always in the linear operating range of
the camera, it is decided not to employ any compensating mechanisms on the
camera itself. Rather, it is decided to use image processing techniques to postprocess, and thus normatize, the images to the equivalent of constant illumination. Propose a method to do this. You are at liberty to use any method you
wish, but state clearly ali the assumptions you made in arriving at your design.Show that the crossover point in Fig. 3.46(d) is given by b = (a + c)/ 2.Use the fuzzy set definitions in Section 3.8.2 and the basic membership functions in Fig. 3.46 to form the membership functions shown below.  *(a) (b) tc}
DomainFilter: A device or material for suppressing
or minimizing woves or oscillations of certainfrequencies. Frequency: The number of times that o periodic
function repeats the same sequence of values during
a unit variation of the independent variable,
Webster's New Collegiate DictionaryPreviewAlthough significant effort was devoted in the previous chapter to spatial filtering, a thorough understanding of this area is impossible without having at
least a working knowledge of how the Fourier transform and the frequency
domain can be used for image filtering. You can develop a solid understanding
of this topic without having to become a signal processing expert. The key lies
in focusing on the fundamentals and their relevance to digital image processing. The notation, usually a source of trouble for beginners, is clarified significantly in this chapter by emphasizing the connection between image
characteristics and the mathematical tools used to represent them. This chapter is concerned primarily with establishing a foundation for the Fourier transform and how it is used in basic image filtering. Later, in Chapters S, 8, 10, and
11, we discuss other applications of the Fourier transform. We begin the discussion with a brief outline of the origins of the Fourier transform and its impact on countless branches of mathematics, science, and engineering. Next, we
start from basic principles of function sampling and proceed step-by-step to
derive the one- and two-dimensional discrete Fourier transforms, the basic staples of frequency domain processing. During this development, we also touch
upon several important aspects of sampling, such as aliasing, whose treatment
requires an understanding of the frequency domain and thus are best covered
in this chapter. This material is followed by a formulation of filtering in the frequency domain and the development of sections that parallel the spatialFiltering in the Frequency221
222 Chapter 4 w Filtering in the Frequency Domainsmoothing and sharpening filtering techniques discussed in Chapter 3. We conclude the chapter with a discussion of issues related to implementing the
Fourier transform in the context of image processing. Because the material in
Sections 4.2 through 4.4 is basic background, readers familiar with the concepts of 1-D signal processing, including the Fourier transform, sampling, aliasing, and the convolution theorem, can proceed to Section 4.5, where we begin
a discussion of the 2-D Fourier transform and its application to digital imageprocessing.ERG Background4.1.1 A Brief History of the Fourier Series and TransformThe French mathematician Jean Baptiste Joseph Fourier was born in 1768 in
the town of Auxerre, about midway between Paris and Dijon. The contribution
for which he is most remembered was outlined in a memoir in 1807 and published in 1822 in his book, La Théorie Analitique de la Chaleur (The Analytic
Theory of Heat). This book was translated into English 55 years later by Freeman (see Freeman [1878]). Basically, Fourier’s contribution in this field states
that any periodic function can be expressed as the sum of sines and/or cosines
of different frequencies, each multiplied by a different coefficient (we now call
this sum a Fourier series). It does not matter how complicated the function is;
if it is periodic and satisfies some mild mathematical conditions, it can be represented by such a sum. This is now taken for granted but, at the time it first
appeared, the concept that complicated functions could be represented as a
sum of simple sines and cosines was not at all intuitive (Fig. 4.1), so it is not surprising that Fourier’s ideas were met initially with skepticism.Even functions that are not periodic (but whose area under the curve is finite) can be expressed as the integral of sines and/or cosines multiplied by a
weighing function. The formulation in this case is the Fourier transform, and its
utility is even greater than the Fourier series in many theoretical and applied
disciplines. Both representations share the important characteristic that a
function, expressed in either a Fourier series or transform, can be reconstructed (recovered) completely via an inverse process, with no loss of information.
This is one of the most important characteristics of these representations because it allows us to work in the “Fourier domain” and then return to the original domain of the function without losing any information. Ultimately, it was
the utility of the Fourier series and transform in solving practical problems
that made them widely studied and used as fundamentai tools.The initial application of Fourier’s ideas was im the field of heat diffusion,
where they allowed the formulation of differential equations representing heat
flow in such a way that solutions could be obtained for the first time. During the
past century, and especially in the past 50 years, entire industries and academic
disciplines have flourished as a result of Fourier’s ideas. The advent of digital
computers and the “discovery” of a fast Fourier transform (FFT) algorithm in
the early 1960s (more about this later) revolutionized the field of signal processing. These two core technologies allowed for the first time practical processing of
4.1 m Background 223VV
NAVA
LIL LDDISIVFIGURE 4.1 The function at the bottom is the sum of the four functions above it.
Fourier’s idea in 1807 that periodic functions could be represented as a weighted sum
of sines and cosines was met with skepticism.a host of signals of exceptional importance, ranging from medical monitors and
scanners to modern electronic communications.We will be dealing only with functions (images) of finite duration, so the
Fourier transform is the tool in which we are interested. The material in the
following section introduces the Fourier transform and the frequency domain.
It is shown that Fourier techniques provide a meaningful and practical way to
study and implement a host of image processing approaches. In some cases,
these approaches are similar to the ones we developed in Chapter 3.4.1.2 About the Examples in this ChapterAs in Chapter 3, most of the image filtering examples in this chapter deal with
image enhancement. For example, smoothing and sharpening are traditionally
associated with image enhancement, as are techniques for contrast manipulation. By its very nature, beginners in digital image processing find enhancement to be interesting and relatively simple to understand. Therefore, using
224 Copter 4 @ Filtering in the Frequency Domainexamples from image enhancement in this chapter not only saves having an
extra chapter in the book but, more importantly, is an effective tool for introducing newcomers to filtering techniques in the frequency domain. We use
frequency domain processing methods for other applications in Chapters 5, 8,
10, and 11.KEE Preliminary ConceptsIn order to simplify the progression of ideas presented in this chapter, we
pause briefly to introduce several of the basic concepts that underlie the material that follows in later sections.4.2.1 Complex Numbers
A complex number, C, is defined as
C=R+ jf (4.2-1)where R and 7 are real numbers, and j is an imaginary number equal to the
square of —1; that is,j = V—I. Here, R denotes the rea part of the complex
number and / its imaginary part. Real numbers are a subset of complex
numbers in which J = 0. The conjugate of a complex number C, denoted C’,
is defined asC=R-jl (4.2-2)Complex numbers can be viewed geometrically as points in a plane (called the
complex plane) whose abscissa is the real axis (values of R) and whose ordinate is the imaginary axis (values of /). That is, the complex number R + jJ is
point (R, 7) in the rectangular coordinate system of the complex plane.
Sometimes, it is useful to represent complex numbers in polar coordinates,C = |Cl(cos 6 + jsin 4) (4.2-3)where |C| = VR? + 7° is the length of the vector extending from the origin of
the complex plane to point (R, /), and @ is the angle between the vector and the
real axis. Drawing a simple diagram of the real and complex axes with the vector in the first quadrant will reveal that tan @ = (7/R) or @ = arctan{//R). The
arctan function returns angles in the range {~2r/2, 2/2]. However, because 7
and R can be positive and negative independently, we need to be able to obtain
angles in the full range [-7, 7]. This is accomplished simply by keeping track
of the sign of / and R when computing #. Many programming languages do this
automatically via so called four-quadrant arctangent functions. For exampie,
MATLAB provides the function atan2(Imag, Real) for this purpose.
Using Euler's formula,e”? = cos@ + jsin@ (4.2-4)where ¢ = 2.71828..., gives the following familiar representation of complex
numbers in polar coordinates,C = |Cle” (4.2-5)
4.2 w Preliminary Concepts 225where [C| and @ are as defined above. For example, the polar representation of
the complex number 1 + j2 is V3e”, where @ = 64.4° or 1.1 radians. The preceding equations are applicable also to complex functions. For example, a
complex function, F(u), of a variable u, can be expressed as the sum
F(u) = R(u) + ji(u), where R(u) and J(u) are the real and imaginary component functions. As previously noted, the complex conjugate is F°(u)
= R(u) ~ jI(u), the magnitude is |F@)| = VR(u)? + 7(u)*, and the angle is
6(u) = arctan[/(u)/R(u)]. We return to complex functions several times in the
course of this and the next chapter.4.2.2 Fourier SeriesAs indicated in Section 4.1.1, a function f(#) of a continuous variable ¢ that is periodic with period, 7, can be expressed as the sum of sines and cosines multiplied
by appropriate coefficients. This sum, known as a Fourier series, has the formfi) = DS cel (4.26)
where
1 Te orn,
CG => fer! dt forn = 0,£1,+2,... {4.2-7)
Thrpare the coefficients. The fact that Eq. (4.2-6) is an expansion of sines and
cosines follows from Euler's formula, Eq. (4.2-4). We will return to the Fourier
series later in this section.4.2.3 Impulses and Their Sifting PropertyCentral to the study of linear systems and the Fourier transform is the concept
of an impulse and its sifting property. A unit impulse of a continuous variable ¢
located at ¢ = 0, denoted 8(t), is defined as00 if* =0
b(t) = {3 ft 20 (4.2-8a)and is constrained also to satisfy the identity
oO
| O(n) dt = 1 (4.2-8b)
00Physically, if we interpret ¢ as time, an impulse may be viewed as a spike of infinity amplitude and zero duration, having unit area. An impulse has the socalled sifting property with respect to integration,[ 140 ae = 700 (42-9)
provided that f(s} is continuous at = 0, a condition typically satisfied in prac
tice. Sifting simply yields the value of the function f(r) at the focation of the impulse (i.¢., the origin, ¢ = 0, in the previous equation). A more general statementAn impulse is nat a function in the usual sense. A
more accuraic name is
distribution or
generalized function
However, one often finds
in the literature the
names iniputse funciion,
delta funttion and Dirac
dela function. despite the
misnomer,To sift means literally to
separate, of to separale
out by pulting through a
sieve.
226 chapter 4 @ Filtering in the Frequency DomainFIGURE 4,2A unit discrete
impulse located at
X = Xo, Variable x
is discrete, and &
is O everywhere
except atx = Xo.of the sifting property involves an impulse located at an arbitrary point fo, denoted by S(t — ft). In this case, the sifting property becomes[fom ~ 19 a = fe) (42-10)which yields the value of the function at the impulse location, fp. For instance,
if f(s) = cos(t}, using the impulse 5(¢ — 2) in Eq. (4.2-10) yields the result
f(a) = cos(7) = —1. The power of the sifting concept will become quite evident shortly.Let x represent a discrete variable. The unit discrete impulse, 5(x), serves the
same purposes in the context of discrete systems as the impulse 6(t) does when
working with continuous variables. It is defined as1 x=0
d(x) = °
(x) {t x20 (4.2-11a)
Clearly, this definition also satisfies the discrete equivalent of Eq. (4.2-8b):
co
Dd sx) =1 (4.2-L1b)
500The sifting property for discrete variables has the form> fax) = FO) (4.2-12)
=o
or, more generally using a discrete impulse located at x = Xo,
D f$)S(x — xo) = F20) (4.2-13)As before, we see that the sifting property simply yields the value of the function at the location of the impulse. Figure 4.2 shows the unit discrete impulse
diagrammatically. Unlike its continuous counterpart, the discrete impulse is anordinary function.
Of particular interest later in this section is an impulse train, ss7(t), defined
as the sum of infinitely many periodic impulses AT units apart:sar(t) = S &(t — nAT) (42-14)Sle — xq)1. ty
4.2 m Preliminary Concepts 227Sar(f)————++ ~3AT -24T -AT 0 AT 2ST 3AT--
Figure 4.3 shows an impulse train. The impulses can be continuous or discrete.4.2.4 The Fourier Transform of Functions of
One Continuous VariableThe Fourier transform of acontinuous function f(t) of a continuous variable, £,
denoted 3{f()}, is defined by the equation’sf} = f poem a (42:15)where 4 is also a continuous variable. Because ¢ is integrated out, *{ f(r}} is a
function only of 4. We denote this fact explicitly by writing the Fourier transform as 3{f(t}} = F(x); that is, the Fourier transform of f(¢) may be written
for convenience asF(u) = | ot (Ne ™ dr (4.2-16)Conversely, given F(z), we can obtain f(s) back using the inverse Fourier
transform, f(t) = 3"'{F(u)}, written as
0f= / ue? dw (4.2-17)where we made use of the fact that variable yu is integrated out in the inverse
transform and wrote simple f(t), rather than the more cumbersome notation
f= STFU). Equations (4.2-16) and (4.2-17) comprise the so-called
Fourier transform pair. They indicate the important fact mentioned in
Section 4.1 that a function can be recovered from its transform.Using Euler’s formula we can express Eq. (4.2-16) asF(u) = [ . f(O[cos(2mp1) - jsin(2zus)] de (4.2-18) ‘Conditions for the existence of the Fourier transform are complicated (o state in general (Champeney
[2987]), but a sufficient condition for its existence is that the integral of the absolute value of f(i). or the
integral of the square of f(r}, be finite. Existence is seldom an issue in practice. except for idealized signals. such as sinusoids that extend forever. These are handled using generalized impulse functions. Our
primary interest is in the discrete Fourier transform pair which, as you wil] see shortly, is guaranteed to
exist for all finite functions.FIGURE 4.3 An
impulse train.
228 Chapter 4 Filtering in the Frequency DomainIf f(® is real, we see that its transform in general is complex. Note that the
Fourier transform is an expansion of f(t} multiplied by sinusoidal terms whose
frequencies are determined by the values of y (variable ¢ is integrated out, as
mentioned earlier). Because the only variable left after integration is frequen
cy, we say that the domain of the Fourier transform is thefrequency domain.We discuss the frequency domain and its properties in more detail later in thisFor consistency intermi- Chapter. In our discussion, £ can represent any continuous variable, and thei din the previ- ‘ . ‘ :
ous teochapters mute. Units of the frequency variable ys depend on the units of ¢. For example, if trep
ous (wo chapters, and to.
be used later in this resents time in seconds, the units of « are cycles/sec or He:rtz (Hz). If ¢ repre
chapter in connecti . : .
we reter sents distance in meters, then the units of yx are cycles/meter, and so on. Inwith images, we reler tothe domain of variabler other words, the units of the frequency domain are cycles per unit of the inde
in general as the spitticddomain, pendent variable of the input function.EXAMPLE 4.1: @ The Fourier transform of the function in Fig. 4.4(a) follows from Eq. (4.2-16):   Obtaining the
Fourier transform . 00 ; we
of a simple F(p) = [ f(t)e?™ dt = / Ae PH de
function. 2 WR
= A [emmy = A [einai — gina)
pap we jin
_~ AL eiteW _ esrew)
jaap
sin(apW)
(7eW)
where we used the trigonometric identity sin 9 = (e/® — e”/*)/2j. In this case
the complex terms of the Fourier transform combined nicely into a real sine
fo F(x) 1F(u)l
4
AW
A
—w/2 0 W/2 “ nay AF ONS, ‘
= 2/W [We Aw pwr le
abc
I functions extend to FIGURE 4.4 (a) A simple function; (b) its Fourier transform; and {c) the spectrum. A
infinity in both directions.
4.2 m@ Preliminary Concepts 229function. The result in the last step of the preceding expression is known as the
sinc function:
sin(wm)
(am)where sinc(0) = 1, and sinc(s) = 0 for all other integer values of m. Figure 4.4(b)
shows a plot of F(j).In general, the Fourier transform contains complex terms, and it is customary for display purposes to work with the magnitude of the transform (a real
quantity), which is called the Fourier spectrum or the frequency spectrum:sinc(m) = (4.2-19)sin(aruW)
(7uW)Figure 4.4(c) shows a plot of |F()| as a function of frequency. The key properties to note are that the locations of the zeros of both F(u) and |F()| are
inversely proportional to the width, W, of the “box” function, that the height of
the lobes decreases as a function of distance from the origin, and that the function extends to infinity for both positive and negative values of 2. As you will
see later, these properties are quite helpful in interpreting the spectra of twodimensional Fourier transforms of images. =|F(u)| = AT  m@ The Fourier transform of a unit impulse located at the origin follows fromEq. (4.2-16):
OO
/ 8(1)eP"™ atOoles}
[ e #27 #! 8(1) dy
00= e ltr — 2?=1
where the third step follows from the sifting property in Eq. (4.2-9). Thus, we
see that the Fourier transform of an impulse located at the origin of the spatial
domain is a constant in the frequency domain. Similarly, the Fourier transform
of an impulse located at ¢ = ry isF(u)F(u) = | S(t — sede
= / e Pt S(¢ — to) dt
00
= glo= cos(2apto) — jsin(2mpto)EXAMPLE 4.2;
Fourier transform
of an impulse and
of an impuise
train.
230 Chapter 4 ® Filtering in the Frequency Domainwhere the third line follows from the sifting property in Eq. (4.2-10) and the
last line follows from Euler’s formula. These last two lines are equivalent representations of a unit circle centered on the origin of the complex plane.In Section 4.3, we make use of the Fourier transform of a periodic impulse train. Obtaining this transform is not as straightforward as we just
showed for individual impulses. However, understanding how to derive the
transform of an impulse train is quite important, so we take the time to derive it in detail here. We start by noting that the only difference in the form
of Eqs. (4.2-16) and (4.2-17) is the sign of the exponential. Thus, if a function
f(t) has the Fourier transform F(z), then the latter function evaluated at 1,
that is, F(t), must have the transform f(—y). Using this symmetry property
and given, as we showed above, that the Fourier transform of an impulse
&(t — f) is eP™%, it follows that the function e”' has the transform
6(—-p ~ to). By letting —to = a, it follows that the transform of e”7% is
5(—p + a} = 3( — a), where the last step is true because 6 is not zero only
when yp = a, which is the same result for either 5(-y + a) or (4 — a), so
the two forms are equivalent.The impulse train sq7(t) in Eq, (4.2-14) is periodic with period AT, so we
know from Section 4.2.2 that it can be expressed as a Fourier series:29 ane,
Sar) = BY ene!
n=—00where
ar/2=o Sar (theta de
AT LlarpCnWith reference to Fig. 4.3, we see that the integral in the interval
[-AT/2, AT/2] encompasses only the impulse of s7{t) that is located at the
origin. Therefore, the preceding equation becomes1 aT?— are TS at
AT JesrpCy1
=—,AT
1~ aT
The Fourier series expansion then becomes1S oe
sar) = a5 yesn=—00Our objective is to obtain the Fourier transform of this expression. Because
summation is a linear process, obtaining the Fourier transform of a sum i:
4.2 m Preliminary Concepts 231the same as obtaining the sum of the transforms of the individual components. These components are exponentials, and we established earlier in this
example thatSeis} = a(n - x)So, S(), the Fourier transform of the periodic impulse train s4/(t}, isSiu) = S{sar}AT me
1 2 zen,
=—~% ef srt
AT {3 12 n
rl - x)This fundamental result tells us that the Fourier transform of an impulse train
with period AT is also an impulse train, whose period is 1/A7. This inverse
proportionality between the periods of s(t) and S(u) is analogous to what
we found in Fig. 4.4 in connection with a box function and its transform. This
property plays a fundamental role in the remainder of this chapter. a4.2.5 ConvolutionWe need one more building block before proceeding. We introduced the idea
of convolution in Section 3.4.2. You learned in that section that convolution of
two functions involves flipping (rotating by 180°) one function about its origin
and sliding it past the other. At each displacement in the sliding process, we
perform a computation, which in the case of Chapter 3 was a sum of products.
In the present discussion, we are interested in the convolution of two continuous functions, f(t) and h(f), of one continuous variable, f, so we have to use integration instead of a summation. The convolution of these two functions,
denoted as before by the operator x, is defined asf(t) kh() = [ f(r)A(t ~ tr) dr (4.2-20)where the minus sign accounts for the flipping just mentioned, ¢ is the
displacement needed to slide one function past the other, and 7 is a dummy
variable that is integrated out. We assume for now that the functions extend
from —© to 09,We illustrated the basic mechanics of convolution in Section 3.4.2, and we
will do so again later in this chapter and in Chapter 5. At the moment, we are
4.3 & Sampling and the Fourier Transform of Sampled Functions4.33 Sampling and the Fourier Transform of Sampled
Functions
In this section, we use the concepts from Section 4.2 to formulate a basis forexpressing sampling mathematically. This will lead us, starting from basic principles, to the Fourier transform of sampled functions.4.3.1 Sampling .
Continuous functions have to be converted into a sequence of discrete values
before they can be processed in a computer. This is accomplished by using
sampling and quantization, as introduced in Section 2.4. In the following discussion, we examine sampling in more detail,With reference to Fig. 4.5, consider a continuous function, f(f, that we
wish to sample at uniform intervals (AT) of the independent variable 1. Wefory 0Sar .HILLEL: “-2AT-ATO AT2AT «°
F(9sa7(0{tl | | ; Ne Let 
 “-2AT-ATO AT2AT °°
fa = (KAT) 233angenFIGURE 4.5{a) A continuous
function. (b) Train
of impulses used
io model the
sampling process.
(c) Sampled
function formed
as the product of
{a) and (b).(d) Sample values
obtained by
integration and
using the sifting
property of the
impulse. (The
dashed line in {c)
is shown for
reference. It is not
part of the data.)
232 Chapter 4 im Filtering in the Frequency Domain‘The same result would
be obtained if the order
of f(¢) and A{i) were
reversed, se convolution
is commutalive.interested in finding the Fourier transform of Eq. (4.2-20). We start with
Eq. (4.2-15):Xf) AY} = [I [rem-o ar lemma| ro | A(t — re Pe a] dtThe term inside the brackets is the Fourier transform of h(t — 7). We show
later in this chapter that X{A(¢ — 7)} = H(w)e7?7*", where H(j2) is the
Fourier transform of #(1), Using this fact in the preceding equation gives ustlMtX{P) & AL) | POH meP™ drsia) [pene Ree ar. H(w) F(u)Recalling from Section 4.2.4 that we refer to the domain of ¢ as the spatial domain, and the domain of y as the frequency domain, the preceding equation
tells us that the Fourier transform of the convolution of two functions in the
spatial domain is equal to the product in the frequency domain of the Fourier
transforms of the two functions. Conversely, if we have the product of the two
transforms, we can obtain the convolution in the spatial domain by computing
the inverse Fourier transform. In other words, f(t) * A(f) and H(u) F(u) are a
Fourier transform pair. This result is one-half of the convolution theorem and
is written as_FG) RAG) > Hw) Fe) (4.2-21)The double arrow is used to indicate that the expression on the right is obtained by taking the Fourier transform of the expression on the left, while the
expression on the left is obtained by taking the inverse Fourier transform of
the expression on the right.Following a similar development would result in the other half of the convolution theorem: *F(A) <> Hy) & Fis) (42-22)which states that convolution in the frequency domain is analogous to multiplication in the spatial domain, the two being related by the forward and inverse Fourier transforms, respectively, As you will see later in this chapter, the
convolution theorem is the foundation for filtering in the frequency domain.
234 Chapter 4 m Filtering in the Frequency Domain‘Taking somples AF units
apart implics a sampling
rate equal to 1/AT. Ef the
units of AT are seconds,
then the sampling rate is
in samples/s, Ef the units
of AT are meters, then
ihe sampling rate is in
samples/m, and so on,assume that the function extends from —0° to 60 with respect to 4. One way
to model sampling is to multiply f(t) by a sampling function equal to a train
of impulses AT units apart, as discussed in Section 4.2.3. That is,FO = fsar= S fae~naT) (43-1)where f(t) denotes the sampled function. Each component of this summation
is an impulse weighted by the value of f(¢) at the location of the impulse, as
Fig. 4.5(c) shows. The value of each sample is then given by the “strength” of
the weighted impulse, which we obtain by integration. That is, the value, f,, of
an arbitrary sample in the sequence is given byf= | f(t) 8(¢ — KAT) at
“~ (43-2)
= f(kAT)where we used the sifting property of 6 in Eq. (4.2-10). Equation (4.3-2) holds
for any integer value kK = ...,-2, -1,0,1,2,.... Figure 4.5(d) shows the result, which consists of equally-spaced samples of the original function.4.3.2 The Fourier Transform of Sampled FunctionsLet F(z) denote the Fourier transform of a continuous function f(t). Asdiscussed in the previous section, the corresponding sampled function, f (¢), is
the product of f(#) and an impulse train. We know from the convolution theorem in Section 4,2.5 that the Fourier transform of the product of two functions
in the spatial domain is the convolution of the transforms of the two functions
in the frequency domain. Thus, the Fourier transform, F(), of the sampledfunction f (¢) is:
F(a) = 3{F(0}
Xf Wsar(O} (43-3)= F(u)* S(p)where, from Example 4.2,S(u) = oe Sau - Z| (43-4)
4.3 Sampling and the Fourier Transform of Sampled Functionsis the Fourier transform of the impulse train s47(1). We obtain the convolution
of F(«) and S(z) directly from the definition in Eq. (4.2-20):F(u) = F(u) * S(u)Hl[ F(t) S(u - 1) dr.a le) > A(u - r~2) dr (43-5)= AF [Feu -1- 2) ae12 n
= 1 3 4( - ar)where the final step follows from the sifting property of the impulse, as given
in Eq. (4.2-10).The summation in the last line of Eq. (4.3-5) shows that the Fourier transform
F(p) of the sampled function F(t) is an infinite, periodic sequence of copies of
F(z), the transform of the original, continuous function. The separation between
copies is determined by the value of 1/A7. Observe that although f(t) is a
sampled function, its transform F (2) is continuous because it consists of copies
of F(z) which is a continuous function. .Figure 4.6 is a graphical summary of the preceding results." Figure 4.6(a) is a
sketch of the Fourier transform, F(«), of a function f(¢), and Fig. 4.6(b) shows
the transform, F(), of the sampled function. As mentioned in the previous section, the quantity 1/A7 is the sampling rate used to generate the sampled function. So, in Fig. 4.6(b) the sampling rate was high enough to provide sufficient
separation between the periods and thus preserve the integrity of F(z). In
Fig. 4.6(c), the sampling rate was just enough to preserve F(j), but in Fig.
4.6(d), the sampling rate was below the minimum required to maintain distinct copies of F(;z) and thus failed to preserve the original transform. Figure
4.6(b) is the result of an over-sampled signal, while Figs. 4.6(c) and (d) are the
results of critically-sampling and under-sampling the signal, respectively.
These concepts are the basis for the material in the following section.  4.3.3 The Sampling TheoremWe introduced the idea of sampling intuitively in Section 2.4. Now we consider the sampling process formally and establish the conditions under which a
continuous function can be recovered uniquely from a set of its samples. *For the sake of clarity in illustrations, sketches of Fourier transforms in Fig. 4.6, and other similar figures
in this chapter, ignore the fact that transforms typically are complex functions.235con eee
236  Chopter 4 m Filtering in the Frequency DomainRateFIGURE 4.6(a) Fourier
transform of a
band-limited
function.(b)-(d)
Transforms of the
corresponding
sampled function
under the
conditions of
over-sampling,
criticallysampling, and
under-sampling,
Tespectively.  —2/aT —l/AT Qo 1/AT 2/aT  -2/aT ~\/AT 0 1/AT Q/aT
Flu) “3/AT -2/AT ~1/AT 0 t/AT 2/AT = -3/ATA function f(t) whose Fourier transform is zero for values of frequencies outside a finite interval (band) [—j4nax» max] about the origin is called a band-limited
function. Figure 4.7(a), which is a magnified section of Fig. 4.6(a), is such a function. Similarly, Fig, 4.7(b) is a more detailed view of the transform of a criticallysampled function shown in Fig. 4.6(c). A lower value of 1/AT would cause the
periods in F(#) to merge; a higher value would provide a clean separation
between the periods.We can recover f(t} from its sampled versiom if we can isolate a copy of
F() from the periodic sequence of copies of this function contained in F(x),
the transform of the sampled function f (1). Recall from the discussion in the
previous section that F(4) is a continuous, periodic function with period
1/AT. Therefore, all we need is one complete period to characterize the entire
transform. This implies that we can recover f(t) from that single period by
using the inverse Fourier transform.
4.3 @ Sampling and the Fourier Transform of Sampled Functions 237  F(x)
.
—Kmax 0 4 max a
F(u)
—H max v
+ yt
ee es
2aT 2aT ATExtracting from F(x) a single period that is equal to F(z) is possible if the
separation between copies is sufficient (see Fig. 4.6). In terms of Fig. 4.7(b),
sufficient separation is guaranteed if 1/2AT > pmax OF *To > 2emax (4.3-6)This equation indicates that a continuous, band-limited function can be recovered completely from a set of its samples if the samples are acquired at a
rate exceeding twice the highest frequency content of the function. This result
is known as the sampling theorem.' We can say based on this result that no information is lost if a continuous, band-limited function is represented by samples acquired at a rate greater than twice the highest frequency content of the
function. Conversely, we can say that the maximum frequency that can be
“captured” by sampling a signal at a rate 1/AT is puma, = 1/247. Sampling at
the Nyquist rate sometimes is sufficient for perfect function recovery, but
there are cases in which this leads to difficulties, as we illustrate later in
Example 4.3. Thus, the sampling theorem specifies that sampling must exceed
the Nyquist rate. ‘The sampling theorem is a cornerstone of digital signal processing theory. It was first formulated in 1928by Harry Nyquist, a Bell Laboratories scientist and engineer. Claude E. Shannon, also from Bell Labs,
proved the theorem formally in 1949. The renewed interest in the sampling theorem in the late 1940s
was motivated by ihe emergence of early digital computing systems and modern communications,
which created a need for methods dealing with digital (sampled) data.abFIGURE 4.7(a) Transform of a
band-limited
function.(b) Transform
resulting from
critically sampling
the same function.A sampling rate equal to
exactly wwice the highest
frequency is called the
Nyquist rate.re
238 Chapter 4 @ Filtering in the Frequency Domainb&&FIGURE 4.8
Extracting one
period of the
transform of a
band-limited
function using an
ideal lowpass
filter.The AT in Eq. (4.3-7)
cancels out the 1/A7 in
Eq. (43-5).F(u)  +
-2/AT -1/ST THmax O MmaTo see how the recovery of F(x) from F(z) is possible in principle, consider
Fig, 4.8, which shows the Fourier transform of a function sampled at a rate slightly
higher than the Nyquist rate. The function in Fig. 4.8(b) is defined by the equation_ AT “Bmax = Hh = max
H(p) = fi otherwise (43-7)When multiplied by the periodic sequence in Fig. 4.8(a), this function isolates
the period centered on the origin. Then, as Fig. 4.8(c) shows, we obtain F(z) by
multiplying F() by H(q):P(u) = H(w) Pe) (43-8)Once we have F(x) we can recover f(¢) by using the inverse Fourier transform: ;fO= / F(wye?™dps (4.3-9)Equations (4.3-7) through (4.3-9) prove that, theoretically, it is possible to
recover a band-limited function from samples of the function obtained at a
rate exceeding twice the highest frequency content of the function. As we
discuss in the following section, the requirement that f(r} must be bandlimited implies in general that f(¢) must extend from —©¢ to o9, a condition
240 Chapter 4 m Filtering in the Frequency Domain -3/AT -2/AT -1/AT 0 N/T aT -3/aT- b   ~Hmox O Bmaxab.‘6FIGURE 4.9 (a) Fourier transform of an under-sampled, band-limited function.
(Interference from adjacent periods is shown dashed in this figure). (b) The same ideal
lowpass filter used in Fig. 4.8(b). (c) The product of (a) and (b). The interference from
adjacent periods results in aliasing that prevents perfect recovery of Fu) and,
therefore, of the original, band-limited continuous function. Compare with Fig. 4.8.components extending to infinity. Therefore, no function of finite duration
can be band-limited. Conversely, a function that is band-limited must extend from —09 to ©o,!We conclude that aliasing is 2n inevitable fact of working with sampled
records of finite length for the reasons stated in the previous paragraph. In
practice, the effects of aliasing can be reduced by smoothing the input function
to attenuate its higher frequencies (e.g., by defocusing in the case of an image).
This process, called anti-aliasing, has to be done before the function is sampled
because aliasing is a sampling issue that cannot be “undone after the fact”
using computational techniques. ‘An important special case is when a fuaction that extends from — 09 to 09 is band-limited and periodic. In
this case, the function can be truncated and still be band-limited. provided that the truncation encompasses exactly an integral number of periods. A single truncated period (and thus the function} can be represented by a set of discrete samples salisfying the sampling theorem, taken over the truncated interval.
4.3 = Sampling and the Fourier Transform of Sampled Functionsthat cannot be met in practice. As you will see shortly, having to limit the duration of a function prevents perfect recovery of the function, except in some
special cases.Function H(z) is called a lowpass filter because it passes frequencies at the
low end of the frequency range but it eliminates (filters out) all higher frequencies. It is called also an ideal lowpass filter because of its infinitely rapid
transitions in amplitude (between 0 and AT at location —,,,, and the reverse
at max), a Characteristic that cannot be achieved with physical electronic components. We can simulate ideal filters in software, but even then there are limitations, as we explain in Section 4.7.2. We will have much more to say about
filtering later in this chapter. Because they are instrumental in recovering (reconstructing) the original function from its samples, filters used for the purpose just discussed are called reconstruction filters.4.3.4 AliasingA logical question at this point is: What happens if a band-limited function is
sampled at a rate that is less than twice its highest frequency? This corresponds
to the under-sampled case discussed in the previous section. Figure 4.9(a) is
the same as Fig. 4.6(d), which illustrates this condition. The net effect of lowering the sampling rate below the Nyquist rate is that the periods now overlap,
and it becomes impossible to isolate a single period of the transform, regardless of the filter used. For instance, using the ideal lowpass filter in Fig. 4.9(b)
would result in a transform that is corrupted by frequencies from adjacent periods, as Fig. 4.9(c) shows. The inverse transform would then yield a corrupted
function of « This effect, caused by under-sampling a function, is known as
frequency aliasing or simply as aliasing. In words, aliasing is a process in which
high frequency components of a continuous function “masquerade” as lower
frequencies in the sampled function. This is consistent with the common use of
the term alias, which means “a false identity.”Unfortunately, except for some special cases mentioned below, aliasing is
always present in sampled signals because, even if the original sampled function is band-limited, infinite frequency components are introduced the moment we limit the duration of the function, which we always have to do in
practice. For example, suppose that we want to limit the duration of a bandlimited function f(#} to an interval, say (0, 7]. We can do this by multiplying
f(t) by the function1 Os1sT
AC) = {} otherwise (43-10)This function has the same basic shape as Fig. 4.4(a) whose transform,
A(z), has frequency components extending to infinity, as Fig. 4.4(b) shows.
From the convolution theorem we know that the transform of the product
of h(t) f (1) is the convolution of the transforms of the functions. Even if the
transform of f(r) is band-limited, convolving it with H(). which involves
sliding one function across the other, will yield a result with frequency239
4.3 & Sampling and the Fourier Transform of Sampled Functions 241® Figure 4.10 shows a classic illustration of aliasing. A pure sine wave
extending infinitely in both directions has a single frequency so, obviously, it is
band-limited. Suppose that the sine wave in the figure (ignore the large dots
for now) has the equation sin(z/), and that the horizontal axis corresponds to
time, ¢, in seconds. The function crosses the axis at¢ = ... ~1,0,1,2,3....The period, P, of sin(at) is 2 s, and its frequency is 1/P, or 1/2 cycles/s.
According to the sampling theorem, we can recover this signal from a set of
its samples if the sampling rate, 1/AT, exceeds twice the highest frequency
of the signal. This means that a sampling rate greater than 1 sample/s
(2 x (1/2) = 1], or AT < 15, is required to recover the signal, Observe that
sampling this signal at exactly twice the frequency (1 sample/s), with samples taken at ¢= ... —1,0,1,2,3..., results in ...sin(—7r), sin(O), sin(z),
sin(27),..., which are all 0. This illustrates the reason why the sampling theorem requires a sampling rate that exceeds twice the highest frequency, as
mentioned earlier.The large dots in Fig. 4.10 are samples taken uniformly at a rate of less than
1 sample/s (in fact, the separation between samples exceeds 2 s, which gives a
sampling rate lower than 1/2 samples/s). The sampled signal /ooks like a sine
wave, but its frequency is about one-tenth the frequency of the original. This
sampled signal, having a frequency well below anything present in the original
continuous function is an example of aliasing. Given just the samples in
Fig. 4.10, the seriousness of aliasing in a case such as this is that we would have
no way of knowing that these samples are not a true representation of the
original function. As you will see in later in this chapter, aliasing in images can
produce similarly misleading results. .4.3.3 Function Reconstruction (Recovery) from Sampled DataIn this section, we show that reconstruction of a function from a set of its samples reduces in practice to interpolating between the samples. Even the simple
act of displaying an image requires reconstruction of the image from its samples  FIGURE 4.10 [ifustration of aliasing. The under-sampled function (black dots) looks
like a sine wave having a frequency much lower than the frequency of the continuous
signal. The period of the sine wave is 2 s, so the zero crossings of the horizontal axis
occur every second. AJ is the separation between samples,EXAMPLE 4,3:
Aliasing.Recall that 1 cycless is
defined as | Iz.
242Chepter 4 m Filtering in the Frequency Domainby the display medium. Therefore, it is important to understand the fundamentals of sampled data reconstruction. Convolution is central to developing this
understanding, showing again the importance of this concept.The discussion of Fig. 4.8 and Eq, (4.3-8) outlines the procedure for perfect
recovery of a band-limited function from its samples using frequency domain
methods. Using the convolution theorem, we can obtain the equivalent result
in the spatial domain. From Eq. (4.3-8), F(a) = H(u)F(y), so it follows thatWWHF @)}= 9 {H(e)F(u)} (43-11)fo= hO*IOwhere the last step follows from the convolution theorem, Eq. (4.2-21). It can
be shown (Problem 4.6) that substituting Eq. (4.3-1) for f (2) into Eq. (4.3-11)
and then using Eq. (4.2-20) leads to the following spatial domain expressionfor f(t):
fO= S f(n AT) sine[(t — nAT)/n AT] (4.3-12)where the sinc function is defined in Eq. (4.2-19). This result is not unexpected
because the inverse Fourier transform of the box filter, H(z), is a sinc function
{see Example 4.1). Equation (4.3-12) shows that the perfectly reconstructed
function is an infinite sum of sinc functions weighted by the sample values, and
has the important property that the reconstructed function is identically equal
to the sample values at multiple integer increments of AT. That is, for any
t = k AT, where k is an integer, f(t) is equa! to the kth sample f(KAT). This
follows from Eq. (4.3-12} because sinc(0) = 1 and sinc(m)=0 for any other
integer value of m. Between sample points, values of f(t) are interpolations
formed by the sum of the sinc functions.Equation (4.3-12) requires an infinite number of terms for the interpolations between samples. In practice, this implies that we have to look for approximations that are finite interpolations between samples. As we discussed
in Section 2.4.4, the principal interpolation approaches used in image processing are nearest-neighbor, bilinear, and bicubic interpolation. We discuss the effects of interpolation on images in Section 4.5.4.(EZ The Discrete Fourier Transform (DFT) of OneVariable
One of the key goals of this chapter is the derivation of the discrete Fourier
transform (DFT) starting from basic principles. The material up to this point
may be viewed as the foundation of those basic principles, so now we have in
place the necessary tools to derive the DFT.
4.4 m The Discrete Fourier Transform (DFT) of One Variable4.4.1 Obtaining the DFT from the Continuous Transform
of a Sampled FunctionAs discussed in Section 4.3.2, the Fourier transform of a sampled, band-limited
function extending from — 0° to 00 is a continuous, periodic function that also
extends from —0©0 to oo. In practice, we work with a finite number of samples,
and the objective of this section is to derive the DFT corresponding to such
sample sets. .Equation (4.3-5) gives the transform, F() of sampled data in terms of the
transform of the original function, but it does not give us an expression for
F(x) in terms of the sampled function f (4 itself. We find such an expression
directly from the definition of the Fourier transform in Eq. (4.2-16):F(u) = [ Fae? at (44-1)
By substituting Eq. (4.3-1) for f (t), we obtainFw = [ Foe Pom ar00S f(t — nAT\e!?™ at00 y= =00(44-2)oO> F(Q8(t — nAT)e 1? arn=—00 J -00oo
Sf, evemanar
nn=~Cowhere the last step follows from Eq. (4.3-2), Although f,, is a discrete function,
its Fourier F(j) is continuous and infinitely periodic with period 1/AT, as we
know from Eq. (4.3-5). Therefore, all we need to characterize F(z) is one period,and sampling one period is the basis for the DFT. .Suppose that we want to obtain M equally spaced samples of F(z) taken
over the period x = 0 to » = 1/AT. This is accomplished by taking the samples at the following frequencies:mw= sap = OQ Mod (4.4-3)Substituting this result for yz into Eq. (4.4-2) and letting F,, denote the result
yieldsM~t
Fy= SfrelrM om = 0,1,2,...,M~1 (4.4-4)n=O243
244 Chapter 4 @ Filtering in the Frequency DomainThis expression is the discrete Fourier transform we are seeking. Given a set
{fn} consisting of M samples of f(9), Eq. (4.4-4) yields a sample set {F,,} of M
complex discrete values corresponding to the discrete Fourier transform of the
input sample set. Conversely, given {F,,}, we can recover the sample set {f,,}
by using the inverse discrete Fourier transform (IDFT)fn = i“ SF F,e7/M =» =0,1,2,...,.M-—1 (4.4-5)
It is not difficult to show (Problem 4.8) that substituting Eq. (4.4-5) for f,,
into Eq. (4,4-4) gives the identity F,, = F,,. Similarly, substituting Eq. (4.4-4)
into Eq. (4.4-5) for F,, yields f,, = f,. This implies that Eqs. (4.4-4) and (4.4-5)
constitute a discrete Fourier transform pair. Furthermore, these identities indicate that the forward and inverse Fourier transforms exist for any set of
samples whose values are finite. Note that neither expression depends explicitly on the sampling interval AT nor on the frequency intervals of Eq.
(4.4-3). Therefore, the DFT pair is applicable to any finite set of discrete
samples taken uniformly.We used nm and » in the preceding development to denote discrete variables
because it is typical to do so for derivations. However, it is more intuitive, especially in two dimensions, to use the notation x and y for image coordinate
variables and u and v for frequency variables, where these are understood to
be integers.t Then, Eqs. (4.4-4) and (4.4-5) becomeM-1
F(a) = DSf@ele — 4 = 0,1,2,...,M—1 (4.4-6)
x0
and
4 Mol .
f) = 55 > Fe? y= 0,1,2,...,M—-1 (44-7)
4=0where we used functional notation instead of subscripts for simplicity, Clearly,
F(u) = F,, and f(x) = f,. From this point on, we use Eqs. (4.4-6) and (4.4-7)
to denote the 1-D DFT pair. Some authors include the 1/M term in Eq. (4.4-6)
instead of the way we show it in Eq. (4.4-7). That does not affect the proof that
the two equations form a Fourier transform pair. tNote from Fig, 4.6(b) that the interval [0, 1/47] covers two back-i-back he/f periods of the transform.
‘This means that the data in F,, tequires re-ordering to obtain samples that are ordered from the lowest
the highest frequency of a peziod. This is the price paid for the notational convenience of taking thesamples at m = 0, 1,..., 4 — 1. instead of using samples on either side of the origin, which would require the use of negative notation. The procedure to order the transform data is discussed in Section
4.6.3.*We have been careful in using ¢ for continuous spatial variables and p for the corresponding continuous
frequency variable. From this point on, we will use x and u 10 denote one-dimensional discrete spatial
and frequency variables. respectively. When dealing with two-dimensional functions, we will use (¢, z)
and (2, v) to denote continuous spatial and frequency domain variables, respectively. Similarly, we will use.
(x,y) and (x, y) to denote their discrete counterparts.
4.4 & The Discrete Fourier Transform (DFT) of One Variable 245It can be shown (Problem 4.9) that both the forward and inverse discrete
transforms are infinitely periodic, with period M. That is,F(u) = F(u+kM) (4.4-8)
and
. F(x) = f(xt+ kM) (4.4-9)where & is an integer,
The discrete equivalent of the convolution in Eq. (4.2-20) isM-1
f(x) w h(x) = 2 Somat -m) (4.4-10)for x = 0, 1,2,..., M — 1. Because in the preceding formulations the functions
are periodic, their convolution also is periodic. Equation (4.4-10) gives one
period of the periodic convolution. For this reason, the process inherent in this
equation often is referred to as circular convolution, and is a direct result of the
periodicity of the DFT and its inverse. This is in contrast with the convolution
you studied in Section 3.4.2, in which values of the displacement, x, were determined by the requirement of sliding one function completely past the other,
and were not fixed to the range [0, Mf — 1] as in circular convolution. We discuss
this difference and its significance in Section 4.6.3 and in Fig. 4.28.Finally, we point out that the convolution theorem giyen in Eqs. (4.2-21) and
(4.2-22) is applicable also to discrete variables (Problem 4.10).4.4.2 Relationship Between the Sampling and Frequency Intervals
If f(x) consists of M samples of a function f(f) taken AT units apart, the
duration of the record comprising the set {fo}, x= 0,1,2,....M@—-1, isT = MAT {4.4-11)The corresponding spacing, Au, im the discrete frequency domain follows from
Eq. (4.4-3):1 1
Au = WAT TF (4.4-12)
The entire frequency range spanned by the M components of the DFT is
1
0 = MAu = AT (4.4-13)Thus, we see from Eqs. (4.4-12) and (4.4-13) that that the resolution in frequency, Au, of the DFT depends on the duration T over which the continuous
function, f(r), is sampled, and the range of frequencies spanned by the DFT
depends on the sampling interval AT. Observe that both expressions exhibit
inverse relationships with respect to T and AT.Eis not obvious why the
discrete function f(x)
should be periodic, considering that the continuous funclion from which
il Was sampted may not
be. One informal way to
reason this out is to keep
in mind that sampling results im a periodic DFT. It
is logical that f(x}, which
is the inverse DFT, has to
be periodic also for the
DFT pair to exisi,
246 — Chopter 4 m Filtering in the Frequency DomainEXAMPLE 4.4:
The mechanics of
computing the
DFT.abFIGURE 4.11(a) A function,
and (b) samples in
the x-domain. In
(a),tisa
continuous
variable; in (b).x
Tepresents integer
values.@ Figure 4.11(a) shows four samples of a continuous function, f(¢), taken ST
units apart. Figure 4.11(b) shows the sampled values in the x-domain. Note
that the values of x are 0, 1, 2, and 3, indicating that we could be referring to
any four samples of f(t).From Eq. (4.4-6),3
FO) = Sifts) = [f(0) + fC) + £(2) + F)]=14+24+44+4=11The next value of F(x) isF(1)u3
Sf (ayers
x=0Mlde? + 2c? + de® + deVF7? = —3 42)Similarly, F(2) = —(1 + 0/) and F(3) = —(3 + 2/). Observe that ai! values of
f(x) are used in computing each term of F(u).If instead we were given F(«) and were asked to compute its inverse, we
would proceed in the same manner, but using the inverse transform. For instance,F(u) e f2au(0)pie
Mef(0) =u=0Bie
oF(u)
0ui}i [11-3 +2j-1-3-2)          1
=-—[4]=1
74
which agrees with Fig. 4.11(b), The other vaiues of f(x} are obtained in a simitar manner. c
fO f9
4
SF 5 .
4h 4 ® ¢
i I I 4
af to 3 i; of
1 | > i \
2b ' H 2 ® 1 1
1 ! 1 | ! i
i 1 i 1 | { 1
H ' ; H I 1 1
0 i 1 i Li. ; 0 I 1 1 x
fy ty t TAT + 27 fy + BAT it) 1 2 3
4.5 m Extension to Functions of Two VariablesEEG Extension to Functions of Two VariablesIn this section, we extend to two variables the concepts introduced in Sections
4.2 through 4.4.4.5.) The 2-D Impulse and Its Sifting PropertyThe impulse, 4(¢, z), of two continuous variables, ¢ and z, is defined as in
Eq. (4.2-8):foe) ifft=z=0
3,2) = {s otherwise (4.5-1a)
and
coal Oo
/ / 51,2) dtdz = 1 (4.5-1b)
—00 J—00As in the 1-D case, the 2-D impulse exhibits the sthing property under
integration,[ [ F(t, 2) S(t, z) dt dz = F(0, 0) (4.5-2)or, more generally for an impulse located at coordinates (fg, Zp),| [ F(t, 2) 8(t — 2 ~ %) dt dz = flo 2) (4.5-3)As before, we see that the sifting property yields the value of the function
F(t, z) at the location of the impulse.
For discrete variables x and y, the 2-D discrete impulse is defined as_ ijl ifx=y=0
Bex, ») = {; otherwise (45-4)
and its sifting property is> s f(x, y)8(x, y) = (0,0) (45-5)x==00 y=where f(x, y) is a function of discrete variables x and y. For an impulse located
at coordinates (xp, yo) (see Fig. 4.12) the sifting property isS >. F(x, y)8(x — Xo. ¥ — Yo) = Fo Yo) (45-6)x= OO yamAs before, the sifting property of a discrete impulse yields the value of the discrete function f(x, y) at the location of the impulse.247
248 Chapter 4 a Filtering in the Frequency DomainFIGURE 4.12
Two-dimensional
unit discrete
impulse. Variables
xan y are
discrete, and dis
zero everywhere
except at
coordinates(Xo. Yo).EXAMPLE 4.5;
Obtaining the 2-D
Fourier transform
of a simple
function.5(x = sm y - Yo} 4.4.2 The 2-D Continuous Fourier Transform PairLet f(t, z) be a continuous function of two continuous variables, f and z. The
two-dimensional, continuous Fourier transform pair is given by the expressionsF(p,v) = | [ F(t, Qe Pt) dr dz (45-7)
and
fee) = ff Fiarye?orau dv (45-8)where yz and v are the frequency variables. When referring to images, f and z
are interpreted to be continuous spatial variables. As in the 1-D case, the domain of the variables 4 and v defines the continuous frequency domain.% Figure 4.13(a) shows a 2-D function analogous to the 1-D case in Example 4.1.
Following a procedure similar to the one used in that example gives the resultoC OO
Joo Joo
Ti2 phf2
[ / Ae l2mutte2) dy dz
f-1;2 J-Zi2
= ATZ sin(apT} sin(avZ)
(wpT) .(mvZ)The magnitude (spectrum) is given by the expressionttF(u,v)t sin(avZ)sin(wrT)[F(u, v)| = ATZ (ru) |  Figure 4.13(b) shows a portion of the spectrum about the origin. As in the 1-D
case, the locations of the zeros in the spectrum are inversely proportional to
4.5 @ Extension to Functions of Two Variables FIGURE 4.13 (a) A 2-D function, and (b) a section of its spectrum (not to scale). The
block is longer along the f-axis, so the spectrum is more “contracted” along the y-axis.
Compare with Fig. 4.4.the values of 7 and Z. Thus, the larger T and Z are, the more “contracted” the
spectrum will become, and vice versa. fe4.5.4 Two-Dimensional Sampling and the 2-D Sampling TheoremIn a manner similar to the 1-D case, sampling in two dimensions can be modeled using the sampling function (2-D impulse train):Sataz (t, 2) = by s 5(t — mAT,z ~ nAZ) (4.5-9)mE—OS 7#=~00where AT and AZ are the separations between samples along the f- and z-axis
of the continuous function f(t, 2). Equation (4.5-9) describes a set of periodic
impulses extending infinitely along the two axes (Fig. 4.14). As in the 1-D case
illustrated in Fig. 4.5, multiplying f(‘,z) by saraz(t, z) yields the sampled
function,Function f (7, z) is said to be band-limited if its Fourier transform is Q outside a rectangle established by the intervals [—pmay; Hmax) ANd [—Ynax- Umax:
that is,F(u.v) = 0 for |yl = femax and [vl = vingx (4.5-10)The two-dimensional sampling theorem states that a continuous, band-limited
function f(t, z) can be recovered with no error from a set of its samples if the
sampling intervals are 1
AT < = (4.5-11)
2max
and
]
AZ <= (4.5-12)
Vinaxor, expressed in terms of the sampling rate, if249
250 Chapter 4 @ Filtering in the Frequency DomainFIGURE 4.14
Two-dimensional
impulse train.abFIGURE 4.15
Two-dimensional
Fourier transforms
of (a) an oversampled, and{b) under-sampled
band-timited
function.Sarsztt, 2) 1
ar > 2pfmax (4.5-13)
and
a > 2», (4.5-14)
AZ max .Stated another way, we say that no information is lost if a 2-D, band-limited, continuous function is represented by samples acquired at rates greater than twice
the highest frequency content of the function in both the y- and v-directions.
Figure 4.15 shows the 2-D equivalents of Figs. 4.6(b) and (d). A 2-D ideal box
filter has the form illustrated in Fig, 4.13(a). The dashed portion of Fig. 4.15(a)
shows the location of the filter to achieve the necessary isolation of a single period of the transform for reconstruction of a band-limited function from its samples, as in Section 4.3.3. From Section 4.3.4, we know that if the function is
under-sampled the periods overlap, and it becomes impossible to isolate a single
period, as Fig. 4.15(b) shows, Aliasing would result under such conditions.4.5.4 Aliasing in ImagesIn this section, we extend the concept of aliasing to images and discuss several
aspects related to image sampling and resampling.     Footprint of an
ideal lowpass
(box) fitter
4.5 @ Extension to Functions of Two Variables 251Extension from 1-D aliasingAs in the 1-D case, a continuous function f (i, z) of two continuous variables, and
z,can be band-limited in general only if it extends infinitely in both coordinate directions. The very act of limiting the duration of the function introduces corrupting
frequency components extending to infinity in the frequency domain, as explained
in Section 4.3.4. Because we cannot sample a function infinitely, aliasing is always
present in digital images, just as it is present in sampled 1-D functions. There are
two principal manifestations of aliasing in images: spatial aliasing and temporal
aliasing. Spatial aliasing is due to under-sampling, as discussed in Section 4.3.4.
Temporal aliasing is related to time intervals between images in a sequence of images. One of the most common examples of temporal aliasing is the “wagon
wheel” effect, in which wheels with spokes in a sequence of images (for example,
in a movie) appear to be rotating backwards, This is caused by the frame rate being
too low with respect to the speed of wheel rotation in the sequence.Our focus in this chapter is on spatial aliasing. The key concerns with spatial
aliasing in images are the introduction of artifacts such as jaggedness in line
features, spurious highlights, and the appearance of frequency patterns not present in the original image. The following example illustrates aliasing in images.m Suppose that we have an imaging system that is perfect, in the sense that it
is noiseless and produces an exact digital image of what it sees, but the number
of samples it can take is fixed at 96 < 96 pixels. If we use this system to digitize
checkerboard patterns, it will be able to resolve patterns that are up to
96 X 96 squares, in which the size of each square is 1 X 1 pixels. In this limiting case, each pixel in the resulting image will correspond to one square in the
pattern. We are interested in examining what happens when the detail (the
size of the checkerboard squares) is less than one camera pixel; that is, when
the imaging system is asked to digitize checkerboard patterns that have more
than 96 X 96 squares in the field of view,Figures 4.16(a) and (b) show the result of sampling checkerboards whose
squares are of size 16 and 6 pixels on the side, respectively. These results are as
expected, However, when the size of the squares is reduced to slightly less than
one camera pixel a severely aliased image results, as Fig. 4.16(c) shows. Finally,
reducing the size of the squares to slightly less than 0.5 pixels on the side yielded
the image in Fig. 4.16(d). In this case, the aliased result looks like a normal
checkerboard pattern. In fact, this image would result from sampling a checkerboard image whose squares were 12 pixels on the side. This last image is a good
reminder that aliasing can create results that may be quite misleading. *The effects of aliasing can be reduced by slightly defocusing the scene to be
digitized so that high frequencies are attenuated. As explained in Section 4.3.4,
anti-aliasing filtering has to be done at the “front-end,” before the image is
sampled. There are no such things as after-the-fact software anti-aliasing filters
that can be used to reduce the effects of aliasing caused by violations of the
sampling theorem. Most commercial digital image manipulation packages do
have a feature called “anti-aliasing.” However, as illustrated in Examples 4.7EXAMPLE 4.6:
Aliasing in
images,This example should not
be construed as being unrealistic. Sampling a
“perfect” scene under
noiseless, distoriion-free
conditions is common.
when converting computer
generated models and
vector drawings lo digital
images.
252Chapter 4 m Filtering in the Frequency Domain   x
FIGURE 4.16 Aliasing in images. In (a) and (b). the lengths of the sides of the squares
are 16 and 6 pixels, respectively, and aliasing is visually negligible. In (c) and (d). the
sides of the squares are 0.9174 and 0.4798 pixels, respectively, and the results show
significant aliasing. Note that (d) masquerades as a “normal” image. and 4.8, this term is related to blurring a digital image to reduce additional
aliasing artifacts caused by resampling. The term does not apply to reducing
aliasing in the original sampled image, A significant number of commercial
digital cameras have true anti-aliasing filtering built in, either in the lens or on
the surface of the sensor itself For this reason, it is difficult to illustrate aliasing using images obtained with such cameras.Image interpolation and resamplingAs in the 1-D case, perfect reconstruction of a band-limited image function
from a set of its samples requires 2-D convolution in the spatial domain with a
sinc function. As explained in Section 4.3.5, this theoretically perfect reconstruction requires interpolation using infinite summations which, in practice,
forces us to look for approximations. One of the most common applications of
2-D interpolation in image processing 1s in image resizing (zooming and
shrinking). Zooming may be viewed as over-sampling, while shrinking may be
viewed as under-sampling. The key difference between these two operations
and the sampling concepts discussed in previous sections is that zooming and
shrinking are applied to digital images.Interpolation was explained in Section 2.4.4. Our interest there was to illustrate the performance of nearest neighbor, bilinear, and bicubic interpolation.
In this section, we give some additional examples with a focus on sampling and
anti-aliasing issues. A special case of nearest neighbor interpolation that ties in
nicely with over-sampling is zooming by pixel replication. which is applicable
when we want to increase the sive of an iniuze an miteeer maimbar of limes. Par
4.8 ® Extension to Functions of Two Variables 253instance, to doubie the size of an image, we duplicate each column. This doubles the image size in the horizontal direction. Then. we duplicate each row of
the enlarged image to double the size in the vertical direction. The same procedure is used to enlarge the image any integer number of times. The intensitylevel assignment of each pixel is predetermined by the fact that new locations
are exact duplicates of old locations.
Image shrinking is done in a manner similar to zooming. Undet-sampling is
achieved by row-column deletion (¢,g., to shrink an image by one-half, we
delete every other row and column). We can use the zooming grid analogy in
Section 2.4.4 to visualize the concept of shrinking by a non-integer factor, except that we now expand the grid fo fit over the original image, do intensilylevel interpolation, and then shrink the grid back to its specified size. To reduce
aliasing, it is a good idea to blur an image slightly before shrinking it (we discuss The process of resan
frequency domain blurring in Section 4.8). An alternate technique is to super- Nee ane
sample the original scene and then reduce (resample) its size by row and co}- _ringisealled decimation
umn deletion. This can yield sharper results than with smoothing, but it clearly
Tequires access to the original scene. Clearly, if we have no access to the original
scene (as typically is the case in practice) super-sampling is not an option.@ The effects of aliasing generally are worsehed when the size of a digital EXAMPLE 47:
image is reduced, Figure 4.17(a) is an image purposely created to illustrate the
effects of aliasing (note the thinly-spaced parallel lines in all garments worn by
the subject). There are no objectionable artifacts in Fig. 4.17(a), indicating thatfiustration of
aliasing in
resampled images.    more blurred than (b). but aliasing is nui Jerjionable. (Oviginal image courtesy of the Signal
Compression Laboratory, University of Calero: :
254 Chapter 4 m Filtering in the Frequency DomainEXAMPLE 4.8:
Illustration of
jaggies in image
shrinking.the sampling rate used initially was sufficient to avoid visible aliasing. In
Fig. 4.17(b), the image was reduced to 50% of its original size using rowcolumn deletion. The effects of aliasing are quite visible in this image (see,
for example the areas around the subject's knees). The digital “equivalent”
of anti-aliasing filtering of continuous images is to attenuate the high frequencies of a digital image by smoothing it before resampling. Figure
4.17(c) shows the result of smoothing the image in Fig. 4.17(a) with a3 x 3
averaging filter (see Section 3.5) before reducing its size. The improvement
over Fig. 4.17(b) is evident. Images (b) and (c) were resized up to their original dimension by pixel replication to simplify comparisons. =When you work with images that have strong edge content, the effects of
aliasing are seen as block-like image components, called jaggies. The following
example illustrates this phenomenon,@ Figure 4.18(a) shows a 1024 x 1024 digital image of a computer-generated
scene in which aliasing is negligible. Figure 4.18(b) is the result of reducing
the size of (a) by 75% to 256 X 256 pixels using bilinear interpolation and
then using pixel replication to bring the image back to its original size in
order to make the effects of aliasing (jaggies in this case) more visible. As in
Example 4.7, the effects of aliasing can be made less objectionable by
smoothing the image before resampling. Figure 4.18(c) is the result of using a
5 x 5 averaging filter prior to reducing the size of the image. As this figure
shows, jaggies were reduced significantly. The size reduction and increase tu
the original size in Fig. 4.18(c) were done using the same approach used to
generate Fig. 4.18(b). ay weFIGURE 4.18 Illustration of jaggies. (a) A 1024 x 1024 digital image of a computer-generated scene with
negligible visible aliasing. (b} Result of reducing (a) to 23%. of its original size using bijinear interpolation.
(c) Resuit of blurring the image in (a) witha 5 « 5 averaging filter prior to resizing it lo 25% using bilinear
interpolation. {Original image courtesy of D. P. Mitchell, Mental Landscape. LLC.)
256 = Chapter 4 Filtering in the Frequency DomainFIGURE 4.20
Examples of the
moiré effect.
These are ink
drawings, not
digitized patterns,
Superimposing
one pattern on
the other is
a beat pattern that has frequencies not present in either of the original patterns, Note in particular the moiré effect produced by two patterns of dots, as
this is the effect of interest in the following discussion,
Color printing uses red, Newspapers and other printed materials make use of so called halftone
produce the se vioniy 088, which are black dots or ellipses whose sizes and various joining schemes
the eye of continuous are used to simulate gray tones. As a rule, the following numbers are typical:
color. newspapers are printed using 75 halftone dots per inch (dpi for short), magazines use 133 dpi, and high-quality brochures use 175 dpi. Figure 4.21 shows  equivalent
mathematically to
multiplying the
Patterns,  FIGURE 4,21 : 1, Bh eeA newspaper eo Soe? 2: =
image of size ‘ art te
246 X 168 pixels
sampled at 75 dpi
showing a moiré
pattern. The
moiré pattera in
this image is the
interference
pattern created
between the +45°
orientation of the
halftone dots and
the north-south
orientation of the
sampling grid
used to digitize
the image.
4.5 ® Extension to Functions of Two Variables 255@ In the previous two examples, we used pixel replication to zoom the small
resampled images. This is not a preferred approach in general, as Fig. 4.19 illustrates, Figure 4.19(a) shows a 1024 x 1024 zoomed image generated by
pixel replication from a 256 X 256 section out of the center of the image in
Fig. 4.18(a). Note the “blocky” edges. The zoomed image in Fig. 4.19(b) was
generated from the same 256 X 256 section, but using bilinear interpolation.
The edges in this result are considerably smoother. For example, the edges of
the bottle neck and the lange checkerboard squares are nol nearly as blocky
in (b) as they are in (a). eMoiré patternsBefore leaving this section, we examine another type of artifact, called moiré
patterns,’ that sometimes result from sampling scenes with periodic or nearly
periodic components. In optics, moiré patterns refer to beat patterns produced between two gratings of approximately equal spacing. These patterns
are a common everyday occurrence. We see them, for example, in overlapping
insect window screens and on the interference between TV raster lines and
striped materials. In digital image processing, the problem arises routinely
when scanning media print, such as newspapers and magazines, or in images
with periodic components whose spacing is comparable to the spacing between samples. it is important to note that moiré patterns are more general
than sampling artifacts, For instance, Fig, 4.20 shows the moiré effect using ink
drawings that have not been digitized. Separately, the patterns are clean and
void of interference. However, superimposing one pattern on the other creates abFIGURE 4.19 Image zooming, {a} A 1024 x 1024 digital image generated by pixcl
replication from a 256 x 256 image extracted from the middie of Fig. 4.18(a).
(b) Image generated using bi-linear interpolation, showing a significant reduction in
jaggies. ‘The term moiré is a French word (not the name of a person) that appears to lave originated with
weavers who first noticed interference patterns visible on sone fabrics: the tern is rested on the word
mohair, a cloth made from Angola goat hairs.EXAMPLE 4.9:
Illustration of
jJaggies in image
zooming.
258 Chopte 4 m Filtering in the Frequency DomainGiven the transform F(u, v), we can obtain f(x, y} by using the inverse discrete Fourier transform (IDFT):1 Nfay) = ms > Fu, ve? ura oy!) (45-16)u=0 vs0for x = 0,1,2,...,M -1 and y = 0,1,2,...,¥—1. Equations (4.5-15) and
(4.5-16) constitute the 2-D discrete Fourier transform pair. The rest of this
chapter is based on properties of these two equations and their use for image
filtering in the frequency domain.| 4.6 | Some Properties of the 2-D Discrete Fourier
TransformIn this section, we introduce several properties of the 2-D discrete Fourier
transform and its inverse.4.6.1 Relationships Between Spatial and Frequency IntervalsThe relationships between spatial sampling and the corresponding frequencydomain intervals are as explained in Section 4.4.2. Suppose that a continuous
function f(t, z) is sampled to form a digital image, f(x, y), consisting of
M X N samples taken in the f- and z-directions, respectively. Let AT and AZ
denote the separations between samples (see Fig. 4.14). Then, the separations
between the corresponding discrete, frequency domain variables are given byAu = MAT {4.6-1)
and
Av = _1_ (4.6-2}
°* NAZrespectively. Note that the separations between samples in the frequency domain are inversely proportional both to the spacing between spatial samples
and the number of samples.4.6.2 Translation and RotationIt can be shown by direct substitution into Eqs. (4.5-15) and (4.5-16) that
the Fourier transform pair satisfies the following translation properties
(Problem 4.16):Fx, pel ttori? 31%) em Flu — ug, ¥ — Wy) (4.6-3)
andP(X = Ko — Yo) > Fla py) P20 ME sQt’N) (4.6-4)
4.5 % Extension to Functions of Two Variableswhat happens when a newspaper image is sampled at 75 dpi. The sampling lattice (which is oriented vertically and horizontally) and dot patterns on the
newspaper image (oriented at +45‘) interact to create a uniform moiré pattern that makes the image look blotchy. (We discuss a technique in Section
4.10.2 for reducing moiré interference patterns.)As a related point of interest, Fig, 4.22 shows a newspaper image sampled at 400 dpi to avoid moiré effects. The enlargement of the region surrounding the subject's left eye illustrates how halftone dots are used to
create shades of gray. The dot size is inversely proportional to image intensity. In light areas, the dots are small or totally absent (see, for example, the
white part of the eye). In light gray areas, the dots are larger, as shown
below the eye. In darker areas, when dot size exceeds a specified value (typically 50%), dots are allowed to join along two specified directions to form
an interconnected mesh (see, for example, the left part of the eye). In some
cases the dots join afong only one direction, as in the top right area below
the eyebrow.4.5.5 The 2-D Discrete Fourier Transform and Its Inverse
A development similar to the material in Sectians 4.3 and 4.4 would yield the
following 2-D discrete Fourier transform (DFT):Af-T N-1F(u,v) = Ss > { (ac. yp) @F2m rt Hee)x=0 r=0(4.5-15)where f(x, y) is a digital image of size Mf & N. Asin the 1-D case, Eq. (4.5-15)
must be evaluated for values of the discrete variables 7 and v in the ranges
w= 0,1,2.....M@-—Landv = 0,1,2...., N-1.'   TAs mentioned in Section 4.4.1. keep in aiind Chat in this chapler we use (6 >) ama Ge ei to denote 2D
continuous spatial and [requency-domain variables. in the 2-19 discrete cose. we use Cra} fin spatialvariables and (u. &) for frequency-domain variables257Sometimes you will find
in the literature theVAN constant in front of
DFT instead of the
IDFT. Al times, the constan is. express
1/4 VMN and is inchided
in froat of the forward
and inverse transforms,
thus creating a more
symmetric pair. Any ol
these formulations is co recl. provided that you
are consistent as FIGURE 4.22A newspaper
image and an
enlargement
showing how
halftone dots are
arranged to
render shades of
gray.
4.6 # Some Properties of the 2-D Discrete Fourier Transform 259That is, multiplying f(x, y) by the exponential shown shifts the origin of the
DFT to (1, v9) and, conversely, multiplying F(u, v) by the negative of that
exponential shifts the origin of f(x. y) to (x9, yo). As we illustrate in
Example 4.13, translation has no effect on the magnitude (spectrum) of
F(u, v).Using the polar coordinatesx=rcos@ p=rsind vu=weose v= wsing
results in the following transform pair:
f(r,8 + 0) => F(a, ge + %) (4.6-5)which indicates that rotating f(x, y) by an angle 9 rotates F(u, v) by the same
angle. Conversely, rotating F(u, v) rotates f(x, y) by the same angle.4.6.3 Periodicity
As in the 1-D case, the 2-D Fourier transform and its inverse are infinitely periodic in the u and v directions; that is,Flu,v) = F(u + k\M,v) = Flu,u + kyN) = Flu + kyM,vu + k,N) (4.6-6)
and
f(xy) = fe + kM. y) = fa, y + kN) = fe + kiMyy + kN) (46-7)where k, and k, are integers.The periodicities of the transform and its inverse are important issues in
the implementation of DFT-based algorithms. Consider the 1-D spectrum in
Fig. 4.23(a). As explained in Section 4.4.1, the transform data in the interval
from 0 to M — 1 consists of two back-to-back half periods meeting at point
M/2. For display and filtering purposes, it is more convenient to have in this
interval a complete period of the transform in which the data are contiguous,
as in Fig. 4.23(b). It follows from Eq. (4.6-3) thatflxje tm) = Flu - uy)In other words, multiplying f(x) by the exponential term shown shifts the data
so that the origin, F(0), is located at ug. If we let uy = M/2, the exponential
term becomes e/** which is equal to (—1)* because x is an integer. In this case,fQx)C-1Y = Fe — M/2)That is, multiplying f(x) by (—1)* shifts the data so that F(0) is at the center of
the interval [0, M — 1), which corresponds to Fig. 4.23(b), as desired.In 2-D the situation is more difficult to graph, but the principle is the same,
as Fig. 4.23(c) shows. Instead of two half periods, there are now four quarter
periods meeting at the point (M@/2, N/2). The dashed rectangles correspond to
260 Chopter 4 # Filtering in the Frequency Domaina Fu)bedFIGURE 4.23 a Two back-to-back ateCentering the vit periods meet here. |
Fourier transform. : ‘ { :(a) A1-D DFT poeta | thet bate
mp umn Nm  showing an infinite qe -M/2 0 M/2-1 winumber of periods.(b) Shifted DFT Fluyobtained bymultiplying f(x)by (~1)* before oy Te Two back-to-back os
2 | ‘ periods meet here. :computing F(u). .
(c) A 2-D DFT :
showing an infinite
number of periods.
The solid area is
the M X N data
array, F(u, v),
obtained with Eq.
(4.5-15). This array
consists of four
quarter periods.
(d) A Shifted DFT
obtained by
multiplying f(x, y)
by (“17before computing
F(u,v). The data i
t
4
'
1
i
'
i
{
+
1N/2~N- 1-4! now contains one Flu.)complete, centered + _period, as in {b). u '
Four back-to-back_ periods meet here.
L 7 = Periods of the DFT.[] = @ « N data array, Fn, v).the infinite number of periods of the 2-D DFT. As in the 1-D case, visualization
is simplified if we shift the data so that F(0,0) is at (M/2, N/2). Letting
(ug, Ug) = (M/2, N/2) in Eq. (4.6-3) results in the expressionF(x, y(-1)**” > F(u — M/2,u - N/2) (4.6-8)Using this equation shifts the data so that F(0,0) is at the center of the
frequency rectangle defined by the intervals [0,M@ — 1] and [0,N — 1], as
desired. Figure 4.23(d) shows the result. We illustrate these concepts later in
this section as part of Example 4.11 and Fig. 4.24.
4.6 @ Some Properties of the 2-D Discrete Fourier Transform 2614.6.4 Symmetry Properties
An important resuit from functional analysis is that any real or complex function, w(x, y), can be expressed as the sum of an even and an odd part (each of
which can be real or complex):w(x, y) = w(x, y) + W(x, y) (4.6-9)
where the even and odd parts are defined aswx, y) = w(x y) + w(=% ~y) > RCE “y) (4.6-10a)andw(x, ¥) wey — Cnn) (4.6-10b)Substituting Eqs. (4.6-10a) and (4.6-10b) into Eq. (4.6-9) gives the identity
w(x, y) = w(x, y), thus proving the validity of the latter equation. It follows
from the preceding definitions thatw(x, ¥) = wx, ~y) (4.6-11a)
and that
welx, ¥) = —Wo(—X, ~y) (4.6-11b)
Even functions are said to be symmetric and odd functions are antisymmetric.
Because all indices in the DFT and IDFT are positive, when we talk about
symmetry (antisymmetry) we are referring to symmetry (antisymmetry) about
the center point of a sequence. In terms of Eq. (4.6-11), indices to the right of
the center point of a 1-D array are considered positive, and those to the left
are considered negative (similarly in 2-D). In our work, it is more convenient
to think only in terms of nonnegative indices, in which case the definitions ofevenness and oddness become:
wAx, y) = wlM — x, N— y) (4.6-12a)andw(x, ¥) = —w,(M — x, N~ y) (4.6-12b)where, as usual, M@ and N are the number of rows and columns of a 2-D array.
262 Chepter 4 m Filtering in the Frequency DomainTo convince yourself that
the samples of an odd
function sum (oe zero,
sketch one period of a
1-D sine wave about the
origin of any other interval spanning one period.EXAMPLE 4.10:
Even and odd
functions.We know from elementary mathematical analysis that the product of two
even or two odd functions is even, and that the product of an even and an
odd function is odd. In addition, the only way that a discrete function can be
odd is if all its samples sum to zero. These properties lead to the important
result thatM-i N-1L
py DY welx, yw(x. y) = 0 (4.6-13)
x=0 y=0for any two discrete even and odd functions w, and w,, In other words, because the argument of Eq. (4.6-13) is odd, the result of the summations is 0.
The functions can be real] or complex.@ Although evenness and oddness are visualized easily for continuous functions, these concepts are not as intuitive when dealing with discrete sequences.
The following illustrations will help clarify the preceding ideas. Consider the
1-D sequencef={fO fA) FR) FE}
={211 1}in which M = 4. To test for evenness, the condition f(x) = f(4 ~ x) must be
satisfied; that is, we require thatFO) = f(4), f(2) = f2), FO) = FG), FB) = FO)Because f(4) is outside the range being examined, and it can be any value,
the value of £(0) is immaterial in the test for evenness. We see that the next
three conditions are satisfied by the values in the array, so the sequence is
even. In fact, we conclude that any 4-point even sequence has to have theform
{a b ¢ b}That is, only the second and last points must be equal in a 4-point even se
quence.
An odd sequence has the interesting property that tts first term, 29(0, 0), is
always 0, a fact that follows directly from Eq. (4.6-10b), Consider the 1-D se
quence
g = {g(0) gM) g(2) 9(3)}
={0 -1 0 1}
4.6 w Some Properties of the 2-D Discrete Fourier Transform 263We easily can confirm that this is an odd sequence by noting that the terms in
the sequence satisfy the condition g(x) = —g(4- x). For example,
g(i) = —g(3). Any 4-point odd sequence has the form{0 -b 0 b}That is, when M is an even number, a 1-D odd sequence has the property that
the points at locations 0 and M/2 always are zero. When M is odd, the first
term still has to be 0, but the remaining terms form pairs with equal value but
Opposite sign.The preceding discussion indicates that evenness and oddness of sequences
depend also on the length of the sequences. For example, we already showed
that the sequence {0 -1 0 1} is odd. However, the sequence
{O ~1 0 1 O} is neither odd nor even, although the “basic” structure appears to be odd. This is an important issue in interpreting DFT results. We
show later in this section that the DFTs of even and odd functions have some
very important characteristics. Thus, it often is the case that understanding
when a function is odd or even plays a key role in our ability to interpret image
results based on DFTs.The same basic considerations hold in 2-D. For example, the 6 x 6 2-D sequence00 0000
00 0 000 ,
00-101 0
00 -2 020
00 -1 01 0
000000is odd. However, adding another row and column of Os would give a result
that is neither odd nor even. Note that the inner structure of this array is a
Sobel mask, as discussed in Section 3.6.4. We return to this mask in
Example 4.15, esArmed with the preceding concepts, we can establish a number of important
symmetry properties of the DFT and its inverse. A property used frequently is
that the Fourier transform of a real function, f(x, y), is conjugate symmetric:F'(u, v) = F(-u, -v) (4.6-14)
If f(Q, y) is anaginary, its Fourier transform is conjugate antisymmetric:F'(—u, —v) = —F(u, v). The proof of Eq. (4.6-14) is as follows:M-1 N=1 ; .
F'(u,v) = > > f(x, y) er 2m ux/M + x1)y=0 y=0As an exercise, you
should use Eq. (4.6-12b)to convince yourself that
this 2-D sequence is odd.Conjugale symmetry also
is called hermigian symmetry. The term
antibermitian is uscd
sometimes to refer 10.
conjugate antisymmetry.
264  Chopter 4 %: Filtering in the Frequency DomainTABLE 4.1Some symmetry
properties of the
2-D DFT and its
inverse. Rite, v)
and /(#, v) are the
real and imaginary
parts of F(t, v).
respectively. The
term complex
indicates that a
function has
nonzero real and
imaginary parts.where the third step follows from the fact that f(x, y) is real, A similar approach can be used to prove the conjugate antisymmetry exhibited by theM-1 N-1= > by h(x, yes 2m es Mt ey/N)x=0 y=0M-1 N= > > f(x, yyeF 2a [mt] 7M + [a] vn)
xv=0 y=1A-t
=O= F(-u, —v)transform of imaginary functions.Table 4.1 lists symmetries and related properties of the DFT that are useful
in digital image processing. Recall that the double arrows indicate Fourier
transform pairs; that is, for any row in the table, the properties on the right are
satisfied by the Fourier transform of the function having the properties listed
on the left, and vice versa. For example, entry 5 reads: The DFT of a real
function f(x, y), in which (x, y) is replaced by (—x,—y), is F’(u, v), whereF(u,v), the DFT of f(r, y), is acomplex function, and vice versa. Spatial Domain*Frequency Domain? 1)
2)
3)
4)
5)
6}
7)
8)
9)
10}
1)
12)
13) ‘Recall chaty. v.u,and ¢ are discrete (integer) vaciables. with x and « in the range (0M — 1. and y.and
vin the range (0,.N — |]. To say that a complex function is even means thal its rcal wed imaginary parts
are even. and similarly for an odd complex fuactionf(x. ¥) realf(t.) imaginary
F(x.) realF(x») imaginary
f(-—x. —y) real
f(~x. -y) complex
f(y) complex
f(x,y) real and evenf(x. y}real and oddf(x. ¥) imaginary and even
f(e.¥) imaginary and odd
f(x. y) complex and evenf(x,y) complex and oddPerret er tesrras=F'(it,v) = P(-a, -v)
F'(-u, ~v) = —F(u, v)
R(u.v) even; Mu, v) odd
R(u, v) odd; (u,v) even
F'(u, v) complexF{-u, ~e) complex
F’(-u — v) complexF (u,v) real and even
F(u.v) imaginary and odd
F (ui, ¥) imaginary and even
Fu. v) real and oddF (u,v) complex and evenF(u.v) complex and odd
266  Chepter 4 @ Filtering in the Frequency DomainNote that we are not
making a change of
variable here. We are
evaluating the DFT of
{(~x, —y)}. so we simply
insert this function into
the equation, as we would
any other function,M-1 NI
Fur) = SD bfelx, ye Prewmronny
x50 y=
M=1 N-1
= S fea, yyje 2a Meatwy/ )
x=0 y=0
M-1 N-1
= [even][even — jodd][even — jodd]
x=0 y=0
M-1-N-1
= > [even][even- even — 2jeven- odd — odd - odd]
x=0 y=0
M-1 N-1 M-1N-1
= [even- even] - 27 ) & [even- odd]
x=0 y=0 x=0 y=0
Mal N- S BS leven- even]
*=0 y=0
= realThe fourth step follows from Euler’s equation and the fact that the cos and sin
are even and odd functions, respectively. We also know from property 8 that, in
addition to being real, fis an even function. The only term in the penultimate
line containing imaginary components is the second term, which is 0 according
to Eq. (4.6-14). Thus, if fis real and even then F is real. As noted earlier, F is
also even because fis real. This concludes the proof.Finally, we prove the validity of property 6. From the definition of the DFT,M=1 N-1
Mex Mb = DY Tix, —ye Peewee
x=0 y=dBecause of periodicity, f(—x, —y) = f(M — x,N - y). If we now define
m= M-—- xandn= N — y,then=zi
f(m n) @ f2mtul Moi] /M rv Nn] /N)
0 ~MlM=1
3{f(-x, -y)} = x
(To convince yourself that the summations are correct, try a 1-D transform
and expand a few terms by hand.) Because exp[—j2m(integer)] = 1, it
follows that
4.6 © Some Properties of the 2-D Discrete Fourier Transform 265% With reference to the even and odd concepts discussed earlier and illustrated in Example 4.10, the following 1-D sequences and their transforms are
short examples of the properties listed in Table 4.1. The numbers in parentheses on the right are the individual elements of F(«), and similarly for f(a) in
the last two properties.Property fix) F(a}
3 Ni 2 3 4} | {(10)(-2 + 2j)(-2) (-2 - 2p}
4 H{1 2 3 AY > ((25/ (5 ~ 5f)(-.5/) (- 5 - .5/}
8 {2 1 1 ty = {65)0)0)0)}
9 {0 —J 0 1} > 4 (0) (2)) (0) (-2))}
10 H2 0 14) @ (5) ()G)(}
u j{O -1 0 1} > 440) (2) (0) (2))
12 {(4 + 4j) (3 + 27} (0 + 2j)B + 2/)} > {CIO + 1DJ} (4 + 2) (-2 + 2A + 2y)}
13 {40 + O/C + TPO + OA) K-1 — A} > (OH + Of) (2 — 24) 0 + OF) (-2 + WYFor example, in property 3 we see that a real function with clements
{1 2 3 4} has Fourier transform whose real part, {10 -2 -2 —2}, is
even and whose imaginary part, {0 2 0 —2}, is odd. Property 8 tells us that
a real even function has a transform that is real and even also. Property 12
shows that an even complex function has a transform that is also complex and
even. The other property examples are analyzed in a similar manner. eS® In this example, we prove several of the properties in Table 4.1 to develop
familiarity with manipulating these important properties, and to establish a
basis for solving some of the problems at the end of the chapter, We prove only
the properties on the right given the properties on the left. The converse is
proved in a manner similar to the proofs we give here.Consider. property 3, which reads: If f(x, y) is a real function. the real part of
its DFT is even and the odd part is odd; similarly, if a DFT has real and
imaginary parts that are even and odd, respectively, then its IDFT is a real
function. We prove this property formally as follows. F(t, v) is complex in
general, so it can be expressed as the sum of a real and an imaginary part:
F(u,v) = R(u,v) + jl(u,v). Then, F(u. v) = RQ) ~ ji (u,v). Also,
F(-u, ~v) = R(-u, ~v) + j/{-u, ~v). But, as proved earlier. if f(x, y) is real
then F’(u. v) = F(—u, —v), which. based on the preceding two equations, means
that R(u. v) = R(~a, -v) and /(,v) = —H{—-u, —v). In view of Eqs. (4.6-] 1a)
and (4.6-11b), this proves that 2 is an even function and / is an odd function.Next, we prove property 8. If f(x, y) is real we know from property 3 that
the real part of F(u, v) is even, so to prove property 8 all we have to do is show
that if f(x, y) is real and even then the imaginary part of F(u. v) is 0 (Le.. Fis
real). The steps are as follows:Af-1 N-1
F(u. v= > > f(x, a«= y=twhich we can write asEXAMPLE 4.11;
1-D illustrations
of properties from
Table 4.1.EXAMPLE 4.12;
Proving several
symmetry
properties of the
DFT from Tabie
4).
4.6 % Some Properties of the 2-D Discrete Fourier Transform 267z1
flim, nef 2m uma + on/N)
0HtM~1
=
m=0= F(-u, —v)S{f(-x, -y)}aThis concludes the proof. ie4.6.5 Fourier Spectrum and Phase AngleBecause the 2-D DFT is complex in general, it can be expressed in polar
form:F(u,v) = [Flu, ve (4.6-15)where the magnitude 12
[F(u,v)| = [R'(u,v) + Plu, »)] (4.6-16)
is called the Fourier (or frequency) spectrum, and
_ I(t, v}
o(u, v) = seta) | (4.6-17)is the phase angle. Recall from the discussion in Section 4.2.1 that the arctan
must be computed using a four-quadrant arctangent, such as MATLAB's
atan2(Imag, Real) function.Finally, the power spectrum is defined asP(u, v) = [Flu vy
(4.6-18)
= R(u,v) + Pu, v)As before, R and / are the real and imaginary parts of F(, ») and al] compu
tations are carried out for the discrete variables uv = 0.1,2,..., M - 1 and
v = 0,1,2,...,M — 1. Therefore, |F(u, v)|, o(u, v), and P(u, v) are arrays of
size M XN.The Fourier transform of a real function is conjugate symmetric (Eq. (4.6-14)],
which implies that the spectrum has even symmetry about the origin:[F(u, vl = |F(-u. -2)] (4.6-19)
The phase angle exhibits the following odd symmetry about the origin:
Hu. v) = ~b(-u, ~v} (4.6-20)
It follows from Eq. (4.5-15) that
M-1 N-1
F0,0)= Y Sfx)220 =O
268 Chapter 4 m Filtering in the Frequency Domainab
edFIGURE 4.24(a) Image.(b) Spectrum
showing bright spots
in the four corners.
(c) Centered
spectrum. (d) Result
showing increased
detail after a log
transformation. The
zero crossings of the
spectrum are closer in
the vertical direction
because the rectangle
in (a) is longer in that
direction. The
coordinate
convention used
throughout the book
places the origin of
the spatial and
frequency domains at
the top left.EXAMPLE 4.13:
The 2-D Fourier
spectrum of a
simple function,| }
i }awhich indicates that the zero-frequency term is proportional to the average
value of f(x, vy). That is,{MI oI
F(0, 0) = MN flay)
ww %, >
= MN F(x, ¥) (4.6.21)
where f denotes the average value of f Then.
PROD = MNP CY, vl (40-22)Because the preportionalily consiant AgN usually iy farpe. : 2 CU. OFF Uepicaléy is
the largest component of the spectrum by a factor that can he several orders of
magnitude larger than other terms. Because freqnency components mand 7
are zero at the orivin, /'(0, 1) sometimes is called the do component of the
transform. This terminology is from electrical snginecring. where “de :
direct current (ic. current of ere freaueniy }        whose values were scaled to ihy range
origins of both the spate:
are apparent in i
4.6 & Some Properties of the 2-D Discrete Fourier Transformtransform contains the highest values (and thus appears brighter in the image).
However, note that the four corners of the spectrum contain similarly high
values. The reason is the periodicity property discussed in the previous section.
To center the spectrum, we simply multiply the image in (a) by (-1)*"" before
computing the DFT, as indicated in Eq. (4,6-8). Figure 4.22(c) shows the result,
which clearly is much easier to visualize (note the symmetry about the center
point). Because the de term dominates the values of the spectrum, the dynamic
range of other intensities in the displayed image are compressed. To bring out
those details, we perform 4 log transformation, as described in Section 3.2.2.
Figure 4.24(d) shows the display of (1 + log|# (a, «)|). The increased rendition
of detail is evident. Most spectra shown in this and subsequent chapters are
scaled in this manner.It follows from Eqs. (4.6-4) and (4.6-5) that the spectrum is insensitive Lo
image translation (the absolute value of the exponential term is 1), but it rotates
by the same angle of a rotated image. Figure 4.25 illustrates these properties.
The spectrum in Fig. 4.25(b) is identical to the spectrum in Fig, 4.24(d), Clearly,
the images in Figs. 4.24(a) and 4.25(a) are different, so if their Fourier spectra
are the same then, based on Eq, (4,6-15), their phase angles must be different.
Figure 4.26 confirms this. Figures 4.26(a) and (b) are the phase angle arrays
(shown as images) of the DFTs of Figs. 4.24(a) and 4.25(a). Note the lack of
similarity between the phase images, in spite of the fact that the only differences
between their corresponding images is simple translation, In general. visual
analysis of phase angle images vields lite intuitive information. For instance,
due to its 45° orientation, one would expect intuitively that the phase angie in  Re errrrrrrrrrrprseeremnnnnrmaniorsnnerncntios etc esc TT Ne ne ne EN269ab
edFIGURE 4.25(a) The rectangle
in Fig, 4,24{a)
translated,and (b) the
corresponding
spectrum.(c) Rotated
rectangle,and (d) the
corresponding
spectrum. The
specirum
corresponding 10
the translated
rectangle is
identical to the
spectrum
corresponding to
the original imagein Pig 4. 24€a5
270 = Chapter 4 # Filtering in the Frequency DomainEXAMPLE 4.14:
Fartherilustration of the
properties of the
Fourier spectrum
and phase angle.   abeFIGURE 4.26 Phase angle array corresponding (a) to the image of the centered rectangle
in Fig, 4.24(a). (b} lo the translated image in Fig. 4.25(a), and {c) to the rotated image in
Fig, 4.25(c).Fig. 4.26(a) should correspond to the rotated image in Fig. 4.25(c), rather than to
the image in Fig. 4.24{a). In fact. as Fig. 4.26{c) shows, the phase angle of the rotated image has a strong orientation that is much less than 45°. %The components of the spectrum of the DFT determine the amplitudes of
the sinusoids that combine to form the resulting image. Al any given frequency in the DFT of an image, a large amplitude implies a greater prominence of
a sinusoid of that frequency in the image. Conversely, a small amplitude implies that less of that sinusoid is present in the image. Although. as Fig. 4.26
shows, the contribution of the phase components is less intuilive, it is just as
important. The phase is a measure of displacement of the various sinusoids
with respect to their origin. Thus, while the magnitude of the 2-D DET is an
array whose components determine the intensities in the image, the corresponding phase is an array of angles that carry much of the information about
where discernable objects are located in the image. The following example
clarifies these concepts further.i Figure 4.27(b) is the phase angle of the DFT of Pig. 4.27(a). There is no detai] in this array that would lead us by visual analysis to associate it with fcatures in its corresponding image (not even the symmetry of the phase angle is
visible}. However, the importance of the phase in determining shape characteristics is evident in Fig. 4.27(c), which was obtained by computing the inverse
DFT of Eq. (4.6-15) using only phase information (i.¢., with |F(u, »)! = 1 in
the equation). Although the intensity informatio has been lost (remember.
that information is carried by the spectrum) the key shape features in this
image are unmistakably from Fig. 4.27(a).Figure 4.27(d) was obtained using only the spectrum in Eq. (4.6-15) and cuin
puting the inverse DFT. This means sctling the exponential term to 1, which in
turn implies setting the phase angle to 0. The result is not unexpected. It contains
only intensity information, with the de term being the most domumaint. Theis ts
no shape information in the image because the phase was set 10 zero
4.6 ® Some Properties of the 2-D Discrete Fourier Transform abedetFIGURE 4.27 (a) Woman. (b) Phase angle. (c) Woman reconstructed using only the
phase angle. (¢) Woman reconstructed using only the spectrum. (¢) Reconstruction
using the phase angte corresponding to the woman and the spectrum corresponding to
the rectangle in Fig. 4.24(a). (f} Reconstruction using the phase, of the rectangle and the
spectrum of the woman.Finally, Figs. 4.27(e) and (f) show yet again the dominance of the phase in determining the feature content of an image. Figure 4.27(€) was obtained by computing the IDFT of Eq. (4.6-15) using the spectrum of the rectangle in Fig. 4.24(a)
and the phase angle corresponding to the woman. The shape of the woman
clearly dominates this result. Conversely, the rectangle dominates Fig. 4.27(f),
which was computed using the spectrum of the woman and the phase angle of
the rectangle. 5S4.6.4 The 2-D Convolution Theorem
Extending Eq, (4.4-19) to two variables results in the following expression for
2-D circular convolution:Mol NwiT(x, yw ACY} > Dy Pathe moyen a) {4.6-23)for x = 0,1,2,....M~J and vo a2... N ei. As in Eq. (44-10),
Eq. (4.6-23) gives one period of a 2-13 periodic sequence. Vhe 2-1 convolution
theorem is given by the expressions Peal yh de ita wo Pre cb ae ay id. 2and, conversely,271
4.6 m Some Properties of the 2-D Discrete Fourier Transform 273f(myAim)mone
0 200 400
A(m)  G0 200 400
h(~mt) 
 ) 200 400 0 200 400hy > m)       @ 200 400 " 0 200 400
fxy* g(x) fQx)* gl)
1200
600
x x
0 200 400 600 800 0 200 400—! Range of ~
Fourier transform
computationtwo transforms, and then computed the inverse DFT, we would have obtained
the erroneous 400-point segment of the convolution shown in Fig. 4.28(j).
Fortunately, the solution to the wraparound error problem is simple. Consider
two functions, f(x) and A(x) composed of A and B samples, respectively. It can be
shown (Brigham [1988}]) that if we append zeros to both functions so that they
have the same length, denoted by P, then wraparound is avoided by choosingP2A+B~1 (4.6-26)In our example, each function has 400 points, so the minimum value we could
use is P = 799, which implies that we would append 399 zeros (o the trailing
edge of each function. This process is called zero padding. As an exercise, youonagre
See reFIGURE 4.28 Left
column;
convolution of
two discrete
functions
obtained using the
approach
discussed in
Section 3.4.2, The
result in (e) is
correct. Right
column:
Convolution of
the same
functions, bul
taking into
account the
periodicity
implied by the
DFT. Note in (j)
how data from
adjacent periads
produce
wraparound error,
yielding an
incorrect
convolution
result. To obtain
the correct result,
function padding
must be used.‘The zeros could be
appended also to the
beginning of the func
tions, ar they could be
divided between the
beginning aad end of the
functions. It is simpler
lo append them atthe
end
272 © Cuopter 4 # Filtering in the Frequency Domain
F(x, yx, y) © Flu, v) & Hu, 2») (46-25)where F and H are obtained using Eq. (4.5-15) and, as before, the double
arrow is used to indicate that the left and right sides of the expressions constitute a Fourier transform pair. Our interest in the remainder of this chapter is in
Eq. (4.6-24), which states that the inverse DFT of the product F(u, v)H(u, v)
yields f(x, y) # A(x, y), the 2-D spatial convolution of f and A. Similarly, the
DFT of the spatial convolution yields the product of the transforms in the frequency domain. Equation (4.6-24) is the foundation of linear filtering and, as
explained in Section 4.7, is the basis for all the filtering techniques discussed in
this chapter.Because we are dealing here with discrete quantities, computation of the
ee oem ways Fourier transforms is carried out with a DFT algorithm. If we elect to compute
Section 411. the spatial convolution using the IDFT of the product of the two transforms,then the periodicity issues discussed in Section 4.6.3 must be taken into account, We give a 1-D example of this and then extend the conclusions to two
variables. The left column of Fig. 4.28 implements convolution of two functions,
fand A, using the 1-D equivalent of Eq. (3.4-2) which, because the two functions are of same size, is written as39
f(x) kAC) = DY POACe ~ m)neo
This equation is identical to Eq. (4.4-10), but the requirement on the displacement x is that it be sufficiently large to cause the flipped (rotated) version of #
to slide completely past f In other words, the procedure consists of (1) mirroring # about the origin (i.¢., rotating it by 180°) [Fig. 4.28(c)]. (2) translating the
mirrored function by an amount « [Fig. 4.28(d)], and (3) for each value x of
translation, computing the entire sum of products in the right side of the preceding equation. In terms of Fig. 4.28 this means multiplying the function in
Fig. 4.28(a) by the function in Fig. 4.28(d) for each value of x.The displacement
x tanges over all values required to completely slide # across f. Figure 4.28(e)
shows the convolution of these two functions. Note that convolution is a function of the displacement variable, x, and that the range of x required in this example to completely slide / past fis from 0 to 799.If we use the DFT and the convolution theorem to obtain the same result as
in the left column of Fig, 4.28, we must take into account the periodicity inherent in the expression for the DFT. This is equivalent to convolving the two periodic functions in Figs. 4.28(f) and (g). The convolution procedure is the same
as we just discussed, but the two functions now a:t periodic. Proceeding with
these two functions as in the previous paragraph would yield the result in
Fig. 4.28(j) which obviously is incorrect. Because we are convolviig two periodic functions, the convolution itself is periodic. The closeness of the periods in
Fig. 4.28 is such that they interfere with each other to cause what is commonly
referred to as wraparound error. According to the convalution theorem, if we
had computed the DFT of the two 400-point functions, f and h, multiplied the
274 Chepter 4 @ Filtering in the Frequency Domainshould convince yourself that if the periods of the functions in Figs. 4.28(f) and
(g) were lengthened by appending to each period at least 399 zeros, the result
would be a periodic convolution in which each period is identical to the correct
result in Fig. 4.28(e). Using the DFT via the convolution theorem would result
in a 799-point spatial function identical to Fig. 4.28(¢). The conclusion, then, is
that to obtain the same convolution result between the “straight” representation of the convolution equation approach in Chapter 3, and the DFT approach, functions in the latter must be padded prior to computing their
transforms.Visualizing a similar example in 2-D would be more difficult, but we would
arrive at the same conclusion regarding wraparound error and the need for appending zeros to the functions. Let f(x, y) and A(x, y) be two image arrays of
sizes A X BandC x D pixels, respectively. Wraparound error in their circular
convolution can be avoided by padding these functions with zeros, as follows:en a ee
and
with
P2A+C-I1 (4.6-29)
and
Q2B+D-1 (4.6-30)The resulting padded images are of size P < Q. If both arrays are of the same
size, M x N. then we require thatP=2M-1 (4.6-31)and
QO22N-1 (4.6-32)We give an example in Section 4.7.2 showing the effects of wraparound error
on images. As rule, DFT algorithms tend to execute faster with arrays of even
size, so it is good practice to select P and Q as the smailest even integers that
satisfy the preceding equations. if the two arrays are of the same size, this
means that P and Q are selected as twice the array size.The two functions in Figs. 4,28(a) and (6) conveniently become zero before
the end of the sampling interval. If one or both of the functions were not zero at
4.6 @ Some Properties of the 2-D Discrete Fourier Transform 275the end of the interval, then a discontinuity would be created when zeros were
appended to the function to eliminate wraparound error. This is analogous to
multiplying a function by a box, which in the frequency domain would imply
convolution of the original transform with a sinc function (see Example 4.1).
This, in turn, would create so-called frequency leakage, caused by the highfrequency components of the sinc function. Leakage produces a blocky effect
on images. Although leakage never can be totally eliminated. it can be reduced
significantly by multiplying the sampled function by another function that tapers smoothly to near zero at both ends of the sampled record to dampen the
sharp transitions (and thus the high frequency components) of the box. This approach, called windowing or apodizing, is an important consideration when fidelity in image reconstruction (as in high-definition graphics) is desired. If you
are faced with the need for windowing, a good approach is to use a 2-D Gaussian
function (see Section 4.8.3). One advantage of this function is that its Fourier
transform is Gaussian also, thus producing low leakage.4.6.7, Summary of 2-D Discrete Fourier Transform PropertiesTable 4.2 summarizes the principal DFT definitions introduced in this chapter.
Separability is discussed in Section 4.11.1 and obtaining the inverse using a
forward transform algorithm is discussed in Section 4.11.2. Correlation is discussed in Chapter 12,  Name Expression(s}
1) Discrete Fourier M-1N~i
transform (DFT) F(u,v) = > D fx phe Fmtix/M + ov/N)
x=0 y=0of f(x, y)2) Inverse discrete
Fourier transform
(IDFT) of F(t, v)} M-)N-} ° At-si9/N
FY) = Fy DD Fle year aonvym)u=0 2=0
3) Polar representation Pu, v) = |F(a, ver”)|Fla,»] = [R(u, v) + Pu, av]4) Spectrum
R= Real{F); 7 = Imag(F)
5) Phase angle (u,v) = wn] 36) Power spectrum Plu, v) = |F(u, v)|?1 M~DN<d i
Fa.) = 5m »» S fy = jan FO.)7) Average value   (Continued)A simple apodizing function is a triangle, centered on the data record,
which tapers to 0 at both
ends of the record. This is
called the Bartlett window. Other common win
dows are the Hamming
and the Hann windows.
We can even use a
Gaussian function. We
return to the issue of
windowing in Section
5.11.5,TABLE 4.2
Summary of DFT
definitions and
corresponding
expressions.
276 Chapter 4 m Filtering in the Frequency DomainTABLE 4.2
(Continued)TABLE 4.3
Summary of DFT
pairs, The closedform expressions
in 12 and 13 are
valid only for
continuous
variables. They
can be used with
discrete variables
by sampling the
closed-form,
continuous
expressions.8) Periodicity (k, and
k> are integers)9) Convolution10) Correlation11) Separability12) Obtaining the inverse Fourier transform
using a forwardtransform algorithm.Expression(s)Flu, 0) = Flu + kM, v) = Fu, v + kN)
= Flu + kM, v + kN)Fx, y) = f(x + kM, y) = f(y + kN)
= f(x + kM, y + kN)
M-1N-1Fos yeh) = SS Sem mya = my ~ 1)M-1N=1
fx. yrh y= DS Sf omnpyatx + my +n)m=0 n=0
The 2-D DFT can be computed by computing 1-D
DFT transforms along the rows (columns) of the
image, followed by 1-D transforms along the columns
(rows) of the result. See Section 4.11.1.Mri N=)MNf"(x, y) = > SF(u, we Amst + ey/N)a=0 190 .
This equation indicates that inputting F"(u, v) into analgorithm that computes the forward transform
(right side of above equation) yields MNf'(x, y).
Taking the complex conjugate and dividing by MN
gives the desired inverse. See Section 4.11.2.   Table 4.3 summarizes some important DFT pairs. Although our focus is on
discrete functions, the last two entries in the table are Fourier transform pairs
that can be derived only for continuous variables (note the use of continuous
variable notation). We include them here because, with proper interpretation,
they are quite useful in digital image processing. The differentiation pair can  Name DFT Pairs1) Symmetry See Table 4.1properties
2) Linearity afi(x, ¥) + bfo(x, yp)  aFi(u, v) + bFo(u, v)
3) Translation F(x, y)e Pao +09) o> F(a — uy. v — )(general) FUE — Xo.» — yo) o> Flu, pe P2mue/M + uy!)
4) Translation f(t, YM -1P*? Fu — M/2,0 ~ N/2)to center of f(x ~ M/2,y — N/2) > F(uw)(-1)4*”the frequencyrectangle,(M2, N?2)
5) Rotation FUG + By) = Fla, g + 6)=rcos@ y=rsiné w=weosp v=wasing
! 6) Convolution fy) AQ, v) > Flu, v)H (ut, v)theorem‘ f(x, y)h(x, y) = Flu, v) ® H(u, v)  (Continued)
4,7 @ The Basics of Filtering in the Frequency Domain  Name DFT Pairs
7) Correlation f(x, yt hts, ye F'(u, v) Hu, v)
theorem‘ f(x, yA, y) & Flu, v) & H(u, v)
8) Discrete unit d(x, y)e>
impulsesin(vrua) sin(avb) grimua-r0b)9) Rectangle rect{a, b] <= ab (rua) (arvb)10) Sine sin(2aupx + 2rupy) =>
1
15 [8lu + Mug» + Nop) ~ 8(u— Mu, v— Noy)
11) Cosine cos(2riiyx + 2arupy) =>[atu + Mu, v+ Nvp) + 8(u~ Muy, v- Nr)The following Fourier transform pairs are derivable only for continuous variables,
denoted as before by ¢ and z for spatial variables and by y and v for frequency
variables. These results can be used for DFT work by sampling the continuous forms.m 4 n .
12) Differentiation (2) (2) f(t, 2) > (j2ap)"j2av)"F(u, v)
(The expressions on the right afGand aft 2)
assume that am (2may" F(a»); Tan (2mvy"F(u. ¥)
f(z, +00) = 0.)13) Gaussian Admate MOE + oy Ae +2" (A is a constant)  * Assumes that the functions have been extended by zero padding. Convolution and correlation are associative, commutative, and distributive,be used to derive the frequency-domain equivalent of the Laplacian defined in
Eq. (3.6-3) (Problem 4.26). The Gaussian pair is discussed in Section 4.7.4.
Tables 4.1 through 4.3 provide a summary of properties useful when working
with the DFT. Many of these properties are key elements in the development of
the material in the rest of this chapter, and some are used in subsequent chapters.The Basics of Filtering in the Frequency DomainIn this section, we lay the groundwork for all the filtering techniques discussed
in the remainder of the chapter.4.7.1 Additional Characteristics of the Frequency DomainWe begin by observing in Eq. (4.5-15) that each term of F(u, v) contains all values of f(x, y), modified by the values of the exponential terms. Thus, with the
exception of trivial cases. it usually is impossible to make direct associations between specific components of an image and its transform. However, some general statements can be made about the relationship between the frequencyTABLE 4.3
(Continued)277
278 Chapter 4 m Filtering in the Frequency Domaincomponents of the Fourier transform and spatial features of an image. For
instance, because frequency is directly related to spatial rates of change, it is not
difficult intuitively to associate frequencies in the Fourier transform with patterns of intensity variations in an image. We showed in Section 4.6.5 that the
slowest varying frequency component (u = v = 0) is proportional to the average intensity of an image. As we move away from the origin of the transform,
the low frequencies correspond to the slowly varying intensity components of
an image. In an image of a room, for example, these might correspond to
smooth intensity variations on the walls und floor. As we move further away
from the origin, the higher frequencies begin to correspond to faster and faster
intensity changes in the image. These are the edges of objects and other camponents of an image characterized by abrupt changes in intensity.Filtering techniques in the frequency domain are based on modifying the
Fourier transform to achieve a specific objective and then computing the inverse DFT to get us back to the image domain, as introduced in Section
2.6.7. It follows from Eq. (4.6-15) that the two components of the transform
to which we have access are (he transform magnitude (spectrum) and the
phase angle. Section 4.6.5 covered the basic properties of these two components of the transform. We learned there that visual analysis of the phase
component generally is nat very useful. The spectrum, however, provides
some useful guidelines as to gross characteristics of the image from which
the spectrum was generated. For example, consider Fig. 4.29{a), which is a
scanning electron microscope image of an integrated circuit, magnified approximately 2500 times. Aside from the interesting construction of the device itself, we note two principal features: strong edges that run
approximately at +45° and two white. oxide protrusions resulting from
thermally-induced failure. The Fourier spectrum in Fig, 4.29(b) shows prominent
components along the +45° directions that correspond to the edges just
mentioned. Looking carefully along the vertical axis, we see a vertical component abFIGURE 4.29 (a) SEM image of a damaged integrated circuit. (b) Fourier spectrum of
(a). (Original image courtesy of Dr. LM. Hudak, Brockhouse institute for Materials
Research, McMaster University. Hamilton, Ontario, Canada}
4.7 # The Basics of Filtering in the Frequency Domain 279that is off-axis slightly to the left. This component was caused by the edges of
the oxide protrusions. Note how the angle of the frequency component with
Tespect to the vertical axis corresponds to the inclination (with respect to the
horizontal axis) of the long white element, and note also the zeros in the vertical frequency component, corresponding to the narrow vertical span of the
oxide protrusions.These are typical of the types of associations that can be made in genera)
between the frequency and spatial domains. As we show later in this chapter.
even these types of gross associations, coupled with the relationships mentioned previously between frequency content and rate of change of intensity
levels in an image, can lead to some very useful results. In the next section,
we show the effects of modifying various frequency ranges in the transform
of Fig. 4.29(a).4.7.2 Frequency Domain Filtering FundamentalsFiltering in the frequency domain consists of modifying the Fourier transform
of an image and then computing the inverse transform to obtain the processed
result. Thus, given a digital image, f(x, y), of size M * N, the basic filtering
equation in which we are interested has the form:a(x, y) = SA, o) Fe, v)] (4.7-1)where 3”! is the IDFT, F(u, v) is the DFT of the input image, f(x, y), H(u, v)
is a filter function (also called simply the filter, or the filter transfer function),
and g(x, y) is the filtered (output) image. Functions F, #/, and g are arrays of
size M X N, the same as the input image. The product H(u, v)F(u, v) is
formed using array multiplication, as defined in Section 2.6.1. The filter function modifies the transform of the input image to yield a processed output,
a(x, y). Specification of H(u, v) is simplified considerably by using functions
that are symmetric about their center, which requires that F(u, v) be centered
also. As explained in Section 4.6.3, this is accomplished by multiplying the
input image by (—1)**” prior to computing its transform."We are now in a position to consider the filtering process in some detail. One
of the simplest filters we can construct is a filter H{u, v) that is 0 at the center of
the transform and 1 elsewhere. This filter would reject the de term and “pass”
(ie, leave unchanged) all other terms of F(u, v) when we form the product
H(u, v)F(u, v). We know from Eq. (4.6-21) that the dc term is responsible for the
average intensity of an image, so setting it to zero will reduce the average intensity of the output image to zero. Figure 4.30 shows the result of this operation using
Eg. (4.7-1). As expected, the image became much darker. (An average of zero 'Many software implementations of the 2-D DFT (e.g., MATLAB) do not center the transform. This implies that filter functions must be arranged to correspond to the same data format as the uncentered
transform (i.e., with the origin at the top left). The net result is that filters are more difficult to generate
and display. We use centering in our discussions 1o aid in visualization. which is crucial in developing a
cleay understanding of filtering concepts. Either method can he used practice, as long as consistency is
maintained.If 4 is real and symmetric and fis real {asis lypicaily the case), then the.
IDFT in Eq. (4.7-1)
should yield real quantities im theory. Jn practice,
the inverse generally
contains parasitic complex terms from roundoff and other
computational inaccuracies. Thus, it is customary
to lake the teal pan of
the IDFT to form g,
4.7. The Basics of Filtering in the Frequency Domain 281Hine)
Fila. e)  abedef .FIGURE 4.31 Top row: frequency domain fillers, Bottom row: corresponding filtered images obtained using,
Eq. (4.7-1), We used ¢ = 0.85 in (c) to obtain (f) (the height of the filter itself is 1). Compare (1) with Fig. 4.29(a).  abcFIGURE 4.32 (a) A simple imas
(c) Result of lowpass filing  
 Hoal bsrpaddiing. Compsfii Be
280 Chapter 4 # Filtering in the Frequency DomainFIGURE 4.30
Result of filtering
the image inFig. 4.29(a) by
setting to 0 the
term F(M/2, N/2)
in the Fourier
transform. implies the existence of negative intensities. Therefore, although it ilustrates the
principle, Fig. 4.30 is not a true representation of the original, as all negative intensities were clipped (set to 0) for display purposes.)As noted earlier, low frequencies in the transtorm are related to slowly
varying intensity components in an image, such as the walls of a room or a
cloudless sky in an outdoor scene. On the other hand. high frequencies are
caused by sharp transitions in intensity, such as edges and noise. Therefore, we
would expect that a filter /7{#, 2) tat altenuates high frequencies while passing
low frequencies (appropriately called a fowpasy filter) would blur an image,
while a filter with the opposite property (called a highpass filter) would enhance sharp detail, but cause a reduction in contrast in the image. Figure 4.3) iilustrates these effects. Nolte the similarity between Figs. 4.31(c) and Pig. 4:
The reason is that the highpass filter shown chminates the de term. resultmg ia
the same basic effect that led to Fig. 4.30. Adding a small constant to the filler
does not affect sharpening appreciably. but it docs prevent climination of the
dc term and thus preserves tonality, as Fig. 4.31(€1) shows.Equation (4,7-1) involves the product of two functions tn the frequency domain which, by the convolution theorem, implics convolution in the spatial domain. We know from the discussion in Section 4.6.6 that if the functions in
question are not padded we can expeel wraparound error, Consider what happens when we apply Eq. (4.7-1) without padding. Figure 4.32(a) shows a simple image, and Fig. 4.32(h) is the result of lowpass filtering the image with a
Gaussian lowpass filter of the form shown in Tig. 4.31 (a). As expected. the
image is blurred. However, the blurring is not unifornn the lop white cdge is
blurred, but the side white edges are not. Padding the inpul image according to
Egs. (4.6-31) and (4.6-32) before applying Fg. Gh 7-1) results in Che filtered
image in Fig. 4.32(c). This resull is as espected,Figure 4.33 illustrates (he reason for the discrepanew felween Pies $32 by
and (c). The dashed areas in t i wai,
Figure 4.33(a) shows the periodicity ipl
plained in Section 4.6.3. Imagine canvelyfblurring filter with this image. Whoo th. filler is pass         
 
 
 
    ww the spedial
282 Chopter4 m Filtering in the Frequency Domain  Boe H ve]FIGURE 4.33 2-D image periodicity inherent in using the DFT, (a) Periodicity without
image padding. (b) Periodicity after padding with Os (black). The dashed areas in the
center correspond to the image in Fig. 4.32(a), (The thin while fines in both images are
superimposed for clarity; they are not part of the data.) dashed image, it will encompass part of the image and also part of the bottom
of the periodic image right above it. When a dark and a light region reside
under the filter, the result is a mid-gray, blurred output. However, when the filter is passing through the top right side of the image. the filter will encompass
only light areas in the image and its right neighbor. The average of a constant
is the same constant, so filtering will have no effect in this area, giving the result in Fig, 4.32(b). Padding the image with Os creates a uniform border around
the periodic sequence, as Fig. 4,33(b) shows. Convolving the blurring function
with the padded “mosaic” of Fig. 4.33(b) gives the correct result in Fig. 4.32(c).
You can see from this example that failure to pad an image. can lead to erroneous results. If the purpose of filtering is only for rough visual analysis, the
padding step is skipped sometimes.Thus far, the discussion has centered on padding the input image, but
Eq. (4.7-1) also involves a filter that can be specified either in the spatial or in
the frequency domain. However, padding is done in the spatial domain. which
raises an important question about the relationship between sparial padding
and filters specified directly in the frequency domain.At first glance, one could conclude that the way to handle padding of a
frequency domain filter is to construct the filter to be of the same size as the
image, compute the IDFT of the filter to obtain the corresponding spatial filter, pad that filter in the spatial domain, and then compute its DFT te retarn
to the frequency domain. The i-D example in Fig. 4.34 illustrates the pitfalls in
this approach. Figure 4,34(a) shows a J-D ideal lowpass {iller in the frequency
domain. The filter is real and has even symmetry. so we Know from property S
in Table 4.1 that its }DFT will be real and symmetric aise:shows the result of multiplying the Glements of the tresses
4,7 & The Basics of Filtering in the Frequency Domain        12 [ —- 0.04 — ; ;
1
0.03 |
08 |
06 0.02
04 i 001 |
0.2
0
0
-9.2 ——_—__1_. -0.
0 128 755 lg 128 256 384 51!
0.04 12
1
0.03
08
0.02 06
0.01 0.4
0.2
0
0
-0.01 ~02
og 128 255 0 128 256 384 suby (~1)“ and computing its [DFT to obtain the corresponding spatial filter.
The extremes of this spatial function are not zero so, as Fig. 4.34(c) shows,
zero-padding the function created two discontinuities (padding the two ends
of the function is the same as padding one end, as Jong as the total number of
zeros used is the same).To get back to the frequency domain, we compute the DFT of the spatial,
padded filter. Figure 4.34(d) shows the result. The discontinuities in the spatial filter created ringing in its frequency domain counterpart, as you would expect
from the results in Example 4.1. Viewed another way, we know from that example that the Fourier transform of a box function is a sinc function with frequency
components extending to infinity, and we would expect the same behavior from
the inverse transform of a box. That is, the spatial representation of an ideal (box)
frequency domain filter has components extending to infinity. Therefore, any
spatial truncation of the filter to implement zero-padding will introduce discontinuities, which will then in general result in ringing in the frequency domain (truncation can be avoided in this case if it is done at zero crossings, but we are
interested in general procedures, and not all filters have zero crossings).What the preceding results tel! us is that, because we cannot work with an infinite number of components, we cannot use an ideal frequency domain filter [as in283ac
bdFIGURE 4,34(a) Original filter
specified in the
(centered)
frequency domain.
{b) Spatial
representation
obtained by
computing the
IDFT of (a).(c) Result of
padding (b) to twice
its length (note the
discontinuities).
(d) Corresponding
filter in the
frequency domain
obtained by
computing the DFT
of (c), Note the
ringing caused by
the discontinuities
in (c). (The curves
appear continuous
because the points
were joined to
simplify visual
analysis.)See the end of Svetion
4.3.3 regarding the detinition of an ideal filter.
284 Chapter 4 m Filtering in the Frequency DomainabFIGURE 4.35(a) Image resulting
from multiplying by
0.5 the phase angle
in Eq. (4.6-15) and
then computing the
IDFT. (b) The
result of
multiplying the
phase by 0.25. The
spectrum was not
changed in either of
the two cases.Fig. 4.34(a)] and simultaneously use zero padding to avoid wraparound error. A
decision on which limitation to accept is required. Our objective is to work with
specified filter shapes in the frequency domain (including ideal filters) without
having to be concerned with truncation issues. One approach is to zero-pad images and then create filters in the frequency domain to be of the same size as the
padded images (remember, images and filters must be of the same size when
using the DFT). Of course, this will result in wraparound error because no
padding is used for the filter, but in practice this error is mitigated significantly by
the separation provided by the padding of the image, and it is preferable to ringing. Smooth filters (such as those in Fig. 4.31) present even less of a problem.
Specifically, then, the approach we will follow in this chapter in order to work
with filters of a specified shape directly in the frequency domain is to pad images
tosize P X Q and construct filters of the same dimensions. As explained earlier, P and Q are given by Eqs. (4.6-29) and (4.6-30).We conclude this section by analyzing the phase angle of the filtered transform. Because the DFT is a complex array, we can express it in terms of its real
and imaginary parts:F(u,v) = Ru, v) + jl (u,v) (4.7-2)
Equation (4.7-1) then becomes
g(x = SR, vyR(u, v) + fA Ca, vi (ee, e)| (4.7-3)The phase angle is not altered by filtering in the manner just described because H(u, v) cancels out when the ratio of the imaginary and real parts is
formed in Eq. (4.6-17). Filters that affect the real and imaginary parts equally,
and thus have no effect on the phase, are appropriately called zero-phase-shift
filters, These are the only types of filters considered in this chapter.Even small changes in the phase angle can have dramatic (usually undesirable) effects on the filtered output. Figure 4.35 illustrates the effect of something as simple as a scalar change. Figure 4.35(a) shows an image resulting
from multiplying the angle array in Eg. (4.6-15) by 0.5, without changing
286 Chopter 4 a Filtering in the Frequency DomainabcdefghFIGURE 4.36
{a)An MX N
image, f.(b) Padded image,
f, of size P X Q.
(c) Result of
multiplying f, by
(-1)**",{d) Spectrum of
F,. (e) Centered
Gaussian lowpass
filter, H, of size
PXxQ.(f) Spectrum of
the product HF,,
(g) gp, the product
of (-1)**” and
the real part of
the IDFT of HF,.
{h) Fina? result, g,
obtained by
cropping the first
M rows and N
columns of 2, spatial domain. Conversely. it follows from a similar analysis and the convalution theorem that, given a spatial filer. we obtain ils frequency domain reprey
sentation by taking the forward Fourier transform of the spatial filter.
Therefore, the two filters form a Fourier transiurm pair:Rex yles Hu, vr) (4,7-4)where f(x. ) is a spatial filler. Because this filter can be obtained from the response of a frequency domain filter fo an impulse. A(x, y) sometimes is referred to as the fnpittse response of E(u, v). Also, because all quantities in a
discrete implementation of Ey. f4.7-4) are finite. such filters are called firiie
impulse response (PIR) filters. These are the only types of dinear spatial Piers
considered in this book
We introduced s;-stiai coayolution in Section 3h) and discussed its unple
mentation in connesitod with Tea (34-2) which involved convolving Jintions of different sizes When wo speak of spatiob Gomvetates
4,7 m The Basics of Filtering in the Frequency Domain 285|F(u, v)|, and then computing the IDFT. The basic shapes remain unchanged,
but the intensity distribution is quite distorted. Figure 4.35(b) shows the result
of multiplying the phase by 0.25, The image is almost unrecognizable.4.7.3 Summary of Steps for Filtering in the Frequency Domain
The material in the previous two sections can be summarized as follows:1. Given an input image f(x, y) of size M X N, obtain the padding parameters P and Q from Eqs. (4.6-31) and (4.6-32). Typically, we select P = 2M
and Q = 2N.2. Form a padded image, f,(x, y), of size P < Q by appending the necessary
number of zeros to f(x, y).3. Multiply f,(x, y) by (-1)** to center its transform.4. Compute the DFT, F{u, v), of the image from step 3.5. Generate a real, symmetric filter function, H(«, v), of size P X Q with center at coordinates (P/2, Q/2).' Form the product G(u, v) = H(u, v)F(u, »)
using array multiplication; that is,G(i, k) = H(i, K)F(, k).6. Obtain the processed image:8 p(X, Y) = {real[9-[G(u, e)])}-ay*where the real part is selected in order to ignore parasitic complex components resulting from computational inaccuracies, and the subscript p indicates that we are dealing with padded arrays.7. Obtain the fina] processed result, g(x, y)}, by extracting the M x N region
from the top, Jeft quadrant of gp(x, y).Figure 4.36 illustrates the preceding steps. The legend in the figure explains the
source of each image. If it were enlarged, Fig. 4.36(c) would show black dots
interleaved in the image because negative intensities are clipped to 0 for display. Note in Fig. 4.36(h) the characteristic dark border exhibited by lowpass
filtered images processed using zero padding.4.7.4 Correspondence Between Filtering in the Spatial and
Frequency DomainsThe link between filtering in the spatial and frequency domains is the convolution theorem, In Section 4.7.2, we defined fiJtering in the frequency domain
as the multiplication of a filter function, H(u, v), times F(u, v), the Fourier
transform of the input image. Given a filter H(u, v), suppose that we want to
find its equivalent representation in the spatial domain. If we let
F(x, y) = (x,y), it follows from Table 4.3 that F(u,v) = 1. Then, from
Eq. (4.7-1), the filtered output is 3'{ H(u, v)}. But this is the inverse transform of the frequency domain filter, which is the corresponding filter in the ‘It H(u, v) is to be generated from a given spatial filter, A(x, y), then we form A, (x. ¥) by padding the
spatial fier to size P x Q, multiply the expanded array by (—1}°'*, and compute the DFT of the result
to obtain a centered ##(u. v). Example 4.15 illustrates this procedure.As noted earlier, cemtering heips in visualizing
the fillering process and
in generating the filter
functions themselves, but
centering is not a fandamental requiremeni.
4.7 % The Basics of Filtering in the Frequency Domain 287convolution theorem and the DFT, it is implied that we are convolving periodic functions, as explained in Fig. 4.28. For this reason, as explained earlier,
Eq. (4.6-23) is referred to as circular convolution. Furthermore, convolution
in the context of the DFT involves functions of the same size, whereas in
Eq. (3.4-2) the functions typically are of different sizes.In practice, we prefer to implement convolution filtering using Eq. (3.4-2)
with smal] filter masks because of speed and ease of implementation in
hardware and/or firmwaré: However, filtering concepts are more intuitive in
the frequency domain. One way to take advantage of the properties of both
domains is to specify a filter in the frequency domain, compute its IDFT,
and then use the resulting, full-size spatial filter as a guide for constructing
smailer spatial filter masks (more formal approaches are mentioned in
Section 4.11.4). This is illustrated next. Later in this section, we illustrate
also the converse, in which a small spatial filter is given and we obtain its
full-size frequency domain representation. This approach is useful for analyzing the behavior of small spatial filters in the frequency domain. Keep in
mind during the following discussion that the Fourier transform and its inverse are linear processes (Problem 4.14), so the discussion is limited to linear filtering.In the foliowing discussion, we use Gaussian filters to illustrate how
frequency domain filters can be used as guides for specifying the coefficients
of some of the small masks discussed in Chapter 3. Filters based on Gaussian
functions are of particular interest because, as noted in Table 4.3, both the
forward and inverse Fourier transforms of a Gaussian function are real
Gaussian functions. We jimit the discussion to 1-D to Nlustrate the underlying principles. Two-dimensional Gaussian filters are discussed later in this
chapter.Let H(z) denote the 1-D frequency domain Gaussian filter:Hu) = Aer’ (4.7-5)where @ is the standard deviation of the Gaussian curve, The corresponding
filter in the spatial domain is obtained by taking the inverse Fourier transform
of H(z) (Problem 4.31): h(x) = VinoAe** (4.7-6)
These equations ' are important for two reasons: (1) They are a Fourier transform pair, both components of which are Gaussian and rea. This facilitates
analysis because we do not have to be concerned with complex numbers. In
addition, Gaussian curves are intuitive and easy to manipulate. (2) The functions behave reciprocally. When H{w) has a broad profile (large value of ), ‘As mentioned in Tabie 4.3, closed forms for the forward and inverse Fourier transforms of Gaussians
are valid only for continuous functions. To use discrete formulations we simply sample the continuous
Gaussian transforms. Our use of discrete variables here implies that we are dealing with sampled
trensforms.
288 Chapter 4 # Filtering in the Frequency DomainacbdFIGURE 4,37(a) A 1-D Gaussian
lowpass filter in the
frequency domain.
{b) Spatiallowpass filter
corresponding to
(a). {c) Gaussian
highpass filter in
the frequency
domain. (d) Spatial
highpass filter
corresponding to
(c). The small 2-D
masks shown are
spatial filters we
used in Chapter 3.h(x) has a narrow profile, and vice versa. In fact. as o approaches infinity, H(t)
tends toward a constant function and A(x) tends toward an impulse, which implies no filtering in the frequency and spatial domains, respectively.Figures 4.37(a) and (b) show plots of a Gaussian lowpass filter in the frequency domain and the corresponding lowpass filter in the spatial domain.
Suppose that we want to use the shape of A(x) in Fig. 4.37(b) as a guide for
specifying the coefficients of a small spatial mask. The key similarity between the two filters is that all their values are positive. Thus, we conclude
that we can implement lowpass filtering in the spatial domain by using a
mask with all positive coefficients (as we did in Section 3.5.1). For reference,
Fig. 4.37(b) shows two of the masks discussed in that section. Note the reciprocal relationship between the width of the filters, as discussed in the previous paragraph. The narrower the frequency domain filter, the more it will
attenuate the low frequencies, resulting in increased blurring, In the spatial
domain, this means that a larger mask rmoust be used to increase blurring, as
ilustrated in Example 3.13.More compiex filters can be constructed using the basic Gaussian function
of Eq. (4.7-5). For example, we can construct a highpass filter as the difference
of Gaussians:H(u) = Aer? — Bee Pe (47-7)with A = Band uv, > 3, The corresponding filter in the spatial domain is
h(x) = V2ar0, Aer — Vine, Bere” (4.7-8)
Figures 4.37(c) and (d) show plots of these two equations. We note again thereciprocity in width, but the most important feature here is that A(x) has a positive center term with negative terms on either side. The small masks shown inHet} F(t)~ mit A(x} hi)
290 Chapter 4 w Filtering in the Frequency DomainabedFIGURE 4.39(a) A spatial
mask and
perspective plot
of its
corresponding
frequency domain
fitter. (b) Filter
shown as an
image. (c) Result
of filteringFig. 4.38(a) in the
frequency domain
with the filter in
(b), (d) Result of
filtering the same
image with the
spatial filter in
(a). The results
are identical.    +0,
XX
.
SUG   602 x 602 pixels, according to Eqs. (4.6-29) and (4.6-30). The Sobel mask exhibits odd symmetry, provided that it is embedded in an array of zeros of even
size (see Example 4.10). To maintain this symmetry, we place A(x. y) so that iis
center is at the center of the 6(12 X 602 padded array. This is an important aspect of filter generation, If we preserve the odd symmetry with respect lo the
padded array in forming f(x, y), we know from property 9 in Table 4.1 that
AH(u, v) will be purely imaginary. As we show at the end of this example. this
will yield results that are identical to filtering the image spatially using icv. vi
If the symmetry were not preserved, the results would no longer be same.
The procedure used to generate A(t, v) is: (1) multiply A,Cx, ¥) by (~ 1)to center the frequency domain filter; (2) compute the forward DFT of the re
sult in (1); (3) set the real part of the resulting DFT to 0 to account for parasivh
real parts (we know that H (u, v) has to be purely imaginary}, and (4) maltipts
the result by (~1)"". This last step reverses the multiplication of f(r.) bs
(-1)"*", which is implicit when #(x, y) was moved to the center of fix, v}
Figure 4.39(a) shows a perspective plot of /7 (nu. 1}. and Fig, 4.39/h) shes:
47) The Basies of Filtering in the Frequency Domain 289Fig, 4.37(d) “capture” this property. These two masks were used in Chapter 3
as sharpening filters, which we now know are highpass filters.Although we have gone through significant effort to get here, be assured
that it is impossible to truly understand fillermg in the frequency domain
without the foundation we have just established. la practice, the frequency
domain can be viewed as a “laboratory” in which we take advantage of the
correspondence between frequency content and image appearance. As is
demonstrated numerous times later in this chapter, some tasks that would be
exceptionally difficult or impossible to formulate directly in the spatial domain become almost trivial in the frequency domain. Once we have selected a
specific filter via experimentation in the frequency domain, the actual implementation of the method usually is done in the spatial domain. One approach
is to specify smal! spatial masks that attempt to capture the “essence” of the
full filter function in the spatial domain. as we explained in Fig. 4.37. A more
formal approach is to design a 2-D digital filler by using approximations
based on mathematical or statistical criteria. We touch on this point again in
Section 4.11.4. @ In this example, we start with a spatial mask and show how to generate its
corresponding filier in the frequency domain. Then. we compare the filtering
results obtained using frequency domain and spatial techniques. This type of
analysis is usefu] when one wisites lo compare the performance of given spatial masks against one or more “full” filter candidates in the frequency domain, or to gain deeper understanding about the performance of a mask. To
keep matters simple, we use the 3 X 3 Sebel vertical edge detector from
Fig. 3.41(e). Figure 4.38(a) shows a 600 X 600 pixel image. f(x, y). that we wish
to filter, and Fig. 4.38(b) shows ils spectrum.Figure 4.39(a) shows the Sobel mask. A(x. v) (the perspective plot is explained below). Because the input image is of size 600 * 600 pixels and the tilter is of size 3 X 3 we avoid wraparound error by padding f and A to size EXAMPLE 4.15:
Obtaining a
frequency domain
filer fromm a smail
spatial mask.abFIGURE 4.38(0) Image of a
huthling. and
(b) its spectrum,
292 Chapter 4 w Filtering in the Frequency DomainH(u, v} wap Fu 2)$n wen —wennenetee EP (14, B)Dy a& Bec
FIGURE 4.40 (a) Perspective plot of an ideal lowpass-filter ansfer function. (b) Filter displayed as an image.
(c) Filter radial cross section.withoitt attenuation, whereas all frequencies outside the circle are completely
attenuated (filtered out). The ideal lowpass filler is radially symmetric about
the origin, which means that the filter is completely defined by a radial cross
section, as Fig. 4.40(c) shows. Rotating the cross section by 360° yields the filter in 2-D.For an ILPF cross section, the point of transition between H(u, v) = 1 and
H(u, v) = Ois called the cutoff frequency. In the case of Fig, 4.40, for example,
the cutoff frequency is Do. The sharp cutoff frequencies of an LLPF cannot be
realized with electronic components, although they certainly can be simulated
in a computer. The effects of using these “nonphysical”™ filters on a digital
image are discussed later in this section.The lowpass filters introduced in this chapter are compared by studying
their behavior as a function of the same cutoff frequencies, One way to establish a set of standard cutoff frequency loci is to compute circles that enclose
specified amounts of total image power /’;. This quantity is obtained by summing the components of the power spectrum of the padded images at each
point (4, v),foru + 0,1,....P-lande = 0,1,..., Q — J; that is,Pel Qa
Pr= SD SP r) (4.8-3)fee ae Owhere P(u, v) is given in Eq. (4.6-18). If the DFT has been centered.a circle of
radius Dy with origin at the center of the frequency rectangle encloses a percent of the power, where™1a © 100 SS Ptaey Py: (4.8-4)and the summation is taker over Valius of i. 2) thul fe inside the circle or on
its boundary.
294 = Chepter 4 w Filtering in the Frequency DomainSena A &
eee eee |raaaaaaa — . iPreteen eerie mere: meemmrintet a nee7yr ie TTTpg   sirablea
oeHTTNINssaanudaddad  eet) aeeert TTie & . wits
eee oeeUNNI) TIILUITaaaaaada aaaaaadd  abcdefFIGURE 4.42 (2) Origin image. (bf) Resulls of filtering using H.PFs with cotott
frequencies sel at radii values Hi UL eG. FOUL and 460. as shown in Pig. 44109. The
power removed by these fillers was 1300.9. 4.3.2.2. and 0.8% of she total respectively.
4.8 % Image Smoothing Using Frequency Domain FiltersH(u, v) as an image. As, expected, the function is odd, thus the antisymmetry
about its center. Function H (u, v) is used as any other frequency domain filter
in the procedure outlined in Section 4.7.3.Figure 4.39(c) is the result of using the filter just obtained in the procedure outlined in Section 4.7.3 to filter the image in Fig. 4.38(a). As expected
from a derivative filter, edges are enhanced and al] the constant intensity
areas are reduced to zero (the grayish tone is due to scaling for display).
Figure 4,39(d) shows the result of filtering the same image in the spatial domain directly, using A(x, y) in the procedure outlined in Section 3.6.4. The results are identical. 2%Image Smoothing Using Frequency Domain FiltersThe remainder of this chapter deals with various filtering techniques in the frequency domain. We begin with lowpass filters. Edges and other sharp intensity
transitions (such as noise) in an image contribute significantly to the highfrequency content of its Fourier transform. Hence, smoothing (blurring) is
achieved in the frequency domain by high-frequency attenuation; that is, by
lowpass filtering. In this section, we consider three types of lowpass filters:
ideal, Butterworth, and Gaussian. These three categories cover the range from
very sharp (ideal) to very smooth (Gaussian) filtering. The Butterworth filter
has a parameter called the filter order. For high order values, the Butterworth
filter approaches the ideal filter. For lower order values, the Butterworth filter
is more like a Gaussian filter. Thus, the Butterworth filter may be viewed as
providing a transition between two “extremes.” Ail filtering in this section follows the procedure outlined in Section 4.7.3, so all filter functions, H(u, v), are
understood to be discrete functions of size P x Q; that is, the discrete frequency
variables are in the range u = 0,1,2,...,P - Landy = 0,1,2,...,.Q—1.4.8.1 Ideal Lowpass FiltersA 2-D lowpass filter that passes without attenuation all frequencies within a
circle of radius Dp, from the origin and “cuts off” all frequencies outside this
circle is called an ideal lowpass filter (ILPF), it is specified by the function1 if Dev) S Do
,v) = . 4.8-1
H(u, 2) {} if Diu, v) > Do (48-1)
where Do is a positive constant and D(x, v) is the distance between a point (, v)
in the frequency domain and the center of the frequency rectangle; that is,
Diu, ve) = [(u- PY + (w- 9/2) |(4.8-2)where, as before, P and Q are the padded sizes from Eqs. (4.6-31) and (4.6-32).
Figure 4.40(a) shows a perspective plot of H(u, v) and Fig. 4.40(b) shows the
filter displayed as an image. As mentioned in Section 4.3.3, the name ideal
indicates that all frequencies on or inside a circle of radius Dy are passed291
4.8 m Image Smoothing Using Frequency Domain Filters 293Figures 4.41(a) and (b) show a test pattern image and its spectrum. The
circles superimposed on the spectrum have radii of 10, 30, 60, 160, and 460
pixels, respectively. These circles enclose a perceni of the image power, for
a = 87.0, 93.1, 95.7, 97.8, and 99.2%, respectively. The spectrum fails off
rapidly, with 87% of the total power being enclosed by a relatively small
circle of radius 10.@ Figure 4.42 shows the results of applying ILPFs with cutoff frequencies at
the radii shown in Fig. 4.41(b). Figure 4.42(b) is useless for all practical purposes, unless the objective of blurring is to elimimate all detail in the image,
except the “blobs” representing the largest objects, The severe blurring in
this image is a clear indication that most of the sharp detail information in
the picture is contained in the 13% power removed by the filter. As the filter
radius increases, less and Jess power is removed, resulting in less blurring.
Note that the images in Figs. 4.42(c) through (e) are characterized by “ringing,” which becomes finer in texture as the amount of high frequency content removed decreases. Ringing is visible even in the image [Fig. 4.42(e)}] in
which only 2% of the total power was removed. This ringing behavior is a
characteristic of ideal filters, as you will see shortly, Finally, the resuit for
a = 99.2 shows very slight blurring in the noisy squares but, for the most
part, this image is quite close to the original. This indicates that litle edge
information is contained in the upper 0.8% of the spectrum power in this
particular case.It is clear from this example that ideal lowpass filtering is not very practical. However, it is useful to study their behavior as part af our development ofaaaaaaadabFIGURE 4.4] (a) Test pattern of size 688 X 688 pixels. and (b) its Fourier spectrum. The
spectrum is doubie the image size due to padding but is shown in half size se that it fits
in the page. The superimposed circles have radii equal to 10, 30, 60, 160. and 460 with
respect to the full-size spectrum image. These radii enclose 87.0), 93.1, 98.7. 97.8, and
99.2% of the padded image power. respectively.  EXAMPLE 4.16;
Image smouthing
using an IPF.
4.8 ~ Image Smoothing Using Frequency Domain Filters 297 
     aaaaaadad388 8-788 8 0 $ arr ng,eee  | aed
ae TIN
| PR. .
I -saa4 aaa aaaaaa a a
So ee “ .
SER EERE
ooo eee mui) TN
aaaaaaad aaaaaadadab
cd  efFIGURE 4.45 (a) Original image. (b)-(1) Resulls of filtering using BLPFs of order 2
with cutoff frequencies at the radii shown m Pie. 4.41. Compare with Fig. 442
4.8 . Image Smoothing Using Frequency Domain Filters 295filtering concepts. Also. as shown in the discussion that follows, some interesting insight is gained by attempting to explain the ringing property of ILPFs in
the spatial domain. rfThe blurring and ringing properties of ILPFs can be explained using the
convolution theorem. Figure 4.43(a) shows the spatial representation, A(x, y), of
an ILPF of radius 10, and Fig. 4.43(b) shows the intensity profile of a line passing
through the center of the image. Because a cross section of the ILPF in the frequency domain looks bke a box filter, it is not unexpected that a cross section of
the corresponding spatial filter has the shape of a sine function. Filtering in the
spatial domain is done by convolving A(x, vy) with the image. Imagine each pixel
in the image being a discrete impulse whose strength is proportional to the intensity of the image al that location. Convalving a sinc with an impulse copies
the sinc at the location of the impulse. The center lobe of the sinc is the principal
cause of blurring, while the outer, smaller lobes are mainly responsibte for ringing. Convolving the sinc with every pixel in the image provides a nice model for
explaining the behavior of ILPFs. Because the “spread” of the sine function is inversely proportional to the radius of /7(u, v), the larger Dy becomes, the more
the spatial sinc approaches an impulse which. in the limit, causes no blurring at
all when convolved with the image. This type of reciprocal behavior should be
routine to you by now. In the next two sections, we show that it is possible to
achieve blurring with little or no ringing. which is an important objective in
lowpass filtering.4.8,2 Butterworth Lowpass Filters .The transfer function of a Butterworth lowpass filter (BLPF) of order n. and
with cutoff frequency at a distance Dy from the origin, is defined asar (4.8-5)
1 + [Da v)/ Dol”Huey =where D(u, v) is given by Eq. (4.8-2). Figure 4.44 shows a perspective plot,
image display, and radial cross sections of the BLPF function. “The wanster function of
the Butterworth lowpass
fitter normatly is written
as the square rac of our
expression. However, our
interest here is in the
basic form of the filter, so
we caclude the square
root for computational
convenienceabFIGURE 4.43(a) Representation
in the spatial
domain of an
ELPF of radius 5
and size1000 x 1000.
(b} Intensity
profile ofa
horizontal Jine
passing through
the center of the
image.
296 Chapter 4 % Filtering in the Frequency Domain“anabe A(a, v) fi (tt, vyrh 1o0.5 FIGURE 4.44 (a) Perspective plot of a Butterworth lowpass-filter transfer function. (b) Filter displayed as an
image. (c} Filter radial cross sections of orders | through 4.EXAMPLE 4.17:
Image smoothing
witha
Butterworth
lowpass filterUnlike the ILPF, the BLPF transfer function does not have a sharp discontinuity that gives a clear cutoff between passed and filtered frequencies. For
filters with smooth transfer functions, defining a cutoff frequency locus at
points for which H(z, v) is down lo a certain fraction of its maximum value is
customary. In Eq. (48-5). (down 50% from ils maximum value of 1} when
D(u, v) = Dy.is Figure 4.45 shows (he results of applying the BLPF of Eq. (4.8-5) to
Fig. 4.45(a), with nv = 2 and Dy equal to the five radii in Fig. 4.41(b), Unlike the
results in Fig. 4.42 for the ILPF. we note here a smooth transition in blurring as
a function of increasing cutoff frequency. Moreover, no ringing is visible in any
of the images processed with this particular BLPF. a fact attributed to the tilter’s smooth transition between tow and high frequencies.A BLPF of order 1} has no ringing in the spatial domain. Ringing generally
is imperceptible in fitters of order 2, but cum become significant in filters of
higher order. Figure 4.46 shows a comparison between the spatial representation of BLPFs of various orders (using a cutalf frequency of 5 in all cases).
Shown also is the intensity profile along u horizontal! scan line through the center of each filter. These filters were abtained and displayed using the same procedure used to generate Fig, 4.43. Io facilitate comparisons, additional
enhancing with a gamma transformation [sce Eq. (3.2-3)] was applied to the
images of Fig. 4.46. The BLPF of order 1 | Fig. 4.46(a)}] has neither ringing nor
negative values. The filter of order 2 does shaw mitd ringing and small negative
values, but they certainly are Jess pronounced than in the ILPF As the remaining images show, ringing in the B}.PF becomes significant for higher-order filters. A Butterworth filter of order 20 uxhibifs characteristics similar to those of
the ILPF (in the limit, both filters are identical). BLPFs of order 2 are a good
compromise between effeclive fowpass fillering and acceptable ringing.
298 Chopter 4 Filtering in the Frequency Domain  te a RY HN feabedFIGURE 4.46 (a)—-(d) Spatial representation of BLPFs of order |, 2, 5, and 20, and corresponding intensity
profiles through the center of the filters (the size in all cases is 1000 x 1000 and the cutoff frequency is 5).
Observe how ringing increases as a function of filler order.  » Gaussian Lowpass FiltersGaussian lowpass filters (GLPFs) of one dimension were introduced in
Section 4.7.4 as an aid in exploring some important relabionships between the
spatial and frequency domains. The form of these fillers in two dimensions is
given byAlar) = @ Rene (48-6)where, as in Eq. (4.8-2), DU. v) is the distance from the center of the frequency
rectangle. Here we do not usc a multiplying constant as in Section 4.74 in
order to be consistent with the filters discussed in the present section, whose
highest value is 1. As before, o is a measure of spread about the center. By Tel
lings = 2)... we can express the filter using the nolavion of the other fitters in
this section:Hany @ PADS (48-7)
where Dy is the cutoff frequency. When OGar) > 2a the GLPE is dawn to0.607 of its maximum value
As lable 4.3 shows, fhe inverse Forrer (ransforei of the Ga PR is Garussian  also, This means thal a spatial Gaugaan fer. obtained by compuung |
HORT of Bg. (40-6) or GES wil hive ve poping. igure fal? sheays a pedayy radial ¢ 
 sscohoms of a GL PP function. und      ua fest i
300 Ghepter 4 m Filtering in the Frequency Domainaaaaaaad    
   eee a
HANA) TININTTTe@eeaaaddd ssaaaaaaaaa a
ANI TINaaaaaaddad aaaaaaddad  Ow
wherFIGURE 4.48 (2) Original image. (db) 10) Results of filtering using OLPPs with cutoff
frequencies at the radii shown in Fig | 41. Compare with Figs. 4.42 and 4.45
4.8 & Image Smoothing Using Frequency Domain Filters 299U(u. 0)H{u, v)    
 
  Dy = 0
» Dy=%Dy = 40» Dy= 1000.667 7 Die, v) abe
FIGURE 4.47 (a) Perspective plot of a GLPF transfer function. (b) Filter displayed as an image. (c) Filter
radial cross sections for various values of Dy,TABLE 4.4
Lowpass filters. Dyis the cutoff frequency and » is the order of the Butrerworth filter. Gaussian |Ideal Butterworth _ 31 if Diu, v) = Dy ee 7 oy ae oun) 208
Hu, 2) = {3 if D(u,v) > Do Hd) = Bn py Dad H(un) = ¢  @ Figure 4.48 shows the results of applying the GLPF of Eq. (4.8-7) to EXAMPLE 4.18:
Fig. 4.48(a), with Dg equal to the five radii in Fig. 4.41(b). As in the case of the Image smoothing
BLPF of order 2 (Fig. 4.45), we note a smooth transition in blurring as a func- With a Gaussian
tion of increasing cutoff frequency. The GLPF achieved slightly less smoothing '°"P#ss filter.
than the BLPF of order 2 for the same value of cutoff frequency, as can beseen, for example, by comparing Figs. 4.45(c) and 4.48(c). This is expected, be
cause the profile of the GLPF is not as “tight” as the profile of the BLPF oforder 2. However, the results are quite comparable. and we are assured of noringing in the case of the GLPF. This is an important characteristic in practice,especially in situations (e.g., medical imaging) in which any type of artifact isunacceptable. In cases where tight contro] of the transition between low andhigh frequencies about the cutoff frequency are needed. then the BLPF pre
sents a more suitable choice. The price of this additional control over the filterprofile is the possibility of ringing. ;:4.8.4 Additional Examples of Lowpass FilteringIn the following discussion, we show several practical applications of lowpass
filtering in the frequency domain. The first example is from the field of machine perception with application to character recognition; the second is from
the printing and publishing industry: and the third is related to processing
302 Chapter4 # Filtering in the Frequency Domain abcFIGURE 4.50 (a) Original image (784 x 732 pixels). (b) Result of filtering using a GLPF with Dy = 100,
(c) Result of filtering using a GLPF with Dy = 80. Note the reduction in fine skin lines in the magnified
sections in (b) and (c).physical cause). Lowpass fillering is a crude but simple way to reduce the effect
of these lines, as Fig. 4.51(b) shows (we consider more effective approaches in
Sections 4.10 and 5.4.1}. This image was obtained using a GLFP with Dy = 50.
The reduction in the effect of the scan lines can simplify the detection of features such as the interface boundaries between ocean currents.Figure 4.51(c) shows the resull of significantly more aggressive Gaussian
lowpass filtering with Dy = 20. Here. the objective is to blur out as much detail as possible while leaving large features recognizable. For instance. this type
oe filtering could be part of a preprocessing stage for an image analys stem
that searches for features in an image bank. An example of such features could
be lakes of a given size. such as Lake Okeechobee in the lower castern region
of Florida, shown as a nearly round dark region in Fig, 4.51 (e). Lowpass filtering helps simipluy the analysis by averaging out features smaller than the ones of interest.Image Sharpening Using Frequency Domain Filters In the previous section. we showed tha: an image can he smoothed by attenuinsform, Becat i    
  vding the high-frequency components of tis Fourier tand other abrupt ¢  es Hy dmensities are associated with high-lrequescy
hieved in the frequency domiun bylow-lTreqhency componsiis wilhout COM POnecnis, TOUR sharpenine can be hod
2 PRON Efe riehpass hing lfphdreartenes aiferoiition ithe bourne: Ganslure Ast silot
48 ® Image Smoothing Using Frequency Domain Fitters 301 Historically, certain computer || Historically, certain computer
programs were written using i programs were written using i
only two digits rather than | Only two digits rather than
four ta define the applicable || four to define the applicable |
year, Accordingly, the | year. Accordingly, the
company's software may company’s software may
recognize a date using "00" recognize a date using "OO" |       as 1900 rather than the year as 1900 rather than the year2000. 2000.
Le m= « a .eajsatellite and aerial images. Similar results can be obtained using the lowpass
spatial filtering techniques discussed in Section 3.5.Figure 4.49 shows a sample of text of poor resolution. One encounters text
like this, for example, in fax transmissions, duplicated material, and historical
records. This particular sample is free of additional difficulties like smudges,
creases, and torn sections The magnified section in Fig. 4.49(a) shows that the
characters in this document have distorted shapes due to lack of resolution,
and many of the characters are broken. Although humans fill these gaps visually without difficulty, machine recognition systems have real difficulties reading broken characters. One approach for handling this problem is to bridge
small! gaps in the input image by blurring it. Figure 4.49(b) shows how well
characters can be “repaired” by this simple process using a Gaussian lowpass
filter with Dy = 80, The images are of size 444 x 508 pixels.Lowpass filtering is a staple in the printing and publishing industry, where it
is used for numerous preprocessing functions, including unsharp masking, as
discussed in Section 3.6.3, “Cosmetic” processing is another use of lowpass filtering prior to'printing. Figure 4.50 shows an application of lowpass filtering
for producing a smoother, softer-looking result from @ sharp original. For
human faces, the typical objective is to reduce the sharpness of fine skin lines
and small blemishes. The magnified sections in Figs. 4.50(b) and (c) clearly
show a significant reduction in fine skin lines around the eyes in this case. In
fact, the smoothed images look quite soft and pleasing.Figure 4,51 shows two applications of lowpass filtering on the same image,
but with totally different objectives. Figure 4.51(a) is an 808 X 754 very high
resolution radiometer (VHRR) image showing part of the Gulf of Mexico
(dark) and Florida (light), taken from a NOAA satellite (note the horizontal
sensor scan lines), The boundaries between bodies of water were caused by
loop currents. This image is illustrative of remotely sensed images in which sensors have the tendency to produce pronounced scan lines along the direction in
which the scene is being scanned (see Example 4.24 for an illustration of a 
 abFIGURE 4.49(a) Sample text of
low resolution
(note broken
characters in
magnified view).
(b) Result of
filtering with a
GLPF (broken
character
segments were
joined),We discuss unsharp
masking in the frequency
domain in Section 4.9.5,
304 Chapter 4 x Filtering in the Frequency Domainabc
def
Zhi   Hu, v) fi (u,v)
—? 16+ Diu, v)Hu, v)vig Dts, vyfi (tv) oa Du, »)FIGURE 4.52 Top row: Perspective plot, image representation, and cross section of a typical ideal highpass
filter. Middle and bottom rows: The same sequence for typical Butterworth and Gaussian highpass filters. where Dy is the cutoff frequency and D(z. v) is given by Eq. (4.8-2). This expression fotlows directly from Eqs. (4.8-1) and (4.9-1}. As intended, the [HPF
is the opposite of the ILPF in the sense that it sets to zero all frequencies inside
a circle of radius Dp while passing, without attenuation, all frequencies outside
the circle. As in the case of the [L.PF. the THPF ts not physically realizable. However, we consider it here for completeness and, as before, because tts properties can be used to explain phenomena such as ringing in the spatial domain.
The discussion will be brief.Because of the way in which they are related [Eq. (4.9-1)], we can expect
THPFs to have the same ringing properties as TLPFs. This is demonstrated
4.9 % Image Sharpening Using Frequency Domain Filters303 abe
FIGURE 4.51 (a) Image showing prominent horizontal scan lines. (b) Result of filtering using a GLPF with
Dy = 50. (c) Result of using a GLPF with Dy = 20. (Original image courtesy of NOAA.)4.8, we consider only zero-phase-shift fillers that are radially symmetric. Al]
filtering in this section is based on the procedure outlined in Section 4,7,3, so
all filter functions, (u,v), are understood to be discrete functions of size
P XQ; that is, the discrete frequency variables are in the range
u=0,1,2,...,P - Landy =0,1.2.....Q~ 1.A highpass filter is obtained from a given lowpass filter using the equationHyp , v) = 1 Ayp(e, v) (4.9-1}where Hp(u, v) is the transfer function of the lowpass filler. That is, when the
lowpass filter attenuates frequencies, the highpass filter passes them, and vice
versa.In this section, we consider ideal, Butterworth, and Gaussian highpass filters. As in the previous section, we illustrate the characteristics of these filters
in both the frequency and spatial domains. Figure 4.52 shows typical 3-D plots.
image representations, and cross sections for these filters. As before, we see
that the Butterworth filter represents a transition between (he sharpness of
the ideal filter and the broad smoothness of the Gaussian filler. Figure 4,53,
discussed in the sections that follow, illustrates what these filters Jook like in
the spatial domain. The spatial filters were obtained and displayed by using the
procedure used to generate Figs. 4.43 and 4.46.4.9.1 Ideal Highpass Filters
A 2-D ideal highpass filier (LHPF) is defined as
fn UDG eye DPHue) 4 . {4.9.7}
‘ li UD e) Dy
4.9% Image Sharpening Using Frequency Domain Filters 305 abcFIGURE 4.53 Spatial representation of typical (a) ideal. (b) Butterworth, and (c) Crussian frequeney dunia
highpass filters, and corresponding intensity profiles through thetr centers.clearly in Fig, 4.54, which consists of various IHPF results using the originat
image in Fig. 4.41 (a) with Dy set lo 30,60, and 160 pixels. respectively. The ringing in Fig. 4.54(a) is so severe that i produced distorted. thickened object
boundaries (e.g., look at the large letter “a”). Edges of the top three circles do
not show well because they are not as strong as the other edges in the image
(the intensity of these three objects is much closer to the background intensity,abe
FIGURE 4.54 Resulis of highpass Hilton the imaye mi Bip coffead osmie i GY) wick 2)
306 Chapter 4 #1 Filtering in the Frequency Domaingiving discontinuities of smaller magnitude). Looking al the “spot” size of the
spatial representation of the [HPF in Fig. 4.53({a) and keeping in mind that filtering in the spatial domain is convolution of the spatial filter with the image
helps explain why the smaller objects and lines appear almost solid white.
Look in particular at the three small squares in the top row and the thin, vertical bars in Fig, 4.54(a). The situation improved somewhat with Dg = 60.
Edge distortion is quite evident still, but now we begin to see filtering on the
smalter objects, Due to the now familiar inverse relationship between the frequency and spatial domains, we know that the spot size of this filter is smaller
than the spot of the filter with Dy = 30, The result for Dy = 160 is closer to
what a highpass-filtered image should look like. Here, the edges are much
cleaner and less distorted, and the smaller objects have been filtered properly. Of course, the constant background in all images is zero in these
highpass-filtcred images because highpass filtering is analogous to differentiation in the spatial domain.Butterworth Highpass FiltersA2-D Buiternvorth highpass filter (BHPF) of order a and cutoff frequency Dy
is defined as Ht B) = mn oh oa (4.9-3)
Ls [Do Dae ey
where D{#, v) is given by Eq. (4.8-2). This expression follows directly from
Eqs. (4.8-5) and (4.9-1). The middle row of Fig. 4.52 shows an image and cross
section of the BHPF function.
As with lowpass filters, we can expect Butterworth highpass filters to behave smoother than IHPFs. Figure 4.55 shows the performance of a BHPF, of aeFIGURE 4.55 Results of highpass fillcring the image in Fig. .41(a) using a BHPF of order 2 with Di = 30, 60,
and 160, corresponding to the circles in Fig, A41(b). These results are much smoather than those obtained
with an [HPF.
FIGURE 4.56 Resulis of highpass filtering the image in Fig. 4.41(a) using 4 GHPF with Dy = 30, 60, and 160,
corresponding to the circles in Fig. 4.41 (b). Compare with Figs, 4.54 and 4.55,order 2 and with D, set to the same values as in Fig. 4.54. The boundaries are
much less distorted than in Fig. 4.54, even for the smallest value of cutoff frequency. Because the spot sizes in the center areas of the IHPF and the BHPF
are similar [see Figs. 4.53(a) and (b)]. the performance of the two filters on the
smaller objects is comparable. The transition into higher values of cutoff frequencies is much smoother with the BHPF.4.9.2 Gaussian Highpass Filters
The transfer function of the Gaussian highpass filter (GHPF) with cutoff frequency Jocus at a distance D, from the center of the frequency rectangle is
given byH(u,v) 21> PnryeDe (4.9-4)where D{u,v) is given by Eq. (4.8-2). This expression follows directly from
Eqs. (4.8-7) and (4.9-!). The third row in Fig. 4.52 shows a perspective plot,
image, and cross section of the GHPF function, Following the same format as
for the BHPF, we show in Fig. 4.56 comparable results using GHPFs. As expected, the results obtained are more gradual than with the previous two filters. Even the filtering of the smaller objects and thin bars is cleaner with the
Gaussian filter. Table 4.5 contains a summary of the highpass filters discussed
in this section.TABLE 4.5
Highpass filters, Dy is the cutoff frequency and n is the order of the Butterworth filter.    Ideal Butterworth 
 1 if D(u.v) = Dy |
) = Heavy t
H(u,) {3 if Dl. w) > B, ee TDS ptie al Me)4.9 ® [mage Sharpening Using Frequency Domain Filters 307 |
Gaussian |
1
|a] °
308 = Chepter 4m Filtering in the Frequency DomainEXAMPLE 4.19:
Using highpass
filtering and
thresholding for
image
enhancement.‘The value Dp = 50 is ap
proximately 2.5% of the
short dimension of the
padded image. The idea
is for Dp ta be close 1o
the origin so low frequencies are aitenuated,but not completely elimi
nated, A range of 2% to
5% of the short dimension is a good starting
point.@ Figure 4.57(a) is a 1026 X 962 image of a thumb print in which smudges
(a typical problem) are evident. A key step in automated fingerprint recognition is enhancement of print ridges and the reduction of smudges, Enhancement is useful also in human interpretation of prints. In this example,
we use highpass filtering to enhance the ridges and reduce the effects of
smudging. Enhancement of the ridges is accomplished by the fact that they
contain high frequencies, which are unchanged by a highpass filter. On the
other hand, the filter reduces low frequency components, which correspond
to slowly varying intensities in the image, such as the background and
smudges. Thus, enhancement is achieved by reducing the effect of all features except those with high frequencies, which are the features of interest
in this case.Figure 4.57(b) is the result of using a Butterworth highpass filter of order 4
with a cutoff frequency of 50. As expected, the highpass-filtered image lost its
gray tones because the dc term was reduced to 0. The net result is that dark
tones typicaily predominate in highpass-filtered images, thus requiring additional processing to enhance details of interest. A simple approach is to threshold the filtered image. Figure 4.57{c) shows the result of setting to black all
negative values and to white all positive values in the filtered image. Note how
the ridges are clear and the effect of the smudges has been reduced considerably. In fact, ridges that are barely visible in the top, right section of the image
in Fig. 4.57(a) are nicely enhanced in Fig. 4.57(c).4,9.4 The Laplacian in the Frequency DomainIn Section 3.6.2. we used the Laplacian for image enhancement in the spatial
domain. In this section, we revisit the Laplacian and show that it yields equivalent results using frequency domain techniques. It can be shown (Problem
4.26) that the Laplacian can be implemented in the frequency domain using
the filterH(u,v) = 4a? + v°) (4.9-5) abcFIGURE 4.57 {4} Thumb print. (b) Result of highpass filtering (a). {c) Result of
thresholding {b). (Original image courtesy of the L!S. National Institute of Standards
and Technology. }
4.9 m Image Sharpening Using Frequency Domain Filters 309or, with respect to the center of the frequency rectangle, using the filterH(u, 0) = —47"[(u— P/2?? + (@@- 9/2)"
(4.9-6)
= ~47°D*(u, v)where D(u, v) is the distance function given in Eq. (4.8-2). Then, the Laplacian
image is obtained as: .V(x, y) = I71{H(u, v)F(u, v)} (4.9-7)where F(x, v) is the DFT of f(x, y). As explained in Section 3.6.2, enhancement is achieved using the equation:g(x,y) = f(x,y) + eV F(x, y) (4.9-8)Here,c = —1 because H(u, wv) is negative. In Chapter 3, f(x, y) and V(x, y)
had comparable values. However, computing V°f(x, y) with Eq. (4.9-7) introduces DFT scaling factors that can be several orders of magnitude Jarger than
the maximum value of f Thus, the differences between f and its Laplacian
must be brought into comparable ranges. The easiest way to handle this problem is to normalize the values of f(x, y) to the range [0, 1] (before computing
its DFT) and divide V°f(x, y) by its maximum value, which wilt bring it to the
approximate range [—1,1] (recall that the Laplacian has negative values).
Equation (4.9-8) can then be applied.
In the frequency domain, Eq. (4.9-8) is written asa(x, y) = SFU, v) — Au, v)F(u, v)}
= y{[1 — H(u, v)|F(u, v)} (4.9-9)
= [1 + 4n°D%(u, v) Fu, v)}Although this result is elegant, it has the same scaling issues just mentioned,
compounded by the fact that the normalizing factor is not as easily computed.
For this reason, Eq. (4.9-8) is the preferred implementation in the frequency
domain, with V’f(x, y) computed using Eq. (4.9-7) and scaled using the approach mentioned in the previous paragraph.@ Figure 4.58(a) is the same as Fig. 3.38(a), and Fig, 4.58(b) shows the result of
using Eq. (4.9-8), in which the Laplacian was computed in the frequency domain using Eq. (4.9-7). Scaling was done as described in connection with that
equation. We see by comparing Figs. 4.58(b) and 3.38(e) that the frequency domain and spatial results are identical visually. Observe that the results in these
two figures correspond to the Laplacian mask in Fig, 3.37(b), which has a —8 in
the center (Problem 4.26). iEXAMPLE 4.20:
Image sharpening
in the frequency
damain using the
Laplacian.
310  Chupter 4 @ Filtering in the Frequency DomainabFIGURE 4.58{a) Original,
blurry image.(b) Image
enhanced using
the Laplacian in
the frequency
domain. Compare
with Fig. 3.38(e). 4.9.3 Unsharp Masking, Highboost Filtering,and High-Frequency-Emphasis Filtering
In this section, we discuss frequency domat formulations of the unsharp
masking and high-boost filtering image sharpening techniques introduced in
Section 3.6.3. Using (requency domain methods, the mask defined in Eq. (3.6-8)
is given byLyrae ¥) = fOr) ~ fp fay) (4.9-10)
with
fiply yy 3 WTF pte wily nf (4.9-11)where ff; p(a. 7) is a lowpass filter and F(a, ¢) is the Fourier transform af
f(x, y). Here, fp (yy) is asmoothed image analogous lo f(y. vy} in Eq. (3.6-8).
Then, as in Eq. (3.6-9),  ete ve flrs he Bane vd (4.9-]2)This expression defines unsharp masking when & © J and bighboust fikering when & > 1. Using the preceding resulis, we can express Eq. (4.9-12)
entirely in terms of frequency domain computations involving a lowpass
filter:Phe Mypte peo a, (49-13) Using Eq. (4.9-1)owe van express this resuttin formas of a highpass (liesa) wd booPdrata hie die} pata
312 Chapter 4 @ Filtering in the Frequency DomainIan image f(x, ») with
intensities in the range
{0. L ~ IJ has any 0 values, a } must be added to
every element of the
image to avoid having to
deal with In{0). The 1 is
then subtracted at the
end of the filtering
process. ab
edFIGURE 4.59 (a) A chest X-ray image. (b) Resull of highpass filtering wilh a Gaussian
filter. (c) Resuit of high-frequency-emphasis filtcring using the same filter. (d) Result of
performing histogram equalization on (c). (Original image courtesy of Dr. Thomas R.Gest, Division of Anatomical Sciences, University of Michigan Medical Schaal.)This equation cannot be used directly to operate on the frequency components of illumination and reflectance because the Fourier transform of a prod
uct is not the product of the transforms:
SfOe a) # AfiOr 9] AlrO, v)]
However, suppose that we define
e(voy) = In firey)
= inex) +t bvréxv)}
Then,
R{olx, vi} - Xin ily, wi~!
shaor1 .
il af vio athe ft AfIn rays(4.9-17) (4.9-19)ghd rhs
4.9 w@ Image Sharpening Using Frequency Domain Filters 311The expression contained within the square brackets is called a high-frequencyemphasis filter. As noted earlier, highpass filters set the de term to zero, thus
reducing the average intensity in the filtered image to 0. The high-frequencyemphasis filter does not have this problem because of the 1 that is added to the
highpass filter. The constant, k, gives control over the proportion of high frequencies that influence the final result. A slightly more general formulation of
high-frequency-emphasis filtering is the expression(xy) = 9 {ky + kot Hurl, Fu, v)} (49-15)where k; = 0 gives controls of the offset from the origin [see Fig. 4.31(c)] and
kz, = Ocontrols the contribution of high frequencies.@ Figure 4.59{a) shows a 416 X 596 chest X-ray with a narrow range of intensity levels. The objective of this example is to enhance the image using highfrequency-emphasis filtering. X-rays cannot be focused in the same manner
that optical Jenses are focused, and the resulting images generally tend to be
slightly blurred. Because the intensities in this particular image are biased
toward the dark end of the gray scale, we also take this opportunity to give
an example of how spatial domain processing can be used to complement
frequency-domain filtering.Figure 4.59(b) shows the result of highpass filtering using a Gaussian filter
with Do = 40 (approximately 5% of the short dimension of the padded
image). As expected, the filtered result is rather featureless, but it shows faintly the principal edges in the image. Figure 4.59(c) shows the advantage of highemphasis filtering, where we used Eq. (4.9-15) with k, = 0.5 and k2 = 0.75.
Although the image is still dark, the gray-level tonality due to the low-frequency
components was not lost,As discussed in Section 3.3.1, an image characterized by intensity levels in a
narrow range of the gray scale is an idea] candidate for histogram equalization. As Fig. 4.59(d) shows, this was indeed an appropriate method to further
enhance the image. Note the clarity of the bone structure and other details
that simply are not visible in any of the other three images. The final enhanced
image is a little noisy, but this is typical of X-ray images when their gray scale
is expanded. The result obtained using a combination of high-frequency emphasis and histogram equalization is superior to the result that would be obtained by using either method alone. Ed4.9.6 Homomorphic FilteringThe illumination-reflectance model introduced in Section 2.3.4 can be used to
develop a frequency domain procedure for improving the appearance of an
image by simultaneous intensity range compression and contrast enhancement. From the discussion in that section, an image f(x, y) can be expressed as
the product of its ilJumination, i(x, y), and reflectance, r(x, y), components:f(xy) = ite, y)r(x, y) (4.9-16)EXAMPLE 4.21:
Image
enhancement
using highfrequencyemphasis filtering.Artifacts such as ringing.
are unacceptable ia med.
ical imaging. Thus. it is.
good practice to avoid
using filters that have the
potential for introducing
artifacts in the processed
image. Because spatial
and (requency domain
Gaussian fillers are
Fourier transform pairs,
these filters produce
smooth results (hat ate
void of artifacts,
4.9 % Image Sharpening Using Frequency Domain Filterswhere F;(u,v) and F,(u,v) are the Fourier transforms of In é(x, ) and
In r(x, y), respectively.
We can filter Z(u, v) using a filter H(u, v) so thatS(u,v) = H(u, v)Z(u, v)(49-21)
= Hu, v)F;(u,v) + Hu, v)F,(u, v)
The filtered image in the spatial domain is
(4, y) = Isc, »)}
(4.9-22)
= WAG, v)F,(u, vy} + S'{A(u, v)F, (u, »)}
By defining
(x,y) = SA, vw F(u, »} (4.9-23)
and
rx y) = TH v)F (a, 2)} (4.9-24)
we can express Eq. (4.9-23) in the form
S(x,y) = iO y) + r(x y) > (4.9-25)Finally, because z(x, y) was formed by taking the natural logarithm of the
input image, we reverse the process by taking the exponential of the filtered
result to form the output image:
g(x,y) = e800)
= efGre"Gy) (4.9-26)fol yoy ¥}whereig(ay y) = ef) (4.9-27)
and
ro(x, y) = eft) (4.9-28)are the illumination and reflectance components of the output (processed)
image.313
314 Chapter 4 # Filtering in the Frequency DomainFIGURE 4.60
Summary of steps
in homomorphic
filtering.FIGURE 4.61
Radial cross
section of a
circularly
symmetric
homomorphic
filter function.
The vertical axis is
at the center of
the frequency
rectangle and
D(u, v) is the
distance from the
center, g(% ¥)flxy) > wb DFT [7 >| Hu, 0)The filtering approach just derived is summarized in Fig. 4.60. This method
is based on a speciai case of a class of systems known as homomorphic systems.
In this particular application, the key to the approach is the separation of the
illumination and reflectance components achieved in the form shown in
Eq. (4.9-20). The homomorphic filter function H(u, v) then can operate on
these components separately, as indicated by Eq. (4.9-21).The illumination component of an image generally is characterized by slow
spatial variations, while the reflectance component tends to vary abruptly, particularly at the junctions of dissimilar objects. These characteristics lead to associating the low frequencies of the Fourier transform of the logarithm of an
image with illumination and the high frequencies with reflectance. Although
these associations are rough approximations, they can be used to advantage in
image filtering, as illustrated in Example 4.22.A good deal of control can be gained over the illumination and reflectance
components with a homomorphic filter. This control requires specification of
a filter function H (u, v) that affects the low- and high-frequency components
of the Fourier transform in different, controllable ways. Figure 4.61 shows a
cross section of such a filter. If the parameters y, and y, are chosen so that
yi < land y,, > 1, the filter function in Fig. 4.61 tends to attenuate the contribution made by the low frequencies (illumination) and amplify the contribution made by high frequencies (reflectance). The net result is simultaneous
dynamic range compression and contrast enhancement.The shape of the function in Fig. 4.61 can be approximated using the basic
form of a highpass filter. For example, using a slightly modified form of the
Gaussian highpass filter yields the function Hw v) = a ~ yeifh — MPP § yy (4.9.28)A{av)Va pooner HS¥).  PAu.)
Sor ERR49. Iniage Sharpening Using Freyuency Domain Filterswhere D(u, v) is defined in Eq. (4.8-2) and the constant ¢ controls the
sharpness of the slope of the function as it transitions between y, and yy,
This filter is similar to the high-cmphasis filter discussed in the previous
section. @ Figure 4.62(a) shows a full body PET (Positron Emission Tomography)
scan of size 1162 x 746 pixels. The image is slightly biurry aud many of its
low-intensity features are obscured by the high intensity of the “hot spots”
dominating the dynamic range of the display. (These hot spots were caused by
a tumor in the brain and one in the lungs.) Figure 4.62(b) was obtained by homomorphic filtering Fig. 4.62(a) using the filter in Eg. (4.9-29} with
yy = 0.25, yy = 2,¢ = 1, and Dy = 80. A cross section of this filter looks
just like Fig. 4.61, with a slightly steeper slope.Note in Fig. 4.62(b) how much sharper the hot spois, the brain. and the
skeleton are in the processed image, and how much more detail is visible in
this image. By reducing the effects of the dominant illumination components
{the hot spots), it became possible for the dynamic range of the display to
allow lower intensities to become much more visible. Similarly, because the
high frequencies are enhanced by homomarphie filtering. the reflectance
components of the image (edge information) were sharpened considerably.
The enhanced image in Fig. 4.62(b) is a significant improvement over the
original.  1Ww
uwEXAMPLE 4.22:
Image
enhancement
using
homomorphic
fiktering.Reeull thal iiltering uses
imape padding, su the 1
trisot site Px O abFIGURE 4.62(a) Full body PET
scan. (b) Image
enhanced using
homonorphic
filtering. {Original
miage courtusy of
Dr. MichactE Casey. OTE
PIT Systems)
4.10 # Selective Filtering 317     center of the frequency rectangle, (M/2, N/2). The distance computations for
each filter are thus carried out using the expressionsDyue, 0) = [(u — M/2— uy)? + (v— N/2~ v4)]!° (4,10-3)
and
D_,(u, 0) = [(a— M/2 + uy)? + (V=N/24+0,)7]!? — (4.10-4)For example, the following is a Butterworth notch reject filter of order 7, containing three notch pairs: .=> | (410-5oR 03 1
Halts ¥) Ti] 1+ [Da/Dile wallwhere D, and D_, are given by Eqs. (4,10-3) and (4.10-4). The constant Dg is
the same for each pair of notches, but it can be different for different pairs.
Other notch reject filters are constructed in the same manner, depending on
the highpass filter chosen. As with the filters discussed earlier, a notch pass filter is obtained from a notch reject filter using the expression   Hyp(u.v) = 1 — Hyr(u, 2) (4.10-6)As the next three examples show, one of the principal applications of notch
filtering is for selectively modifying local regions of the DFT. This type of processing typically is done interactively, working directly on DFTs obtained
without padding, The advantages of working interactively with actual DFTs
{as opposed to having to “translate” from padded to actual frequency values}
outweigh any wraparound errors that may result from not using padding in
the filtering process. Also, as we show in Section 5.4.4, even more powerful
notch filtering techniques than those discussed here are based on unpadded
DFTs. To get an idea of how DFT values change as a function of padding, sec
Problem 4.22.abFIGURE 4.63(a) Bandreject
Gaussian filter.
(b) Corresponding
bandpass filter.
The thin black
border in (a) was
added for clarity; it
is not part of the
data.
316 Chopter 4 wa Filtering in the Frequency Domain4.10 | Selective FilteringThe filters discussed in the previous two sections operate over the entire frequency rectangle. There are applications in which it is of interest to process
specific bands of frequencies or small regions of the frequency rectangle. Filters in the first category are called bandreject or bandpass filters, respectively.
Filters in the second category are called notch filters.4.16.1 Bandreject and Bandpass FiltersThese types of filters are easy to construct using the concepts from the previous two sections. Table 4.6 shows expressions for ideal, Butterworth, and
Gaussian bandreject filters, where D(u, v) is the distance from the center of
the frequency rectangle, as given in Eq. (4.8-2), Do is the radial center of the
band, and W is the width of the band. Figure 4.63(a) shows a Gaussian bandteject filter in image form, where black is 0 and white is 1.A bandpass filter is obtained from a bandreject filter in the same manner
that we obtained a highpass filter from a lowpass filter:Hep(u, ¥) = 1 ~ Agp(u, v) (4.10-1)
Figure 4.63(b) shows a Gaussian bandpass filter in image form.4.10.2 Notch FiltersNotch filters are the most useful of the selective filters. A notch filter rejects
(or passes) frequencies in a predefined neighborhood about the center of the
frequency rectangle. Zero-phase-shift filters must be symmetric about the origin, so a notch with center at (up, vg) must have a corresponding notch at location (~u9, ~vp}. Notch reject filters are constructed as products of highpass
filters whose centers have been transtated to the centers of the notches. The
general form is:Q
Hyplu, v) = Tile, 0) Hae, 0) (4.10-2)
ke}where 7,(u, v) and H_,(a, v) are highpass filters whose centers are at (1;, 0;)
and (~u,, ~v,), respectively. These centers are specified with respect to theTABLE 4.6
Bandreject filters. W is the width of the band, D is the distance D(u, v) from the center of the filter, Dy is thecutoff frequency, and 1 is the order of the Butterworth filter. We show D instead of D(u, v) to simplify the
notation in the table. Butterworth Gaussian  1pw |)" Hiv =1-e!
lt) oT Rp
D— Da0 itdy- P= Ds n+ H(u,v) = H(u,v) = 1 otherwise
318 Chapter 4 Filtering in the Frequency DomainEXAMPLE 4.23:
Reduction of
moiré patterns
using notch
filtering.ab
edFIGURE 4.64(a) Sampled
newspaper image
showinga |
moiré pattern.
{b) Spectrum(c) Butterworth
notch reject filter
multiplied by the
Fourier
transform.(d) Filtered
image.m Figure 4.64{a) is the scanned newspaper image from Fig. 4.21, showing a
prominent moiré pattern, and Fig. 4.64(b) is its spectrum. We know from
Table 4.3 that the Fourier transform of a pure sine, which is a periodic function, is a pair of conjugate symmetric impulses. The symmetric “impulsc-like™
bursts in Fig. 4.64(b) are a result of the near periodicity of the moiré pattern.
We can attenuate these bursts by using notch filtering.
4.10 © Splective Filtering 319Figure 4.64(c) shows the result of multiplying the DFT of Fig. 4.64{a) by a
Butterworth notch reject filter with Dy = 3 and n = 4 for all notch pairs. The
value of the radius was selected (by visual inspection of the spectrum) to encompass the energy bursts completely, and the value of 7 was selected to give
notches with mildly sharp transitions. The locations of the center of the notches were determined interactively from the spectrum. Figure 4.64{d) shows the
result obtained with this filter using the procedure outlined in Section 4.7.3.
The improvement is signifitant, considering the law resolution and degradation of the original image.
@ Figure 4.65(a) shows an image of part of the rings surrounding the planet
Saturn. This image was captured by Cassini, the first spacecraft to enter the
planet’s orbit. The vertical sinusoidal pattern was caused by an AC signal superimposed on the camera video signal just prior to digitizing the image. This
was an unexpected problem that corrupted some images from the mission.
Fortunately, this type of interference is fairly casy to correct by postprocessing.
One approach is to use notch filtering,Figure 4.65(b) shows the DFT spectrum. Careful analysis of the vertical axis
reveals a series of small bursts of energy which correspond to the nearly sinusoidal    EXAMPLE 4.24:
Enhancement of
corrupted Cassini
Saturn image by
notch filtering.ab
cdFIGURE 4.65(a) 674 % 674
image of the
Saturn rings
showing nearly
perioctic
interference.(b) Spectrum: The
bursts of energy
in the vertical axis
near the origin
correspond to the
interference
pattern. (¢) A
vertical notch
reject filter,(d) Result of
filtering, The thin
black border in
(c) was added for
clarity; it is not
part of the data.
{Oviginal image
courtesyof Dy. RobertAL West,
NASA/IP).)
320 Chapter 4 # Filtering in the Frequency DomainabFIGURE 4.66(a) Result
(spectrum) of
applying a notch
pass filter tothe DFT ofFig. 4.65(a).(b) Spatial
pattern obtained
by computing the
IDFT of (a). interference. A simple approach is to use a narrow notch rectangle filter starting
with the lowest frequency burst and extending for the remaining of the vertical
axis. Figure 4.65(c) shows such a filter (white represents 1 and black 0). Figure
4,65(d) shows the result of filtering the corrupted image with this filter. This result
is a significant improvement over the original image.We isolated the frequencies in the vertical axis using a notch pass version of
the same filter (Fig. 4.66(a)]. Then. as Fig. 4.66(b) shows, the [DFT of these frequencies yielded the spatial interference pattern itself. 3ei ImplementationWe have focused attention thus far on theoretical concepts and on examples ct
filtering in the frequency domain. One thing that should be clear by now is that
computational requirements in this area of image processing are not trivial.
Thus, it is important to develop a basic understanding of methods by which
Fourier transform computations can be simplified and speeded up. This section deals with these issucs.
3.41.2 Separability of the 2-D DFT
As mentioned in Table 4.2. the 2-D DFP as separable into 1-D transforms. We
can write Eq. (4.5-15) as
N= Vi .
Facey = Se BRM ST pry ype PRONmf) vi ~(11-1wd
oy. bene
SM Foxe '
wot1
wherefraep oo Novas ; Atl)
322 = Chapter 4 @ Filtering in the Frequency DomainAlthough the FFT is a topic covered extensively in the literature on signal
processing, this subject matter is of such significance in our work that this
chapter would be incomplete if we did not provide at least an introduction explaining why the FFT works as it does. The algorithm we selected to accomplish this objective is the so-called successive-doubling method, which was the
original algorithm that led to the birth of an entire industry. This particular algorithm assumes that the number of samples is an integer power of 2, but this
is not a general requirement of other approaches (Brigham [1988]). We know
from Section 4.11.1 that 2-D DFTs can be implemented by successive passes
of the 1-D transform, so we need to focus only on the FFT of one variable.When dealing with derivations of the FFT, it is customary to express Eq.
(44-6) in the formF(u) = Seog (4.11-4)
=
u=0,1,...,M — 1, where
Wy = Pri (4.11-5)
and M is assumed to be of the form
M = 2" . (4.11-6)
with n being a positive integer. Hence, M can be expressed as
M = 2K (4.11-7)with K being a positive integer also. Substituting Eq. (4,11-7) into Eq. (4.11-4)
yields '2K-1
Flu) = >» FQ)WEK(4.11-8)K-i K~I
Daxywik? + Thx + wage?
x0 x=0However, it can be shown using Eq. (4.11-5) that W3% = Wy", so Eq. (4.11-8)
can be expressed asK-i K~1
Fu) = SfQxwe + S fx + DWE Wo, (4.11-9)
x= +0
Defining
K-!
Fever tt) = DS Awe (4.11-10)
x=0foru = 0,1,2,...,K ~- 1, and
4.11 & Implementation 321For each value of x and for v = 0,1,2,..., M — 1, we see that F(x, v) is simply the 1-D DFT of a rew of f(x, y). By varying x from 0 to M — 1 in Eq.
(4.11-2), we compute a set of 1-D DFTs for all rows of f(x, y). The computations in Eq. (4.11-1) similarly are 1-D transforms of the columns of F(x, v).Thus, we conclude that the 2-D DFT of f(x, y) can be obtained by computing the 1-D transform of each row of f(x, y) and then computing the 1-D
transform along each column of the result. This is an important simplification
because we have to deal gnly with one variable at a time. A similar development applies to computing the 2-D IDFT using the 1-D IDFT. However, as we
show in the following section, we can compute the IDFT using an algorithm
designed to compute the DFT.4.11.2 Computing the IDFT Using a DFT AlgorithmTaking the complex conjugate of both sides of Eq. (4.5-16) and multiplying the
results by MN yieldsM-IN-I
MN (x,y) = SS SF (avy er PreiMersiN) (4.11-3)w=0 v0But, we recognize the form of the right side of this result as the DFT of
F'(u, v). Therefore, Eq. (4.11-3) indicates that if we substitute F"(w, v) into an
algorithm designed to compute the 2-D forward Fourier transform, the result
will be MNf"(x. y). Taking the complex conjugate and multiplying this result
by MN yields f(x, y), which is the inverse of F(x, v).Computing the 2-D inverse from a 2-D forward DFT algorithm that is based
on successive passes of 1-D transforms {as in the previous section) is a frequent
source of confusion involving the complex conjugates and multiplication by a
constant, neither of which is done in the 1-D algorithms. The key concept to
keep in mind is that we simply input F’(u, v) into whatever forward algorithm
we have. The result will be MNf (x, y). All we have to do with this result to
obtain f(x, y) is to take its complex conjugate and multiply it by the constant
MN. Of course, when f(x, y) is reat, as typically is the case, f(x, y) = f(x, y).4.{1.3 The Fast Fourier Transform (FFT)Work in the frequency domain would not be practical if we had to implement
Egs. (4.5-15) and (4,5-16) directly. Brute-force impJementation of these equations
requires on the order of (A/V)? summations and additions, For images of moderate size (say, 1024 x 1024 pixels), this means on the order of a trillion multiplications and additions for just one DFT, excluding the exponentials, which could be
computed once and stored in a look-up table. This would be a challenge even for
super computers. Without the discovery of the fast Fourier transform (FFT),
which reduces computations to the order of MNlogo»MN multiplications and additions, it is safe to say that the material presented in this chapter would be of little practica) value. The computational reductions afforded by the FFT are
impressive indeed. For exampte, computing the 2-D FFT of a 1024 x 1024 image
would require on the order of 20 million multiplication and additions, which is a
significant reduction from the one trillion computations mentioned above.We could have expressed
Ey. (4.11-1) and (411-2)
in the form of 1-D column transforms followed
by row transforms The
final resuslt would have
been the same.Muhiplication by AfN in
this development assumes the forms in Eqs.
(45-158) and (4.5-16). A
different constant multiplication scheme is re~
quired if the constants
are distributed differently beiween the forward
and inverse Wransforms.
4.11 © ImplementationK-1
Foas(u) = > f(2x + WEE (4.11-11)
x=0foru = 0,1,2,..., K — 1, reduces Eq. (4.11-9) to
F(u) = Feven() + Fog wax (4.11-12)Also, because Wij*™ = Wi and W8i = -W4y, Eqs. (4.11-10) through
(4.11-12) giveFlu + K) = Feyen(u) — Foaal4)Wix (4.11-13)Analysis of Eqs. (4.11-10) through (4.11-13) reveals some interesting properties of these expressions. An M-point transform can be computed by dividing the original expression into two parts, as indicated in Eqs. (4.11-12) and
(4.11-13). Computing the first half of F(«) requires evaluation of the two
(M/2)-point transforms given in Eqs. (4,11-10) and (4.11-11). The resultingvalues of Foyen(u) and Foga(u) are then substituted into Eq. (4.11-12) to obtain
F(u) for « = 0,1,2,...,(M/2 ~ 1). The other half then follows directly fromEq. (4.11-13) without additional transform evaluations.In order to examine the computational implications of this procedure, let s(s)
and a(n) represent the number of complex multiplications and additions, respectively, required to implement it. As before, the number of samples is 2” with 7 a
positive integer. Suppose first that 7 = 1. A two-point transform requires the
evaluation of F(0); then F(1) follows from Eq. (4.11-13).To obtain F(0) requires
computing Feyen(0) and Fygy(0), In this case K = 1 and Eqs. (4.11-10) and (4.11-11)
are one-point transforms. However, because the DFT of a single sample point is
the sample itself, no multiplications or additions are required to obtain Fever(0)
and F,4q(0). One multiplication of Fygg(0) by WS and one addition yield F(0)
from Eq. (4.11-12). Then F(1) follows from (4.11-13) with one more addition
(subtraction is considered to be the same as addition). Because Fygq(0)W3 has alteady been computed, the total number of operations required for a two-pointtransform consists of (1) = 1 multiplication and a(1) = 2 additions.The next allowed vaiue for n is 2. According to the above development, a
four-point transform can be divided into two parts. The first half of F(u) requires evaluation of two, two-point transforms, as given in Eqs. (4.11-10) and
(4.11-11) for K = 2. As noted in the preceding paragraph, a two-point transform requires m(1} multiplications and a(1) additions, so evaluation of these
two equations requires a total of 2(1) multiplications and 2a(1) additions.
Two further multiplications and additions are necessary to obtain F(0) and
F(1) from Eq. (4.11-12). Because Fyya(u)W$x already has been computed for
u = {0,1}, two more additions give F(2) and F(3). The total is then
m(2)} = 2m(1) + 2 and a(2) = 2a(1) + 4.When n is equal to 3, two four-point transforms are considered in the evaluation of Fyyen(a) and Foag(u). They require 2(2) multiplications and 2a(2)
additions. Four more multiplications and eight more additions yield the complete transform. The total then is 77(3) = 2m(2) + 4 and a(3) = 2a(2) + 8.323
324 Chapter 4 m Filtering in the Frequency DomainContinuing this argument for any positive integer value of n leads to recursive expressions for the number of multiplications and additions required to
implement the FFT:m(n) = 2m(n ~ 1) +2") net (4.11-14)and
a(n) = 2a(n - 1) +2" n21 (411-15)where (0) = 0 and a(0) = 0 because the transform of a single point does not
require any additions or multiplications.Implementation of Eqs. (4.11-10) through (4.11-13) constitutes the successive doubling FFT algorithm. This name comes from the method of computing
a two-point transform from two one-point transforms, a four-point transform
from two two-point transforms, and so on, for any M equal to an integer power
of 2. It is Jeft as an exercise (Problem 4.41} to show thatm(n) = 5M logs M (4.11-16)and
a(n) = M log, M (4.11-17)The computational advantage of the FFT over a direct implementation of the
1-D DFT is defined asMm
M log, M-—_
log, Me(M) =
(4.11-18)Because it is assumed that M = 2", we can write Eq. (4.11-18) in terms of n:nAe(n) = 2 (4.11-19)Figure 4.67 shows a plot of this function. It is evident that the computational
advantage increases rapidly as a function of n. For instance, when n = 15
(32,768 points), the FFT has nearly a 2,200 to 1 adyantage over the DFT. Thus,
we would expect that the FFT can be computed nearly 2,200 times faster than
the DFT on the same machine.There are so many excellent sources that cover details of the FFT that we will
not dwell on this topic further (see, for example, Brigham [1988]), Virtually all
comprehensive signal and image processing software packages have generalized
implementations of the FFT that handle cases in which the number of points is
not an integer power of 2 (at the expense of less efficient computation). Free
FFT programs also are readily available, principally over the Internet.
Re |  C(n) 1200  oli
123 4 5 6 7 8 9 1011 12 13 14 15n4.11.4 Some Comments on Filter DesignThe approach to filtering discussed in this chapter is based strictly on fundamentals, the focus being specifically to explain the effects of filtering in the frequency domain as clearly as possible. We know of no better way to do that
than to treat filtering the way we did here. One can view this development as
the basis for “prototyping” a filter. In other words, given a problem for which
we want to find a filter, the frequency domain approach is an ideal tool for experimenting, quickly and with full contro] over filter parameters.Once a filter for a specific application has been found, it often is of interest to implement the filter directly in the spatial domain, using firmware and/or hardware.
This topic is outside the scope of this book. Petrou and Bosdogianni (1999] present
a nice tie between two-dimensional frequency domain filters and the corresponding digital filters. On the design of 2-D digital filters, see Lu and Antoniou [1992].SummaryThe materia! in this chapter is a progression from sampling to the Fourier transform,
and then to filtering in the frequency domain. Some of the concepts, such as the sampling theorem, make very little sense if not explained in the context of the frequency
domain. The same is true of effects such as aliasing. Thus, the material developed in the
Preceding sections is a solid foundation for understanding the fundamentals of digital
signal processing. We took special care to develop the material starting with basic principles, so that any reader with a modest mathematical background would be in a position not only to absorb the material, but also to apply it.Asecond major objective of this chapter was the development of the discrete Fourier transform and its use for filtering in the frequency domain. To get there, we had to introduce the convolution theorem. This result is the foundation of linear systems, and
underlies many of the restoration techniques developed in Chapter 5. The types of filters we discussed are representative of what one finds in practice. The key point in presenting these filters, however, was to show how simple it is to formulate and implement
filters in the frequency domain. While final implementation of a solution typically is
based on spatial filters, the msight gained by working in the frequency domain as a
guide in the selection of spatial filters cannot be overstated.Although most filtering examples in this chapter are in the area of image enhancement,
the procedures themselves are general and are utilized extensively in subsequent chapters.w Summary 325FIGURE 4.67
Computational
advantage of the
FFT over a direct
implementation
of the 1-D DFT.
Note that the
advantage
increases rapidly
as a function of 7.
326 Chopter 4 mt Filtering in the Frequency DomainSip,
Detailed solutions to the
problems marked with a
stat can be found in the
book Web site. The site
also contains suggested
projects based on the material in this chapterReferences and Further ReadingFor additional reading on the material in Section 4.1, see Hubbard [1998]. The books by
Bracewell [2000, 1995] are good intraductions to the continuous Fourier transform and its
extensions to two dimensions for image processing. These two books, in addition to Lim
[1990], Castleman [1996], Petrou and Bosdogianni [1999], Brigham [1988], and Smith
[2003], provide comprehensive background for most of the discussion in Sections 4,2
through 4.6. For an overview of early work on the topic of moiré patterns, see Oster and
Nishijima [1963]. Creath and Wyant [1992] discuss the state of the art in that field thirty
years later. The sampling, aliasing, and image reconstruction issues discussed in Section 4.5
are also topics of significant interest in computer graphics, as exemplified by Shirley [2002].For additional background on the material in Sections 4,7 through 4.11, see Castleman
[1996], Pratt (2001), and Hall [1979]. To learn more about the imaging sensors in the
Cassini spacecraft (Section 4.10.2), see Porco, West, et al. [2004]. Effective handling of issues on filter implementation (like ringing) still is a topic of interest, as exemplified by
Bakir and Reeves [2000]. For unsharp masking and high-frequency-emphasis filtering,
see Schowengerdt [1983]. The material on homomorphic filtering (Section 4.9.5) is based
on a paper by Stockham [1972]; see also the books by Oppenheim and Schafer [1975] and
Pitas and Venetsanopoulos [1990]. Brinkman et al. [1998] combine unsharp masking and
homomorphic filtering for the enhancement of magnetic resonance images,As noted in Section 4.1.1, “discovery” of the Fast Fourier transform (Section 4.11.3)
was a major milestone in the popularization of the DFT as a fundamental signal processing tool. Our presentation of the FFT in Section 4.11,3 is based on a paper by Cooley and
Tuckey [1965] and on the book by Brigham {1988], who also discusses several implementations of the FFT, including bases other than 2. Formulation of the fast Fourier transform
is often credited to Cooley and Tukey [1965]. However, the FFT has an interesting history worth sketching here. In response to the Cooley~Tukey paper, Rudnick [2966] reported that he was using a similar technique, whose number of operations also was
proportional to Nlog)N and which was based on a method published by Danielson and
Lanczos [1942]. These authors, in turn, referenced Runge [1903, 1905] as the source of
their technique. The latter two papers, together with the lecture notes of Runge and
Kénig [1924], contain the essential computational advantages of present FFT algorithms.
Similar techniques also were published by Yates [1937], Stumpff [1939], Good [1958], and
Thomas [1963]. A paper by Cooley, Lewis, and Welch [1967a] presents a historical summary and an interesting comparison of results prior to the 1965 Cooley-Tukey paper.The FFT algorithm in Section 4.11.3 is from the original paper by Cooley and Tukey
[1965]. See Brigham [1988] and Smith [2003] for complementary reading. For the design of digital filters (Section 4.11.4) based on the frequency domain formulations discussed in this chapter, see Lu and Antoniou [1992] and Petrou and Bosdogianni [1999].
For software implementation of many of the approaches discussed in Sections 4.7
through 4.11, see Gonzalez, Woods, and Eddins {2004}.Problems .
41 Repeat Example 4.1, but using the function f(*) = 2A for -W/4 31 = Wi4
and f(c) = Q for all other values of s. Explain the reason for any differences between your results and the results in the example.
#42 Show that F(j) in Eq. (4.4-2) is infinitely periodic in both directions, with period 1/AT.
*43° It can be shown (Bracewell [2000]} that 1 <= 5() and &(r) <= 1. Use the first of
these properties and the translation property from Table 4.3 to show that the
Fourier transform of the continuous function f(t) = cos(2war), where 7 isa real
number, is f(x) = (1/2)[(u + n) + dp — n)).
328 Chopter 4m Filtering in the Frequency Domain4.164.174.184.194.206 4.21was split as two constants 1/W/MN in front of both the forward and inverse
transforms, How can you find where the term(s) is (are) included if this information is not available in the documentation?Prove that both the continuous and discrete 2-D Fourier transforms are translation and rotation invariant.You can infer-from Problem 4.3 that 1 > S(u. v) and 6&{#, z) <> t. Use the first of
these properties and the translation property in Table 4.3 to show that the Fourier
transform of the continuous function f(t, z) = A cos(2rpyt + 2avy 2) is1
fr) = 5[8H + Hoe +H) + Ble ~ my — 0]Show that the DFT of the discrete function f(x, y) = 1 is1 ife=v=0
Sl} = d{u,v) = :
{1} (uv) {; otherwise
Show that the DFT of the discrete function f(x, y) = cos(2ruyx + 2729}! is
l
F(u,v} = ALG + Mi, vp + Ney) + Su — Mig, v — Nv) }The following problems are related to the properties in Table 4.1.
x(a) Prove the validity of property |.
y(b) Prove the validity of property 3.
(c) Prove the validity of property 6.
x(d) Prove the validity of property 7.
(e} Prove the validity of property 9.
{f) Prove the validity of property 10.
x (g) Prove the validity of property 11.
(h) Prove the validity of property 12.
(i) Prove the validity of property 13.
The need for image padding when filtering in the frequency domain was discussed in Section 4.6.6. We showed in that section that images needed to be
padded by appending zeros to the ends of rows and columns in the image (sce
the following image on the left). Do you think it would make a difference if we
4.4 Consider the continuous function f(i) = cos(2mnt).
*(a) What is the period of f(s)?
*%(b) What is the frequency of f(t)?
The Fourier transform, F (4), of f{#) is real (Problem 4.3), and because the transform of the sampled data consists of periodic copies of F(,), the transform of
the sampled data, F (yu), will also be real. Draw a diagram similar to Fig. 4.6,
and answer the following questions based on your diagram (assume that sampling
starts att = 0). .
*&(c) What would the sampled function and its Fourier transform look like in
general if f(t) is sampled at a rate higher than the Nyquist rate?
{d) What would the sampled function look like in general if f(r) is sampled at a
rate lower than the Nyquist rate?
(e) What would the sampled function look like if {(¢) is sampled at the Nyquist
sate with samples taken at = 0, AT, 2 AT,,..?7
*45 Prove the validity of the 1-D convolution theorem of a continuous variable, as
given in Eqs. (4.2-21) and (4.2-22).4.6 Complete the steps that led from Eq. (4.3-11) to Eq, (4.3-12).4.7 As the figure below shows, the Fourier transform of a “tent” function (on the left) is
a squared sinc function (on the right), Advance an argument that shows that the
Fourier transform of a tent function can be obtained from the Fourier transform of a
box function, (Hint: The tent itseif can be generated by convolving two equal boxes.)4.8 (a) Show that Eqs. (4.4-4) and (4.4-S)} constitute a Fourier transform pair. |
&(b) Repeat (a) for Eqs. (4.4-6) and (4.4-7). You will need the following orthogonality property of exponentials in both parts of this problem:Sea Mer flmusiM M ifr=u
= 0 otherwise
4.9 — Prove the validity of Eqs, (4.4-8) and (4.4-9).
*4.10 Prove the validity of the discrete convolution theorem of one variable [see Eqs.
(4.2-21), (4.2-22), and (4.4-10)]. You will need to use the translation properties
f(xje? to o> F(u — up) and conversely, f(x ~— x9) > Fue Pe,*4.51 Write an expression for 2-D discrete convolution.4.12 Consider a checkerboard image in which each square is 0.5 X 0.5 mm. Assuming that the image extends infinitely in both coordinate directions, what is the
minimum sampling rate (in samples/mm) required to avoid aliasing?413 We know from the discussion in Section 4.5.4 that shrinking an image can cause
aliasing. Is this true also of zooming? Explain.
* 4.14 Prove that both the 1-D continuous and discrete Fourier transforms are linear
operations (see Section 2.6.2 for a definition of linearity).415 You are given a “canned” program that computes the 2-D, DFT pair. However,
it is not known in which of the two equations the 1/MN term is included or if it@ Problems327
& 4,224.234.24
4.254.26centered the image and surrounded il by a border of zeros instead (see image on
the right), but without changing the total number of zeros used? Explain.The two Fourier spectra shown are of the same image. The spectrum on the left
corresponds to the original image, and the spectrum on the right was obtained
after the image was padded with zeros. Explain the significant increase in signal
strength along the vertical and horizontal axes of the spectrum shown on the
right. You know from Table 4,2 that the de term, F(0, 0), of a DFT is proportional to
the average value of ils corresponding spatial] image. Assume that the image is of
size M X N. Suppose that you pad the image with zeros to size P X Q, where P
and Q are given in Eqs. (4.6-31) and (4.6-32), Let F,(0, 0) denote the de term of
the DFT of the padded function.% (a) What is the ratio of the average values of the original and padded images?(b) Is F,,(0,0) = (0,0)? Support your answer mathematically,Prove the periodicity properties {entry 8} in Table 4.2.The following problems are related to the entries in Table 4.3.*(a) Prove the validity of the discrete convolution theorem (entry 6) for the 1-D
case.(b) Repeat (a) for 2-D.(c) Prove the validity of entry 9.(d) Prove the validity of entry 13.(Note: Problems 4.18, 4.19, and 4.31 are related to Table 4.3 also.)(a) Show that the Laplacian of a continuous function f{7, z) of continuous variables ¢ and z satisfies the following Fourier transform pair [sce E.g. (3.6-3)
for a definition of the Laplacian}:VF (tz) > Am? + PF, v)
[Hint: Study entry 12 in Table 4.3 and see Problem 4.25(d).]wx (b) The preceding closed form expression is valid only for continuous variables.
However, it can be the basis for implementing the Laplacian in the disciete
frequency domain using the M x N filterAwe) = 4 Gh a ey# Problems329
330 Chapter 4 m Filtering in the Frequency Domain* 4,274.284,294044314324.33for u = 0,1,2,...,M@ - 1 and v = 0,1,2,...,.N — 1. Explain how you
would implement this filter,(c} As you saw in Example 4.20, the Laplacian result in the frequency domain
was similar to the result of using a spatial mask with a center coefficient
equal to ~8, Explain the reason why the frequency domain result was not
similar instead to the result of using a spatial mask with a center coefficient
of —4: See Section 3.6.2 regarding the Laplacian in the spatial domain.Consider a 5 X 5 spatial mask that averages the 12 closest neighbors of a point{x, y), but excludes the point itself from the average.(a) Find the equivalent filter, A(z, v),in the frequency domain.(b) Show that your result is a lowpass filter.Based on Eg. (3.6-4), one approach for approximating a discrete derivative in 2-Dis based on computing differences of the form f(x + 1. y) + f(x-L, »)-2f( xy)and f(x, y+1) + f(x y~1)-2f(xy).(a) Find the equivalent filter, 47 (uv, v),in the frequency domain.{b) Show that your result is a highpass filter.Find the equivaient filter, 7 («, v), that implements in the frequency domain thespatial operation performed by the Laplacian mask in Fig. 3.37(b).Can you think of a way to use the Fourier transform to compute (or partially com
pute} the magnitude of the gradient {Eq. (3.6-11)] for use in image differentiation?If your answer is yes, give a method to do it. Ii your answer is no, explain why,A continuous Gaussian lowpass filter in the continuous frequency domain hasthe transfer functionHig.v) = He
Show that the corresponding filter in the spatial domain is
het, 2) = rer OD
As explained in Eq. (4.9-1), itis possible to obtain the transfer function. A yp, of
a highpass filter from the transfer function of a lowpass filter as
Hip = 1- HypUsing the information given in Problem 4.31, what is the form of the spatial do
main Gaussian highpass filter?Consider the images shown. The image on the right was obtained by: (a) multi
plying the image on the Jeft by (—1)°' *: (hb) computing the DFT; (c} taking thecomplex conjugate of the transform: (d) computing the inverse DFT; and(e) multiplying the real part of the result by (~1)*"*. Explain (mathematically)why the image on the right appears as it does.
434«4.354364.374.38What is the source of the nearly periodic bright points in the horizontal axis ofFig. 4.41(b)?Each filter in Fig. 4.53 has a strong spike in its ccnler. Explain the source of thesespikes.Consider the images shown. The image on the right was obtained by Jowpass fil
tering the image on the left with a Gaussian iowpass fitter and then highpass fil
tering the result with a Gaussian highrass filter. The dimension of the images is420 X 344, and Dy = 25 was used for both filters.(a) Explain why the center part of the finger ring in the figure on the right appears so bright and solid, considering that the dominant characteristic of the
filtered image consists of edges on the outer boundary of objects (e.g., fingers, wrist bones) with a darker area in between. In other words, would you
not expect the highpass filter to render the constant area inside the ring
dark, since a highpass filter eliminates the dc term? {b) Do you think the result would have been different if the order of the filtering process had been reversed? (Original) image courtesy of Dr. Thomas R. Gest,
Division of Anatomical Sciences, University of Michigan
Medical School.)Given an image of size M x N, you are asked to perform an experiment that
consists of repeatedly Jowpass filtering the image using a Gaussian lowpass filter
with a given cutoff frequency Dy. You may ignore computational round-off erTors, Let ¢pin denote the smallest positive number representable in the machine
in which the proposed experiment will be conducted.x(a) Let K denote the number of applications of the filter, Can you predict (without doing the experiment) what the result (image) will be for a sufficiently
large value of K? Hf so, what is that result?(b) Derive an expression for the neniniui value of K that will guarantee the re
sult that you predicted.Consider the sequence of images shown. [he image on the lefl is a segment of an
X-ray image of a commercial printed circuil board. The images following it are.
respectively, the results of subjectiny the image to}. 1). and 100 passes of a
Gaussian highpass filter with Dy = 30, The images are of size 330 % 3M pixels,
with each pixel being represented by & bits af gray. The images were sealed for
display, but this has no effect on th: jreblem slatement,   xProblems331
332Qeopter 4 mw Filtering in the Frequency Domain {a) It appears from the images that changes will cease to take place after some
finite number of passes. Show whether or not this in fact is the case. You
may ignore computational round-off errors. Let cyiq denote the smallest
positive number representable in the machine in which the proposed experiment will be conducted.(b) If you determined in (a) that changes would cease after a finite number of
iterations, determine the minimum value of that number. Original image courtesy of Mr. Joseph E. Pascente, Lixi, Inc.4394.40«4.41
4.42443As illustrated in Fig. 4.59, combining high-frequency emphasis and histogram
equalization is an effective method for achieving edge sharpening and contrast
enhancement.(a) Show whether or not it matters which process is applied first.{b) Ifthe order does matter, give a rationale for using one or the other methad first.Use a Gaussian highpass filter to construct a homomorphic filter that has the
same general shape as the filter in Fig. 4.61.Show the validity of Eqs. (4.11-16) and (4.11-17). (Hint: Use proot by induction.)
Suppose that you are given a set of images generated by an experiment dealing
with the analysis of stellar events. Each image contains a set of bright, widely
scattered dots corresponding to stars in a sparsely occupied section of the universe. The problem is that the stars are barely visible, due to superimposed illumination resulting from atmospheric dispersion. lf these images are modeled as
the product of a constant illumination component with a set of impulses, give an
enhancement procedure based on homomorphic filtering designed to bring out
the image components due to the stars themselves.Askilled medical technician is assigned the job of inspecting a certain class of images generated by an electron microscope. In order to simplify the mspection
task, the technician decides to use digital image enhancement and, to this end, ex
amines a set of representative images and finds the following problems:
(1) bright, isolated dots that are of no interest; (2) lack of sharpness: (3) not
enough contrast in some images: and (4) shifts in the average intensity. when this
value should be V to perform correctly certain intensity measurements. The tech
nician wants to correct these problems and then display in white all intensities in
a band between /, and /.. while keeping normal tonality in the remaining intensities. Propose a sequence of processing steps tbat the technician can follow io
achieve the desired goal. You may use techniques from both Chapters 3 and 4
Image Restoration
and Reconstruction   Things which we see are not by themselves what we see. ...
It emains completely unknown to us what the objects may be by
themselves and apart from the receptivity of our senses. We know
nothing but our manner of perceiving them.Immanuel KantPreviewAs in image enhancement, the principal goal of restoration techniques is to improve an image in some predefined sense. Although there are areas of overlap,
image enhancement is largely a subjective process, while image restoration is for
the most part an objective process. Restoration attempts to recover an image
that has been degraded by using a priori knowledge of the degradation phenomenon. Thus, restoration techniques are oriented toward modeling the degradation and applying the inverse process in order to recover the original image.This approach usually involves formulating a criterion of goodness that will
yield an optimal estimate of the desired result. By contrast, enhancement tech~
niques basically are heuristic procedures designed to manipulate an image in
order to take advantage of the psychophysica! aspects of the human visual system. For example, contrast stretching is considered an enhancement technique
because it is based primarily on the pleasing aspects it might present to the
viewer, whereas removal of image blur by applying a deblurring function is
considered a restoration technique.The material developed in this chapter is strictly introductory. We consider
the restoration problem only from the point where a degraded, digital image
is given; thus we consider topics dealing with sensor, digitizer, and display
degradations only superficially. These subjects, although of importance in the
overall treatment of image restoration applications, are beyond the scope of
the present discussion.333
334  Chopter 5 mm Image Restoration and ReconstructionFIGURE 5.1A mode) of the
image
degradation/
restoration
process.As discussed in Chapters 3 and 4, some restoration techniques are best formulated in the spatial domain, while others are better suited for the frequency
domain. For example, spatial processing is applicable when the only degradation is additive noise. On the other hand, degradations such as image blur are
difficult to approach in the spatial domain using small filter masks. In this
case, frequency domain filters based on various criteria of optimality are the
approaches of choice. These filters also take into account the presence of
noise. As in Chapter 4, a restoration filter that solves a given application in
the frequency domain often is used as the basis for generating a digital filter
that will be more suitable for routine operation using a hardware/firmware
implementation.Section 5.1 introduces a linear model of the image degradation/restoration
process. Section 5.2 deals with various noise models encountered frequently in
practice. In Section 5.3, we develop several spatial filtering techniques for reducing the noise content of an image, a process often referred to as image
denoising. Section 5.4 is devoted to techniques for noise reduction using
frequency-domain techniques. Section 5.5 introduces linear, position-invariant
models of image degradation, and Section 5.6 deals with methods for estimating degradation functions. Sections 5.7 through 5.10 include the development
of fundamental image-restoration approaches. We conclude the chapter (Section
5.11) with an introduction to image reconstruction from projections. The principal application of this concept is computed tomography (CT), one of the
most important commercial applications of image processing, especially in
health care.A Model of the Image Degradation/Restoration ProcessAs Fig. 5.1 shows, the degradation process is modeled in this chapter as a degradation function that, together with an additive noise term, operates on an input
image f(x, y) to produce a degraded image g(x, y). Given g(x, y), some knowledge about the degradation function H,and some knowledge about the additive noise term (x, y), the objective of restoration is to obtain an estimate
f(x, y) of the origina! image. We want the estimate to be as close as possible to the
original input image and, in general, the more we know about H and 7, the closer
f(x, y) will be to f(x, y). The restoration approach used throughout most of
this chapter is based on various types of image restoration filters. 
  
  
  Degradation
function
H    
 Restorationfilter(s) fle y)fx > 
  DEGRADATION RESTORATION
5.2 & Noise Models 337plz) 
   
 on 2
= 0.607 3  Rayleigh    Caussian   _ aoa een  
  plz) p(z) P(2)) Exponential b-a Uniform Impulse  abc
defFIGURE 5.2 Some important probability density functions.and
gis b(4 — 7)
4
Figure 5.2(b) shows a plot of the Rayleigh density. Note the displacement from
the origin and the fact that the basic shape of this density is skewed to the right.
The Rayleigh density can be quite useful for approximating skewed histograms. _(5.2-4)Erlang (gamma) noise
The PDF of Erlang noise is given bybb-1
ee as——e
p(z) = 4 (b - 1)!
0 forz <Qf =
orz=0 (5.25)where the parameters are such that a > 0, b is a positive integer, and “!” indicates factorial. The mean and variance of this density are given by~_0
Zao (5.2-6)
and(5.2-7)9
!
pe
336 Chapter S @ Image Restoration and Reconstruction   Be
Consult the book Web sitefor a brief review of probability theory.uncorrelated with respect to the image itself (that is, there is no correlation
between pixel values and the values of noise components). Although these
assumptions are at least partially invalid in some applications (quantumlimited imaging, such as in X-ray and nuclear-medicine imaging, is a good example), the complexities of dealing with spatially dependent and correlated
noise are beyond the scope of our discussion.5.2.2 Some Important Noise Probability Density FunctionsBased on the assumptions in the previous section, the spatial noise descriptor
with which we shall be concerned is the statistical behavior of the intensity
values in the noise component of the model in Fig. $5.1. These may be considered random variables, characterized by a probability density function
{PDF). The following are among the most common PDFs found in image processing applications. .Gaussian noiseBecause of its mathematical tractability in both the spatial and frequency
domains, Gaussian (also called normal) noise models are used frequently in
practice. In fact, this tractability is so convenient that it often results in
Gaussian models being used in situations in which they are marginally applicable at best.The PDF of a Gaussian random variable, z, is given byple) = Retire (52-1)V 200where z represents intensity, Z is the mean’ (average) value of z, and ¢ is its standard deviation. The standard deviation squared, a”, is called the variance of z.A
plot of this function is shown in Fig. 5.2(a). When z is described by Eq. (5.2-1),
approximately 70% of its values will be in the range [(Z — a), (z + o)J, and
about 95% will be in the range [(Z — 20), (Z + 20)]. Rayleigh noise ‘
The PDF of Rayleigh noise is given by2 >
“te - (za) jb s
p(z) = pe aje forz 2a (52-2)
9 forz <aThe mean and variance of this density are given byF=at Vab/4 (5.2-3) ’ ss . - .
We use 2 instead of vr to denote the nian in this section to avoid confusion wheo we use vz and later
to denote neighborhood size.Rete ba:
5.2 m Noise ModelsIt is shown in Section 5,5 that if # is a linear, position-invariant process,
then the degraded image is given in the spatial domain byg(x, y) = hx, y) w f(x, ¥) + n(x, y) (5.1-1)where /#(x, y) is the spatial representation of the degradation function and, as in
Chapter 4, the symbol “*” indicates convolution. We know from the discussion
in Section 4.6.6 that convolution in the spatial domain is analogous to multiplication in the frequency domain, so we may write the model in Eq. (5.1-1) in an
equivalent frequency domain representation:G(u, v) = Hu, v)F(u, v) + N(u, v) (5.1-2)where the terms in capital letters are the Fourier transforms of the corresponding terms in Eq. (5.1-1). These two equations are the bases for most of
the restoration material in this chapter.In the following three sections, we assume that H is the identity operator,
and we deal only with degradations due to noise. Beginning in Section 5.6 we
consider a number of important image degradations functions and look at several methods for image restoration in the presence of both H and n,ER Noise ModelsThe principal sources of noise in digital images arise during image acquisition and/or transmission. The performance of imaging sensors is affected by a
variety of factors, such as environmental conditions during image acquisition,
and by the quality of the sensing elements themselves. For instance, in acquiring images with a CCD camera, light levels and sensor temperature are
major factors affecting the amount of noise in the resulting image. Images are
corrupted during transmission principally due to interference in the channel
used for transmission. For example, an image transmitted using a wireless
network might be corrupted as a result of lightning or other atmospheric
disturbance.5.2.1 Spatial and Frequency Properties of NoiseRelevant to our discussion are parameters that define the spatial characteristics of noise, and whether the noise is correlated with the image, Frequency
properties refer to the frequency content of noise in the Fourier sense (i.e., as
opposed to frequencies of the electromagnetic spectrum). For example, when
the Fourier spectrum of noise is constant, the noise usually is called white
noise. This terminology is a carryover from the physical properties of white
tight, which contains nearly aj] frequencies in the visible spectrum in equal
proportions. From the discussion in Chapter 4, it is not difficult to show that
the Fourier spectrum of a function containing all frequencies in equa) proportions is a constant.With the exception of spatially periodic noise (Section 5.2.3), we assume
in this chapter that noise is independent of spatial coordinates, and that it is335
338 Chapter 5 m Image Restoration and ReconstructionFigure 5.2(c) shows a plot of this density. Although Eq, (5.2-5) often is referred
to as the gamma density, strictly speaking this is correct only when the denominator is the gamma function, (6). When the denominator is as shown, the
density is more appropriately called the Erlang density.Exponential noise
The PDF of exponential noise is given by
ae? forz 20
= (5.2-8)
Pte) {e forz < 0
where a > 0. The mean and variance of this density function are
1
Zs> (5.2-9)
. and 4
1
zs (5.2-10)
aNote that this PDF is a special case of the Erlang PDF, with 6 = 1. Figure 5.2(d)
shows a plot of this density function.Uniform noise
The PDF of uniform noise is given by; | oitasz<o
Piz)=4b-a ~ es (5.2-11)0 otherwiseThe mean of this density function is given by  +b
z= (5.2-12)
and its variance by
2
2. - 4) 2-13
o D (5.2-13)
Figure 5.2(e) shows a plot of the uniform density.
Impulse (salt-and-pepper) noise
The PDF of (bipolar) impuise noise is given by
P, forz =a
p(2) = 4 Py forz = 6 (5.2-14)
0 otherwiseIf > a, intensity & will appear as a light dot in the image. Conversely, level a will
appear like a dark dot. If either P, or P, is zero, the impulse noise is called
unipolar. If neither probability is zero, and especially if they are approximately
equal, impulse noise values will resemble salt-and-pepper granules randomly distributed over the image. For this reason, bipolar impulse noise also is called saltand-pepper noise. Data-drop-out and spike noise also are terms used to refer to this
type of noise. We use the terms impulse or salt-and-pepper noise interchangeably.
5.2 @ Noise Models 341    Exponential Uniform, Salt & Pepperendaoe |FIGURE 5.4 (Continued) Images and histograms resulting from adding exponential, uniform, and salt-andpepper noise to the image in Fig. 5.3sinusoid is a pair of conjugate impulses’ located at the conjugale frequencies of
the sine wave (Table 4.3). Thus, if the amplitude of a sine wave in the spatial domain is strong enough, we would expect to see in the spectrum of the image a
pair of impulses for each sine wave in the image. As shown in Fig. 5.5(b), this is
indeed the case, with the impulses appearing in an approximate circle because
the frequency values in (his particular case are so arranged. We will have much
more to say in Section 5.4 about this and other examples of periodic noise.5.2.4 Estimation of Noise Parameters
The parameters of periodic noise typically are estimated by inspection of the
Fourier spectrum of the image. As noted in the previous section. periodic noise
tends to produce frequency spikes that offen can be detected even by visual
analysis. Another approach is 10 allempt fo infer the periodicity of noise com
ponents directly from the image. but this is possible only in sonplstic TASES  "Be careful not to confuse ih <
impulse noise.
340 © Chopter 5 m Image Restoration and Reconstruction  Gaussian Rayleigh Gammawee
FIGURE 5.4 [mages and histograms resulling from adding Gaussian. Rayleigh, and gamma noise to the image
in Fig, 5.3.We see a close correspondence in comparing the histograms in Fig. 5.4 with
the PDFs in Fig. 5.2. The histogram for the salt-and-pepper example has an
extra peak at the white end of the intensity scale because the noise components were pure black and white, and the lightest component of the test pattern (the cirele) is light gray. With the exception of slightly different overall
intensity, it is difficult to differentiate visually between the first five images in
Fig. 5.4, even though their histograms are significantly different. The satt-andpepper appearance of the image corrupted by impulse noise is the only one
that is visually indicative of the type of noise causing the degradation. oJ 3.2.3 Periodic Noise    
 
  interference during image acquisition. This is the only type of s
dent noise that wilf be considered in this chapter. As discussed in Se
riodic noise can be reduced significantly via frequency domain filter
example. consider ite image io Fig. SS{a}. Phis image is severe wrruphed by
(spatial) sinusoid! noise of various frequencies. The Fourier tans         OYAED OF GL Prue
§.2 @ Noise Models 339Noise impulses can be negative or positive. Scaling usually is part of the image
digitizing process. Because impulse corruption usually is large compared with the
strength of the image signal, impulse noise generaily is digitized as extreme (pure
black or white) values in an image. Thus, the assumption usually is that @ and b
are “saturated” values, in the sense that they are equal to the minimum and maximum allowed values in the digitized image. As a result. negative impulses appear
as black (pepper) points in an image. For the same reason, positive impulses appear as white (salt) noise. For an 8-bit image this means typically that a = 0
(black) and 6 = 255 (white), Figure 5.2(f) shows the PDF of impulse noise.AS a group, the preceding PDFs provide useful tools for modeling a broad
range of noise corruption situations found in practice. For example, Gaussian
noise arises in an image due to factors such as electronic circuit noise and sensor
noise due to poor illumination and/or high temperature. The Rayleigh density is
helpful in characterizing noise phenomena in range imaging. The exponential
and gamma densities find application in laser imaging, Impulse noise is found in
situations where quick transients, such as faulty switching, take place during
imaging, as mentioned in the previous paragraph. The uniform density is perhaps the least descriptive of practical situations. However. the uniform density is
quite useful as the basis for numerous random number generators that are used
in simulations (Peebles [1993] and Gonzalez, Woods. and Eddins [2004)).% Figure 5.3 shows a test pattern well suited for illustrating the noise models
just discussed, This is «1 suitable pattern to use hecause jt is composed of simple. constant areas thal span the gray scale from black to near while in only
three increments. This facilitates visual analysis of the characteristics of the
various noise components added to the image.Figure 5.4 shows the test pattern after addition of the six types of noise discussed thus far in this section. Shown below cach image is the histogram computed directly from that image. The parameters of the noise were chosen in
each case so that the histogram corresponding lo the three intensity levels in
the test pattern would start to merge. This made the noise quite visible. without
obscuring the basic structure of the underlying image EXAMPLE 5,1:
Noisy images and
their histograms.FIGURE 5.3 Test
pattern used to
iNustrate the
characteristics of
the noise PDFs
shown in Pig, §.2.
342 Chopter § # Image Restoration and Reconstruction¥
bFIGURE 5.5
(a) Image
corrupted bysinusoidal noise.{b) Spectrum
(each pair of
conjugate
impulses
corresponds to
one sine wave).
(Original image
courtesy of
NASA.} Automated analysis is possible in situations in which the noise spikes are either exceptionally pronounced, or when knowledge is available about the general location of the frequency components of the interference.The parameters of noise PDFs may be known partially from sensor specifications, but it is often necessary to estimate them for a particular imaging
arrangement. If the imaging system is available, one simple way to study the
characteristics of system noise is to capture a set of images of “fat” environments, For example, in the case of an optical sensor. this is as simple as imaging
a solid gray board that is illuminated uniformly. The resulling images typically
are good indicators of system noise.When only images already generated by a sensor are available, frequently it
is possible to estimate the parameters of the PDF from small patches of rea
sonably constant background intensity, For example, the vertical strips (of
150 X 20 pixels) shown in Fig. 3.6 were cropped from the Gaussian, Rayleigh,
and uniform images in Fig. 5.4. The histograms shown were calculated using
image data from these sinall strips. Thu histograms in Fig. §.4 Chal correspond
to the histograms in Fig. 5.6 are fhe ones in the anddle of the group of fhree ia
5.2 & Noise ModelsFigs. 5.4(d), (e), and (k). We see that the shapes of these histograms correspond
quite closely to the shapes of the histograms in Fig. 5.6. Their heights are dif
ferent due to scaling, but the shapes are unmistakably similar.The simplest use of the data from the image strips is for calculating the mean
and variance of intensity levels. Consider a strip (subimage) denoted by S, and
let ps(2;), 4 = 0,1,2,....£ — 1. denote the probability estimates (normalized
histogram values) of the intensities of the pixels in S, where ZL is the number of
possible intensities in thesenlire image (c.g.. 256 for an 8-bit image), As in
Chapter 3, we estimate the mean and variance of the pixels in S$ as follows:i-]
2= DVaps(zi) (5.2-15)
i=0
and
5 L~t 5
w= Sei 2 pslzi) (5.2-16)
fetThe shape of the histogram identifies the closest PDF match. If the shape
is approximately Gaussian, then the mean and variance are all we need because the Gaussian PDF is completely specified by these two parameters.
For the other shapes discussed in Section 5.2.2, we use the mean and variance to solve for the parameters a and }. Impulse noise is handled differentlybecause the estimate needed is of the actual probability of occurrence ofwhite and black pixels. Obtaining this estimate requires that both black and
white pixels be visible, so a midgray, relatively constant area is needed in the
image in order to be able to compute a histogram. The heights of the peaks
corresponding to black and white pixels are the estimates of P, and FP, in
Eq. (5.2-14).Ww
w4 abe
FIGURE 5.6 Histograms computed using small strips (shown as inseris) rom (#4) the Gaussian, (bh) the
Rayleigh, and (c} the uniform noisy umapes in Fig. 5.4
344  Qhopter § m Image Restoration and ReconstructionWe assume that rt and 7
are odd integers.EEN Restoration in the Presence of Noise
Only—Spatial FilteringWhen the only degradation present in an image is noise, Eqs. (5.1-1) and (5.1-2)
becomeg(x,y) = f(x,y) + nlx, y) (5.3-1)
andG(u,v) = Flu, v) + N(u,v) (5.3-2). The noise terms are unknown, so subtracting them from g(x, y) or G(x, v) is notaTealistic option. In the case of periodic noise, it usually is possible to estimate
N(u, v) from the spectrum of G(u, v), as noted in Section 5.2.3. In this case N (te, v)
can be subtracted from G(u, v) to obtain an estimate of the original image. In general, however, this type of knowledge is the exception, rather than the rule.Spatial filtering is the method of choice in situations when only additive
random noise is present. Spatial filtering is discussed in detail in Chapter 3.
With the exception of the nature of the computation performed by a specific
filter, the mechanics for implementing all the filters that follow are exactly as
discussed in Sections 3.4 through 3.6.5.3.1 Mean Filters
In this section we discuss briefly the noise-reduction capabilities of the spatialfilters introduced in Section 3.5 and develop several other filters whose performance is in many cases superior to the filters discussed in that section.Arithmetic mean filterThis is the simplest of the mean filters. Let S,, represent the set of coordinates in
a rectangular subimage window (neighborhood) of size m X n, centered at point
(x, y). The arithmetic mean filter computes the average value of the corrupted
image g(x, y) in the area defined by S,,. The value of the restored image f at
point (x, y) is simply the arithmetic mean computed using the pixels in the region
defined by S,,. In other words,F(x, y) _ D ats, oD) (5.3-3)mn (5. DeSeyThis operation can be implemented using a spatial filter of size m X n in
which all coefficients have value l/r. A mean filter smooths local variations
in an image, and noise is reduced as a result of blurring.
5.3 ® Restoration in the Presence of Noise Only-—Spatial Filtering 345Geometric mean filter
An image restored using a geometric mean filter is given by the expression7
fan =| I]. sts. ofr (5.3-4)(s.eS,,Here, each restored pixel is.given by the product of the pixels in the subimage
window, raised to the power 1/znn. As shown in Example 5.2, a geometric mean
filter achieves smoothing comparable to the arithmetic mean filter, but it tends
to lose less image detail in the process.Harmonic mean filterThe harmonic mean filtering operation is given by the expressionf(xy) = —T (53-5){s. eS. ats, 9 The harmonic mean filter works well for salt noise, but fails for pepper noise.
It does well also with other types of noise like Gaussian noise.Contraharmonic mean filterThe contraharmonic mean filter yields a restored image based on the expressionD alse"
fox, yy = 2 —__ (5.3-6)DY als,o?(s.NeS,,where Q is called the order of the filter. This filter is well suited for reducing or
virtually eliminating the effects of salt-and-pepper noise. For positive values of Q,
the filter eliminates pepper noise. For negative values of Q it eliminates salt noise.
It cannot do both simultaneously. Note that the contraharmonic filter reduces to
the arithmetic mean filter if @ = 0, and to the harmonic mean filter if Q = —1.& Figure 5.7(a) shows an 8-bit X-ray image of a circuit board, and Fig. 5.7(b)
shows the same image, but corrupted with additive Gaussian noise of zero
mean and variance of 400. For this type of image this is a significant level of
noise. Figures 5.7(c) and (d) show, respectively, the result of filtering the noisyEXAMPLE 5.2:
[lustration of
mean filters.
346 Chopter 5 & Image Restoration and ReconstructionabedFIGURE 5.7{a) X-ray image.
(b) Image
corrupted by
additive Gaussian
noise. (c) Result
of filtering with
an arithmetic
mean filter of size
3X 3.(d) Result
of filtering with a
geometric mean
filter of the same
size.(Original image
courtesy of Mr.
Joseph E.
Pascente, Lixi,
inc.) image with an arithmetic mean filter of size 3 X 3 and a geometric mean filter
of the same size. Although both filters did a reasonable job of attenuating the
contribution due to noise, the geometric mean filter did not blur the image as
much as the arithmetic filter. For instance, the connecter fingers at the top of
the image are sharper in Fig. 5.7(d) than in (c}. The same is true in other parts
of the image.Figure 5.8(a) shows the same circuit image, but corrupted now by pepper
noise with probability of 0.1. Similarly. Fig. 5.8(b) shows the image corrupted by salt noise with the same probability. Figure 5.8(c) shows the result of
filtering Fig. 5.8(a) using a contraharmonic mean filler with O = LS, and
Fig. 5.8(d)} shows the result of filtcring Fig. 5.8(b) with © “1.5. Both filters did a good job in reducing the effect of the noise. The positive-order filter did a better job of cleaning the background, at the expense of slightly
thinning and blurring the dark areas. The opposite was true ol Hiv negative:
order filter.
5.3 & Restoration in the Presence of Noise Only-—Spatial Filtering 347 In general, the arithmetic and geometric mean filters (particularly the latter) are well suited for random noise like Gaussian or uniform noise. The contraharmonic filter is well suited for impulse noise, but it has the disadvantage
that it must be known whether the noise is dark or light in order to select the
proper sign for Q. The results of choosing the wrong sign for Q can be disastrous, as Fig. 5.9 shows. Some of the filters discussed in the following sectionseliminate this shortcoming. &5.3.2 Order-Statistic FiltersOrder-statistic filters were introduced in Section 3.5.2. We now expand the
discussion in that section and introduce some additional order-statistic fillers.
As noted in Section 3.5.2, order-statistic filters are spatial filters whose response js based on ordering (ranking) the values of the pixcis contained in
the image area encompassed by the filter. The ranking result determines the
response of the filter.ab
edFIGURE 5.8{a) Image
corrupted by
pepper noise with
a probability of
0.1. (b) Image
corrupted by salt
noise with the
same probability.
{c} Result of
fillcring (a) with a
3X 3contraharmonic filter of
order 1.5.{d) Result of
filtering (b) with
Q= -i5.
348 Chapter 5 m Image Restoration and ReconstructionabFIGURE 5.9
Results of selecting the wrong sign
in contraharmonic
filtering,(a) Result of
filteringFig. 5.8(a) with a
contraharmonic
filter of size 3 x 3
and Q = -1.5.
(b) Result of
filtering 5.8(b)
with @ = 1.5,Sce the second margin
note in Section 10.3.5 regarding percentiles,  Median filterThe best-known order-statistic filter is the median filter, which, as its name implies, replaces the value of a pixel by the median of the intensity levels in the
neighborhood of that pixel:. F(xy) = median{g(s.#)} (5.3-7}The value of the pixel at (x, y) is included in the computation of the median.
Median filters are quite popular because, for certain types of random noise,
they provide excellent noise-reduction capabilities, with considerably less
blurring than linear smoothing filters of similar size. Median filters are particularly effective in the presence of both bipolar and unipolar impulse noise, In
fact, as Example 5.3 below shows, the median tilter yields excellent results for
images corrupted by this type of noise. Computation of the median and implementation of this filter are discussed in Section 3.5.2.Max and min filters
Although the median filter is by far the order-statistic filter most used in image
processing. il is by no means the only one. The median represents the 50th percentile of a ranked set of numbers. but you will recall from basic statistics that
ranking lends itself to many other possibilities. For example, using the 100th
percentile results in the so-called mucx filter. given byf(x) mas Lets. oh (5.3-8)}Lsfh3This filter is useful tor finding the brightest porits in an unage. Also, because
d by this filler as a result of the pepper noise has very low values mis redumax selection procs. inte
5.3 m Restoration in the Presence of Noise Only—Spatial Filtering 349The Oth percentile filter is the min filter:
f(xy) = min {g(s,1)} (5.3-9)
(s.NeSyyThis filter is useful for finding the darkest points in an image. Also, it reduces
salt noise as a result of the min operation.Midpoint filter .
The midpoint filter simply computes the midpoint between the maximum and
minimum values in the area encompassed by the filter:f(y) = ; cmag {8.9} + min {a(s.9} (53-10)Note that this filter combines order statistics and averaging. It works best for
randomly distributed noise, like Gaussian or uniform noise.Alpha-trimmed mean filterSuppose that we delete the d/2 lowest and the d/2 highest intensity values of
a(s, ) in the neighborhood S,,. Let g,(s, 1) represent the remaining mn — d
pixels. A filter formed by averaging these remaining pixels is called an alphatrimmed mean filter: D als.) (5.3-11)G.neS,,fay =a
where the value of d can range from 0 to mn — 1. When d@ = 0, the alphatrimmed filter reduces to the arithmetic mean filter discussed in the previous
section. If we choose d = mn — 1, the filter becomes a median filter, For other
values of d, the alpha-trimmed filter is useful in situations involving multiple
types of noise, such as a combination of salt-and-pepper and Gaussian noise.M@ Figure 5.10(a) shows the circuit board image corrupted by salt-and-pepper EXAMPLE 5.3:
noise with probabilities P, = P, = 0.1. Figure 5.10(b) shows the result of median _ Illustration of
filtering with a filter of size 3 X 3. The improvement over Fig. 5.10(a) is signifi- Bl der-statistic
cant, but several noise points still are visible. A second pass [on the image in ilers,
Fig. 5.10(b)] with the median filter removed most of these points, leaving only few,
barely visible noise points. These were removed with a third pass of the filter.
These results are good examples of the power of median filtering in handling
impulse-like additive noise. Keep in mind that repeated passes of a median filter
will blur the image, so it is desirable to keep the number of passes as low as possible.
Figure 5.11(a) shows the result of applying the max filter to the pepper noise
image of Fig, 5.8(a).The filter did a reasonable job of removing the pepper noise,
but we note that it also removed (set to a light intensity level} some dark pixels
from the borders of the dark objects. Figure 5.11(b) shows the result of applying
the min filter to the image in Fig. 5.8(b). In this case, the min filter did a better
job than the max filter on noise removal, but it removed some white points
around the border of light objects. These made the light objects smaller and
350 Chapter $ m Image Restoration and ReconstructionabedFIGURE 5.10(a) Image
corrupted by saltand-pepper noise
with probabilities
P, = P, = 0.1.
{b) Result of one
pass with a
median filter of
size 3 X 3.{c) Result of
processing (b)
with this filter.
(d) Result of
processing (c}
with the same
filter.abFIGURE 5.11(a) Result of
filteringFig. 5,8(a) with a
max filter of size
3 X 3. (b) Result
of filtering 5.8{b)
with a min filter
of the same size.
5.3 # Restoration in the Presence of Noise Only—Spatial Filtering 351ab
cddb efataFIGURE 5,12(a) Image
corruptedby additive
uniform noise,
(b) Image
additionally
corrupted by
additive salt-andpepper noise.
Image (b) filtered
witha S x 5;(c) arithmetic
mean filter;(d) geometric
mean filter;{e) median filter:
and (f) alphaimmed mean
filler with d = 5. some of the dark objects larger (like the connector fingers in the top of the
image) because white points around these objecis were sct to a dark level.The alpha-trimmed filter js illustraied next. Figure $.12(a) shows the circuit
board image corrupted this time by additive. uniform noise of variance 800 and
352Chapter 5 1 Image Restoration and Reconstructionzero mean. This is a high level of noise corruption that is made worse by further
addition of salt-and-pepper noise with P, = P, = 0.1, as Fig. 5.12(b) shows. The
high level of noise in this image warrants use of larger filters. Figures 5.12(c)
through (f) show the results obtained using arithmetic mean, geometric mean,
median, and alpha-trimmed mean (with d = 5) filters of size 5 x 5S. As expected, the arithmetic and geometric mean filters (especially the latter) did not do
well because of the presence of impulse noise. The median and alpha-trimmed
filters performed much better, with the alpha-trimmed filter giving slightly better noise reduction. Note, for example, that the fourth connector finger from the
top left is slightly smoother in the alpha-trimmed result. This is not unexpected
because, for a high value of d, the alpha-trimmed filter approaches the performance of the median filter, but still retains some smoothing capabilities. is5.3.3 Adaptive FiltersOnce selected, the filters discussed thus far are applied to an image without regard for how image characteristics vary from one point to another, In this section we take a look at two adaptive filters whose behavior changes based on
statistical characteristics of the image inside the filter region defined by the
m X n rectangular window S,,. As the following discussion shows, adaptive
filters are capable of performance superior to that of the filters discussed thus
far. The price paid for improved filtering power is an increase in filter complexity. Keep in mind that we still are dealing with the case in which the degraded image is equal to the original image plus noise. No other types of
degradations are being considered yet.Adaptive, local noise reduction filterThe simplest statistical measures of a random variable are its mean and variance. These are reasonable parameters on which to base an adaptive filter because they are quantities closely related to the appearance of an image. The
mean gives a measure of average intensity in the region over which the mean
is computed, and the variance gives a measure of contrast in that region.Our filter is to operate on a local region, S,y. The response of the filter at
any point (x, y) on which the region is centered is to be based on four quantities: (a) g(x, y), the value of the noisy image at (x, y); (b) a, the variance of
the noise corrupting f(x, y) to form g(x, y); (c) m,, the local mean of the pixels in S,y; and (d) o%. the local variance of the pixels in S,,. We want the behavior of the filter to be as follows:L if a is zero, the filter should return simply the value of g(x, y). This is the
trivial, zero-noise case in which g(x, y) is equal to f(x, y).2. If the local variance is high relative to v2, the filter should return a value
close to g(x, y). A high local variance typically is associated with edges, and
these should be preserved.3. If the two variances are equal, we want the filler to return the arithmetic
mean value of the pixels in 5,,. This condition occurs when the local area
has the same properties as the overall image, and local noise is to be reduced simply by averaging.
5.3 Restoration in the Presence of Noise Only—Spatial Filtering 353An adaptive expression for obtaining fix. y) based on these assumptions may
be written as2
fn y) = az y) - let y)— mm] (5.3-12)The only quantity that heeds to be known or estimated is the variance of
the overall noise, v4. The other parameters are computed from the pixels in
Syy at each location (x, y} on which the filter window is centered. A tacit assumption in Eq. (5.3-12) is that 0) = 3, The noise in our model is additive
and position independent, so this is a reasonable assumption to make because
S,y is a subset of g(x, y). However, we seldom have exact knowledge of ov,
Therefore, it is possible for this condition to be violated in practice. For that
teason, a test should be built into an implementation of Eq. (5.3-12} so that the
ratio is set to 1 if the condition of > a7, occurs, This makes this filter nonlinear. However, it prevents nonsensical results (i.e., negative intensity levels, depending on the value of #71,) due to a potential lack of knowledge about the
variance of the image noise. Another approach is to allow the negative values
to occur, and then rescale the intensity values at the end. The result then would
be a loss of dynamic range in the image.Mm Figure 5.13(a) shows the circuit-board image, corrupted this time by addi- EXAMPLE 5.4:
tive Gaussian noise of zero mean and a variance of 1000. This is a significant Mtustration of
level of noise corruption, but it makes an ideal test bed’ on which to compare adaptive, local
relative filter performance. Figure 5.13(b) is the result of processing the noisy filtering. uetion
image with an arithmetic mean filter of size 7 X 7. The noise was smoothed
out, but at the cost of signiftcant blurring in the image. Similar comments are
applicable to Fig. 5.13(c), which shows the result of processing the noisy image
with a geometric mean filter, also of size 7 X 7. The differences between these
two filtered images are analogous to those we discussed in Example 5.2; only
the degree of blurring is different.
Figure 5.13(d) shows the result of using the adaptive filter of Eq. (5.3-12) with
a = 1000. The improvements in this resull compared with the two previous filters are significant. In terms of overall noise reduction, the adaptive filter
achieved results similar to the arithmetic and geometric mean filters. However,
the image filtered with the adaptive filter is much sharper. For example, the connector fingers at the top of the image are significantly sharper in Fig. 5.13(d).
Other features, such as holes and the eight legs of the dark component on the
lower left-hand side of the image, are much clearer in Fig, 5.13(d). These results
are typical of what can be achieved with an adaptive filter. As mentioned earlier,
the price paid for the improved performance is additional filter complexity.
The preceding results used a value for v7, that matched the variance of the
noise exactly. If this quantity is not known and an estimate is used that is too low,
the algorithm will return an image that closely resembles the original because
the corrections wil! be smaller than they should be. Estimates that are too high
5.3 % Restoration in the Presence of Noise Only—Spatial Filtering 355
Consider the following notation:Zmin = Minimum intensity value in S,,Zmax = Maximum intensity value in 5,,Zmed = median of intensity values in S,,
Zyy = intensity value at coordinates (x, y)Sax = maximum allowed size of S,,
The adaptive median-filtering algorithm works in two stages, denoted stage A
and stage B, as follows:Stage A: Al = med ~ Zmin
A2 = 2med ~ Zmax
If Al > OAND A2 < 0. go tostage B
Else increase the window size
If window size = S,,,, repeat stage A
Else output ZedStage B: Bl = 2,y ~ Zmin
B2 = Zry — 2max
If B1 > 0 AND 82 <0, output z,,
Else output ZmcaThe key to understanding the mechanics of this algorithm is to keep in mind that
it has three main purposes: to remove salt-and-pepper (impulse) noise, to provide
smoothing of other noise that may not be impulsive, and to reduce distortion, such
as excessive thinning or thickening of object boundaries. The values Zig a0 Zmax
are considered statistically by the algorithm to be “impulse-like” noise components,
even if these are not the lowest and highest possible pixel values in the image.With these observations in mind, we see that the purpose of stage A is to determine if the median filter output, Zmeq, is an impulse (black or white) or not. If
the condition Zin < Zmea < Zmax Holds, then Zm¢q Cannot be an impulse for the
reason mentioned in the previous paragraph. In this case, we go to stage B and
test to see if the point in the center of the window, z,,, is itself an impulse (recall
that z,, is the point being processed). If the condition B1 > 0 AND B2 < Ois
true, then Zmin < Z,y < Zmax, and Z,, Cannot be an impulse for the same reason
that Zpeg Was not. In this case, the algorithm outputs the unchanged pixel value,
Zxy- By not changing these “intermediate-level” points, distortion is reduced in
the image. If the condition B1 > O AND B2 < Ois false, then either z,, = Zmin
OF Zzy = Zax. In cither case, the value of the pixel is an extreme value and the
algorithm outputs the median value Z.4, Which we know from stage A is not a
noise impulse. The last step is what the standard median filter does. The problem
is that the standard median filter replaces every point in the image by the median of the corresponding neighborhood. This causes unnecessary loss of detail.Continuing with the explanation. suppose that stage A does find an impulse
(e., it fails the test that would cause it to branch to stage B). The algorithm then
increases the size of the window and repeats stage A. This Jooping continues until
358 Chapter 5 m Image Restoration and Reconstructionab
edFIGURE 5.16(a) Image
corrupted by
sinusoidal noise.
{b) Spectrum of (a).
(c) Butterworth
bandreject filter
(white represents
1). (d) Result of
filtering,
(Original image
courtesy of
NASA.)Figure 5.16(a), which is the same as Fig, 5.5(a}, shows an image heavily corrupted by sinusoidal noise of various frequencies. The noise components are easily seen as symmetric pairs of bright dots in the Fourier spectrum shown in
Fig. 5.16(b). In this example, the components lie on an approximate circle about
the origin of the transform, so a circularly symmetric bandreject filter is a good
choice. Figure 5.16(c} shows a Butterworth bandreject filter of order 4, with the
appropriate radius and width to enclose completely the noise impulses, Since it is
desirable in general to remove as little as possible from the transform, sharp, narrow filters are common in bandreject filtering. The result of filtering Fig. 5.16(a)
with this filter is shown in Fig, 5.16(d). The improvement is quite evident. Even
smail details and textures were restored effectively by this simple filtering approach. It is worth noting also that it would not be possible to get equivalent results
by a direct spatial domain filtering approach using small convolution masks. —&5.4.2 Bandpass FiltersA bandpass filter performs the opposite operation of a bandreject filter, We
showed in Section 4.10.1 how the transfer function Ayp{#. v) of a bandpass filter is obtained from a corresponding bandreject filter with transfer function
Age(u, v} by using the equationHyp(u,v) = 1 ~ Hg, v) (5.4-1)It is left as an exercise (Problem 5.12) to derive expressions for the bandpass
filters corresponding to the bandreject equations in Table 4.6.
356 Chopter 5 m Image Restoration and ReconstructionEXAMPLE 5.5:
Illustration of
adaptive median
filtering.  FIGURE 5.14 (a) Image corrupted by salt-and-pepper noise with probabilities P, =
filtering with a? x 7 median filter. (c} Result of adaptive median filtering with Sings the algorithm either finds a median value that is not an impulse {and branches to
stage B), or the maximum window size is reached. If the maximum window size is
reached, the algorithm returns the value of z,,.3- Note that there is no guarantee
that this value is not an impulse. The smaller the noise probabilities P, and/or P,
are, or the larger Sina, is allowed to be, the less Jikely it is that a premature exit condition will occur. This is plausible. As the density of the impulses increases, it stands
to reason that we would need a larger window to “clean up” the noise spikes,
Every time the algorithm outputs a value, the window S,, is moved to the next
location in the image. The algorithm then is reinitialized and applied to the pixels
in the new location. As indicated in Problem 3.18, the median value can be updated iteratively using only the new pixels, thus reducing computational load,M@ Figure 5.14({a) shows the circuit-board image corrupted by salt-and-pepper
noise with probabilities P, = P, = 0.25, which is 2.5 times the noise level used
in Fig. 5.10(a). Here the noise level is high enough to obscure most of the detail in the image. As a basis for comparison, the image was filtered first using
the smallest median filter required to remove most visible traces of impulse
noise. A 7 X 7 median filter was required to do this, and the result is shown in
Fig. 5.14(b). Although the noise was effectively removed, the filter caused significant loss of detail in the image. For instance, some of the connector fingers
at the top of the image appear distorted or broken. Other image details are
similarly distorted.Figure 5.14{c) shows the result of using the adaptive median filter with
Smax = 7. Noise removal performance was similar to the median filter, However, the adaptive filter did a better job of preserving sharpness and detail. The
connector fingers are less distorted, and some other features that were either
obscured or distorted beyond recognition by the median filter appear sharper
and better defined in Fig. 5.14(c). Two notable examples are the feed-through
smail white holes throughout the board, and the dark component with eight
legs in the bottom, left quadrant of the image. , = 1.25. (b) Result of
5.4 ® Periodic Noise Reduction by Frequency Domain Filtering 357Considering the high level of noise in Fig. 5.14(a), the adaptive algorithm performed quite well. The choice of maximum allowed window size depends on the
application, but a reasonable starting value can be estimated by experimenting
with various sizes of the standard median filter first. This will establish a visual
baseline regarding expectations on the performance of the adaptive algorithm. @EES Periodic Noise Reduction by Frequency
Domain FilteringPeriodic noise can be analyzed and filtered quite effectively using frequency
domain techniques. The basic idea is that periodic noise appears as concentrated
bursts of energy in the Fourier transform, at locations corresponding to the
frequencies of the periodic inierference. The approach is to use a selective filter (see Section 4.10) to isolate the noise. The three types of selective filters
(bandreject, bandpass, and notch, introduced in Section 4.10) are used in
Sections 5.4.1 through 5.4.3 for basic periodic noise reduction. We also develop
an optimum notch filtering approach in Section 5.4.4,5.4.1 Bandreject FiltersThe transfer functions of ideal, Butterworth, and Gaussian bandreject filters,
introduced in Section 4.10.1, are summarized in Table 4.6. Figure 5.15 shows
perspective plots of these filters, and the following exampie illustrates using a
bandreject filter for reducing the effects of periodic noise.@ One of the principal applications of bandreject filtering is for noise removal in EXAMPLE 5.6:
applications where the general location of the noise component(s) in the fre- Use of bandreject
quency domain is approximately known. A good example is an image corrupted __Iiltering for
by additive periodic noise that can be approximated as two-dimensional sinu- periodic poise
: : . tees . . emoval,
soidal functions, It is not difficult to show that the Fourier transform of a sine consists of two impulses that are mirror images of each other about the origin of the
transform. Their Jocations are given in Table 4.3. The impuises are both imaginary
(the real part of the Fourier transform of a sine is zero) and are complex conjugates of each other. We will have more to say about this topic in Sections 5.4.3 and5.4.4. Our purpose at the moment is to illustrate bandreject filtering. abe
FIGURE 5.15 From {eft to right. perspective plots of ideal. Butterworth (of order 1), and Gaussian bandreject
filters.
5.4 # Periodic Noise Reduction by Frequency Domain Filtering 359FIGURE 5.17
Noise pattern of
the image inFig. 5.16(a)}
obtained by
bandpass filtering. Performing straight bandpass filtering on an image is not acommon proce- EXAMPLE 8,7:dure because it generally removes too much image detai]. However, bandpass Bandpass filteringfiltering is quite useful in isolating the effects on an image caused by selected for extracting
naar . a noise patterns.frequency bands. This is illustrated in Fig. 5.17. This image was generated by(1) using Eq. (5.4-1) to obtain the bandpass filter corresponding to the band
reject filter used in Fig. 5.16; and {2} taking the inverse transform of thebandpass-filtered transform. Most image detail was lost, but the informationthat remains is most useful, as it is clear that the noise pattern recovered usingthis method is quite close to the noise that corrupted the image in Fig. 5.16{a).In other words, bandpass filtering helped isolate the neise pattern. This is auseful result because it simplifies analysis of the noise, reasonably indepen
dently of image content. &5.4.3 Notch FiltersA notch filter rejects (or passes} frequencies in predefined neighborhoods
about a center frequency. Equations for notch filtering are detailed in Section
4.10.2, Figure 5.18 shows 3-D plots of ideal, Butterworth, and Gaussian notch
(reject) filters. Due to the symmetry of the Fourier transform, notch filters must
appear in symmetric pairs about the origin in order to obtain meaningful results. The one exception to this rule is if the notch filter is located at the origin,
in which case it appears by itself. Ahough we show only one pair for illustrative purposes, the number of pairs of notch fillers that can be implemented is
arbitrary. The shape of the notch areas also can be arbitrary (e.g., rectangular).As explained in Section 4.10.2, we can obtain notch filters that pass, rather
than suppress, the frequencies containcd in the notch areas. Since these filters
perform exactly the opposite function as the notch reject filters, their transfer
functions are given byHyp(u.v) = f > Hyetu, v) (5.4-2)where Hyp(u, v) is the transfer function of the notch pass filter corresponding
to the notch reject filter with transfer function Aydt).
5.4 = Periodic Noise Reduction by Frequency Domain Filtering 361 question. The starlike components were caused by the interference. and sever
al pairs of components are present. indicating that the pattern contains more
than just one sinusoidal component.When several interference components are present. the methods discussed
in the preceding sections are not always acceptable because they may remove
too much image information in the filtering process (a highly undesitahle feature when images are unique and/or expensive to acquire}. In addition. the interference components generaly are not single-frequency bursts, Instead.
they tend to have broad skirts thal carry information about the pueriorancs
pattern. These skirts are not always causily deteuta
form background. Alternative ft   
     
    ing oicihods 1aeaaFIGURE 5.19(a) Satellite image
of Florida and the
Gulfof Mexico
showing horizontal
scan lines.(b) Spectrum.(c) Notchpass filter
superimposed on
(b). (d) Spatial
noise pattern.(c) Result of notch
reject filtering.
(Onginal image
courtesy of
NOAA.)
360 Chapter 5 Image Restoration and Reconstructiona
beFIGURE 5.18
Perspective plots
of (a) ideal,(b) Butterworth
(of order 2), and
{c) Gaussian
notch (reject}
filters.EXAMPLE 5.8:
Removal of
periodic noise by
notch filtering. # Figure 5.19(a) shows the same image as Fig. 4.51(a). The notch filtering approach that follows reduces the noise in this image, without introducing the
appreciable blurring we saw in Section 4.8.4. Unless blurring is desirable for
reasons we discussed in that section, notch filtering is preferable if a suitable
filter can be found.Just by looking at the nearly horizontal lines of the noise pattern in Fig. 5.19(a),
we expect its contribution in the frequency domain to be concentrated along the
vertical axis. However, the noise is not dominant enough to have a clear pattern
along this axis, as is evident from the spectrum shown in Fig. 5.19(b). We can get
an idea of what the noise contribution looks like by constructing a simple ideal
notch pass filter along the vertical axis of the Fourier transform, as shown in Fig.
5.19(c). The spatial representation of the noise pattern (inverse transform of the
notch-pass-filtered result) is shown in Fig. 5.19(d). This noise pattern corresponds
closely to the pattern in Fig. 5.19(a). Having thus constructed a suitable notch
pass filter that isolates the noise to a reasonable degree, we can obtain the corresponding notch reject filter from Eq. (5.4-2). The result of processing the image
with the notch reject filter is shown in Fig. 5.19(e). This image contains significantly fewer visible noise scan lines than Fig. 5.19(a). EJ
44.2 Optimum Notch Filtering
Figure 5.20(a), another example of periodic image degradation, shows a digital
image of the Martian terrain taken by the Mariner 6 spacecraft. The tnterference pattern is somewhat similar to the one in Fig, 5.16(a), but the former pattern is considerably more subtle and, consequently, harder to detect in the
frequency plane. Figure 5.20(b)} shows the Fourier spectrum of the image in
362 Chapter 5 @ Image Restoration and ReconstructionabFIGURE 5.20(a) Image of the
Martian terrain
taken by Mariner 6.
({b) Fourier
spectrum showing
periodic
interference.
(Courtesy of
NASA.) these degradations are quile useful in many applications. The method discussed here is optimum, in the sense that it minimizes local variances of the
restored estimate f(x, y).The procedure consists of first isolating the principal contributions of the
interference pattern and then subtracting a variable, weighted portion of the
pattern from the corrupted image. Although we develop the procedure in
the context of a specific application, the basic approach is quite general and
can be applied to other restoration tasks in which multiple periodic interference is a problem.The first step is to extract the principal frequency components of the interference pattern. As before, this can be done by placing a notch pass filter,
Apu, v), at the location of each spike. If the filter is constructed to pass only
components associated with the interference pattern, then the Fourier transform of the interference noise paltern is given by the expressionanN(u.v) = Hyp(u, v)G(te, v) (5.4-3)
where, as usual, G(r, #). denotes the Fourier transform of the corrupted image.Formation of Axp(i. 2) requires considerable judgment about what is or is
not an interference spike. For this reason, the notch pass filter generally is con
structed interactively by observing the spectrum of Gu, v) on a display. After
a particular filter has been selected, the corresponding pattern in the spatial
domain is obtained from the expressionaly, ¥) ay 'Hapla, nO. ’); (3.44)Because the corrupted image is assumed fo be formed by the addition of the
uncorrupted image (Cv. v) and the interference Tf oy(y yd) were known com
pletely, subtracting the pattem from g(x,y) to obtain Pie yy woutkd be a simple matter. The problem. of course, is that this lilkering procedure usualls    yields only an appre ion af the true patiora. Phi effect of companerts
5.4 & Periodic Noise Reduction by Frequency Domain Filteringnot present in the estimate of n(x, y) can be minimized instead by subtracting from g(x, y) a weighied portion of n(x. y) to obtain an estimate of f(x, y):fen y) = g(x. y) — wlx, yn(x y) (5.4-5)where, as before, fix y) is the estimate of f(x, y) and w(x, ¥) is to be determined. The function w(x, y) is called a weighting or modulation function, and
the objective of the procedure is to select this function so that the result is optimized in some meaningful way. One approach is to select w(x, y) so that the
variance of the estimate Fx, y) is minimized over a specified neighborhood
of every point (x, y).Consider a neighborhood of size (2a + 1) by (2b + 1) about a point (x, y).
The “local” variance of f(x, y) at coordinates (x, y) can be estimated from the
samples, as follows:a bf, x 2
DS Dd ifotsyto—fw »| (5.4-6)2f 4. _ ee
POY Gre Db +1) ,2, 2where Fx, y) is the average value of f in the neighborhoods that is,= ]
f(x,y) = Garde pS, Sie +sy+t) (84-7)Points on or near the edge of the image can be treated by considering partial
neighborhoods or by padding the border with Os.
Substituting Eq. (5.4-5) into Eq. (5.4-6) yields .a(x, y) = were > Sloe + sy t+)
-w(xt sy t Ontrts,y+ 9] (5.4-8)
~ [B(x ») ~ wl, Yn IYAssuming that w(x, y) remains essentially constant over the neighborhood
gives the approximationw(x + s,y +t) = w(x, y) (5.4-9)
for-@ = s = aand —b 1 = b. This assumption also results in the expression
w(x, y)a(x, y) = w(x, yA, y) (5.4-10)in the neighborhood. With these approximations, Eq. (5.4-8) becomes
1 ab
2, =-—_ + ts
RY) = BGs 1D, >, {la syn— w(x, yn + s,y¥ +9] (5.4-11)— [Bx y) - wx, yale, y]}?363neesate
5.5 m@ Linear, Position-Invariant Degradations 365 | 5.5 | Linear, Position-Invariant DegradationsThe input-output relationship in Fig, 5.1 before the restoration stage is ex
pressed as
a(t y) = ALP, yy] + abn y) (55-1)For the moment, let us assume that y(x. ¥) = Oso that g(x,y) = Al f(x,y].
Based on the discussion in Section 2.6.2, 4 is finear ifHlafi(s. y) + bhlx y)) = aH ey] + bAT AC.) (5.52)where a and b are scalars and f,(x, ¥) and jo(x, y) are any two inpul images.
Ifa = b = 1, Eq. (5.5-2} becomes abFIGURE 5.22(a) Fourier
spectrum ofN(u, v), and(b) corresponding
Noise interterence
patiern (x, \).
{Courtesy of
NASA.) Consult the book Web sitefor a brief review ol linear
system theory.FIGURE 5.23
Processed image.
(Courtesy of
NASA.}
364 Chapter 5 @ Image Restoration and ReconstructionEXAMPLE 5.9:
Illustration of
optimum notch
filtering.FIGURE 5,21
Fourier spectrum
(without shifting)
of the image
shown in Fig,
$.20(a).
(Courtesy of
NASA.)To minimize 7(x, y), we solveao7(x, y) _ .
note 9) = 0 (5.4-12)
for w(x, y). The result is
w(x, y) = gS y)nlx ¥) ¥) — Be vate, ¥) (5.4-13)Wx y) ~ Wey)To obtain the restored image F(x, y), we compute w(x, y) from Eq. (5.4-13)
and then use Eq. (5.4-5). As w(x, y) is assumed to be constant in a neighborhood, computing this function for every value of x and y in the image is unnecessary. Instead, w{x, y) is computed for one point in each nonoverlapping
neighborhood (preferably the center point) and then used to process all the
image points contained in that neighborhood.@ Figures 5.21 through 5.23 show the result of applying the preceding technique
to the image in Fig. 5.20(a). This image is of size $12 x 512 pixels, and a neighborhood with a = 5 = 15 was selected. Figure 5.21 shows the Fourier spectrum
of the corrupted image. The origin was not shifted to the center of the frequency
plane in this particular case, so wu = v = Q is at the top left corner of the transform image in Fig, 5.21. Figure 5.22(a) shows the spectrum of M(u, v), where only
the noise spikes are present. Figure 5,.22(b) shows the interference pattern
n(x, y) obtained by taking the inverse Fourier transform of N(u, v). Note the similarity between this pattern and the structure of the noise present in Fig, 5.20(a).
Finally, Fig, 5.23 shows the processed image obtained by using Eq. (5.4-5). The periodic interference was removed for all practical purposes. w
366 Chapter 5 a Image Restoration and ReconstructionALAC y) + ACs »)] = ALA, »)] + ALA. yD] (55-3)which is called the property of additivity. This property simply says that, if H is
a linear operator, the response to a sum of two inputs is equal to the sum of the
two responses,With f(x, y) = 0, Eq. (5.5-2) becomesAlafi(x, y)] = aHLACs yl (5.5-4)which is called the property of homogeneity. It says that the response to a constant multiple of any input is equal to the response to that input multiplied by
the same constant. Thus a linear operator possesses both the property of additivity and the property of homogeneity.An operator having the input-output relationship g(x, y) = H[Lf(x, y)] is
said to be position (or space) invariant ifH[f(x ~ a y — B)] = g(x — a, y — B) (5.5-5)for any f(x, y) and any a and Q. This definition indicates that the response at
any point in the image depends only on the value of the input at that point, not
on its position.With a slight (but equivalent) change in notation in the definition of the im
Sec the footnote i . ;
301 reqonding contianes pulse in Eq. (4.5-3), f(x, y) can be expressed as:
and discrete variables.f(xy) = [. [fe B)S(x — a, y ~ B) da dB (5.5-6)Assume again for a moment that 7(x, y) = 0. Then, substitution of Eq. (5.5-6)
into Eq. (5.5-1) results in the expressiong(x,y) = HLF, y)] = a / . [ _ Fla, Bax ~ ay ~ B)da as | (55-1)If His a linear operator and we extend the additivity property to integrals, theng(x,y) = [. [ave B(x - ay B)|dadB (5.58)Because f(a, 8) is independent of x and y, and using the homogeneity property,
it follows thatg(a y) = f. [fe B)H[S(x — a, vy — B)] da dB (5.5-9)
The term
h(x, a, y, B) = H[6(x - a, y ~ B)] (5.5-10)is called the impulse response of H.In other words, if n(x, y} = Oin Eq. (5.5-1),
then A(x, a, y, 8) is the response of H to an impulse at coordinates (x, y). In
5.5 & Linear, Position-Invariant Degradationsoptics, the impulse becomes a point of light and A(x, @, y, 8) is commonly referred to as the point spread function (PSF). This name arises from the fact
that all physical optical systems blur (spread) a point of light to some degree,
with the amount of blurring being determined by the quality of the opticalcomponents.
Substituting Eq. (5.5-10) into Eq. (5.5-9) yields the expressiona(x. y) = f. [fe B)A(x, a, y, B) da dB (5.5-11)which is called the superposition (or Fredholm) integral of the first kind. This
expression is a fundamental result that is at the core of linear system theory. It
states that if the response of H to an impulse is known, the response to any
input f(a, 8) can be calculated by means of Eq. (5.5-11). In other words, a linear system H is completely characterized by its impulse response.If H is position invariant, then, from Eq. (5.5-5),H[8(x ~ a, y ~ B)] = A(x — ay ~ B) (5.5-12)Equation (5.5-11) reduces in this case towoy= ff flapymx-ay~ p)dadp (5513)This expression is the convolution integral introduced for one variable in
Eq. (4.2-20) and extended to 2-D in Problem 4.11. This integral tells us that
knowing the impulse response of a linear system allows us to compute its
response, g, to any input f. The result is simply the convolution of the impulse response and the input function.3n the presence of additive noise, the expression of the linear degradationmodel [Eq. (5.5-11}] becomesg(x,y) = / ; f _ Fla, AYA. « 9, B)da dp + nlx.y) (55-14)If H is position invariant, Eq. (5.5-14) becomesg(x, y) = [. [ se.pyncs ~a,y~ B)dadB +7(x,y) — (5.5-15)The values of the noise term n(x, y) are random, and are assumed to be independent of position. Using the familiar notation for convolution, we can write
Eq. (5.5-15) asa(x, y) = h(x, ye SCY ¥) + n(x, y) (5.5-16)
or, based on the convolution theorem (see Section 4.6.6), we can express it in
the frequency domain asG(u.v) = H(u, v)F(u. v) + Nu, v) (5,5-17)367
370 — Chopter 5 a Image Restoration and ReconstructionabFIGURE 5.24
Degradation
estimation by
impulse
characterization.
{a) An impulse of
light (shown
magnified).(b) Imaged
(degraded)
impulse. ebedFIGURE 5.25
Illustration of the
atmospheric
turbulence model.
(a) Negligible
turbulence.(b} Severe
turbulence,& = 0,0025,(c) Mild
turbulence,k = 0,001.(d) Low
turbulence,& = 0.00025.
(Original image
courtesy of
NASA.)
368  Chogter Sm Image Restoration and ReconstructionThese two expressions agree with Eqs. (5.1-1) and (5.1-2). Keep in mind that,
for discrete quantities, all products are term by term. For example, term ij of
H(u, v) F(a, v) is the product of term i of H(u, v) and term i of F(u, v).In summary, the preceding discussion indicates that a linear, spatiallyinvariant degradation system with additive noise can be modeled in the spatial
domain as the convolution of the degradation (point spread) function with an
image, followed by the addition of noise, Based on the convolution theorem,
the same process can be expressed in the frequency domain as the product of
the transforms of the image and degradation, followed by the addition of the
transform of the noise. When working in the frequency domain, we make use
of an FFT algorithm, as discussed in Section 4.11. Keep in mind also the need
for function padding in the implementation of discrete Fourier transforms, as
outlined in Section 4.6.6.Many types of degradations can be approximated by linear, position-invariant
processes. The advantage of this approach is that the extensive tools of linear
system theory then become available for the solution of image restoration
problems. Nonlinear and position-dependent techniques, although more general (and usually more accurate), introduce difficulties that often have no
known solution or are very difficult to solve computationally. This chapter focuses on linear, space-invariant restoration techniques. Because degradations
are modeled as being the result of convolution, and restoration seeks to find
filters that apply the process in reverse, the term image deconvolution is used
frequently to signify linear image restoration. Similarly, the filters used in the
restoration process often are called deconvolution filters.a4 Estimating the Degradation FunctionThere are three principal ways to estimate the degradation function for use in
image restoration: (1) observation, (2) experimentation, and (3) mathematical
modeling. These methods are discussed in the following sections. The process
of restoring an image by using a degradation function that has been estimated
in some way sometimes is called blind deconvolution, due to the fact that the
true degradation function is seldom known completely.24.3 Estimation by Image ObservationSuppose that we are given a degraded image without any knowledge about the
degradation function H. Based on the assumption that the image was degraded
by a linear, position-invariant process, one way to estimate H is to gather information from the image itself. For example, if the image. is blurred, we can
look at a small rectangular section of the image containing sample structures,
like part of an object and the background. In order to reduce the effect of
noise, we would look for an area in which the signal content ts strong (e.g., an
area of high contrast). The next step would be to process the subimage to arrive at a result that is as unblurred as possible. For example. we can do this by
sharpening the subimage with a sharpening filler and even by processing small
areas by hand.
5.6 & Estimating the Degradation FunctionLet the observed subimage be denoted by g,(x, y), and let the processed
subimage (which in reality is our estimate of the original image in that area) be
denoted by f,(x, »). Then, assuming that the effect of noise is negligible because of our choice of a strong-signal area, it follows from Eq. (5.5-17) thatG,(u, v)A 6-1
F,(u, ») ee)H,(u, v) =
From the characteristics of this function, we then deduce the complete degradation function H(u, v) based on our assumption of position invariance. For example, suppose that a radial plot of H,(u, v) has the approximate shape of a
Gaussian curve. We can use that information to construct a function H(u, v) on
a larger scale, but having the same basic shape. We then use H(u, v) in one of
the restoration approaches to be discussed in the following sections, Clearly,
this is a laborious process used only in very specific circumstances such as, for
example, restoring an old photograph of historical value.5.6.2 Estimation by ExperimentationIf equipment similar to the equipment used to acquire the degraded image is available, it is possible in principle to obtain an accurate estimate of the degradation.
Images similar to the degraded image can be acquired with various system settings
until they are degraded as closely as possible to the image we wish to restore. Then
the idea is to obtain the impulse response of the degradation by imaging an impulse (small dot of light) using the same system settings, As noted in Section 5.5, a
linear, space-invariant system is characterized completely by its impulse response.
An impulse is simulated by a bright dot of light, as bright as possible to reduce the effect of noise to negligible values. Then, recalling that the Fourier
transform of an impulse is a constant, it follows from Eq. (5.5-17) thatH(u, v) = ee (5.6-2)where, as before, G(u, v) is the Fourier transform of the observed image and A is
a constant describing the strength of the impulse. Figure 5.24 shows an example.5.6.3 Estimation by ModelingDegradation modeling has been used for many years because of the insight it
affords into the image restoration problem. In some cases, the model can even
take into account environmental conditions that cause degradations. For example,
a degradation model proposed by Hufnagel and Stanley [1964] is based on the
physical characteristics of atmospheric turbulence. This model has a familiar form:Hu, v) = eke (5.63)where x is a constant that depends on the nature of the turbulence. With the exception of the 5/6 power on the exponent, this equation has the same form as
the Gaussian lowpass filter discussed in Section 4.8.3, In fact, the Gaussian LPF
is used sometimes to model mild. uniform blurring. Figure 5.25 shows examples369
5.6 # Estimating the Degradation Functionobtained by simulating blurring an image using Eq. (5.6-3) with values
k = 0.0025 (severe turbulence), & = 0.001 (mild turbulence), and k = 0.00025
(low turbulence), All images are of size 480 X 480 pixels.Another major approach in modeling is to derive a mathematical model
starting from basic principles. We illustrate this procedure by treating in some
detail the case in which an image has been blurred by uniform linear motion
between the image and the sensor during image acquisition. Suppose that an
image f(x, y) undergoes planar motion and that xo(#) and yo(r) are the timevarying components of motion in the x- and y-directions, respectively. The
total exposure at any point of the recording medium (say, film or digital memory) is obtained by integrating the instantaneous exposure over the time interval during which the imaging system shutter is open.Assuming that shutter opening and closing takes place instantaneously, and
that the optical imaging process is perfect, isolates the effect of image motion.
Then, if T is the duration of the exposure, it follows thatT
B(x y) = [ Flx — xo), y ~ yo] ar (5.6-4)where g(x, y) is the blurred image.
From Eq. (4.5-7), the Fourier transform of Eq, (5.6-4) isco no
Guy= ff ea nerrerrasdy"120 oo T .
= [. / if fle — x0, ¥ ~ wl] ar fraronem dx dyReversing the order of integration allows Eq. (5.6-5) to be expressed in the formT| oo co
Glu, v) = [ [ / . | _ fle = 200s. = wolfe Pe ax ‘| dt (56-6)The term inside the outer brackets is the Fourier transform of the displaced
function f [x — x(t), » — yo(t)]. Using Eq. (4.6-4) then yields the expression(5.6-5)r
G(u,v) = [ F(u, pe Prlert + 6] dt
0T
= F(u,v) [ en Pamluxol)+ yl] ge (5.6-7)
()where the last step follows from the fact that F(u, v) is independent of ¢.
By definingr
H(u,v) = [ eae + oyu) gy (5.6-8)
0371
372 Chopter 5 m Image Restoration and ReconstructionAs explained at the end
of Table 4.3, we sample
Eg (5.6-1)) inw and v to
generate a discrele filter.EXAMPLE 5.10:
Image blurring
due to motion.abFIGURE 5.26(a) Original image.
{b) Result of
blurring using the
function in Eq.
(5.6-11) with
a=6=0,l and
T=1. Eq. (5.6-7) can be expressed in the familiar form
Glu, v) = H(u, v)F(u, v) (5.6-9)Ifthe motion variables x9(z) and y(t) are known, the transfer function //(, v)
can be obtained directly from Eq. (5,6-8). As an illustration, suppose that the
image in question undergoes uniform linear motion in the x-direction only, at
arate given by x9{¢) = at/7’. When ¢ = 7, the image has been displaced by a
total distance a. With yo(t) = 0, Eq. (5.6-8) yieldsr
[ eo Pears) at
0+
= | er semua? dt (5.6-10)
0i(u,v)ilTs, .
--— sin(ruaje I"
Wwua 
Observe that H vanishes at values of u given by « = 1/a, where vis an integer.
If we allow the y-component to vary as well, with the molion given by
¥o = bt/T, then the degradation function becomesPr
py se ee 8] +n, pe im{uat vb) 56-11
Hu, vy nua + vb) sin{ar(ua + vb)je (5.6-11}Mi Figure 5.26{b) is an image blurred by computing the Fourier transform of the
image in Fig. 5.26(a), multiplying the transform by A (, v) from Eq, (5,6-11).
and taking the inverse transform. The images are of size 688 688 pixels, and
the parameters used in Fa, (5.6-11} were a = 6 = 0.1 and T = 1. As discussed
in Sections 5.8 and 5.9, recovery of the original image from its blurred counterpart presents some interesting challenges, particularly when noise is present in
the degraded image. #™“
374 Chupter S w Image Restoration and Reconstructionab
edFIGURE 5.27
RestoringFig, 5.25(b) with
Eq. (5.7-1).(a) Result of
using the full
filter. (b) Result
with H cut off
outside a radius of
40; {c) outside a
radius of 70; and
(d) outside a
radius of 85.   desired radius. Radii near 70 yielded the best visual results [Fig. 5.27(c)].
Radius values below that tended toward blurred images, as illustrated in
Fig. 5.27(b), which was obtained using a radius of 40. Values above 70 started
to produce degraded images, as illustrated in Fig. 5.27(d), which was obtained using a radius of 85. The image content is almost visible in this image
behind a “curtain” of noise, but the noise definitely dominates the result.
Further increases in radius values produced images that looked more and
more like Fig. 5.27(a). xThe results in the preceding example are ifluslyalive of the poor performance of direct inverse filtering in general. The basic theme of the three sec
tions that follow is how to improve on direct iverse filtering.Minimum Mean Square Error (Wiener) FilteringThe inverse filtering approach discussed in the previous section makes no exin approach that        
 plicit provision for handling noise. In this section we <incorporates both the degradation function and sin uo characteristics of
5.7 m Inverse Filtering 373Inverse FilteringThe material in this section is our first step in studying restoration of images
degraded by a degradation function H, which is given or obtained by a method
such as those discussed in the previous section. The simplest approach to
restoration is direct inverse fillering, where we compute an estimate, F(u, v),
of the transform of the original image simply by dividing the transform of the
degraded image, G(u, v), wy the degradation function:
G(u, v)H(u, v)The division is an array operation, as defined in Section 2.6.1 and in connection with Eq. (5.5-17). Substituting the right side of Eq. (5.1- 2) for G(x, v) in
Eq. (5.7-1) yieldsF(u,v) = (5.7-1)N(u, v)
H(u, v)
This is an interesting expression. It tells us that even if we know the degradation function we cannot recover the undegraded image [the inverse Fourier
transform of F(u, v)] exactly because N(u, v) is not known. There is more bad
news. If the degradation function has zero or very small values, then the ratio
N(u, v)/H(u, v) could easily dominate the estimate F(u, v). This, in fact, is frequently the case, as will be demonstrated shortly.One approach to get around the zero or small-value problem is to limit the
filter frequencies to values near the origin. From the discussion of Eq. (4.6-21)
we know that H(0, 0) is usually the highest value of H(u, v) in the frequency
domain. Thus, by limiting the analysis to frequencies near the origin, we reduce
the probability of encountering zero values. This approach is illustrated in the
following example.F(u,v) = F(u,v) + (5.7-2)M@ The image in Fig. 5.25(b) was inverse filtered with Eq. (5.7-1) using the EXAMPLE 5.11:
exact inverse of the degradation function that generated that image. That is, Ynverse filtering.
the degradation function used wasH(u, v) = eHlu- MAF +(e Ni2yIMwith k = 0.0025, The M/2 and N/2 constants are offset values; they center the
function so that it will correspond with the centered Fourier transform, as discussed on numerous occasions in the previous chapter. In this case,
M = N = 480. We know that a Gaussian-shape function has no zeros, so that
will not be a concern here. However, in spite of this, the degradation values became so small that the result of full inverse filtering [Fig. 5.27(a)] is useless. The
reasons for this poor result are as discussed in connection with Eq, (5.7-2).
Figures 5.27(b) through (d) show the results of cutting off values of the
Tatio G(u, v)/H(u, v) outside a radius of 40, 70, and 85, respectively. The
cut off was implemented by applying to the ratio a Butterworth lowpass
function of order 10. This provided a sharp (but smooth) transition at the
5.8 B Minimum Mean Square Error (Wiener) Filtering 375Noise into the restoration process. The method is founded on considering images
and noise as random variables, and the objective is to find an estimate f of the
uncorrupted image f such that the mean square error between them is minimized. This error measure is given bye = E{(f - f)} (58-1)where E{+} is the expected value of the argument. Jt is assumed that the
noise and the image are uncorrelated; that one or the other has zero mean; and
that the intensity levels in the estimate are a linear function of the levels in the
degraded image. Based on these conditions, the minimum of the error function
in Eq. (5.8-1) is given in the frequency domain by the expressionH’(u, v)Sp(u, v)
Sy(u, v)|H(u, vl? + S,(u,v)-|— Fwy) :
7 lm v)|? + S,(u, v)/S;(u, = low.) (5-8-2)_ | 1 |H(u, v)?Fu, v) = [ lew v) Hu, v) [Hu 0? + S,(u, 0/8; (te 5 Ot *)where we used the fact that the product of a complex quantity with its conjugate is equal to the magnitude of the complex quantity squared. This resuit is
known as the Wiener filter, after N. Wiener [1942], who first proposed the concept in the year shown. The filter, which consists of the terms inside the brackets, also is commonly referred to as the minimum mean square error filter or
the least square error filter. We include references at the end of the chapter to
sources containing detailed derivations of the Wiener filter. Note from the first
line in Eq. (5.8-2) that the Wiener filter does not have the same problem as the
inverse filter with zeros in the degradation function, unless the entire denominator is zero for the same value(s) of u and v.
The terms in Eq. (5.8-2) are as follows:H(u,v) = degradation functionH"(u, v) = complex conjugate of H(u, v)|H(u, v) |? = H"(u, v)H(u, v)S, (u,v) = INC, v)|? = power spectrum of the noise [see Eq. (4.6-18)]*
S;(u, v) = |F(u, v) ? = power spectrum of the undegraded image ‘The term | N(iz, v) |? also is referred to as the aurocorrelation of the noise. This terminology comes from
the correlation theorem (first tine of entry 7 in Table 4.3). When the two functions are the same, correlation becomes autocorrelation and the right side of that entry becomes N“(u. v) Nc, v), which is equal
to |N(u, »)I?, Similar comments apply to | F(u. v)|?, which is the autocorrelation of the image. We discuss correlation in more detail in Chapter 12.Nolte that entire images
are being considered random variables, as discussed at the end of
Section 2.6.8.
376 — Gaapter § @ Image Restoration and ReconstructionAs before, H(u, v) is the transform of the degradation function and G(#, v) is
the transform of the degraded image. The restored image in the spatial domain
is given by the inverse Fourier transform of the frequency-domain estimate
F(u, v). Note that if the noise is zero, then the noise power spectrum vanishes
and the Wiener filter reduces to the inverse filter.A number of useful measures are based on the power spectra of noise and
of the undegraded image. One of the most important is the signal-to-noise
ratio, approximated using frequency domain quantities such asM-IN-iY TIF, v)/?SNR = Oo (58-3). Y TW»?
n=0 v=0This ratio gives a measure of the level of information bearing signal power
(i.e., of the original, undegraded image) to the level of noise power. Images
with low noise tend to have a high SNR and, conversely, the same image with
a higher level of noise has a lower SNR. This ratio by itself is of limited value,
but it is an important metric used in characterizing the performance of
restoration algorithms.The mean square error given in statistical form in Eq. (5.8-1) can be approximated also in terms a‘summation involving the original and restored images:~ MN & al (x,y) — Pex, )P (5.8-4)In fact, if one considers the restored image to be “signal” and the differencebetween this image and the original to be noise, we can define a signal-to-noise
ratio in the spatial domain asM-I
=
re0N-1_
Sfx»?
y=0 SNR = aw - (5.8-5)
> > [fo ») - fa, »PThe closer f and f are, the larger this ratio will be. Sometimes the square root
of these measures is used instead, in which case they are referred to as the
root-mean-square-signal-to-noise ratio and the reet-mean-square-error, respectively. As we have mentioned several times before, keep in mind that
quantitative metrics do not necessarily relate well to perceived image quality.When we are dealing with spectrally white noise, the spectrum |N(u, vy I? is
a constant, which simplifies things considerably, However, the power spectrum
of the undegraded image seidom is known. An approach used frequently when
these quantities are not known or cannot be estimated is to approximate
Eq. (5.8-2) by the expression
378 Chapter 5 @ Image Restoration and Reconstruction  bs
FIGURE 5,29 (a) 8-bit image corrupted by motion blur and additive noise. (b} Reswe of inverse filtering.
(c} Result of Wiener filtering. (d)-(f) Same sequence. but with notse variance one order of magnitude less.
(g)-(i) Same sequence, but noise variance reduced by five orders of magnitude from (a). Note in (h) how
the deblurred image is quite visible through a “curtain” of nevise“
§.8 © Minimum Mean Square Error (Wiener) Filtering 377iu) CD see
u,v) = | more eee ,u 5.8
Hur) [Hae wie + kK [C8 G6)
where K is a specified constant that is added to all terms of |H(u, a)l*. The
following examples illustrate the use of this expression.if Figure 5.28 illustrates thé advantage of Wiener filtering over direct inverse filtering. Figure 5.28(a) is the full inverse-filtered result from Fig. 5.27(a). Similarly,
Fig. 5.28(b) is the radially limited inverse filter result of Fig, 5.27(c). These images are duplicated here for convenience in making comparisons. Figure 5.28(c)
shows the result obtained using Eg. (5.8-6) with the degradation function used in
Example 5.11. The value of K was chosen interactively to yield the best visual results. The advantage of Wiener filtering over the direct inverse approach is evident in this example. By comparing Figs. 5.25(a) and 5.28(c}, we see that the
Wiener filter yielded a result very close in appearance to the original image. &@ The first row of Fig. 5.29 shows, from left to right, the blurred image of Fig.
5.26(b) heavily corrupted by additive Gaussian noise of zero mean and variance of 650; the result of direct inverse filtering; and the result of Wiener filtering. The Wiener filter of Eg. (5.8-6) was used, with H(u,«) from Example
5.10, and with K chosen interactively to give the besi possible visual resul. As
expected, the inverse filter produced an unusable image. Note that the noise in
the inverse filtered image is so strong that its structure ig in the direction of the
deblurring filter. The Wiener filter result is by no means perfect, but it does
give us a hint as to image content. With some difficulty, the text is readable.
The second row of Fig. 5.29 shows the same sequence, but with the level of
noise variance reduced by one order of magnitude. This reduction had little effect
on the inverse filter, but the Wiener results are considerably improved. The text abc
FIGURE 5.28 Comparison of inverse aid Wiener Iiliciing.  (b) Radially limited inverse filter result. (c) Wiener filtercsuliod ful aivesse Glicriny of Lig. S28):EXAMPLE 5.12;
Comparison of
inverse and
Wiener filtcring.EXAMPLE 5.13:
Further
comparisons of
Wiener filtering.
5.9 @ Constrained Least Squares Filtering 381Wi Figure 5.30 shows the result of processing Figs. 5.29(a), (d),and (g) with con- EXAMPLE §,14:
strained least squares filters, in which the values of y were selected manually © Comparison of
to yield the best visual results. This is the same procedure we used to generate Wiener andthe Wiener filtered results in Fig. 5.29(c), (f), and (i). By comparing the con- squates Bitte,
strained least squares and Wiener results, it is noted that the former yielded "
slightly better results for the high- and medium-noise cases, with both filtersgenerating essentially equal results for the low-noise case. It is not unexpectedthat the constrained Jeast squares filter would outperform the Wiener filterwhen selecting the parameters manually for better visual results, The parame
ter y in Eq. (5.9-4) is a scalar, while the value of K in Eq. (5.8-6) is an approxi
mation to the ratio of two unknown frequency domain functions; this ratioseldom is constant. Thus, it stands to reason that a result based on manually se
lecting y would be a more accurate estimate of the undegraded image. sAs shown in the preceding example. it is possible to adjust the parameter y
interactively until acceptable results are achieved. If we are interested in optimality, however, then the parameter y must be adjusted so that the constraint in
Eq. (5.9-3) is satisfied. A procedure for computing y by iteration is as follows.Define a “residual” vector r as .r=g-Hf (5.9-6)Since, from the solution in Eq. (5.9-4), F(u, v) (and by implication f) is a function
of y, then r also is a function of this parameter. It can be shown (Hunt [1973]) thatoy) = a'r
= (rll? (S.9-7) FIGURE 5.30 Results of constrained least squares fillering, Compare (a). (b). and (c) with the Wiener filtering
results in Figs. 5.29(c), (f), and (i). respectively,
382Gupte: 5 @ Image Restoration and Reconstructionis a monotonically increasing function of y. What we want to do is adjust y so
thatIr? = Ini? + @ (5.9-8)where a is an accuracy factor. In view of Eq. (5.9-6), if |r? = In|, the constraint in Eq. (5.9-3) will be strictly satisfied.Because $(y)} is monotonic, finding the desired value of y is not difficult.
One approach is to1. Specify an initial value of y.2. Compute |r|”.3. Stop if Eq, (5.9-8) is satisfied; otherwise return to step 2 after increasing y
if rl? < nl? - @ or decreasing y if |r|? > |ly|? + a. Use the new value
of y in Eq. (5.9-4) to recompute the optimum estimate F(z, v).Other procedures, such as a Newton-Raphson algorithm, can be used to improve the speed of convergence.In order to use this algorithm, we need the quantities ||r[? and |||”. To compute |[r]?, we note from Eq. (5.9-6) thatR(u, v) = Glu, v) — Hu v)F(u, v) (5.9-9)
from which we obtain r(x, y) by computing the inverse transform of R(u, v). Then
M-IN-I
I? = 3 Sry) (5.9-10)x=0 y=0Computation of ||? leads to an interesting result. First, consider the variance
of the noise over the entire image, which we estimate by the sample-average
method, as discussed in Section 3.3.4:1 ~ITN-L
MN Zine. y) ~ mJ? (5.9-11)
where
1 M-1N-1
My = TaN = = n(x, y) (5.9-12)is the sample mean. With reference to the form of Eq. (5.9-10), we note that
the double summation in Eq. (5.9-11) is equal to ||y|?, This gives us the
expressionlal? = MN[o2 + 2) (5.9-13)This is a most useful result. It tells us that we can implement an optimum
restoration algorithm by having knowledge of only the mean and variance of
the noise. These quantities are not difficult to estimate (Section 5.2.4), assuming that the noise and image intensity values are not correlated. This is a basic
assumption of ai! the methods discussed in this chapter.
5.9 # Constrained Least Squares Filtering 379now is much easier to read. In the third row of Fig. 5.29, the noise variance has decreased move than five orders of magnitude from the first row. Jn fact, image
5.29g) has no visible noise, The inverse filter result is interesting in this case. The
noise is still quite visible, but the text can be seen through a “curtain” of noise.
This is a good example of the comments made regarding Eq. (5.7-2). In other
words, as is evident in Fig. 5.29(h), the inverse filter was quite capable of essentially eliminating the blur in the image. However, the noise still dominates the result. If we could “look” behind the noise in Figs. 5.29(b) and (e), the characters
also would show little blurring, The Wiener filter result in Fig. 5.29(i) is excellent,
being quite close visually to the original image in Fig, 5.26(a). These types of results are representative of what is possible with Wiener filtering, as Jong as a reasonable estimate of the degradation function is available. sEXE Constrained Least Squares FilteringThe problem of having to know something about the degradation function
is common to al] methods discussed in this chapter. However, the Wiener filter
presents an additional difficulty: The power spectra of the undegraded image
and noise must be known. We showed in the previous section that it is possible
to achieve excellent results using the approximation given in Eq. (5.8-6). However, a constant estimate of the ratio of the power spectra is not always a suitable solution.The method discussed in this section requires knowledge of only the mean
and variance of the noise. As discussed in Section 5.2.4, these parameters usually can be calculated from a given degraded image, so this is an important advantage. Another difference is that the Wiener filter is based on minimizing a
statistical criterion and, as such, it is optimal in an average sense. The algorithm presented in this section has the notable feature that it yields an optimal
result for each image to which it is applied. Of course, it is important to keep in
mind that these optimality criteria, while satisfying from a theoretical point of
view, are not related to the dynamics of visual perception. As a result, the
choice of one algorithm over the other will almost always be determined (at
least partially) by the perceived visual quality of the resulting images.By using the definition of convolution given in Eq. (4.6-23), and as explained in Section 2.6.6, we can express Eq. (5.5-16) in vector-matrix form:g= H+ 7 (5.9-1)For example, suppose that g(x, y) isof size M x N.Then we can form the first V
elements of the vector g by using the image elements in first row of g(x, y), the
next NV elements from the second row, and so on. The resulting vector will have dimensions MN X 1. These are also the dimensions of f and 4, as these vectors are
formed in the same manner. The matrix H then has dimensions MN x MN. Its
elements are given by the elements of the convolution given in Eq. (4.6-23).It would be reasonable to arrive at the conclusion that the restoration problem can now be reduced to simple matrix manipulations. Unfortunately, this is
not the case. For instance, suppose that we are working with images of medium
size,say M = N = 512. Then the vectors in Eq. (5.9-1) would be of dimension Consult the book Web
site for a brief review of
vectors and matrices.
380 — Chopter 5 @ Image Restoration and Reconstruction&Consult the Tutoriais seclion in the book Web site
for an entire chapter devoted to the topic of alge~
braic techniques for image
restoration.262,144 x 1, and matrix H would be of dimensions 262,144 x 262,144. Manipulating vectors and matrices of such sizes is not a trivial task. The problem
is complicated further by the fact H is highly sensitive to noise (after the expe-~
riences we had with the effect of noise in the previous two sections, this should
not be a surprise). However, formulating the restoration problem in matrix
form does facilitate derivation of restoration techniques.Although we do not fully derive the method of constrained least squares
that we are about to present, this method has its roots in a matrix formulation.
We give references at the end of the chapter to sources where derivations are
covered in detail. Central to the method is the issue of the sensitivity of H to
noise. One way to alleviate the noise sensitivity problem is to base optimality
of restoration on a measure of smoothness, such as the second derivative of an
image (our old friend the Laplacian). To be meaningful, the restoration must
be constrained by the parameters of the problems at hand. Thus, what is desired is to find the minimum of a criterion function, C, defined asM-1N~1
C= 2D ZIV, yP (5.9-2)
x=0 y=0
subject to the constraint
Ig ~ HE/?=In/P (5.9-3)where ||w||? = w’w is the Euclidean vector norm,' and f is the estimate of the
undegraded image. The Laplacian operator V” is defined in Eq. (3.6-3).The frequency domain solution to this optimization problem is given by the
expressionH’(u, v)|H(u, v)P + pao OM ° 6)F(u,v) = [where y is a parameter that must be adjusted so that the constraint in Eq. (5.9-3)
is satisfied, and P(u, v) is the Fourier transform of the function0 -1 «0
p(xy)=]}-1 4 =1 (5.9-5)
0 -1 0We recognize this function as the Laplacian operator introduced in Section
3.6.2. As noted earlier, it is important to keep in mind that p(x, y), as well as all
other relevant spatial domain functions, must be properly padded with zeros
prior to computing their Fourier transforms for use in Eq. (5.9-4), as discussed
in Section 4.6.6. Note that Eq. (5.9-4) reduces to inverse filtering if y is zero. 7
‘Recall that. for a vector w with n components, ww = Sw, where zw, is the kth component of w.
R=L
5.10 ®& Geometric Mean Filter @ Figure 5.31(a) shows the result obtained by using the algorithm just described to estimate the optimum filler for restoring Fig, 5.25(b). The initialvalue used for y was 10™°, the correction factor for adjusting y was 10°, and
the value for a was 0.25. The noise parameters specified were the same used to
generate Fig. 5.25(a): a noise variance of 10°°, and zero mean. The restored result is almost as ; 90d as Fig, 5.28(c), which was obtained by Wiener filtering
with K manually specified for best visual results. Figure 5.31{b) shows what
can happen if the wrong estimaic of noise parameters are used. In this case.
the noise variance specified was 10°? and the mean was left at a value of 0. The
result in this case is considerably more blurred. &As stated at the beginning of this section, it is important to keep in mind that
optimum restoration jn the sense of constrained least squares does not necessarily imply “best” in the visual sense, Depending on the nature and magnitude of
the degradation and noise, the other parameters in the algorithm for iteralively
determining the optimum estimate also play a role in the final result. In general,
automatically determined restoration filters yield inferior results to manual adjustment of filler parameters. This is particularly true of the constrained least
squares filter, which is completely specitied by a smgle. scalar parameter5.103 Geometric Mean FilterIt is possible to generalize slightly the Wiener filter discussed in Section 5.8.
The generalization is in the form of the so-called geometric meant filter:y F(t, v)[ Su u, ll Glu,
[Spas v)(5.10-1) [FQ4, 0) |? + 8) with a and 8 being positive. rca constants. Phe geometric mean filter consists
the two expressions in brackets raised to the powers a aud to. respectively.383abFIGURE 5.31(a) Hteratively
determined
constrained least
squares
restoration of
Fig. 5.16(b), using
correct noise
parameters,(b) Result
obtained with
wrong noise
parameters,EXAMPLE 4.15:
Iterative
estimation of the
optimum
constrained least
squares filler,
384  Chopter 5m Image Restoration and ReconstructionAs noted in Chapter 1,
the term computerized
axial tomography {CAT)
is used interchangeably
to denote CT.ab
edeFIGURE 5.32(a) Flat region
showing a simple
object, an input
parallel beam, anda detector strip.(b) Result of backprojecting the
sensed strip data
(ie., the 1-D absorption profile), (c) The
beam and detectors
rotated by 90°.(d) Back-projection.
{e) The sum of (b}
and (d), The intensity where the backprojections intersect
is twice the intensity
of the individual
back-projections.When a = | this filter reduces to the inverse filter. With @ = 0 the filter becomes the so-called parametric Wiener filter, which reduces to the standard
Wiener filter when 8 = 1. If w = 1/2, the filter becomes a product of the two
quantities raised to the same power, which is the definition of the geometric
mean, thus giving the filter its name. With 8 = 1, as @ decreases below 1/2, the filter performance will tend more toward the inverse filter. Similarly, when @ increases above 1/2, the filter will behave more like the Wiener filter. When a = 1/2
and 8 = 1, the filter also is commonly referred to as the spectrum equalization filter. Equation (5.10-1) is quite useful when implementing restoration filters because it represents a family of filters combined into a single expression.El Image Reconstruction from ProjectionsIn the previous sections of this chapter, we dealt with techniques for restoring
a degraded version of an image. In this section, we examine the problem of
reconstructing an image from a series of projections, with a focus on X-ray
computed toniography (CT). This is the earliest and still the most widely used
type of CT and is currently one of the principal applications of digital image
processing in medicine.5.41.1 IntroductionThe reconstruction problem is simple in principle and can be explained qualitatively in a straightforward, intuitive manner. To begin, consider Fig. 5.32(a).
which consists of a single object on a uniform background. To bring physical/Absorption profile   
 
  BeamB) Xe Ray Detector strip —4TIT TTT WIN WHAT | |
388 Chapter 5 m Image Restoration and Reconstructionab
edFIGURE 5.35 Four
generations of CT
scanners. The
dotted arrow
lines indicate
incremental
linear motion.
The dotted arrow
arcs indicate
incremental
rotation. The
cross-mark on
the subject’s head
indicates linear
motion
perpendicular to
the plane of the
paper. The
double arrows in
(a) and (b}
indicate that the
source/detector
unit is transiated
and then brought
back into its
original position. digital computers made them practical. The theoretical foundation of CT dates
back to Johann Radon, a mathematician from Vienna who derived a method in
1917 for projecting a 2-D object along parallel rays as part of his work on line integrals. The method now is referred to commonly as the Radon transform, a topic
we discuss in the following section. Forty-five years later, Allan M. Cormack, a
physicist at Tufts University, partially “rediscovered” these concepts and applied
them to CT. Cormack published his initial findings in 1963 and 1964 and showed
how they could be used to reconstruct cross-sectional images of the body from
X-ray images taken at different angular directions. He gave the mathematical
formulae needed for the reconstruction and built a CT prototype to show the
practicality of his ideas. Working independently, electrical engineer Godfrey N.
Hounsfield and his colleagues at EMI in London formulated a similar solution
and built the first medical CT machine. Cormack and Hounsfield shared the 1979
Nobel Prize in Medicine for their contributions to medical tomography.
First-generation (G1) CT scanners employ a “pencil” X-ray beam and a single
detector, as Fig, 5.35(a) shows. For a given angle of rotation, the source/detector
5.11 # Image Reconstruction from ProjectionsMeaning to the following expianation, suppose that this image is a cross section of a 3-D region of a human body. Assume also that the background in the
image represents soft, uniform tissue, while the round object is a tumor, also
uniform, but with higher absorption characteristics.' Suppose next that we pass a thin, flat beam of X-rays from left to right
(though the plane of the image), as Fig. 5.32(a) shows, and assume that the energy of the beam is absorbed more by the object than by the background, as
typically is the case. Using a strip of X-ray absorption detectors on the other
side of the region will yield the signal (absorption profile) shown, whose amplitude (intensity) is proportional to absorption.’ We may view any point in
the signal as the sum of the absorption values across the single ray in the beam
corresponding spatially to that point (such a sum often is referred to as a
raysum). At this juncture, all the information we have about the object is this
1-D absorption signal.We have no way of determining from a single projection whether we are
dealing with a single object or a multitude of objects along the path of the
beam, but we begin the reconstruction by creating an image based on just this
information. The approach is to project the 1-D signal back across the direction from which the beam came, as Fig, 5.32(b) shows. The process of backprojecting a 1-D signal across a 2-D area sometimes is referred to as smearing
the projection back across the area. In terms of digital images, this means duplicating the same 1-D signal across the image perpendicularly to the direction
of the beam. For example, Fig. 5.32(b) was created by duplicating the 1-D signal in all columns of the reconstructed image. For obvious reasons, the approach just described is called backprojection.Next, suppose that we rotate the position of the source-detector pair by
90°, as in Fig. 5.32(c). Repeating the procedure explained in the previous
paragraph yields a backprojection image in the vertical direction, as Fig.
5.32(d) shows. We continue the reconstruction by adding this result to the
previous backprojection, resulting in Fig. 5.32(e). Now, we can tell that the
object of interest is contained in the square shown, whose amplitude is twice
the amplitude of the individual backprojections. A little thought will reveal
that we should be able to learn more about the shape of the object in question by taking more views in the manner just described. In fact, this is exactly
what happens, as Fig. 5.33 shows. As the number of projections increases, the
strength of non-intersecting backprojections decreases relative to the
strength of regions in which multiple backprojections intersect, The net effect is that brighter regions will dominate the result, and backprojections
with few or no intersections wil] fade into the background as the image is
scaled for display.Figure 5.33(f), formed from 32 projections, illustrates this concept. Note,
however, that while this reconstructed image is a reasonably good approximation to the shape of the original object, the image is blurred by a “halo” effect, 7A treatment of the physics of X-ray sources and detectors is beyond the scope of our discussion. which
focuses on the image processing aspects of CT. See Prince and Links [2006] for an excellent introduction
to the physics of X-ray image formation,385
5.11 & Image Reconstruction from Projections 391y FIGURE 5.37
Geomeiry of aA point e(p,, 0) in parallel-ray beam.
Q the projection   
  Complete projection, g(p. 8;),
for a fixed angieThe projection of a parallel-ray beam may be modeled by a set of such lines, as
Fig. 5.37 shows. An arbitrary point in the projection signal is given by the raysum along the line x cos 6 + y sin @ = p;. Working with continuous quantities' for the moment, the raysum is a line integral, given byoo oo
8(p;. Ox) = / / f(x, y)8(x cos & + ysin® — pjdxdy — (5.11-2)where we used the properties of the impulse, 6, discussed in Section 4.5.1. In other
words, the right side of Eq. (5.11-2) is zero unless the argument of 4 is zero, indicating that the integral is computed only along the line x cos 6, -+ y sin @, = pj.
If we consider all values of p and @, the preceding equation generalizes tu&(p, 4) = | [ f(x, y8(xcos#@ + ysin@ - p)dxdy (511-3)This equation, which gives the projection (line integral) of f(x, )) along an arbitrary
line in the xy-plane, is the Radon transform mentioned in the previous section. The
notation H{ f(x, y)} or M{f} is used sometimes in place of g{p, 9) in Eq. (5.11-3)
to denote the Radon transform of f, but the type of notation used in Eq. (5.11-3) is
more customary. As will become evident in the discussion that follows. the Radon
transform is the comerstone of reconstruction from projections, with computed tomography being its principal application in the field of image processing.  ‘In Chapter 4, we exercised great care in denoting continuous image coordinates by and discrete coordinates by (x, y). At that time, the distinction was important because we were developing basic concepts to take us from continuous to sampled quantities. In the present discussion, we go back and forth
$0 many times between continuous and discrete coordinates that adhering lo this convention is likely lo
generate unnecessary confusion. For this reason. and also to follow the pubtished titerature in this field
(e.g. see Prince and Links {2006]), we let the context determine whether coordinates (x. 1) are continuous or discrete. When they are continuous, you will see integrals, otherwise you will see sammations.
390 Ghopter § @ Image Restoration and ReconstructionThronghout this section,
we follow CT convention
and place the origin of
the xy-plane in the center. instead of at our cusltomary top left comer
(see Section 2.4.2). Note,
however, that both are
right-handed coordinate
systems, the only dilference being that our
image coordinate system
bas no negative axcs, We
can accaunt far the difference with a simpic
translation of the origin,
so both representations
are interchangeable,chest scans) that require the patient to hold his/her breath during image acquisition. Completing these procedures for, say, 30 images, may require several minutes. An approach whose use is increasing is helical CT, sometimes referred to as
sixth-generation (G6) CT. in this approach, a G3 or G4 scanner is configured
using so-called stip rings that eliminate the need for electrical and signal cabling
between the source/detectors and the processing unit. The source/detector pair
then rotates continuously through 360° while the patient is moved at a constant
speed along the axis perpendicular to the scan. The result is a continuous helical
volume of data that is then processed to obtain individual slice images.Seventh-generation (G7) scanners (also called multislice CT scanners) are
emerging in which “thick” fan beams are used in conjunction with parallel
banks of detectors to collect volumetric CT data simultaneously. That is, 3-D
cross-sectional “slabs,” rather than single cross-sectional images are generated
per X-ray burst. In addition to a significant increase in detail, this approach has
the advantage that it utilizes X-ray tubes more economically, thus reducing
cost and potentially reducing dosage.Beginning in the next section, we develop the mathematical tools necessary
for formulating image projection and reconstruction algorithms. Our focus is on
the image-processing fundamentals that underpin all the CT approaches just
discussed. Information regarding the mechanical and source/detector characteristics of CT systems is provided in the references cited at the end of the chapter.5.11.3 Projections and the Radon TransformIn what follows, we develop in detail the mathematics needed for image reconstruction in the context of X-ray computed tomography, but the same basic
principles are applicable in other CT imaging modalities, such as SPECT (single photon emission tomography), PET (positron emission tomography), MRI
(magnetic resonance imaging), and some modalities of ultrasound imaging.A Straight line in Cartesian coordinates can be described either by its slopeintercept form, y = ax + b, or, as in Fig. 5.36, by its normal representation:xcos@é + ysin@d = p (5.11-1) FIGURE 5.36 Normal representation of a straight line.
392 Chapter 5 # Image Restoration and ReconstructionEXAMPLE-5.17:
Using the Radon
transform to
obtain the
projection of a
circular region.a
bFIGURE 5.38 (a) A
disk and (b) a plot of
its Radon transform,
derived analytically.
Here we were able to
plot the transform
because it depends
only on one variable.
When g depends on
both p and 6, the
Radon transform
becomes an image
whose axes are p and
@, and the intensity
of a pixel is
proportional to the
value of g at the
location of that pixel.In the discrete case, Eq. (5.11-3} becomesM-1N-1
gp, 8) = SD flx, y)d(x cos @ + y sing — p)x=0 y=0
where x, y, p, and @ are now discrete variables. If we fix @ and allow p to vary,
we see that (5.11-4) simply sums the pixels of f(x, y) along the line defined by
the specified values of these two parameters, Incrementing through all values
of p required to span the image (with @ fixed) yields one projection. Changing
6 and repeating the foregoing procedure yields another projection, and so
forth. This is precisely how the projections in Section 5.11.1 were generated.(5.11-4)® Before proceeding, we illustrate how to use the Radon transform to obtain
an analytical expression for the projection of the circular object in Fig. 5.38(a):A rtysP0 otherwisefy) = {where A is a constant and r is the radius of the object. We assume that the circle is centered on the origin of the xy-plane. Because the object is circularly
symmetric, its projections are the same for all angles, so all we have to do is obtain the projection for @ = 0°, Equation (5,11-3) then becomes[. [1 y)5(x ~ p) dx dy&(p, 6)Il| oS (p. ) dy
5.11 # Image Reconstruction from Projectionspair is translated incrementally along the linear direction shown. A projection
(like the ones in Fig, 5,32), is generated by measuring the output of the detector
at each increment of translation. After a complete linear translation, the
source/detector assembly is rotated and the procedure is repeated to generate
another projection at a different angle. The procedure is repeated for all desired
angles in the range [0°, 180°] to generate a complete set of projections, from
which one image is generated by backprojection, as explained in the previous
section. The cross-mark on the head of the subject indicates motion in a direction perpendicular Lo the plane of the source/detector pair. A set of cross sectional images (slices) is generated by incrementally moving the subject (after
each complete scan) past the source/detector plane. Stacking these images computationally produces a 3-D volume of a section of the body. G1 scanners are no
longer manufactured for medical imaging but, because they produce a parallelray beam (as in Fig. 5.32), their geometry is the one used predominantly for introducing the fundamentals of CT imaging. As discussed in the following
section, this geometry is the starting point for deriving the equations necessary
to implement image reconstruction from projections.Second-generation (G2) CT scanners [Fig. 5.35(b)] operate on the same
principle as G1 scanners, but the beam used is in the shape of a fan. This allows
the use of multiple detectors, thus requiring fewer translations of the
source/detector pair. Third-generation (G3) scanners are a significant improvement over the earlier two generations of CT geometries, As Fig. 5.35(c)
shows, G3 scanners employ a bank of detectors long enough (on the order of
1000 individual detectors) to cover the entire field of view of a wider beam.
Consequently, each increment of angle produces an entire projection, eliminating the need to translate the source/detector pair, as in the geometry of G1
and G2 scanners. Fourth-generation (G4) scanners go a step further. By employing a circular ring of detectors (on the order of 5000 individual detectors),
only the source has to rotate. The key advantage of G3 and G4 scanners is
speed. Key disadvantages are cost and greater X-ray scatter, which requires
higher doses than G1 and G2 scanners to achieve comparable signal-to-noise
characteristics.Newer scanning modalities are beginning to be adopted. For example, fifthgeneration (G5) CT scanners, also known as electron beam computed tomography (EBCT) scanners, eliminate all mechanical motion by employing electron
beams controlled electromagnetically. By striking tungsten anodes that encircle the patient, these beams generate X-rays that are then shaped into a fan
beam that passes through the patient and excites a ring of detectors, as in G4
scanners,The conventional manner in which CT images are obtained is to keep the patient stationary during the scanning time required to generate one image. Scanning is then halted while the position of the patient is incremented in the
direction perpendicular to the imaging plane using a motorized table. The next
image is then oblained and the procedure is repeated for the number of increments required to cover a specified section of the body. Although an image may
be obtained in less than one second, there are procedures (e.g., abdominal and389
394 Chapter 5 mw Image Restoration and Reconstruction  ed
FIGURE 5.39. ‘Two images and their stnograms (Radon transforms). Each row of a sinogram
is a projection along the corresponding angie on the vertical axis. [mage (c) is called the
Shepp-Logan phantom. tn its original form, the contrast of the phantom is quite low. It is
shown enhanced here to facilitate viewing.The key objective of CT is to obtain a 3-D representation of a volume from
its projections. As introduced intuitively in Section 5.11.1, the approach is to
back-project each projection and then sum all the backprojections to generate
one image (slice), Stacking all the resulting images produces a 3-D rendition of
the volume.To obtain a formal expression for a back-projected image from the
Radon transform, let us begin with a single point, g(p;, 0), of the complete
projection, g(p, 4), for a fixed value of rotation, &, (see Fig. $.37}. Forming
part of an image by back-projecting this sirgle point is nothing more than
copying the line .(p,. @,) onto the image, where the value of each point in that
line is g(p;, 8). Repeating this process of all values-of p, in the projected signal
(but keeping the value of @ fixed al 4.) resulis in the follawing expression:fae. vy glp. Ah} ety cos fy vesin Gi)
$.11  fmage Reconstruction from Projections 395we may write in general that the image formed from a single backprojection
obtained at an angle @ is given byfalx. y) = g(x cos + ysin é,@) (5.11-5)We form the final image by integrating over ail the back-projected images:
fQ.y) = [i x,y) dO (5.11-6)In the discrete case, the integral becomes a sum of all the back-projected images:
f(xy) = Show) (5.11-7)where, 2, y, and @ are now discreie quantities. Recai] from the discussion in
Section 5.11.1 that the projections at 0° and 180° are mirror images of each other,
so the summations are carried out to the last angle increment before 180°. For example, if 0.5° increments are being used, the summation is from 0 to 179.5 in halfdegree increments. A back-projected image formed in the manner just described
sometimes is referred to as a faminogram. It ts understood implicitly that a
laminogram is only an approximation to the image from which the projections
were generated, a fact that is illustrated clearly in the following example.& Equation (5.11-7) was used to generate the back-projected images in Figs.
5.32 through 5.34, from projections obtained with Eq. (5.11-4). Similarly,
these equations were used to generate Figs. 5.40(a) and (b). which show the
back-projected images corresponding to the sinograms in Fig. 5.39(b) and
(d), respectively. As with the earlier figures, we note a significant amount of
blurring, so it is obvious that a straight use of Eqs. (5.11-4) and (5.11-7) will
not yield acceptable results. Early, experimental CT systems were based on
these equations. However, as you will see in Section 5.11.5, significant improvements in reconstruction are possible by reformulating the backprojection approach. x EXAMPLE 5.18:
Obtaining backproiected images
from sinograms.abFIGURE 5.40
Backprojections
of the sinograms
in Fig. 5.39.
5.11 # Image Reconstruction from Projections 393where the second line follows from Eq. (4.2-10}. As noted earlier, this is a lineintegral (along the line L(p, 0) in this case). Also, note that g(p, @) = 0 whenlel > r. When |p| = 7 the integral is evaluated from y = —V?r? — p° toyeV P~ p*. Therefore,VPoe
sto.) [Fo yayPopVFae
= / AdyVPCarrying out the integration yields8) = gp) = 2AVr — fp [pj Sr8(0, 8) = 80) 0 otherwisewhere we used the fact mentioned above that g(p,6) = 0 when lpl > r.
Figure 5.38(b) shows the result, which agrees with the projections illustrated in
Figs. 5.32 and 5.33. Note that g{p, 9) = g(p); that is, g is independent of @ because the object is symmetric about the origin.When the Radon transform, g(p, 9), is displayed as an image with p and @ as
rectilinear coordinates, the result is called a sinogram, similar in concept to displaying the Fourier spectrum (unlike the Fourier transform, however, g(p, #) is
always a real function). Like the Fourier transform, a sinogram contains the
data necessary to reconstruct f(x,y). As is the case with displays of the Fourier spectrum, sinograms can be readily interpreted for simple regions, but become increasingly difficult to “read” as the region being projected becomes
more complex. For example, Fig. 5.39(b) is the sinogram of the rectangle on To generate arrays with
the left. The vertical and horizontal axes correspond to 6 and g, respectively. sows of the same size. the
Thus, the bottom row is the projection of the rectangle in the horizontal direc- Hine
tion (i.e.,@ = 0°), and the middle row is the projection in the vertical direction corresponds to the
(6 = 90°). The fact that the nonzero portion of the bottom row is smaller than lee ser aaine neice:
the nonzero portion of the middle row tells us that the object is narrower in tion. For example. the
the horizontal direction. The fact that the sinogram is symmetric in both direc- minimum ize of asinotions about the center of the image tells us that we are dealing with an object Mx M obuined wing
that is symmetric and parallel to the x and y axes. Finally, the sinogram is Tare oh here ois he
smooth, indicating that the object has a uniform intensity. Other than these  cmaliest integer greater
types of general observations, we cannot say much more about this sinogram. — h V2Figure 5.39(c) shows an image of the Shepp-Logan phantom, a widely used
synthetic image designed to simulate the absorption of major areas of the
brain, including smal] tumors. The sinogram of this image is considerably more
difficult to interpret, as Fig. 5.39(d) shows. We still can infer some symmetry
properties, but that is about all we can say. Visual] analysis of sinograms is of limited practical use, but sometimes it is helpful in algorithm development.
396 Chapter § m Image Restoration and Reconstruction§.11.4 The Fourier-Slice TheoremIn this section, we derive a fundamental resuit relating the 1-D Fourier transform of a projection and the 2-D Fourier transform of the region from which
the projection was obtained. This relationship is the basis for reconstruction
methods capable of dealing with the blurring problem just discussed.The 1-D Fourier transform of a projection with respect to p isooG(w, 6) = / 2(p, de?" do (5.11-8)
00where, as in Eq. (4.2-16), w is the frequency variable, and it is understood that
this expression is for a given value of §. Substituting Eq. (5.11-3) for g(p, @) results in the expressionco ~ oO
Go, 6) = [ [ [ f(x, y)6(x cos + ysin — pe? dx dy dp
oo Joo J~ooco] 00 oO
[ [ f(x, of f &(x cos 8 + ysin @ — pe 7? «| dxdy
—co Joo —000 pce (5.11-9)
[ [ fx, ye Prete cos 0+ y sit #) dx dyIwhere the last step follows from the property of the impulse mentioned earlier in this section. By letting u = w cos @ and v = w sin 6, Eq. (5.11-9) becomeslu.0) =| / / f(x, ye Perr) aray| (5.11-10)
-00 J—90 i= wos d; v=wsin dOWe recognize this expression as the 2-D Fourier transform of f(x, y) [see
Eq. (4.5-7)] evaluated at the values of uw and v indicated. That is,G(w, 8) = [FCs v)),.<0005 @; o=wsin
(w ) [ ¢ 9} C08 8; v 8 (5.1111)= F(w cos 6, w sin 6}where, as usual, F(u, v) denotes the 2-D Fourier transform of f(x, y).
Equation (5.11-11) is known as the Fourier-slice theorem (or the
projection-slice theorem). It states that the Fourier transform of a projection is a slice of the 2-D Fourier transform of the region from which the
projection was obtained. The reason for this terminology can be explained
with the aid of Fig. 5.41. As this figure shows, the 1-D Fourier transform of
an arbitrary projection is obtained by extracting the values of F(u, v) along
a line oriented at the same angle as the angle used in generating the projection. In principle, we could obtain f(x, y) simply by obtaining the inverse
5.1) @ Image Reconstruction from Projections   2-D Fourier
transform
Flu, v} 
   
    
 
 
 Projection    
 1-D Fourier
transform  Fourier transform of F(u, v).? However, this is expensive computationally,
as it involves inverting a 2-D transform. The approach discussed in the following section is much more efficient.5.11.5 Reconstruction Using Parallel-Beam Filtered
BackprojectionsAs we saw in Section 5.11.1 and in Example 5.18, obtaining backprojections
directly yields unacceptably blurred results. Fortunately, there is a straightforward solution to this problem based simply on filtering the projections before
computing the backprojections. From Eq, (4.5-8), the 2-D inverse Fourier
transform of F(u, v) iseo iso]
f(y) = [ / F(u, vje?s*) du dy (5.11-12)
—00 J-00If, as in Eqs. (5.11-10) and (5.11-11), we let u = wcos @ and » = wsin9, then
the differentials become du dv = w dw d@, and we can express Eq. (5.11-12} in
polar coordinates:Qn ~
fay) = f [ F(w cos 8, w sin Oe? 7 8 +75) 6 dey d@ {5.11-13)
0 Jo
Then, using the Fourier-slice theorem,ar po
f(y y) = [ [ Geo, Oe PR 5 8*5 ey degd@ — (5,11-14)
0 tu ‘Keep in mind that blurring will still be present in an image recovered using the inverse Fourier
transform, because the result is equivalent to the result obtained using the approach discussed in theprevious section.397FIGURE 5.41
Tilustration of the
Fourier-slice theorem. The 1-D
Fourier transform
of a projection is
a slice of the 2-D
Fourier transform
of the region from
which the projection was obtained.
Note the correspondence of the
angle 8.The relationshipdudv = wdw dé is from
basic integral cateulus,
where the Jacobian is
‘used as the basis for a
change of variables.
398 Chapter 5 ® Image Restoration and Reconstructionab
edeFIGURE 5.42(a) Frequency
domain plot of the
filter || after bandlimiting: it with a
box filter. (b) Spatiat
domain
representation,(c) Hamming
windowing function.
(d) Windowed ramp
filter, formed as the
product of (a) and
(c). (e) Spatial
representation of the
product (note the
decrease in ringing).By splitting this integral into two expressions, one for 6 in the range 0° to
180° and the other in the range 180° to 360°, and using the fact that
G(w, 8 + 180°) = G(—w, @) (see Problem 5.32), we can express Eq. (5.11-14) as7 poo
fay f / jw|G(o, ae e564 y5i0) doy dQ (5.11-15)
Jo J-00In terms of integration with respect to w, the term x cos @ + y sin @ is aconstant, which we recognize as p from Eg, (5.11-1}. Thus, Eq. (5.11-15) can be
written as:ooTy
f(xy) = [ I/ jolt 0} a6 (5.11-16)
0 eaied JP=.x cos P+ y singThe inner expression is in the form of an inverse 1-D Fourier transform [see
Eq. (4.2-17)], with the added term |w| which, based on the discussion in
Section 4.7, we recognize as a one-dimensional filter function. Observe that ||
is a ramp filter [see Fig. 5.42(a)].’ This function is not integrable because its
amplitude extends to +o in both directions, so the inverse Fourier transform
is undefined. Theoretically, this is handled by methods such as using so-called
generalized delta functions. In practice, the approach is to window the ramp so
it becomes zero outside of a defined frequency interval. That is, a window
band-limits the ramp filter.      
 SpatialFrequency
domaindomain  Spatial
domainFrequencyFrequency
domaindomain “The ramp filter often is referred to as the Rem-Lak fitter after Ramachandran and Lakshminarayanan
[1971] who generaily are credited with having been first (0 suggest it.
5.11 # Image Reconstruction from ProjectionsThe simplest approach to band-limit a function is to use a box in the frequency domain. However, as we saw in Fig. 4.4, a box has undesirable ringing
properties, so a smooth window is used instead. Figure 5.42(a) shows a plot of
the ramp filter after it was band-limited by a box window, and Fig. 5.42(b)
shows its spatial domain representation, obtained by computing its inverse
Fourier transform. As expected, the resulting windowed filter exhibits noticeable ringing in the spatial domain. We know from Chapter 4 that filtering in
the frequency domain is equivalent to convolution in the spatial domain, so
spatial filtering with a function that exhibits ringing will produce a result corrupted by ringing also. Windowing with a smooth function helps this situation.
An M-point discrete window function used frequently for implementation
with the 1-D FFT is given by27w
+(e- am OS ws (M-1
Mw) = e+(e- 1) cosy <T sas (M ) (5.11-17)0 otherwiseWhen c = 0.54, this function is called the Hamming window (named after
Richard Hamming) and, when c = 0.5, it is called the Hann window (named
after Julius von Hann).' The key difference between the Hamming and Hann
windows is that in the latter the end points are zero. The difference between
the two generally is imperceptible in image processing applications.Figure 5.42(c) is a plot of the Hamming window, and Fig, 5.42(d)} shows the
product of this window and the band-limited ramp filter in Fig. 5.42(a). Figure
5.42(e) shows the representation of the product in the spatial domain, obtained as usual by computing the inverse FFT. It is evident by comparing this
figure and Fig. 5.42(b) that ringing was reduced in the windowed ramp (the ratios of the peak to trough in Figs. 5.42(b) and (e) are 2.5 and 3.4, respectively).
On the other hand, because the width of the central lobe in Fig. 5.42(e) is
slightly wider than in Fig. 5.42(b), we would expect backprojections based on
using a Hamming window to have less ringing but be slightly more blurred. As
Example 5.19 shows, this indeed is the case.Recail from Eq. (5.11-8) that G(w, @) is the 1-D Fourier transform of
&(p, 9), which is a single projection obtained at a fixed angle, 8. Equation
(5.11-16) states that the complete, back-projected image f(x,y) is obtained as
follows:1. Compute the 1-D Fourier transform of each projection.2, Multiply each Fourier transform by the filter function |@| which, as explained
above, has been multiplied by a suitable (e.g,, Hamming) window.3. Obtain the inverse 1-D Fourier transform of each resulting filtered
transform.4, Integrate (sum) al] the 1-D inverse transforms from step 3. *Sometimes the Hann window is referred (o as the Hanning window in analogy to the Hamming window,
However, this terminology is incorrect and is a frequent source of confusion.399
400 Chapter 5 m Image Restoration and ReconstructionEXAMPLE 5.19:
Image reconstruction using filtered
backprojections.Because a filter function is used, this image reconstruction approach ts appropriately called filtered backprojection. In practice, the data are discrete, so all
frequency domain computations are carried out using a 1-D FFT algorithm,
and filtering is implemented using the same basic procedure explained in
Chapter 4 for 2-D functions. Alternatively, we can implement filtering in the
spatial domain using convolution, as explained later in this section.The preceding discussion addresses the windowing aspects of filtered backprojections. As with any sampled data system, we also need to be concerned
about sampling rates. We know from Chapter 4 that the selection of sampling
tates has a profound influence on image processing results. In the present discussion, there are two sampling considerations. The first is the number of rays
used, which determines the number of samples in each projection. The second
is the number of rotation angle increments, which determines the number of
reconstructed images (whose sum yields the final image). Under-sampling results in aliasing which, as we saw in Chapter 4, can manifest itself as artifacts in
the image, such as streaks. We discuss CT sampling issues in more detail in
Section 5.11.6.@ The focus of this example is to show reconstruction using filtered backprojections, first with a ramp filter and then using a ramp filter modified by a
Hamming window. These filtered backprojections are compared against the
results of “raw” backprojections in Fig. 5.40. In order to focus on the difference
due only to filtering, the results in this example were generated with 0.5°
increments of rotation, which is the increment we used to generate Fig. 5.40.
The separation between rays was one pixel in both cases. The images in both
examples are of size 600 x 600 pixels, so the length of the diagonal is
V2 x 600 = 849. Consequently, 849 rays were used to provide coverage of
the entire region when the angle of rotation was 45° and 135°.Figure 5.43(a) shows the rectangle reconstructed using a ramp filter. The
most vivid feature of this result is the absence of any visually detectable blurring. As expected, however, ringing is present, visible as faint lines, especially
around the corners of the rectangle. These lines are more visible in the zoomed
section in Fig. 5.43(c). Using a Hamming window on the ramp filter helped
considerably with the ringing problem, at the expense of slight blurring, as
Figs. 5.43(b) and (d) show. The improvements (even with the ramp filter without windowing) over Fig. 5.40(a) are evident. The phantom image does not
have transitions that are as sharp and prominent as the rectangle so ringing,
even with the un-windowed ramp filter, is imperceptible in this case, as you can
see in Fig. 5.44(a). Using a Hamming window resulted in a slightly smoother
image, as Fig. 5.44(b) shows. Both of these results are considerable improvements over Fig. 5.40(b), illustrating again the significant advantage inherent in
the filtered-backprojection approach.In most applications of CT (especially in medicine), artifacts such as ringing are a serious concern, so significant effort is devoted to minimizing
them. Tuning the filtering algorithms and, as explained in Section 5.11.2,
5.17 & Smage Reconstruction from Projections 401 using a large number of detectors are among the design considerations that
help reduce these effects. iThe preceding discussion is based on obtaining liltered back projections via
an FFT implementation. However. we know from the convolution theorem in
Chapter 4 that equivaicnt results can be obtained using spatial convolution.
In particular, note that the term inside the brackets in Eq. (5.11-16) is the inverse Fourier transform of the product of two frequency domain funcuions ab
cdFIGURE 5.43
Filtered backprojections of the
rectangle using
{a)a ramp filter,
and (b)a
Hammingwindowed ramp
filter. The second
row shows
zoomed details of
the images in the
first row. Compare
with Fig. 5,.40(a).abFIGURE 5.44
Filteredhack projections
of the head
phantom using
(a) a ramp filter,
and (b) a
Hammingwindowed ramp
filter, Compare
with Fig, 3.40(b).
5.11 # Image Reconstruction from Projections 403result using the procedure described in Eqs. (2.6-10) and (2.6-11). This is the
approach followed in this section. When knowledge about what a “typical” average value should be is available, that value can be added to the filter in the
frequency domain, thus offsetting the ramp and preventing zeroing the de
term (see Fig, 4.31(c)]. When working in the spatial domain with convolution,
the very act of truncating the length of the spatial filter (inverse Fourier transform of the ramp) prevents it from having a zero average value, thus avoiding
the zeroing problem altogether.5.1}. Reconstruction Using Fan-Beam Filtered BackprojectionsThe discussion thus far has centered on parallel beams. Because of its simplicity and intuitiveness, this is the imaging geometry used traditionally to
introduce computed tomography. However, modern CT systems use a fanbeam geometry (see Fig. 5.35), the topic of discussion for the remainder of
this section.Figure 5.45 shows a basic fan-beam imaging geometry in which the detectors
are arranged on a circular arc and the angular increments of the source are assumed to be equal. Let p(a, 8) denote a fan-beam projection, where a is the
angular position of a particular detector measured with respect to the center
ray, and 8 is the angular displacement of the source, measured with respect to
the y-axis, as shown in the figure. We also note in Fig. 5.45 that a ray in the fan
beam can be represented as a line, £(p, @), in normal form, which is the approach we used to represent a ray in the parallel-beam imaging geometry discussed in the previous sections. This allows us to utilize parallel-beam results as Lip. 4)   Center ray FIGURE 5.45
Basic fan-beam
geometry. The line
passing through
the center of the
source and the
origin (assumed
here to be the
centér of rotation
of the source) is
called the center
ray.
402Chapter 5 @ Image Restoration and Reconstructionwhich, according to the convolution theorem, we know to be equal to the convolution of the spatial representations (inverse Fourier transforms) of these
two functions. In other words, letting s(@) denote the inverse Fourier transform of |w],’ we write Eq. (5.11-16) asa 00
[ | [ |w|G(@, de?" aa | a6
hy J-0 p=x cosé+y sin a[ [s(o) ® g(0, ®) px cosa+y sing 48 (5.11-18)af poo
[ L/ &(p, O)s(x cos @ + ysin @ — p) «| dé
'O —owhere, as in Chapter 4, “*” denotes convolution. The second line follows from
the first for the reasons explained in the previous paragraph. The third line follows from the actual definition of convolution given in Eq. (4.2-20).The last two lines of Eq. (5.11-18) say the same thing: Individual backprojections at an angle @ can be obtained by convolving the corresponding projection, g(p, @), and the inverse Fourier transform of the ramp filter, s(p). As
before, the complete back-projected image is obtained by integrating (summing) all the individual back-projected images. With the exception of roundoff differences in computation, the results of using convolution will be
identical to the results using the FFT. Jn practical CT implementations, convolution generally turns out to be more efficient computationally, so most modern CT systems use this approach. The Fourier transform does play a central
role in theoretical formulations and algorithm development (for example, CT
image processing in MATLAB is based on the FFT). Also, we note that there
is no need to store all the back-projected images during reconstruction. Instead, a single running sum is updated with the latest back-projected image. At
the end of the procedure, the running sum will equal] the sum total of all the
backprojections.Finally, we point out that, because the ramp filter (even when it is windowed) zeros the dc term in the frequency domain, each backprojection image
will have a zero average value (see Fig. 4.30). This means that each backprojection image will have negative and positive pixels. When alt the backprojections are added to form the final image, some negative locations may become
positive and the average value may not be zero, but typically, the final image
will still have negative pixels. .There are several ways to handle this problem. The simplest approach,
when there is no knowledge regarding what the average values should be, is to
accept the fact that negative values are inherent in the approach and scale thef(x y) "Y€ a windowing function, such as a Hamming window, is used, then the inverse Fourier transform is per
formed on the windowed ramp. Also, we again ignore the issue mentioned earlier regarding the existence of the continuous inverse Fourier transform because all implementations are carried out using
discrete quantities of finite length.
404  Chopter 5 m Image Restoration and Reconstructionthe starting point for deriving the corresponding equations for the fan-beam
geometry. We proceed to show this by deriving the fan-beam filtered backprojection based on convolution.‘We begin by noticing in Fig. 5.45 that the parameters of line L(p, 6) are related to the parameters of a fan-beam ray by@=Bta (5,11-19)
and
p= Dsing (5.11-20)where D is the distance from the center of the source to the origin of the xyplane.The convolution backprojection formula for the parallel-beam imaging geometry is given by Eq, (5.11-18). Without loss of generality, suppose that we focus attention on objects that are encompassed within a circular area of radius T about
the origin of the plane. Then g(p, 6) = 0 for |p| > T and Eq. (5.11-18) becomes2 pT
f(xy) = 5f i a(p, s(x cos@ + ysin@ — p)dpd@ —(5.11-21)
0 J-rwhere we used the fact stated in Section 5.11.1 that projections 180° apart are
mirror images of each other. In this way, the limits of the outer integral in Eq.
(5.11-21) are made to span a full circle, as required by a fan-beam arrangement
in which the detectors are arranged in a circle.We are interested in integrating with respect to a and §. To do this, we start
by changing to polar coordinates (r,¢). That is, we let x = rcos@ and
y =r sin y, from which it follows thatxcos@+ ysin@ = rcos pcos 6 + rsin g sin 6
y ? ¢ (5.11-22)=rcos(é — ¢)Using this result, we can express Eq. (5.11-21) as. 2 pT
f(xy) = sf [ 26. @)s[r cos(@ — aw) - p] dpdéThis expression is nothing more than the parallel-beam reconstruction formula written in polar coordinates. However, integration stil} is with respect to p
and @. To integrate with respect to a and £ requires a transformation of coordinates using Eqs. (5.11-19) and (5.11-20): The Fourier-slice theorem was derived for a parallel-beam geometry and is not directly applicable to fan
beams. However. Eqs. (5.11-19) and (5.11-20) provide the basis for converting a fan-beam geometry to a
parallel-beam geometry, thus allowing us to use the filtered parallel backprojection approach developed
in the previous section. for which the slice theorem is applicable, We discuss this in more detail at the
end of this section.
4.11 m Image Reconstruction from Projections 405 1 Yar-a — psin”{T/D)
f(r, ¢) -3/ [. 8(D sin a, a+ B)
-a — dsin“(-7/D) (5.11-23)s[rcos(B + a ~ y) — Dsina]D cosa dadpwhere we used dp d@ = Dcos a da df [see the explanation of Eq. (5.11-13)].This equation can be simplified further. First, note that the limits —a to
27 — a for f span the entire range of 360°. Because all functions of 8 are pe-~
riodic, with period 277, the limits of the outer integral can be replaced by 0 and
2m, respectively. The term sin“(7/D) has a maximum value, a,,, corresponding to |p| > T, beyond which g = 0 (see Fig. 5.46), so we can replace the limits of the inner integral by —a,, and @,,, respectively. Finally, consider the line
L(p, 6) in Fig. 5.45. A raysum of a fan beam along this line must equal the raysum of a parallel beam along the same line (a raysum is a sum of all values
along a line, so the result must be the same for a given ray, regardless of the coordinate system is which it is expressed). This is true of any raysum for corresponding values of (a, 8) and (p, @). Thus, letting p(@, 8) denote a fan-beam
projection, it follows that p(a, 8) = g(p, @) and, from Eqs. (5.11-19) and (5.11-20),
that p(a, 8) = g(Dsina,a + 8). Incorporating these observations into Eq.
(5.11-23) results in the expression27 ptm
f(r, 9) = sf [ p(a, B)s[r cos(B + w - ¢) ~ Dsina) (5.11.24)
. D cos a da dpFIGURE 5.46
Maximum value
of a needed to
encompass a
region of interest.
406  Chopter 5 m@ Image Restoration and ReconstructionFIGURE 5.47
Polar representation of an arbitrary point on aray of a fan beam.  This is the fundamental fan-beam reconstruction formula based on filtered
backprojections.Equation (5.11-24) can be manipulated further to put it in a more familiar convolution form. With reference to Fig. 5.47, it can be shown (Problem 5.33) thatreos(B + a ~~) — Dsina = Rsin(a' — a) (5.11-25)where R is the distance from the source to an arbitrary point in a fan ray, and
a’ is the angle between this ray and the center ray. Note that R and a’ are determined by the values of r, 9, and 8. Substituting Eq. (5.11-25) into Eq.
(5.11-24) yieldsan Eaee
fr. e) = if / pla, B)s[R sina’ - a)|DcosadadB  (5.11-26)It can be shown (Problem 5.34) that2
s(R sin a) = (x25) s(a) > (5.11-27)Using this expression, we can write Eq. (5.11-26) asFlr.) = [ : al | “gla Bala’ - «) da lag (5.11-28)a,
5.11 & Image Reconstruction from Projectionswhere
1 a 2
h(a) = (ss) s(a) (5.11-29)
and
g(a, B) = pla, B) D cos « (5.11-30)We recognize the inner integral in Eq. (5.11-28) as a convolution expression, thus showing that the image reconstruction formula in Eq. (5.11-24)
can be implemented as the convolution of functions q(e, 8) and A(@). Unlike the reconstruction formula for parallel projections, reconstruction
based on fan-beam projections involves a term 1/R?, which is a weighting
factor inversely proportional to the distance from the source. The computational details of implementing Eq. (5.11-28) are beyond the scope of the
present discussion (see Kak and Slaney (2001] for a detailed treatment of
this subject).Instead of implementing Eq. (5.11-28) directly, an approach used often, particularly in software simulations, is (1) to convert a fan-beam geometry to a
parallel-beam geometry using Eqs. (5.11-19) and (5.11-20), and (2) use the
parallel-beam reconstruction approach developed in Section 5.11.5, We conclude this section with an example of how this is done. As noted earlier, a fanbeam projection, p, taken at angle 8 has a corresponding parallel-beam
projection, g, taken at a corresponding angle @ and, therefore,P(e, B) = g(p, 9)
g(Dsina,a + B) (5.11-31)
where the second line follows from Eqs. (5.11-19) and (5.11-20).Let AB denote the angular increment between successive fan-beam
projections and Jet Aa@ be the angular increment between rays, which determines the number of samples in each projection. We impose the restriction thati]AB = hazy (5.11-32)Then, 8 = my and a = ny for some integer values of m and n, and we can
write Eq. (5.11-31) aspry, my) = g[D sin wy, (m + n)y) (5.11-33)This equation indicates that the #th ray in the mth radial projection is equal to
the nth ray in the (m + ”)th parallel projection. The D sin y term on the right
side of (5.11-33) implies that parallel projections converted from fan-beam
projections are not sampled uniformly, an issue that can lead to blurring, ringing, and aliasing artifacts if the sampling intervals Aa and AB are too coarse,
as the foilowing example illustrates,407
408 Chapter 5 @ Image Restoration and ReconstructionEXAMPLE 5.20:
Image
reconstruction
using filtered fan
backprojections.ab
edFIGURE 5.48
Reconstruction of
the rectangle
image from
filtered fan
backprojections.
(a) 1° increments
of a and B. (b} 0.5°
increments.(c) 0.25° increments, (d) 0.125°
increments.
Compare (d) with
Fig. 5.43(b).Mi Figure 5.48(a) shows the results of (1) generaling fan projections of the reetangle image with Aa = AB = 1°, (2) converting each fan ray to the corresponding parallel ray using Eq. (5.11-33). and (3) using the filtered
backprojection approach developed in Section 5.11.5 for parallel rays. Figures
5.48(b) through (d) show the results using 0.5°. 0.25°, and 0.125° increments. A
Hamming window was used in all cases. This variety of angle increments was
used to illustrate the effects of under-sampling.The result in Fig. 5.48(a) is a clear indication that 1° increments are too
coarse, as blurring and ringing are quite evident. The result in (b) is interesting,
in the sense that it compares poorly with Fig. 5.43(b), which was generated
using the same angie increment of 0.5°. In fact, as Fig. 5.48(c) shows, even with
angle increments of 0.25° the reconstruction still is not as good as in Fig.
§.43(b). We have to use angle increments on the order of 0.125° before the two
results become comparable, as Fig. 5.48(d) shows. This angle increment results
in projections with 180 x (1/0.25) = 720 samples, which is close to the 849
rays used in the parallel projections of Example 5.19. Thus, it is not unexpected that the results are close in appearance when using Aa = 0.125°.Similar results were obtained with the head phantom, except that aliasing is
much more visible as sinusoidal interference. We see in Fig, 5.49(c) that even
with Aa = AB = 0.25 significant distortion still is present, especially in the periphery of the ellipse. As with the rectangle, using increments of (.1257 finally
produced results that are comparable with the back-projected image of the head
phantom in Fig. 5.44(b). These results illustrate one of the principal reasons why
thousands of detectors have to be used in the fan-beam geometry of modern CTsystems in order to reduce aliasing artifacts. &SummaryThe restoration results in this chapter are based on the assumption that image degradation
can be modeled as a Jinear, position invariant process followed by addilive noise that is not
correlated with image values, Even when these assumptions are not entirely valid, it often is
possible to obtain useful results by using the methods developed in the preceding sections.Some of the restoration techniques derived in this chapter are based on various criteria of optimality. Use of the word “optimal” in this context refers strictly to a mathematical concept, not to optimal response of the human visual system. In fact, the
present lack of knowledge about visual perception preciudes a general formulation of
the image restoration problem that takes into account observer preferences and capabilities. In view of these Jimitations, the advantage of the concepts introduced in this
chapter is the development of fundamental approaches that have reasonably predictable behavior and are supported by a solid body of knowledge.As in Chapters 3 and 4, certain restoration tasks. such as random-noise reduction, are
carried out in the spatial domain using convolution masks. The frequency domain was
found ideal for reducing periodic noise and for modeling some iimportaut degradations.
such as blur caused by motion during image acyuesitien, W Hoquerey   
     % Summary 409ab
cdFIGURE 5.49
Reconstruction of
the head phantom
image from
filtered fan
backprojections.
{a} 1° increments
of wand B.(b) 0.5° increments.
(c) 0.25° increments. (d) 0,125°
increments,
Compare (d) with
Fig. 5.44(b).
Umbaugh [2005]. and Petrou and Bosdogianni [1999]. This last reference also presents
a nice tie between two-dimensional frequency domain filters and the corresponding
digital filters. On the design of 2-D digital filters, sec Lu and Antoniou [1992].Basic references for computed tomography are Rosenfeld and Kak {1982}, Kak and
Slaney [2001], and Prince and Links [2006]. For further rcading on the Shepp-Logan
phantom see Shepp and Logan [}974], and for additional details on the origin of the
Ram-Lak filter see Ramachandran and Lakshminarayanan [1971]. The paper by
O'Connor and Fessler [2006] is representative of current research in the signal and
image processing aspects of computed lomography.For software techniques to implement most of the material discussed in this chapter
see Gonzalez, Woods, and Eddins [2004].Problems
w5.1 > The white bars in the test pattern shown are 7 pixels wide and 210 pixels high.
The separation between bars is 17 pixels. What would this image look like after
application of
(a) A3 X 3 arithmetic mean filter?
{b) A5 x 5 arithmetic mean filter?
(c)} A9 X Sarithmetic mean filter?Note: This problem and the ones that follow it, related to filtering this image.
may seem a bit tedious. However, they are worth the effort. as they help develop
a real understanding of how these filters work. After you understand how a particular filter affects the image, your answer can be a brief verbal description oi
the result. For example, “the resulting image will consist of vertical bars 3 pixels
wide and 206 pixels high.” Be sure to describe any deformation of the bars. such
as rounded corners. You may ignore image border e/fects, in which the masks
only partially contain image pixels.
5.2 Repeat Problem 5.1 using a geometric mean filter,
¥*5.3 Repeat Problem 5.1 using a harmonic mean filter
5.4 Repeat Problem 5.1 using a contraharmonic mean filter with O » 1.5
*5.5 Repeat Problem 5.1 using a contraharmonie mean filter with @ iA
§.6 Repeat Problem S.J using a median filter
*%5.7 Repeat Problem 5.1 using a max filter.® Problems 411Tetailed solutions to the
pblems marked with a
stip can be found in the
book Web site. The site aise
contains suggested projects
based on the material in
this chapter.
410 Chapter 5 m Image Restoration and Reconstructiondomain to be a useful tool for formulating restoration filters, such as the Wiener and
constrained least-squares filters.As mentioned in Chapter 4, the frequency domain offers an intuitive, solid base for
experimentation, Once an approach (filler) has been found to perform satisfactorily
for a given application, implementation usually is carried out via the design of a digital
filter that approximates the frequency domain solution, but runs much faster in a computer or in a dedicated hardware/lirmware system, as indicated at the end of Chapter 4.Our treatment of image reconstruction from projections, though introductory, is the
foundation for the image-processing aspects of this field. As noted in Section 5.11,computed tomography (CT) is the main application area of image reconstruction from projections. Although we focused on X-ray tomography, the principles established in
Section 5.11 are applicable in other CT imaging modalities, such as SPECT (single photon emission tomography), PET (positron emission tomography), MRI (magnetic resonance imaging), and some modalities of ultrasound imaging.References and Further ReadingFor additional reading on the linear modet of degradation in Section 5.1, see Castleman
[1996] and Pratt [1991]. The book by Peebles {1993] provides an intermediate-level coverage of noise probability density functions and their properties (Section 5.2). The book by
Papoulis [1991] is more advanced and covers these concepts in more detail. References for
Section 5.3 are Umbaugh [2005], Boie and Cox [1992], Hwang and Haddad [1995], and
Wilburn (1998). See Eng and Ma (2001, 2006) regarding adaptive median filtering. The
general area of adaptive filter design is good background for the adaptive filters discussed
in Section 5.3. The book by Haykin [1996] is a good introduction to this topic. The filters in
Section 5.4 are direct extensions of the material in Chapter 4. For additional reading on
the material of Section 5.5, see Rosenfeld and Kak [1982] and Pratt [1991].The topic of estimating the degradation function (Section 5.6) is an area of considerable current interest. Some of the early techniques for estimating the degradation function are given in Andrews and Hunt [1977], Rosenfeld and Kak [1982], Bates and
McDonnell [1986], and Stark [1987}. Since the degradation function seldom is known exactly, a number of techniques have been proposed over the years, in which specific aspects of restoration are emphasized. For example, Geman and Reynolds (1992] and Hurn
and Jennison [1996} deal with issues of preserving sharp intensity transitions in an attempt
to emphasize sharpness, while Boyd and Meloche [1998] are concerned with restoring
thin objects in degraded images. Examples of techniques that dea) with image blur are
Yitzhaky et al. [1998], Harikumar and Bresler [1999], Mesarovic [2000], and Giannakis
and Heath [2000]. Restoration of sequences of images also is of considerable interest. The
book by Kokaram [1998] provides a good foundation in this area.The filtering approaches discussed in Sections 5.7 through 5.10 have been explained
in various ways over the years in numerous books and articles on image processing.
There are two major approaches underpinning the development of these filters. One is
based on a general formulation using matrix theory, as introduced by Andrews and
Hunt {1977]. This approach is elegant and general, but 2 is difficult for newcomers to
the field because it lacks intuitiveness. Approaches based directly on frequency domain
filtering (the approach we followed in this chapter) usually are easier to follow by those
who first encounter restoration, but fack the unifying mathematical rigor of the matrix
approach. Both approaches arrive at the same results, but our experience in teaching
this material] in a variety of settings indicates that students first entering this field favor
the latter approach by a significant margin. Complementary readings for our coverage
of the filtering concepts presented in Sections 5.7 through 5.10 are Castleman [1996],
414 Chapter 5 a Image Restoration and Reconstruction§.21Ww 5.225.235.245.255.26* §.27A certain X-ray imaging geometry produces a blurring degradation that can be
modeled as the convolution of the sensed image with the spatial, circularly symmetric functione+ry-e?
h(x, y) = 2 Assuming continuous variables, show that the degradation in the frequency domain is given by the expressionH(u,v) = -V2ae(u? + pje tr au +r?)(Hint: Refer to Section 4.9.4, entry 13 in Table 4,3, and Problem 4.26.)Using the transfer function in Problem 5.21, give the expression for a Geometric
mean filter, assuming that the ratio of power spectra of the noise and undegraded
signal is a constant. Hence obtain an expression for a Wiener filter.Using the transfer function in Problem 5.21, give the resulting expression for the
constrained Jeast squares filter.Assume that the mode] in Fig. 5.1 is linear and position invariant and that the
noise and image are uncorrelated. Show that the power spectrum of the output isIG, v)P = [He v)71Fs, a) PF + ING, v)?Refer to Eqs. (5.5-17) and (4.6-18).
Cannon [1974] suggested a restoration filter R(, v) satisfying the conditioniF{u, »)/* = IR, v)P71G(a, v)!?and based on the premise of forcing the power spectrum of the restored image,
|F(u, v)|?, to equal the power spectrum of the original image, | F(u, v)|?. Assume
that the image and noise are uncorrelated.x(a) Find R(u, v) in terms of |F(u, v)[?, |H(u, v)/?, and |N(u, vl. (Hint: Referto Fig, 5.1, Eq. (5.5-17), and Problem 5.24.]{b) Use your result in (a) to state a result in the form of Eq. (5.8-2).
An astronomer working with a large-scale telescope observes that her images
are a little blurry. The manufacturer tells the astronomer that the unit is operating within specifications, The telescope lenses focus images onto a high-resolution, CCD imaging array, and the images are then converted by the telescope
electronics into digital images. Trying to improve the situation by conducting
controlled lab experiments with the lenses and imaging sensors is not possible
due to the size and weight of the telescope components. The astronomer, having
heard about your success as an image processing expert, calls you to help her
formulate a digital image processing solution fonsharpening the images a little
more. How would you go about solving this problem, given that the only images
you can obtain are images of stellar bodies?
A professor of archeology doing research on currency exchange practices during the Roman Empire recently became aware that four Roman coins crucial to
his research are listed in the holdings of the British Museum in London. Unfortunately, he was told after arriving there that the coins recently had been stolen.
Further research on his part revealed that the museum keeps photographs ot
5.1§
«5.165.17w 5.185.19© $.20[Hint: Use the continuous version of the Fourter transform in Eg, (4,5-7), and
express the cosine in terms of exponentials. }Start with Eq. (5.4-11) and derive Eq. (5.4-13).
Consider a linear, position-invariant image degradation system with impulse
responseAxe - ayy ~ B) = Ke ertiesSuppose that the inputJo the system is an image consisting of a line of infinitesimal width located at x = a, y = 6 and modeled by f(x, y} = 6(s — a. y — 6).
where 6 is an impulse, Assuming no noise, what is the output image g(x,y)?
During acquisition, an image undergoes uniform linear motion in the vertical direction for a time 7;. The direction of motion then switches ta the horizontal direction for a time interval 7>, Assuming that the lime it takes the image lo
change directions is negligible, and that shutter opening and closing times are
negligible also, give an expression for the blurring function, // (zz, v).Consider the problem of image blurring caused by uniform acceleration in the
x-direction. If the image is at rest at time ¢ = 0 and accelerates with a uniform
acceleration xo(t) = af? for a time T, find the blurring function H(u, v), You
may assume that shutter opening and closing limes are negligible.A space probe is designed to transmit images from a planet as it approaches it for
landing. During the last stages of landing, one of the control thrusters fails, resulting in rapid rotation of the craft about its vertical axis. The images sent during the
last two seconds prior to landing are blurred as 4 consequence of this circular motion. The camera is located in the bottom of the probe, along its vertical axis, and
pointing down. Fortunately, the rotation of the craft is alse about its vertical axis,
so the images are blurred by uniform rotational motion. During the acquisition
time of each image the craft rotation was limited to 7/12 radians. The image acquisition process can be modeled as an ideal shutter that is open only during the
time the craft rotated the 7/12 radians. You may assume that vertical motion was
negligible during image acquisition. Formulate a solution for restoring the images.
The image shown is a blurred, 2-D projection of a volumetric rendition of a
heart, Jt is known that each of the cross hairs on the right bottom part of the
image was 4 pixels wide, 20 pixels long, and had an intensity value of 255 before
blurring. Provide a step-by-step procedure indicating how you weuld use the information just given to obtain the blurring function H (u,v) (Original image courtesy of GE. Medical Systeis.}# Problems413
412 Chapter w Image Restoration and Reconstruction58
*59
5.10$11w 5412§.13* 5.14Repeat Problem 5.1 using a Alpha-trimmed mean filter,Repeat Problem 5.1 using a midpoint filter,The two subimages shown were extracted from the top right corners of Figs.
5.7(c)} and (d), respectively. Thus, the subimage on the left is the result of using
an arithmetic mean filter of size 3 x 3: the other subimage is the result of using
a geometric mean filter of the same size.(a) Explain why the subimage obtained with geometric mean filtering is less
blurred. (Hint: Start your analysis by examining a 1-D step transition in
intensity.)(b) Explain why the black components in the right image are thicker, Refer to the contraharmonic filter given in Eq. (5.3-6).(a) Explain why the filter is effective in elimination pepper noise when Q is
positive. .(b) Explain why the filler is effective in eliminating salt noise when Q is
negative,(c) Explain why the filter gives poor results (such as the resulis shown in Fig.
5.9) when the wrong polarity is chosen for Q.(d) Discuss the bebavior of the filter when Q = -1.5.(e) Discuss (for positive and negative Q) the behavior of the filter in areas of
constant intensity levels.Obtain equations for the Gaussian and Butterworth bandpass {ters corresponding to the bandreject filters in Table 4.6.Obtain equations for Gaussian, Butterworth, and ideal notch rejeet filters inthe form of Eq. (4.10-5).Show that the Fourier transform of the 2-D contihuous cosine functionf(x. ¢) = A cas(uyx + tyy)is the pair of conjugate impulsesAl by fo ty
F(u.v) = “Z Ee se r | - au i fs !
5.15
5.165.17*5.185.19*$.20[Hine Use the continuous version of the Fauricr transform in Eq. (4.5-7), and
express the cosine in terms of cxponcatials.]Start with Eq, (5.4-11) and derive Eq, (5.4-13).Consider a linear, position-invariant image degradation system with impulse
responseh(x -~a.y~ B) =e Kara fe gy]Suppose that the inputto the system is an image consisting of a line of infinttesimal width located at x = a, y = b and modeled by f(x, v) = (x ~ a, y ~ BY),
where & is an impulse. Assuming no noise. what is the output image g(x, ¥)?
During acquisition, an image undergoes uniform linear motion in the vertical direction for a time 7). The direction of motion then switches to the horizontal direction for a time interval 72. Assuming that the time it takes the image to
change directions is negligible, and that shutter opening and closing times are
negligible also, give an expression for the blurring function, H(i, v),Consider the problem of image blurring caused by uniform acceleration in the
x-direction. If the image is at rest at time ¢ = 0 and accelerates with a uniform
acceleration xp(t) = at? for a time T, find the blurring function H (a, 2). You
may assume that shutter opening and closing times are negligible,A space probe is designed to transmit images from a planet as it approaches it for
landing. During the last stages of landing. one of the control thrusters fails, resuiting in rapid rotation of the craft about its vertical axis. The images sent during the
last two seconds prior to landing are blurred as a consequence of {his circular motion. The camera is focated in the bottom of the probe, along its vertical axis. and
pointing down. Fortunately, the roiation of the craft is also about its vertical axis.
so the images are blurred by uniform rotational motion. During the acquisition
time of each image the craft rotation was limited to z/'12 radians. The image acquisition process can be modeled as an ideal shutter that is open only during the
time the craft rotated the 77/12 radians. You may assume that verlical motion was
negligible during image acquisition, Formulate. a solution for restoring the images.
The image shown is a blurred, 2-D projection of a volumetric rendition of a
heart. It is known that each of the cross hairs on the right bottom part of the
image was 4 pixels wide, 20 pixels long. and gad an intensity valuc of 255 before
blurring, Provide a step-by-step procedure iridicating how you would use the information just given to obtain the blurring function H(u, #).  (Original image courtesy of C.F. Medicat Systems.® Problems413
414 Chapter 5 m Image Restoration and Reconstruction5.21* 5.225.23$.24$.25§.26*§,27A certain X-ray imaging geometry produces a blurring degradation that can be
modeled as the convolution of the sensed image with the spatial, circularly symmetric function 2 2
rt+y~-o
h(x, y) = ~~~
oe
Assuming continuous variables, show that the degradation in the frequency domain is given by the expressionAlu, v) = -Viro(u? + wre emer ter)(Hint: Refer to Section 4.9.4, entry 13 in Table 4.3, and Problem 4.26.)Using the transfer function in Problem 5.21, give the expression for a Geometric
mean filter, assuming that the ratio of power spectra of the noise and undegraded
signal is a constant. Hence obtain an expression for a Wiener filter.Using the transfer function in Problem 5.21, give the resulting expression for the
constrained least squares filter.Assume that the model in Fig. 5.1 is linear and position invariant and that the
noise and image are uncorrelated. Show that the power spectrum of the output isIG, a)? = LA (a, vo) PIF, vo)? + NG, »)/?Refer to Eqs. (5.5-17) and (4.6-18).
Cannon {1974} suggested a restoration filter R(u, v) satisfying the condition[Fu v7 = [RO vl7GEs, wl?and based on the premise of forcing the power spectrum of the restored image,
| F(z, v)|?, to equal the power spectrum of the original image, | F(, v)|’, Assume
that the image and noise aré uncorrelated.(a) Find R(u, v) in terms of |F(u, v2, | (ez, 0) 2, and |N(u, e) 27. [Hint Referto Fig. 5.1, Eq. (5.5-17), and Problem 5,24.](b) Use your result in (a) to state a result in the form of Eq. (5.8-2).
An astronomer working with a large-scale telescope observes that her images
are a little blurry, The manufacturer tells the astronomer that the unit is operating within specifications. The telescope lenses focus images onto a high-resolution, CCD imaging array, and the images are then converted by the telescope
electronics into digital images. Trying to improve the situation by conducting
controjled lab experiments with the lenses and imaging sensors is not possible
due to the size and weight of the telescope components. The astronomer, having
heard about your success as an image processing expert, calls you to heip her
formulate a digital image processing solution for sharpening the images a little
more. How would you go about solving this problem, given that the only images
you can obtain are images of stellar bodies?
A professor of archeology doing research on currency exchange practices during the Roman Empire recently became aware that four Roman coins crucial to
his research are listed in the holdings of the British Museum in London. Unfortunately, he was told after arriving there that the coins recently had been stolen.
Further research on his part revealed that the museum keeps photographs of
every item for which if is responsible. Unfortunately. the photos of the coins in
question are blurred to the poimt where the date and other smal] markings are
not readable. The cause of the blurring was the camera being out of focus when
the pictures were taken. As an iniage processing expert and friend of the professor, you are asked as a favor to determine whethes computer processing can be
utilized to restore the images to the point where the professor can read the
markings. You are told that the original camera used to take the photos is still
available, as are other representative coins of the same era. Propose a step-bystep solution to this problem.5.28 Sketch the Radon transform of the following square images. Label quantitatively
all the important features of your sketches. Figure (a) consists of one dot in the
center, and (b) has two dots along the diagonal. Describe your solution to (c) by
an intensity profile, Assume a paraliel-beam geometry. (a) * (b) {c)$.29 Show that the Radon transform |Eg. (3.11-3)] of the Gaussian shapeory) — 2
fy) = Aew(— 5) is g(p. 0) = AV2ac.exp(—p*). (Him: Refer to
Example 5.17, where we used symmetry 10 simplify integration.)§.30 * (a) Show that the Radon transform [Eq. (5.11-3)] of the unit impulse &(x, y) isa
straight vertical line in the g@-plane passing through the origin.
(b) Show that the radon transform of the impulse S(x -- x.y — yp) is a simuscidal curve in the pé@-plane. ar
5.31 Prove the validity of the following properties of the Radon transform [Eq. (5.11-3)}:*(a)} Linearity: The Radon transform is a linear operator. (See Section 2.6.2
regarding the definition of linear operators.)(b) Translation property: The radon transform of f(x -> xq, y o> yy) is
8(p — XyCOSy ~ Fy SiN, 4).*(¢) Convolution property: Show that the Radon transform of the convolution
of two functions is equal to the convolution of the Radon transtorms of the
two functions.5.32 Provide the steps leading from Eq. ($.11-14) te (5.11.44). You will need to use
the property G(w, 6 + 180°) ~ Gl w, 0}
533 Prove the validity of Eq. (6.11-25}5.34 Prove the validity of Eq. (S.t1-27)& Problems415
416   tis only alter years of preparation that the young artist should touch
°“color—not color used descriptively, that is, but as a means of
personal expression. Henri MotisseFor a long time | limited myself to one color—as a form of discipline.
Pablo PicassoPreviewThe use of color in image processing is motivated by two principal factors.
First, color is a powerful descriptor that often simplifies object identification
and extraction from a scene. Second, humans can discern thousands of color
shades and intensities, compared to about only two dozen shades of gray. This
second factor is particularly important in manual {i.e., when performed by humans) image analysis.Color image processing is divided into two major areas: full-color and
pseudocolor processing. In the first category, the images in question typically
are acquired with a full-color sensor, such as a color TV camera or color scanner. [In the second category, the problem is one of assigning a color to a particular monochrome intensity or range of intensities. Until relatively recently,
most digital color image processing was done at the pseudocolor level. However, in the past decade, color sensors and hardware for processing color images have become available at reasonable prices. The result is that full-color
image processiag techniques are now used in a broad range of applications, including publishing. visualization, and the Internet.Tt will become evident in the discussions that follow that some of the gray-scale
methods covered m previous chapters are directly applicable to color images.
6.1 m@ Color Fundamentals 417Others require reformulation to be consistent with the properties of the color
spaces developed in this chapter. The techniques described here are far from exhaustive; they illustrate the range of methods available for color image processing.ra Color FundamentalsAlthough the process foliowed by the human brain in perceiving and interpreting color is a physiopsychological phenomenon that is not fully understood, the physical nature of color can be expressed on a formal basis
supported by experimental and theoretical results.In 1666, Sir Isaac Newton discovered that when a beam of sunlight passes
through a glass prism, the emerging beam of light is not white but consists instead of a continuous spectrum of colors ranging from violet at one end to red
at the other. As Fig. 6.1 shows, the color spectrum may be divided into six
broad regions: violet, blue, green, yellow, orange, and red. When viewed in full
color (Fig. 6.2), no color in the spectrum ends abruptly, but rather each color
blends smoothly into the next.Basically, the colors that humans and some other animals perceive in an object
are determined by the nature of the light reflected from the object. As illustrated
in Fig, 6.2, visible light is composed of a relatively narrow band of frequencies in
the electromagnetic spectrum. A body that reflects light that is balanced in alll visible wavelengths appears white to the observer. However, a body that favors reflectance in a limited range of the visible spectrum exhibits some shades of color.
For example, green objects reflect light with wavelengths primarily in the 500 to
570 nm range while absorbing most of the energy at other wavelengths.FIGURE 6.1 Color
spectrum seen by
passing white
light through a
prism. (Courtesy
of the General
Electric Co.,
Lamp Business
Division.)  
 JO ION Too hWTRAMOLET a vistane sracTRUM 
 1900 1500WAVELENGTH (ManemeteresFIGURE 6.2 Wavelengths comprising the visible range of the electromagnetic spectrum.
(Courtesy of the General Electric Co., Lamp Business Division.)
6.3 # Color Fundamentals 419buman eye, colors are seen as variable combinations of the so-called primary
colors red (R), green (G), and blue (B), For the purpose of standardization, the
CIE (Commission Internationale de }’Eclairage—the International Commission on Illumination) designated in 1931 the following specific wavelength values to the three primary colors: blue = 435.8nm, green = 546,] nm, and
red = 700 nm. This standard was set before the detailed experimental curves
shown in Fig. 6.3 became available in 1965. Thus, the CIE standards correspond
only approximately with experimental data. We note from Figs. 6.2 and 6.3 that
no single color may be called red, green, or blue. Also, it is important to keep in
mind that having three specific primary color wavelengths for the purpose of
standardization does not mean that these three fixed RGB components acting
alone can generate al] spectrum colors. Use of the word primary has been widely
misinterpreted to mean that the three standard primaries, when mixed in various intensity proportions, can produce aff visible colors. As you will see shortly,
this interpretation is not correct unless the wavelength also is allowed to vary,
in which case we would no longer have three fixed, standard primary colors.The primary colors can be added to produce the secondary colors of light—
magenta (red plus blue), cyan (green plus blue), and yellow {red plus green).
Mixing the three primaries, or a secondary with its opposite primary color, in
the right intensities produces white light. This result is shown in Fig. 6.4(a),
which also illustrates the three primary colors and their combinations to produce thé secondary colors.JHN EC RES OP ELGITEprinnuiestVISTMESIEY USD SLANG SEN LOD IES
OV IGAEL AND VICAIE NE a
bFIGURE 6.4
Primary and
secondary colors
of light and
pigments.
(Courtesy of the
General Electric
Co., Lamp
Business
Division.)
418 — Chopter 6 m@ Color Image Processing,FIGURE 6.3
Absorption of
light by the red,
green, and blue
cones in the
human eye as a
function of
wavelength.Characterization of light is central to the science of color. If the light is
achromatic (void of color), its only attribute is its intensity, or amount. Achromatic light is what viewers see on a black and white television set, and it has
been an implicit component of our discussion of image processing thus far. As
defined in Chapter 2, and used numerous times since, the term gray level
tefers to a scalar measure of intensity that ranges from black, to grays, and finally to white.Chromatic light spans the electromagnetic spectrum from approximately
400 to 700 nm. Three basic quantities are used to describe the quality of a
chromatic light source: radiance, luminance, and brightness. Radiance is the
total amount of energy that flows from the light source, and it is usually measured in watts (W). Luminance, measured in lumens (Im), gives a measure of
the amount of energy an observer perceives from a light source. For example,
light emitted from a source operating in the far infrared region of the spectrum could have significant energy (radiance), but an observer would hardly
perceive it; its luminance would be almost zero. Finally, brighiness is a subjective descriptor that is practically impossible to measure. It embodies the
achromatic notion of intensity and is one of the key factors in describing
color sensation.As noted in Section 2.1.1, cones are the sensors in the eye responsible for
color vision. Detailed experimental evidence has established that the 6 to 7 million cones in the human eye can be divided into three principal sensing categories, corresponding roughly to red, green, and blue. Approximately 65% of all
cones are sensitive to red light, 33% are sensitive to green light, and only about
2% are sensitive to blue (but the blue cones are the most sensitive). Figure 6.3
shows average experimental curves detailing the absorption of light by the red,
green, and blue cones in the eye. Due to these absorption characteristics of the445 am 535nm = 575mAbsorption (arbitrary units)   400 450 500 550 600 650 700 nm
2 y 2 S 5 z= % v Bod
<4 2 a eo o cry op OD ao
fs 2 #f 8 gf  #
= Z 2 ° = ~o °
ed 4 2 2 4
‘3 5 « z 8
a * 3 3
xy m4
420 Chepter'6. & Color Image ProcessingDifferentiating between the primary colors of light and the primary colors
of pigments or colorants is important. In the latter, a primary color is defined
as one that subtracts or absorbs a primary color of light and reflects or transmits the other two. Therefore, the primary colors of pigments are magenta,
cyan, and yellow, and the secondary colors are red, green, and blue. These colors are shown in Fig. 6.4(b). A proper combination of the three pigment primnaries, or a secondary with its opposite primary, produces black.Color television reception is an example of the additive nature of light colors. The interior of CRT (cathode ray tube) color TV screens is composed of a
large array of triangular dot patterns of electron-sensitive phosphor. When excited, each dot in a triad produces light in one of the primary colors. The intensity of the red-emitting phosphor dots is modulated by an electron gun inside
the tube, which generates pulses corresponding to the “red energy” seen by
the TV camera. The green and blue phosphor dots in each triad are modulated
in the same manner. The effect, viewed on the television receiver, is that the
three primary colors from each phosphor triad are “added” together and received by the color-sensitive cones in the eye as a full-color image. Thirty successive image changes per second in all three colors complete the illusion of a
continuous image display on the screen.CRT displays are being replaced by “flat panel” digital technologies, such as
liguid crystal displays (LCDs) and plasma devices. Although they are fundamentally different from CRTs, these and similar technologies use the same
principle in the sense that they all require three subpixels (red, green, and
blue) to generate a single color pixel. LCDs use properties of polarized light to
block or pass light through the LCD screen and, in the case of active matrix
display technology, thin film transistors (TFTs) are used to provide the proper
signals to address each pixel on the screen. Light filters are used to produce
the three primary colors of light at each pixel triad location. In plasma units,
pixels are tiny gas cells coated with phosphor to produce one of the three primary colors. The individual cells are addressed in a manner analogous to
LCDs. This individual! pixel triad coordinate addressing capability is the foundation of digital displays.The characteristics generally used to distinguish one color from another are
brightness, hue, and saturation. As indicated earlier in this section, brightness
embodies the achromatic notion of intensity. Hue is an attribute associated
with the dominant wavelength in a mixture of light waves. Hue represents
dominant color as perceived by an observer. Thus, when we call an object red,
orange, or yellow, we are referring to its hue. Saturation refers to the relative
purity or the amount of white light mixed with a hue. The pure spectrum colors
are fully saturated. Colors such as pink (red and white) and javender (violet
and white) are less saturated, with the degree of saturation being inversely
proportional to the amount of white light added.Hue and saturation taken together are called chromaticity, and, therefore, a
color may be characterized by its brightness and chromaticity. The amounts of
red, green, and blue needed to form any particular color are called the
6.1 m Color Fundamentals 421tristimulus values and are denoted, X, Y, and Z, respectively. A color is then
specified by its trichromatic coefficients, defined asx
*"XSY4Z ery)
_ Y
Y"X4¥42 (19)
and .
Zz“XYZ
It is noted from these equations that*
xtyt+ze=l (6.1-4)z (6.1-3)For any wavelength of light in the visible spectrum, the tristimulus values
needed to produce the color corresponding to that wavelength can be obtained directly from curves or tables that have been compiled from extensive
experimental results (Poynton [1996]. See also the early references by Walsh
[1958] and by Kiver [1965]).Another approach for specifying colors is to'use the CIE chromaticity diagram (Fig. 6.5), which shows color composition as a function of x (red) and y
(green). For any value of x and y, the corresponding value of z (blue) is obtained from Eq. (6.1-4) by noting that z = 1 ~ (x + y). The point marked
green in Fig. 6.5, for example, has approximately 62% green and 25% red content. From Eq. (6.1-4), the composition of blue is approximately 13%.The positions of the various spectrum colors— from violet at 380 nm to red
at 780 nm—are indicated around the boundary of the tongue-shaped chromaticity diagram. These are the pure colors shown in the spectrum of Fig. 6.2.
Any point not actually on the boundary but within the diagram represents
some mixture of spectrum colors. The point of equal energy shown in Fig. 6.5
corresponds to equal fractions of the three primary colors; it represents the
CIE standard for white light. Any point located on the boundary of the chromaticity chart is fully saturated. As a point leaves the boundary and approaches the point of equa] energy, more white light is added to the color and it
becomes less saturated. The saturation at the point of equal energy is zero.The chromaticity diagram is useful for color mixing because a straight-line
segment joining any two points in the diagram defines al! the different color
variations that can be obtained by combining these two colors additively. Consider, for example, a straight line drawn from the red to the green points shown
in Fig. 6.5. If there is more red light than green light, the exact point representing the new color will be on the line segment, but it will be closer to the red
point than to the green point. Similarly, a line drawn from the point of equal ‘The use of x, y, z in this context follows notational convention. These should not be confused with the
use of (x, y) to denole spatial coordinates in other sections of the book.
422 Cuapter 6 @ Color Image ProcessingFIGURE 6.5
ceomateity Sf CHROMATICITY D
(Courtesy of the
General Electric
Co., Lamp
Business
Division.) energy to any point on the boundary of the chart will define all the shades of
that particular spectrum color.
Extension of this procedure to three colors is straightforward. lo determine
the range of colors that can be obtained from any three given colors in the
chromaticity diagram, we simply draw connecting Jines to each of the three
color points. The result is a Wviangle, and any colog on the boundary or inside
the triangle can be produced by various combinations of the three initial colors, A triangle with verlices af any three fixed colors cannot enclose the entire
color region in Fig. 6.5. This observation supports graphically the remark nade
garlier that not ail colors can be obtained with Unvee single. tived pranarics.
The triangle in Figure 6.6 show feolors |      sical the colar    a UYpMCat rage +
6.2 w Color Models 423 y-axis  X-axis™~“ :
devices. The boundary of the color printing gamut is irregular because color
printing is a combination of additive and subtractive color mixing, a process
that is much more difficult to contro! than that of displaying colors on a
monitor, which is based on the addition of three highly controllable light
primaries.| 6.2 | Color ModelsThe purpose of a color model (also called color Space or color system) is to facilitate the specification of colors in some standard, generally accepted way. In
essence, a color model is a specification of a coordinate system and a subspace
within that system where each color is represented by a single point.Most color models in use today are oriented either toward hardware (such
as for color monitors and printers) or toward applications where color manipulation is a goal (such as in the creation of color graphics for animation). InFIGURE 6.6
Typical color
gamut of color
monitors
(triangle) and
color printing
devices (irregular
region).
424  Cepter& m Color Image ProcessingFIGURE 6.7
Schematic of the
RGB color cube.
Points along the
main diagonal
have gray values,
from black at the
origin to white at
point (1, 1,1).terms of digital image processing, the hardware-oriented models most commonly used in practice are the RGB (red, green, blue) model for color monitors and a broad class of color video cameras; the CMY (cyan, magenta,
yellow) and CMYK (cyan, magenta, yellow, black) models for color printing;
and the HSI (hue, saturation, intensity) model, which corresponds closely with
the way humans describe and interpret color. The HSI model also has the advantage that it decouples the color and gray-scale information in an image,
making it suitable for many of the gray-scale techniques developed in this
book. There are numerous color models in use today due to the fact that color
science is a broad field that encompasses many areas of application. It is
tempting to dwell on some of these models here simply because they are interesting and informative. However, keeping to the task at hand, the models discussed in this chapter are leading models for image processing. Having
mastered the material in this chapter, you will have no difficulty in understanding additional color models in use today.6.2.) The RGB Color ModelIn the RGB model, each color appears in its primary spectral components of
red, green, and blue. This model is based on a Cartesian coordinate system.
The color subspace of interest is the cube shown in Fig. 6.7,in which RGB primary values are at three corners; the secondary colors cyan, magenta, and yellow are at three other corners; black is at the origin; and white is at the corner
farthest from the origin. In this model, the gray scale (points of equal RGB
values) extends from black to white along the line joining these two points.
The different colors in this model are points on or inside the cube, and are defined by vectors extending from the origin. For convenience, the assumption
is that all color values have been normalized so that the cube shown in Fig. 6.7
is the unit cube. That is, all values of R, G, and B are assumed to be in therange [0, 1].Magenta{0, 2,0)
Green Red Yellow
426 Chapter 6 & Color Image Processing‘*
5FIGURE 6.9(a) Generating
the RGB image of
the cross-sectional
color plane (127,
G, B). (b} The
three hidden
surface planes in
the color cube of
Fig. 6.8,Color Ro 8
monitor [a    (R=) (G =0) (8 =0)While high-end display cards and monitors provide a reasonable rendition
of the colors in a 24-bit RGB image, many systems in use today are limited to
256 colors. Also, there are numerous applications in which it simply makes no
sense to use more than a few hundred, and sometimes fewer, colors. A good
example of this is provided by the pseudocotor image processing techniques
discussed in Section 6.3. Given the variety of systems in current use, it is of
considerable interest to have a subset of colors thal are likely to be reproduced faithfully, reasonably independently of viewer hardware capabilities.
This subset of colors is called the set of sefe RGB colors, or the set of allsystems-safe colors. In Internet applications, they are called safe Web colors or
safe browser colors.On the assumption that 256 colors is the minirgum number of colors that
can be reproduced faithfully by any sysiem in which a desired result is likely to
be displayed, it is useful to have an accepted standard notation to refer to
these colors. Forty of these 256 colors are known to be processed differently by
various operating systems, leaving aly 216 colors that are conimon to most
systems. These 216 colors have become the de factu standard for safe colors,
especially in Internet applications. They are used whenever it is desired that
the colors viewed by most people appear the same.
6.2 @ Color Modeis 427   ee rs a TABLE 6.1
Coler E lents
Number System _____ Color Equivalents” ee Valid vatues of
Hex 00 33 66 99 ce FF each RGB
Decimal 0 St 102 153 204 258 component in a safe color,  Each of the 216 safe colors is formed from three RGB values as before, but
each value can only be 0, St, £02, 153, 204, or 255. Thus, RGB triplets of these
values give us (6)* = 216 possible values (note that al] values are divisible by
3). It is customary to express these values in the hexagonal number system, as
shown in Table 6.1. Recall that hex numbers 0, 1,2,....9, A, B,C, D, E, F
correspond to decimal numbers 0,1, 2,..., 9,10, 11, 12, 13,14, 15. Recall
also that (0),, = (0006), and (F)j, = (1111). Thus, for example,
(FF)i6 = (255)19 = (11111111). and we sce that a grouping of two hex numbers forms an 8-bit byte.Since it takes three numbers to form an RGB color, each safe color is
formed from three of the two digit hex numbers in Table 6.1. For example, the
purest red is FF0000. The vaiues 000000 and FFFFFF represent black and
white, respectively. Keep in mind that the same result is obtained by using the
more familiar decimal notation. For instance, the brightest red in decimal notation has R = 255 (FF) andG = B = 0,Figure 6.10(a) shows the 216 safe colors, organized in descending RGB values. The square in the top lelt array has value FFFFFF (white), the second
square to its right has value FFFFCC, the third square has value FFFF99, anda
bFIGURE 6.10
{a) The 216 sate
RGB colors.
(b) All the grays
in the 256-color
RGB system
(grays that are
part of the safe
eolor group are
shown
underlined). tay k wy
now << 2 oa B ke
8S a3 2 8F 6B tm Gewe
agqstine« 2&8 FF cm Ge GE
a ase 8 FE as a2 oa
8 8 3 4 BS © 8 A tm OF Ag ot
6.2 m Color Models 425 Images represented in the RGB color model consist of three component
images, one for each primary color. When fed into an RGB monitor, these
three images combine on the screen to produce a composite color image, as
explained in Section 6.1. The number of bits used to represent each pixel in
RGB space is called the pixel depth. Consider an RGB image in which each of
the red, green, and blue images is an 8-bil image. Under these conditions each
RGB color pixel [that is, a triplet of values (R, G, B)] is said to have a depth of
24 bits (3 image planes times the number of bits per plane). The term full-color
image is used often to denote a 24-bit RGB color image. The total number of
colors in a 24-bit RGB image is (2°)? = 16,777,216. Figure 6.8 shows the 24-bit
RGB color cube corresponding to the diagram in Fig. 6.7.W@ The cube shown in Fig. 6.8 is a solid, composed of the (28)? = 16,777,216
colors mentioned in the preceding paragraph. A convenient way to view these
colors is to generate color planes (faces or cross sections of the cube). This is
accomplished simply by fixing one of the three colors and allowing the other
two to vary. For instance, a cross-sectional plane through the center of the cube
and parallel to the GB-plane in Fig. 6.8 is the plane (127, G, B) for
G, B = 0,1,2,...,255. Here we used the actual pixel values rather than the
mathematically convenient normalized values.in the range [0, 1] because the
former values are the ones actually used in a computer to generate colors,
Figure 6.9(a) shows that an image of the cross-sectional plane is viewed simply
by feeding the three individual component images into a color monitor. In the
component images, 0 represents black and 255 represents white (note that
these are gray-scale images). Finally, Fig. 6.9(b) shows the three hidden surface
planes of the cube in Fig. 6.8, generated in the same manner.It is of interest to note that acquiring a color image is basically the process
shown in Fig. 6.9 in reverse. A color image can be acquired by using three filters, sensitive to red, green, and blue, respectively. When we view a color scene
with a monochrome camera equipped with one of these filters, the result is a
monochrome image whose intensity is proportional to the response of that filter. Repeating this process with each filter produces three monochrome images that are the RGB component images of the color scene. (In practice,
RGB color image sensors usually integrate this process inte a single device.}
Clearly, displaying these three RGB component images in the form shown in
Fig. 6.9(a) would yield an RGB color rendition of the original color scene. iFIGURE 6.8 RGB
24-bit color cube.EXAMPLE 6.1:
Generating the
hidden face
Planes and a cross
section of the
RGB color cube.
428 Gupter 6 m Color Image ProcessingFIGURE 6.11
The RGB safecolor cube,so on for the first row. The second row of that same array has values FFCCFF,
FFCCCC, FFCC99, and so on. The final square of that array has value FFO000
{the brightest possible red). The second array to the right of the one just examined starts with value CCFFFF and proceeds in the same manner, as do the
other remaining four arrays. The final (bottom right) square of the last array
has value 000000 (black). It is important to note that not all possible 8-bit gray
colors are included in the 216 safe colors. Figure 6.10(b) shows the hex codes
for ail the possible gray colors in a 256-color RGB system. Some of these values are outside of the safe color set but are represented properly (in terms of
their relative intensities) by most display systems. The grays from the safe
color group, (KK KKKK)j, for K = 0, 3,6, 9, C, KE are shown underlined in
Fig. 6.10(b).Figure 6.11 shows the RGB safe-color cube, Unlike the full-color cube in
Fig. 6.8, which is solid, the cube in Fig. 6.11 has valid colors only on the surface planes. As shown in Fig. 6.10(a), each plane has a total of 36 colors, so
the entire surface of the safe-color cube is covered by 216 different colors, as
expected.6.2.2 The CMY and CMYK Color ModelsAs indicated in Section 6.1, cyan, magenta, and yellow are the secondary colors
of light or, alternatively, the primary colors of pigments. For example, when a
surface coated with cyan pigment is illuminated with white light, no red light is
reflected from the surface. That is, cyan subtracts red light from reflected white
light, which itself is composed of equal amounts of red, green, and blue light.Most devices that deposit colored pigments on paper, such as color printers
and copiers, require CMY data input or perform an RGB to CMY conversion
internally. This conversion is performed using the simple operationCc 1 R
Mj=|1]|-|G (6.2-1)
Y 1 Bwhere, again, the assumption is that alf cofor values have been normalized to
the range (0, 1]. Equation (6.2-1) demonstrates that light reflected from a
6.2 w Color Modelssurface coated with pure cyan does not contain red (that is,C = 1 — Rin the
equation). Similarly, pure magenta does not reflect green, and pure yellow
does not reflect blue. Equation (6.2-1) also reveals that RGB values can be
obtained easily from a set of CMY values by subtracting the individual CMY
values from 1. As indicated earlier, in image processing this color model is
used in connection with generating hardcopy output, so the inverse operation from CMY to RGB generally is of little practical interest.According to Fig. 6.4, equal amounts of the pigment primaries, cyan, magenta, and yellow should produce black. In practice, combining these colors
for printing produces a muddy-looking black. So, in order to produce true
black (which is the predominant color in printing), a fourth color, black, is
added, giving rise to the CMYK color model. Thus, when publishers talk about
“four-color printing,” they are referring to the three colors of the CMY color
model plus black.6.2.3 The HSI Color ModelAs we have seen, creating colors in the RGB and CMY models and changing
from one model to the other is a straightforward process. As noted earlier,
these color systems are ideally suited for hardware implementations. In addition, the RGB system matches nicely with the fact that the human eye is
strongly perceptive to red, green, and blue primaries. Unfortunately, the
RGB, CMY, and other similar color models are not well suited for describing
colors in terms that are practical for human interpretation. For example, one
does not refer to the color of an automobile by giving the percentage of each
of the primaries composing its color. Furthermore, we do not think of color
images as being composed of three primary images that combine to form that
single image.When humans view a color object, we describe it by its hue, saturation, and
brightness. Recall from the discussion in Section 6.1 that hue is a color attribute that describes a pure color (pure yellow, orange, or red), whereas saturation gives a measure of the degree to whicha pure color is diluted by white
light. Brightness is a subjective descriptor that is practically impossible to measure. It embodies the achromatic notion of intensity and is one of the key factors in describing color sensation. We do know that intensity (gray level) is a
most useful descriptor of monochromatic images. This quantity definitely is
measurable and easily interpretable. The model we are about to present, called
the HSI (hue, saturation, intensity) color model, decouples the intensity component from the color-carrying information (hue and saturation) in a color
image. As a result, the HSI model is an ideal tool for developing image processing algorithms based on color descriptions that are natural and intuitive to
humans, who, after all, are the developers and users of these algorithms. We
can summarize by saying that RGB is ideal for image color generation (as in
image capture by a color camera or image display in a monitor screen), but its
use for color description is much more limited. The material that follows provides an effective way to do this.429
430  Ghopter 6 m Color Image ProcessingabFIGURE 6.12
Conceptual
teJationships
between the RGB
and HSI color
models.As discussed in Example 6.1, an RGB color image can be viewed as three
monochrome intensity images (representing red, green, and blue), so it should
come as no surprise that we should be able to extract intensity from an RGB
image. This becomes rather clear if we take the color cube from Fig. 6.7 and stand
it on the black (0, 0,0) vertex, with the white vertex (1,1, 1) directly above it, as
shown in Fig. 6.12(a). As noted in connection with Fig. 6.7, the intensity (gray
scale) is along the line joining these two vertices, In the arrangement shown in
Fig. 6.12, the line (intensity axis) joining the black and white vertices is vertical.
Thus, if we wanted to determine the intensity component of any color point in
Fig. 6.12, we would simply pass a plane perpendicular to the intensity axis and
containing the color point. The intersection of the plane with the intensity axis
would give us a point with intensity value in the range [0, 1]. We also note with a
little thought that the saturation (purity) of a color increases as a function of distance from the intensity axis, In fact, the saturation of points on the intensity axis
is zero, as evidenced by the fact that all points along this axis are gray.In order to see how hue can-be determined also from a given RGB point,
consider Fig. 6.12(b), which shows a plane defined by three points (black,
white, and cyan). The fact that the black and white points are contained in the
plane tells us that the intensity axis also is contained in the plane. Furthermore, we see that ad! points contained in the plane segment defined by the intensity axis and the boundaries of the cube have the same hue (cyan in this
case), We would arrive at the same conclusion by recalling from Section 6.1
that all colors generated by three colors lie in the triangle defined by those colors. If two of those points are black and white and the third is a color point, all
points on the triangle would have the same hue because the black and white
components cannot change the hue (of course, the intensity and saturation of
points in this triangle would be different). By rotating the shaded plane about
the vertical intensity axis, we would obtain different hues. From these concepts
we arrive at the conclusion that the hue, saturation, and intensity values required to form the HSI space can be obtained from the RGB color cube. That
is, we can convert any RGB point to a corresponding point in the HSI color
model by working out the geometrical formulas describing the reasoning outlined in the preceding discussion.WhiteYellowBlue Red
6.2 m Color ModelsThe key point to keep in mind regarding the cube arrangement in Fig. 6.12
and its corresponding HSI color space is that the HSI space is represented by a
vertical intensity axis and the locus of color points that lie on planes
perpendicular to this axis. As the planes move up and down the intensity axis,
the boundaries defined by the intersection of each plane with the faces of the
cube have either a triangular or hexagonal shape. This can be visualized much
more readily by looking at the cube down its gray-scale axis, as shown in
Fig, 6.13(a). In this plane we see that the primary colors are separated by 120°.
The secondary colors are 60° from the primaries, which means that the angle
between secondaries also is 120°. Figure 6.13(b) shows the same hexagonal
shape and an arbitrary color point (shown as a dot). The hue of the point is determined by an angle from some reference point. Usually (but not always) an
angle of 0° from the red axis designates 0 hue, and the hue increases counterclockwise from there. The saturation (distance from the vertical axis) is the
length of the vector from the origin to the point. Note that the origin is defined
by the intersection of the color plane with the vertical intensity axis. The important components of the HSI color space are the vertical intensity axis, the
length of the vector to a color point, and the angle this vector makes with the
red axis. Therefore, it is not unusual to see the HSI planes defined is terms of
the hexagon just discussed, a triangle, or even a circle, as Figs. 6.13(c) and (d)
show. The shape chosen does not matter because any one of these shapes can
be warped into one of the other two by a geometric transformation. Figure 6.14
shows the HSI model based on color triangles and also on circles.Green YellowCyan Red   
  
  Blue Magenta :Green Yellow Yellow              Cyan » Red *Red
Blue Magenta Blue Magenta — Red Blue ‘Magenta
a
bedFIGURE 6.13 Hue and saturation in the HSI color model. The dot is an arbitrary color
point. The angle from the red axis gives the hue, and the length of the vector is the
saturation. The intensity of ail colors in any of these planes is given by the position of
the plane on the vertical intensity axis.431
432  Chpter6 mm Color Image Processing@%FIGURE 6.14 The
HSI color model
based on(a) triangular and
(b) circular color
planes, The
triangles and
circles are
perpendicular to
the vertical
intensity axis.Computations from RGB
to HS] and back are
carried out on a per pixel
basis. We omitted the
dependence on (x. y) of
the conversion equations
for notational clarity.        BlackConverting colors from RGB to HSI
Given an image in RGB color format. the #7 component of each RGR pixel is
obtained using the equation_ ‘' HBG2-2
360-8 if Be eG (62-2)
434  Chapter6 @ Color Image ProcessingEXAMPLE 6.2:
The HSI values
corresponding to
the image of the
RGB color cube.abeFIGURE 6.15 HIS{ components of the image in Fig. 6.8. 4a) Ane. (b) saturation, and tc) intensity images.Then the RGB components areR= 11-58) (6.2-9)
a _ ScosH |
G= Ill + Sep | (6.2-10)
and
B=3I -(R+G) (6.2-11)BR sector (240° = H = 360°): Finally, if H is in this range, we subtract 240°
from it: H = H ~ 240° (6.2-12)
Then the RGB components are
G=I(i- S$) (6.2-13)
S cos H
B= flit cos(60° — A) (6.2-14)
and
R= 31 -(G+ B) (6.2-15)Uses of these equations for image processing are discussed in several of the
following sections.@ Figure 6.15 shows the hue, saturation, and intensity images for the RGB
values shown in Fig. 6.8. Figure 6,15(a) is the hue image. lts most distinguishing
feature is the discontinuity in value along a 45° line in the front (red) plane of
the cube. To understand the reason for this discontinuity, refer to Fig. 6.8, draw
a line from the red to the white vertices of the cube, and select a point in the
middle of this line. Starting at that point, draw a path to the right, following the
cube around until you return to the starting point. The major colors encountered in this path are yellow, green. cyan, blue, magenta. and back to red. According to Fig, 6.13, the values of hue along this path should increase from 0°
6.2 m@ Color Models 433with’ 4 a(R — G) + (R- BD]
0 = cos 3 77
[((R - GY + (R~ BYG ~ B)]
The saturation component is given bySs [min(R, G, B)| (62-3)~* (R+G +B)
Finally, the intensity component is given byI= HR + Gt B) {6.2-4)It is assumed that the RGB values have been normalized to the range (0, 1]
and that angle @ is measured with respect to the red axis of the HSI space, as
indicated in Fig. 6.13. Hue can be normalized to the range [0, 1] by dividing by
360° ail values resulting from Eq. (6.2-2). The other two HSI components already are in this range if the given RGB values are in the interval (0, 1].The results in Eqs. (6.2-2) through (6.2-4) can be derived from the geometry
shown in Figs. 6.12 and 6.13. The derivation is tedious and would not add significantly to the present discussion. The interested reader can consult the
book’s references or Web site for a proof of these equations, as well as for the
following HSI to RGB conversion results.Converting colors from HSI to RGBGiven values of HS] in the interval (0, 1), we now want to find the corresponding RGB values in the same range. The applicable equations depend on the
values of H. There are three sectors of interest, corresponding to the 120° intervals in the separation of primaries (see Fig. 6.13). We begin by multiplying
H by 360°, which returns the hue to its original range of [0°, 360°].RG sector (0° = H < 120°): When # is in this sector, the RGB components
are given by the equationsBeili-S) Xs (62-5)
S cos H
R=i¢+ cos(60° — H) {6.2-6)
and
G = 31 -(R+ B) (62-7)GB sector (120° = H < 240°): If the given value of H is in this sector, we first
subtract 120° from it:
H = H — 120° (6.2-8) 4It is good practice to add @ small number in the denominator of this expression to avoid dividing by 0
when R = G = B, in which case 6 will be 90°. Note that when all RGB componenis are equal, Eq. (6.2-3)
gives § = 0. In addition, the conversion from HSI back to RGB in Eqs. (6.2-5) through (6.2-7) will give
R=G = B = /, as expected, because when R = C7 = B, we are dealing with a gray-scale image.Consult the Tutorials section of the hook Web site
for a detailed derivation
of ihe conversion equations between RGB and
HSI, and vice versa,
436  Chopter 6 m Color Image Processing
ae
a2FIGURE 6.17
{a)~(c) Modified
HSI component
images.(d) Resulting
RGB image. (See
Fig. 6.16 for the
original HSI
images.) To change the individual color of any region in the RGB image, we change
the values of the corresponding region in the hue image of Fig. 6.16(b). Then
we convert the new H image, along with the unchanged Sand / images, back to
RGB using the procedure explained in connection with Eqs. (6.2-S) through
(6.2-15). To change the saturation (purity) of the color in any region, we follow
the same procedure, except that we make the changes in the saturation image
in HSI space. Similar comments apply to changing the average intensity of any
region. Of course, these changes can be made simultaneously. For example, the
image in Fig. 6.17(a) was obtained by changing to 0 the pixels corresponding to
the blue and green regions in Fig. 6.16(b). In Fig. 6.17(b) we reduced by half
the saturation of the cyan region in component image S from Fig. 6.16(c}. In
Fig. 6.17(c) we reduced by half the intensity of the central white region in the
intensity image of Fig. 6.16(d). The result of converting this modified HSI
image back to RGB is shown in Fig. 6.17(d). As expected, we see in this figure
that the outer portions of all circles are now red; the purity of the cyan region
was diminished, and the central region became gray rather than white. Although these results are simple, they illustrate clearly the power of the HSI
color model in allowing independent control over hue, saturation, and intensi
ty, quantities with which we are quite familiar when describing colors,Pseudocolor Image ProcessingPseudocolor (also called fulse color) image proc
ors to gray values based on a specified evi
is used to differentiate the process of assigning cal‘Siig Consists of assigniz
6.3 m Pseudocolor Image Processing 437from the processes associated with true color images, a topic discussed starting
in Section 6.4. The principal use of pseudocolor is for human visualization and
interpretation of gray-scale events in an image or sequence of images. As noted
at the beginning of this chapter, one of the principal motivations for using color
js the fact that humans can discern thousands of color shades and intensities,
compared to only two dozen or so shades of gray.6.3.1 Intensity Slicing ~The technique of intensity (sometimes called density) slicing and color coding is
one of the simplest examples of pseudocolor image processing, If an image is interpreted as a 3-D function (see Fig, 2.18(a)}, the method can be viewed as one
of placing planes parallel to the coordinate plane of the image; each plane then
“slices” the function in the area of intersection. Figure 6.18 shows an example of
using a plane at f(x, y) = {; to slice the image function into two levels. :If a different color is assigned to each side of the plane shown ia Fig. 6.18,
any pixel whose intensity level is above the plane will be coded with one color,
and any pixel below the plane will be coded with the other. Levels that lie on
the plane itself may be arbitrarily assigned one of the two colors. The result is
a two-color image whose relative appearance can be controlled by moving the
slicing plane up and down the intensity axis.In generai, the technique may be summarized as follows. Let (0, £ — 1]
represent the gray scale, let level /) represent black [ f(x, y} = 0], and level
{,_, represent white [f(a, y) = L — 1]. Suppose that P planes perpendicular
to the intensity axis are defined at levels 1;, 4,...,/p. Then, assuming that
0<P<L-—1, the P planes partition the gray scale into P + 1 intervals,
V,, Va, .-., Vpay. Intensity to color assignments are made according to the relationfay=c if fe, yew (6.3-1)
f(x,y) FIGURE 6.18
Intensity axis ~ Geometric
A _ ae interpretation of
(White) L— 1 the intensity
Slicing plane slicing technique.
6.3 # Pseudocolor Image Processing 439In the preceding simple example, the gray scale was divided into intervals and
a different color was assigned to each region. without regard for the meaning of
the gray levels in the image. Interest in that case was simply to view the different
pray levels constituting the image. Intensity slicing assumes a much more meaningful and useful role when subdivision of the gray scale is based on physical
characteristics of the image. For instance, Fig. 6.21(a) shows an X-ray image of a
weld (the horizontal dark region) containing several cracks and porosities (the
bright, white streaks running horizontally through the middle of the image), It
is known that when there is a porosity or crack in a weld, the full strength of the
X-rays going through the object saturates the imaging sensor on the other side of
the object. Thus, intensity values of 255 in an 8-bit image coming from such a system automatically imply a problem with the weld. If a human were to be the ultimate judge of the analysis, and manual processes were employed to inspect welds
(still a common procedure today), a simple color coding that assigns one color to a
bFIGURE 6.21(a) Monochrome
X-ray image of a
weld. (b) Result
of color coding.
(Original image
courtesy of
X-TEK Systems,
Ltd.)
438 Chapter 6 m@ Color Image ProcessingFIGURE 6.19 An
alternative
representation of
the intensityslicing technique.EXAMPLE 6.3:
Intensity slicing.abFIGURE 6.20(a) Monochrome
image of the Picker
Thyroid Phantom.
(b) Result of
density slicing into
eight colors.
(Courtesy of Dr.
JL. Blankenship,
Instrumentation
and Controls
Division, Oak
Ridge National
Laboratory.}|QtColor  Intensity levelswhere c, is the color associated with the Ath intensity interval V, defined by
the partitioning planes al? = k.-~- land! = &.The idea of planes is useful primarily for a geometric interpretation of the
intensity-slicing technique. Figure 6.19 shows an alternative representation
that defines the same mapping as in Fig. 6.18. According to the mapping Tunc:
tion shown in Fig. 6.19, any input intensity level is assigned one of two colors,
depending on whether if is above or below the value of /;; When more levels
are used, the mapping function takes on a staircase form.@ A simple, but practical, use of intensity slicing is shown in Fig. 6.20. Figure
6.20(a) is a monochrome image of the Picker Thyroid Phantom (a radiation
test pattern), and Fig. 6.20(b) is the result of intensity slicing this image into
eight color regions. Regions that appear of constant intensity in the monochrome image are really quite variable, as shown by the various colors in the
sliced image. The left lobe. for instance, is a dull gray in the monochrome
image, and picking out variations in intensity is difficult. By contrast, the color
image clearly shows eight different regions of constant intensity, one for each
of the colors used. a
6.3 m Pseudocolor Image Processing 441 amesFIGURE 6.22 (a) Gray-scale image in which intensity (in the lighter horizontal band shown) corresponds to
average monthly rainfall. {b) Colors assigned to intensity values. (c) Color-coded image. (d) Zoom of the
South American region. (Courtesy of NASA.)FIGURE 6.23
Functional block
diagram for
pseudocolor
image processing.
fr fon and fy are
fed into the
fod y) corresponding
red, green, and
blue inputs of an
RGB color
momtor, 
    
   
     
 
   
    
 transformationGreeo
transformationBluc
transformation
440 Chapter 6 m Color Image ProcessingEXAMPLE 6.4:
Use of color to
highlight rainfall
levels,level 255 and another to all other intensity levels would simplify the inspector’s
job considerably. Figure 6.21(b) shows the result. No explanation is required to
arrive at the conclusion that human error rates would be lower if images were
displayed in the form of Fig. 6.21(b), instead of the form shown in Fig. 6.21{a). In
other words, if the exact intensity value or range of values one is looking for is
known, intensity slicing is a simple but powerful aid in visualization, especially if
numerous images are involved. The following is a more complex example.@ Measurement of rainfall levels, especially in the tropical regions of the
Earth, is of interest in diverse applications dealing with the environment. Accurate measurements using ground-based sensors are difficult and expensive to
acquire, and total rainfall figures are even more difficult to obtain because a
significant portion of precipitation occurs over the ocean. One approach for obtaining rainfall figures is to use a satellite. The TRMM (Tropical Rainfall Measuring Mission) satellite utilizes; among others, three sensors specially designed
to detect rain: a precipitation radar, a microwave imager, and a visible and infrared scanner (see Sections 1.3 and 2.3 regarding image sensing modalities),
The results from the various rain sensors are processed, resulting in estimates of average rainfall over a given time period in the area monitored by the
sensors, From these estimates, it is not difficult to generate gray-scale images
whose intensity values correspond directly to rainfall, with each pixel representing a physical land area whose size depends on the resolution of the sensors. Such an intensity image is shown in Fig. 6.22(a), where the area monitored
by the satellite is the slightly lighter horizontal band in the middle one-third of
the picture (these are the tropical regions). In this particular example, the rainfall values are average monthly values (in inches) over a three-year period.
Visual examination of this picture for rainfall patterns is quite difficuit, if
not impossible. However, suppose that we code intensity levels from 0 to 255
using the colors shown in Fig. 6.22(b). Values toward the blues signify low values of rainfall, with the opposite being true for red. Note that the scale tops out
at pure red for values of rainfall greater than 20 inches. Figure 6.22(c) shows
the result of color coding the gray image with the color map just discussed. The
results are much easier to interpret, as shown in this figure and in the zoomed
area of Fig. 6.22(d). In addition to providing global coverage, this type of data
allows meteorologists to calibrate ground-based rain monitoring systems with
greater precision than ever before. a6.3.2 Intensity to Color TransformationsOther types of transformations are more generab and thus are capable of
achieving a wider range of pseudocolor enhancement results than the simple
slicing technique discussed in the preceding section. An approach that is particularly attractive is shown in Fig. 6.23. Basically, the idea underlying this approach is to perform three independent transformations on the intensity of any
input pixel. The three results are then fed separately into the red, green, and
blue channels of a color television monitor. This method produces a composite
image whose color content is modulated by the naturc of the transformation
6.3 @ Pseudocolor Image Processing 443       
       Intensity
0 | } 4 L-4J .
Explosive Garment Background
bag
L-j
Red
L-1
Green
L-i
Blue
Intensity 0 t 4 4271
Explosive Garment Background
bagThe image shown in Fig. 6.24(b) was obtained with the transformation
functions in Fig. 6.25(a), which shows the gray-level bands corresponding to
the explosive, garment bag, and background, respectively. Note that the explosive and background have quite different intensity levels, but they were
both coded with approximately the same color as a result of the periodicity of
the sine waves. The image shown in Fig. 6.24(c) was obtained with the transformation functions in Fig, 6.25(b), In this case the explosives and garment
bag intensity bands were mapped by similar transformations and thus received essentially the same color assignments. Note that this mapping allows
an observer to “see” through the explosives. The background mappings were
about the same as those used for Fig. 6.24(b), producing almost identical color
assignments. tja
bFIGURE 6.25
Transformation
functions used to
obtain the images
in Fig. 6.24.
442EXAMPLE 6.5:
Use of
pseudocolor for
highlighting
explosives _
contained in
luggage.BeFIGURE 6.24
Pseudocolor
enhancement by
using the gray
level to color
transformations in
Fig. 6.25.
(Original image
courtesy ofDr. Mike Hurwitz,
Westinghouse.)Chapter 6 @ Color Image Processingfunctions. Note that these are transformations on the intensity values of an
image and are not functions of position.The method discussed in the previous section is a special case of the technique just described. There, piecewise linear functions of the intensity levels
(Fig. 6.19) are used to generate colors. The method discussed in this section, on
the other hand, can be based on smocth, nonlinear functions, which, as might
be expected, pives the technique considerable flexibility.M@ Figure 6.24(a) shows two monochrome images of luggage obtained from an
airport X-ray scanning system. The image on the teft contains ordinary articles.
The image on the right contains the same articles, as well as a block of simulated
plastic explosives. The purpose of this example is to illustrate the use of intensity level to coior transformations to obtain various degrees of enhancement.Figure 6.25 shows the transformation functions used. These sinusoidal functions contain regions of relatively constant value around the peaks as well as
regions that change rapidly near the valleys. Changing the phase and frequency of each sinusoid can emphasize (in color) ranges in the gray scale. For instance, if all three transformations have the same phase and frequency, the
output image will be monochrome. A small change in the phase between the
three transformations produces little change in pixels whose intensities correspond to peaks in the sinusoids, especially if the sinusoids have broad profiles
(low frequencies). Pixels with intensity values in the steep section of the sinusoids are assigued a much stronger color content as a result of significant differences between the amplitudes of the three sinusoids caused by the phase
displacement between them.
6.3 M Pseudocolor Image Processing 445 FIGURE 6.27 (a)-(d) Images in bands 1-4 in Fig. 1.10 (sce Table 1.1). (¢) Color
composite image obtained by treating (a), (b), and (c} as the red, green, blue components of an RGB image. (f) Image obtained in the same manner, but using in the
red channel the near-infrared image is (1). (Original multispectral images courtesyof NASA.)
446 Ghopter6 m Color Image Processing
«
&FIGURE 6.28(a) Pseudocolor
rendition of
Jupiter Moon Io.
(b) A close-up.
(Courtesy of
NASA.) from an active volcano on Jo, and the surrounding yellow materials are older
sulfur deposits, This image conveys these characteristics much more readily
than would be possible by analyzing the component images individually, xEEE Basics of Full-Color Image ProcessingIn this section, we begin the study of processing techniques applicable to full
color images. Although they are far frorn being exhaustive, the techniques developed in the sections that follow are illustrative of how Lull-color images ars
handled for a variety of image processing tasks. Full-color image processing
approaches fail into two major catcgories. In the tirst category, we process
each component image individually and then fornt a composite peo :
color image from thy indivs
we work with color pivels «|
444 = Chaptor 6: m Color Image ProcessingFIGURE 6.26 A
pseudocolor
coding approach
used when several
monochrome
images are
available.EXAMPLE 6.5:
Color coding of
multispectral
images. Agix.y)8104.9)
Sil, yo Transformation T;8205 ))) ditional
SAlx, yc>| Transformation T, [—— > processing Ag(x, y)fe NED emt ts FSThe approach shown in Fig. 6.23 is based on a single monochrome image.
Often, it is of interest to combine several monochrome images into a single
color composite, as shown in Fig. 6.26. A frequent use of this approach (illustrated in Example 6.6) is in multispectral image processing, where different
sensors produce individual monechrome images, each in a different spectral
band. The types of additional processes shown in Fig. 6.26 can be techniques
such as color balancing (see Section 6.5.4), combining images, and selecting
the three images for display based on knowledge about response characteristics of the sensors used to generate the images.  ha(x. y)eee
rs
x
*
=@ Figures 6.27(a) through (d) show four spectral satellite images of Washington, D.C., including part of the Potomac River. The first three images are in the
visible red, green, and blue, and the fourth is in the near infrared (see Table 1.1
and Fig, 1.10). Figure 6.27(e) is the full-color image obtained by combining the
first three images into an RGB.image. Full-color images of dense areas are difficult to interpret, but one notable feature of this image is the difference in
color in various parts of the Potomac River. Figure 6.27(f) is a little more interesting. This image was formed by replacing the red component of Fig. 6.27(e)
with the near-infrared image. From Table 1.1, we know that this band is strongly responsive to the biomass components of a scene. Figure 6.27(f) shows quite
clearly the difference between biomass (in red) and the human-made features
in the scene, composed primarily of concrete and asphalt, which appear bluish
in the image.The type of processing just illustrated is quite powerful in helping visualize
events of interest in complex images, especially when those events are beyond
our normal sensing capabilities. Figure 6.28 is an excellent illustration of this.
These are images of the Jupiter moon Io, shown im pseudocolor by combining
several of the sensor images from the Galileo spacecraft, some of which are in
spectral regions not visible to the eye. However, by understanding the physical
and chemical processes likely to affect sensor response, it is possible to combine
the sensed images into a meaningful pseudocolor map. One way to combine the
sensed image data is by how they show either differences in surface chemical
composition or changes in the way the surface reflects sunlight. For example, in
the pseudocolor image in Fig. 6.28(b), bright red depicts material newly ejected
6.4 m Basics of Full-Color Image Processing 447three components, color pixels are vectors. For example, in the RGB system,
each color point can be interpreted as a vector extending from the origin to
that point in the RGB coordinate system (see Fig. 6.7).Let ¢ represent an arbitrary vector in RGB color space:CR R
¢=|G|)=(G (64-1)
a cy BThis equation indicates that the components of ¢ are simply the RGB components of a color image at a point. We take into account the fact that the color
components are a function of coordinates (x, y) by using the notationerly) | [ R(x, y)
e(x, y) = | co, ¥) | = | GC, y) (6.4-2)
cats, y) B(x, y)For an image of size M x N, there are MN such vectors, e(x, y), for
x=0,1,2,...,M—-1;y=0,1,2,....N—-1.It is important to keep in mind that Eq, (6.4-2) depicts a vector whose components are spatial variables in x and y. This is a frequent source of confusion
that can be avoided by focusing on the fact that our interest lies in spatial
processes. That is, we are interested in image processing techniques formulated in x and y. The fact that the pixels are now color pixels introduces a factor
that, in its easiest formulation, allows us to process a color image by processing
each of its component images separately, using standard gray-scale image processing methods. However, the results of individual color component processing are not always equivalent to direct processing in color vector space, in
which case we must formulate new approaches.In order for per-color-component and vector-based processing to be equivalent, two conditions have to be satisfied: First, the process has to be applicable
to both vectors and scalars, Second, the operation on each component of a vector must be independent of the other components, As an ilhustration, Fig. 6.29
shows neighborhood spatial processing of gray-scale and full-color images.abFIGURE 6.29
Spatial masks for
gray-scale and
RGB color
images. Spatial mask wi Spatial mask JS  Gray-scale imageRGB color image
6.5 # Color Transformations 449 Full color Cyan Magenta . Yellow Black Hue Saturation IntensityFIGURE 6.30 A full-color image and its various color-space components. (Original image courtesy of Med)ata
Interactive.)The full-color image in Fig. 6.30 shows a high-resolution color image of a
bow] of strawberries and a cup of coffee that was digitized from a large format
(4" « 5”) color negative. The second row of the figure contams The compornens
448 — Ghepter6 @ Color Image ProcessingSuppose that the process is neighborhood averaging. In Fig. 6.29(a), averaging
would be accomplished by summing the intensities of all the pixels in the
neighborhood and dividing by the total number of pixels in the neighborhood.
In Fig. 6.29(b), averaging would be done by summing all the vectors in the
neighborhood and dividing each component by the total number of vectors in
the neighborhood. But each component of the average vector is the sum of the
pixels in the image corresponding to that component, which is the same as the
result that would be obtained if the averaging were done on a per-colorcomponent basis and then the vector was formed. We show this in more detail
in the following sections. We also show methods in which the results of the two
approaches are not the same.EEEB Color TransformationsThe techniques described in this section, collectively called color transformations, deal with processing the components of a color image within the context
of a single color model, as opposed to the conversion of those components between models (like the RGB-to-HSI and HSI-to-RGB conversion transformations of Section 6.2.3).6.5.1 Formulation
As with the intensity transformation techniques of Chapter 3, we model color
transformations using the expressiong(x,y) = THF (x, y)] (6.5-1)where f(x, y) is a color input image, g(x, y) is the transformed or processed
color output image, and T is an operator on f over a spatial neighborhood of
(x, y). The principal difference between this equation and Eq. (3.1-1) is in its
interpretation. The pixel values here are triplets or quartets (i.e., groups of
three or four values) from the color space chosen to represent the images, as illustrated in Fig. 6.2%(b).Analogous to the approach we used to introduce the basic intensity transformations in Section 3.2, we will restrict attention in this section to color
transformations of the form8, = Tr, ta.  Pa)s P= 1,2,...,n" (6.5-2)where, for notational simplicity, 7; and s; are variables denoting the color components of f(x, y) and g(x, y) at any point (x, y), 7 is the number of color components, and {7}, 7,...,T,} is a set of transformation or color mapping
functions that operate on r; to produce s;. Note that 2 transformations, 7;, combine to implement the single transformation function, T, in Eq. (6.5-1). The
color space chosen to describe the pixels of f and g determines the value of x.
If the RGB color space is selected, for example, a = 3 and ry, r2, and r; denote
the red, green, and blue components of the input image, respectively. If the
CMYK or HSI color spaces are chosen, n = 40r 2 = 3.
6.5 % Color Transformationsselected, however, the output is the same. Figure 6.31(b) shows the result of
applying any of the transformations in Eqs. (6.5-4) through (6.5-6) to the fulcolor image of Fig. 6.30 using & = 0.7, The mapping functions themselves are
depicted graphically in Figs. 6.31{c) through (e).It is important to note that cach transformation defined in Eqs. (6.5-4)
through (6.5-6) depends only on one component within its color space. For
example, the red output component, s;, in Eq. (6.5-5) is independent of the
green (r2) and blue (r3) inputs; it depends only on the red {r,) input. Transformations of this type are among the simplest and most used coior processing tools and can be carried out on a per-color-component basis, as
mentioned at the beginning of our discussion. In the remainder of this section we examine several such transformations and discuss a case in which the
component transformation functions are dependent on all the color components of the input image and, therefore, cannot be done on an individual
color-component basis.    ab
wae
FIGURE 6.31 Adjusting the intens' ity of an image using color transformations.
(a) Original image. (b) Result of decreasing its intensity by 30% {Le., letting k = 0,7),
(c)-(e) The required RGB, CMY. and HST transformation functions. (Original image
courtesy of MedData Interactive.)    451
450 Chapter6 mi Color Image Processingof the initial CMYK scan, In these images, black represents 0 and white represents 1 in each CMYK color component. Thus, we see that the strawberries
are composed of large amounts of magenta and yellow because the images
corresponding to these two CMYK components are the brightest. Black is
used sparingly and is generally confined to the coffee and shadows within the
bowl of strawberries. When the CMYK image is converted to RGB, as shown
in the third row of the figure, the strawberries are seen to contain a large
amount of red and very little (although some) green and blue. The last row of
Fig. 6.30 shows the HSI components of the full-color image —computed using
Eggs. (6.2-2) through (6.2-4). As expected, the intensity component is a monochrome rendition of the full-color original. In addition, the strawberries are
relatively pure in color; they possess the highest saturation or least dilution by
white light of any of the hues in the image. Finally, we note some difficulty in: interpreting the hue component. The problem is compounded by the fact
that (1) there is a discontinuity in the HSI model where 0° and 360° meet (see
Fig. 6.15), and (2) hue is undefined for a saturation of 0 (i.e., for white, black,
and pure grays). The discontinuity of the model is most apparent around the
strawberries, which are depicted in gray level values near both black (0) and
white (1). The result is an unexpected mixture of highly contrasting gray levels to represent a single color—red.Any of the color-space components in Fig. 6.30 can be used in conjunction
with Eq. (6.5-2). In theory, any transformation can be performed in any color
model. In practice, however, some operations are better suited to specific models. For a given transformation, the cost of converting between representations
must be factored into the decision regarding the color space in which to implement it. Suppose, for example, that we wish to modify the intensity of the fullcolor image in Fig, 6.30 usingg(x,y) = kKf(x, y) (6.5-3)
where 0 < k <1. In the HSI color space, this can be done with the simple
transformation53 = krs (6.5-4)where 5s, =r; and 5) = r2. Only HSI intensity component r3 is modified. In
the RGB color space, three components must be transformed:S = ke £2 1,2,3 {6.5-5)
The CMY space requires a similar set of linear transformations:
= ket - ky) §=1,2,3 (6.5-6)Although the HSI transformation involves the fewest number of operations, the computations required to convert an RGB or CMY(K) image to the
HSI space more than offsets (in this case) the advantages of the simpler
transformation—the conversion calculations are more computationally intense than the intensity transformation itself. Regardiess of the color space
452 Chapter 6 m Color Image ProcessingFIGURE 6.32Complements onthe color circle.EXAMPLE 6.7:Computing color
. imagecomplements.MagentaCyan RedGreen 6.5.2 Color Complements ~The hues directly opposite one another on the color circle’ of Fig. 6.32 are called
complemenis. Our interest in complements stems from the fact that they are
analogous to the gray-scale negatives of Section 3.2.1. As in the gray-scale case,
color complements are useful for enhancing detail that is embedded in dark regions of a color image—particularly when the regions are dominant jn size.@ Figures 6.33(a) and (c) show the full-color image from Fig. 6.30 and its color
complement. The RGB transformations used to compute the complement are
plotted in Fig. 6.33(b). They are identical to the gray-scale negative transformation defined in Section 3.2.1. Note that the computed complement is reminiscent of conventional photographic color film negatives. Reds of the original
image are replaced by cyans in the complement. When the original image is
black, the complement is white, and so on. Each of the hues in the complement
image can be predicted from the original image using the color circle of
Fig. 6.32, and each of the RGB component transforms involved in the computation of the complement is a function of ently the corresponding input color
component,Unlike the intensity transformations of Fig. 6.31, the RGB complement
transformation functions used in this example do not have a straightforward
HSI space equivalent. It is left as an exercise for the reader (see Problem 6,18)
to show that the saturation component of the complement cannot be computed from the saturation component of the input image alone. Figure 6.33(d)
provides an approximation of the complement using the hue, saturation, and
intensity transformations given in Fig. 6.33(b). Note that the saturation component of the input image is unaltered; it is responsible for the visual differences between Figs. 6.33(c) and {d). * ‘The color circle originated with Sir Isaac Newton, who in the seventeenth centacy joined the ends of the
color spectrum to form the first color circle.
6.5 @ Color Transformations 453           ~6.5.3 Color Slicing =
Highlighting a specific range of colors in an image is useful for separating objects from their surroundings. The basic idea is either to (1) display the colors
of interest so that they stand out from the background or (2) use the region defined by the colors as a mask for further processing. The most straightforward
approach is to extend the intensity slicing techniques of Section 3.2.4, Because
a color pixel is an n-dimensional quantity, however, the resulting color transformation functions are more complicated than their gray-scale counterparts
in Fig, 3.11. In fact, the required transformations are more complex than the
coler component transforms considered thus far. This is because all practical
color-slicing approaches require cach pixel’s transformed color components to
be a function of all 7 original pixe!’s color components,One of the simplest ways to “slice” a color image is to map the colors outside
some range of interest to a nonprominent neutral color, ff the colors of interesi
are enclosed by a cube (or Aypercube for n° 3) of width W and centered at a ab
edFIGURE 6,33
Color
complement
transformations.
(a) Originai
image.(b) Complement
transformation
functions,(c) Complement
of (a) based on
the RGB mapping
functions. (d) An
approximation
of the RGB
complement
using HSI
transformations.
454  Chopter6 m Color Image ProcessingEXAMPLE 6.8:
An illustration of
color slicing.prototypical (e.g., average) color with components (a), @),..., @,), the necessary set of transformations is
ip Ww
0.5 if h — al > “|
5 = 2 any b=jsutrj otherwise p= 12.40 (65-7)These transformations highlight the colors around the prototype by forcing al!
other colors to the midpoint of the reference color space (an arbitrarily chosen
neutral point). For the RGB color space, for example, a suitable neutral point
is middle gray or color (0.5, 0.5, 0.5).If a sphere is used to specify the colors of interest, Eq. (6.5-7) becomestt
0.5 if SC; — a; > Rijel . ;
r, — otherwise = 1,2,...,0 (6.5-8)Here, Rp is the radius of the enclosing sphere (or hypersphere for n > 3) and
(a, a2,...,@,) are the components of its center (i¢., the prototypical colar).
Other useful variations of Eqs, (6.5-7) and (6.5-8) include implementing multiple color prototypes and reducing the intensity of the colors outside the region
of interest —rather than setting them to a neutral constant.W@ Equations (6.5-7) and (6.5-8) can be used to separate the edible part of the
strawberries in Fig. 6.31(a) from the background cups, bowl, coffee, and table.
Figures 6.34(a) and (b) show the results of applying both transformations. In abFIGURE 6.34 Color-slicing transformations that detect (a) reds within an RGB cube of
width W = 0.2549 centered at (0.6863, 0.1608, 0.1922), and (b) reds within an ROB
sphere of radius 0,1765 centered at the same point. Pixels outside the cube and sphere
were replaced by color (00.5. 0.5, 0.5).
6.5 @ Color Transformations 455each case, a prototype red with RGB color coordinate (0.6863, 0.1608, 0.1922)
was selected from the most prominent strawberry; W and Ry were chosen so
that the highlighted region would not expand to undesirable portions of the
image. The actual values, W = 0.2549 and Ry = 0.1765, were determined interactively. Note that the sphere-based transformation of Eq. (6.5-8) is slightly
better, in the sense that it includes more of the strawberries’ red areas. A
sphere of radius 0.1765 does not completely enclose a cube of width 0.2549 but
is itself not completely enclosed by the cube.6.5.4 Tone and Color CorrectionsColor transformations can be performed on most desktop computers. Jn conjunction with digital cameras, flatbed scanners, and inkjet printers, they turn a
personal computer into a digital darkroom—allowing tonal adjustrnents and
color corrections, the mainstays of high-end color reproduction systems, to be
performed without the need for traditionally outfitted wet processing (ie.,
darkroom) facilities. Although tone and color corrections are useful in other
areas of imaging, the focus of the current discussion is on the most common
uses— photo enhancement and color reproduction.The effectiveness of the transformations examined in this section is
judged ultimately in print. Because these transformations are developed, refined, and evaluated on monitors, it is necessary to maintain a high degree of
color consistency between the monitors used and the eventual output devices, In fact, the colors of the monitor should represent accurately any digitally scanned source images, as well as the final printed output. This is best
accomplished with a device-independent color model that relates the color
gamuts (see Section 6.1) of the monitors and output devices, as well as any
other devices being used, to one another. The success of this approach is a
function of the quality of the color profiles used to map each device to the
model and the model itself. The model of choice for many color management
systems (CMS) is the CIE L*a* b* model, also called CIELAB (CIE [1978],
Robertson (1977]). The L*a*b* color compohents are given by the following equations:Lt = uo-A( 2) = 16 (6.5-9)a) wmbt om 20} 1 *-} - (2) {6.5-11)at = soo]
u
6.5 m Color Transformations 457Corrected Dark CorrectedFIGURE 6.35 Tonal corrections for flat. litt (high key}. und dark ¢
green, and blue components equally does not akways alter the ta   Adiasting the red,
456 Chopter 6 m Color Image ProcessingEXAMPLE 6.9:
Tonal
transformations.where3,
hq) = \na q > 0.008856 (65-12)7.781q + 16/116 q = 0.008856and Xy, Yy, and Zy are reference white tristimulus values—typically the
white of a perfectly reflecting diffuser under CIE standard D65 illumination
(defined by x = 0.3127 and y = 0.3290 in the CIE chromaticity diagram of
Fig. 6.5). The L*a*b* color space is colorimetric (i.e., colors perceived as
matching are encoded identically), perceptually uniform (i.e., color differences
among various hues are perceived uniformly—see the classic paper by
MacAdams [1942]), and device independent. While not a directly displayable
format (conversion to another color space is required), its gamut encompasses
the entire visible spectrum and can represent accurately the colors of any display, print, or input device. Like the HSI system, the L *a*b* system is an excellent decoupler of intensity (represented by lightness L*) and color
(represented by a* for red minus green and b* for green minus blue), making
it useful in both image manipulation (tone and contrast editing) and image
compression applications.’The principal benefit of calibrated imaging systems is that they allow tonal
and color imbalances to be corrected interactively and independently — that is,
in two sequential operations. Before color irregularities, like over- and undersaturated colors, are résolved, problems involving the image's tonal range are
corrected. The tonal range of an image, also called its key type, refers to its general distribution of color intensities. Most of the information in high-key images is concentrated at high (or light) intensities; the colors of fow-key images
are located predominantly at low intensities; middle-key images lie in between. As in the monochrome ease, it is often desirable to distribute the intensities of a color image equally between the highlights and the shadows. The
following examples demonstrate a variety of color transformations for the correction of tonal and color imbalances.B® Transformations for modifying image tones normally are selected interactively. The idea is to adjust experimentally the image's brightness and contrast to provide maximum detail over a suitable range of intensities. The
colors themselves are not changed. In the RGB and CMY(K) spaces, this
means mapping all three (or four) color components with the same transformation function; in the HSI color space, only the intensity component is
modified. ~.Figure 6.35 shows typical transformations used for correcting three common tonal imbalances —flat, light, and dark images. The S-shaped curve in the ‘Studies in-licate that the degree 10 which the fuminance {lightness) information is separated from the
color information in L*a*b* is greater than in other color models—such as CIELUV, YIQ, YUV,YCC, and XYZ (Kasson and Plouffe [1992]).
458 Chapter 6 m Color Image ProcessingEXAMPLE 6.10:
Color balancing.first row of the figure is ideal for boosting contrast [see Fig. 3.2(a)]. Its midpoint is anchored so that highlight and shadow areas can be lightened and
darkened, respectively. (The inverse of this curve can be used to correct excessive contrast.) The transformations in the second and third rows of the figure correct light and dark images and are reminiscent of the power-law
transformations in Fig. 3.6. Aithough the color components are discrete, as
are the actual transformation functions, the transformation functions themselves are displayed and manipulated as continuous quantities —typically
constructed from piecewise linear or higher order (for smoother mappings)
polynomials. Note that the keys of the images in Fig. 6.35 are directly observable; they could also be determined using the histograms of the images’ colorcomponents.B@ After the tonal characteristics of an image have been properly established,
any color imbalances can be addressed. Although color imbalances can be determined objectively by analyzing—with a color spectrometer—a known
color in an image, accurate visual assessments are possible when white areas,
where the RGB or CMY(K) components should be equal, are present. As can
be seen in Fig. 6.36, skin tones also are excellent subjects for visual color assessments because humans are highly perceptive of proper skin color. Vivid
colors, such as bright red objects, are of little value when it comes to visual
color assessment.When a color imbalance is noted, there are a variety of ways to correct
it. When adjusting the color components of an image, it is important to realize that every action affects the overall color balance of the image. That
is, the perception of one color is affected by its surrounding colors. Nevertheless, the color wheel of Fig. 6.32 can be used to predict how one color
component will affect others. Based on the color wheel, for example, the
proportion of any color can be increased by decreasing the amount of the
opposite (or complementary) color in the image. Similarly, it can be increased by raising the proportion of the two immediately adjacent colors
or decreasing the percentage of the two colors adjacent to the complement. Suppose, for instance, that there is an abundance of magenta in an
RGB image. It can be decreased by (1) removing both red and blue or (2) adding
green.Figure 6.36 shows the transformations used to correct simple CMYK output imbalances, Note that the transformations depicted are the functions required for correcting the images; the inverses of these functions were used
to generate the associated color imbalances. Together, the images are analogous to a color ring-around print of a darkroom environment and are useful
as a reference tool for identifying color printing problems. Note, for example, that too much red can be due to excessive magenta (per the bottom left
image) or too little cyan (as shown in the rightmost image of the second
row). b I
460 Chapter 6 @ Color Image ProcessingEdeeFIGURE 6.37
Histogram
equalization
(followed by
saturation
adjustment) in the
HSI color space.    
    6.5.5 Histogram ProcessingUnlike the interactive enhancement approaches of the previous section, the
gray-level histogram processing transformations of Section 3.3 can be applied
to color images in an automated way. Recall that histogram equalization automatically determines a transformation that seeks to produce an image with a
uniform histogram of intensity values. In the case of monochrome images, it
was shown (see Fig. 3.20) to be reasonably successful at handling low-, high-,
and middle-key images. Since color images are composed of multiple components, however, consideration must be given to adapting the gray-scale technique to more than one component and/or histogram. As might be expected, it
is generally unwise to histogram equalize the components of a color image independently. This results in erroneous color. A more logical approach is to
spread the color intensities uniformly, teaving the colors themselves (e.g.,
hues) unchanged. The following example shows that the HSI color space is
ideally suited to this type of approach.    Histogram before processing(median = 636)  
   po —~ et
stagram after processing |E
Wl, (median = 0.5)
L
6.6 & Smoothing and Sharpening 463 
 
 
 
 
 
 
 
  ab
edFIGURE 6.38(a) RGB image.
(6) Red
component image,
(c) Green component. (d) Blue
component.#BE
FIGURE 6.39 HSI components of the RGB color sip big. 6.5800}. GO Hee. (b} Saturation, (c) Totensity
6.6 m Smoothing and Sharpening 461@ Figure 6.37(a) shows a color image of a caster stand containing cruets and
shakers whose intensity component spans the entire (normalized) range of
possible values, (0, 1]. As can be seen in the histogram of its intensity component prior to processing [Fig. 6.37(b)], the image contains a large number of
dark colors that reduce the median intensity to 0.36. Histogram equalizing the
intensity component, without altering the hue and saturation, resulted in the
image shown in Fig, 6.37(c). Note that the overall image is significantly
brighter and that several ntoldings and the grain of the wooden table on which
the caster is sitting are now visible. Figure 6.37(b) shows the intensity histogram of the new image, as well as the intensity transformation used to equalize the intensity component [see Eq. (3.3-8)].Although the intensity equalization process did not alter the values of hue
and saturation of the image, it did impact the overall color perception. Note, in
particular, the loss of vibrancy in the oil and vinegar in the cruets. Figure
6.37(d) shows the result of correcting this partially by increasing the image's
saturation component, subsequent to histogram equalization, using the transformation in Fig. 6.37(b). This type of adjustment is common when working
with the intensity component in HSI space because changes in intensity usually affect the relative appearance of colors in an image. .| 6.6 | Smoothing and SharpeningThe next step beyond transforming each pixel of a color image without regard
to its neighbors (as in the previous section) is to modify its value based on the
characteristics of the surrounding pixels. In this section, the basics of this type
of neighborhood processing are illustrated within the context of color image
smoothing and sharpening.6.6.1 Color Image Smoothing .With reference io Fig. 6.29(a) and the discussion in Sections 3.4 and 3.5, grayscale image smoothing can be viewed as a spatial filtering operation in which
the coefficients of the filtering mask have the same value. As the mask is slid
across the image to be smoothed, each pixel is replaced by the average of the
pixels in the neighborhood defined by the mask. As can be seen in Fig. 6.29(b),
this concept is easily extended to the processing of full-color images. The principal difference is that instead of scalar intensity values we must deal with
component vectors of the form given in Eq. (6.4-2).Let S,, denote the set of coordinates defining a neighborhood centered at
(x, y) in an RGB color image. The average of the RGB component vectors in
this neighborhood is@(x, y) =< SY e(s.2) (6.6-1)
(soeS,,.EXAMPLE 6.11:
Histogram
equalization in the
HSI color space.
462 Chapter 6 m Color Image ProcessingConsult the book Web site
for a brief review of vectors and matrices.EXAMPLE 6.12:
Color image
smoothing by
neighborhood
averaging.It follows from Eq. (6.4-2) and the properties of vector addition thatDY Rs,t)(8,65,SX Gs.9 (6.6-2)(SNeSeySY Bs.(5, DeSiyalienE(x, y) =l= mlWe recognize the components of this vector as the scalar images that would be
obtained by independently smoothing each plane of the starting RGB image
using conventional gray-scale neighborhood processing. Thus, we conclude
that smoothing by neighborhood averaging can be carried out on a per-colorplane basis. The result is the same as when the averaging is performed using
RGB color vectors.@ Consider the RGB color image in Fig. 6.38(a). Its red, green, and blue component images are shown in Figs. 6.38(b) through (d). Figures 6.39(a) through
(c) show the HSI components of the image. Based on the discussion in the previous paragraph, we smoothed each component image of the RGB image in
Fig. 6.38 independently using a 5 X 5 spatial averaging mask. We then combined the individually smoothed images to form the smoothed, full-color RGB
result shown in Fig, 6.40(a). Note that this image appears as we would expect
from performing a spatial smoothing operation, as in the examples given in
Section 3.5. :In Section 6.2, we noted that an important advantage of the HSI color
model is that it decouples intensity and color information, This makes it
suitable for many gray-scale processing techniques and suggests that it
might be more efficient to smooth only the intensity component of the HSI
representation in Fig. 6.39. To illustrate the merits and/or consequences of
this approach, we next smooth only the intensity component (leaving the
hue and saturation components unmodified) and convert the processed result to an RGB image for display. The smoothed color image is shown in
Fig. 6.40(b). Note that it is similar to Fig. 6.40(a), but, as you can see from
the difference image in Fig. 6.40(c), the two smoothed images are not identical. This is because in Fig. 6.40(a) the color of each pixel is the average
color of the pixels in the neighborhood, On the other hand, by smoothing
only the intensity component image in Fig. 6.40(b), the hue and saturation
of each pixel was not affected and, therefore, the pixel colors did not
change. It follows from this observation that the difference between the
two smoothing approaches would become more pronounced as a function
of increasing filter size. a
464  Chopter6 m Color Image Processing FIGURE 6.40 Image smoothing with a 5 x 5 averaging mask. (a) Result of processing each RGB
component image. (b) Result of processing the intensity component of the HS] image and converting toRGB. (c) Difference between the two results.6.6.2 Color Image Sharpening
In this section we consider image sharpening using the Laplacian (see Section
3.6.2). From vector analysis, we know that the Laplacian of a vector is defined
as a vector whose components are equal to the Laplacian of the individual
scalar components of the input vector. In the RGB color system, the Laplacian
of vector ¢ in Eq, (6.4-2) is
WR, ¥)
Wle(x, yf =| VG. ¥) (6.6-3)
| VBC, mIwhich, as in the previous section. tells us that we can compute the Laplacian of a
full-color image by computing the Laplacian of each compouent image separately.ane
FIGURE 6.41 Image sharpi ning with i
processing the HS] interisity Conipeine       Laphiiaa. gal R  Aa vou Yet ne
466  Chapteré m Color Image Processing    Ri€e FIGURE 6.42 Image segmentation in LES) space. (a) Original. (b) Hue. (c) Saturation.
s (d) Intensity. (e) Binary saturation mask (black = 0). (2) Product of (is) and (e).
Gh  (g) Histogram of (f). (h) Scementrtion oof red components fs Gr}
67 @ Image Segmentation Based on Color 467by experimentation that the regions shown in white in Fig. 6.42(h) are about
the best this method can do in identifying the reddish components of the original image. The segmentation method discussed in the following section is capable of yielding considerably better results. “6.7.2 Segmentation in RGB Vector SpaceAlthough, as mentioned numerous times in this chapter, working in HSI space
is more intuitive, segmentation is one area in which better results generally are
obtained by using RGB color vectors. The approach is straightforward. Suppose that the objective is to segment objects of a specified color range in an
RGB image. Given a set of sample color points representative of the colors of
interest, we obtain an estimate of the “average” color that we wish to segment.
Let this average color be denoted by the RGB vector a. The objective of segmentation is to classify each RGB pixel in a given image as having a color in
the specified range or not. In order to perform this comparison, it is necessary
to have a measure of similarity. One of the simplest measures is the Euclidean
distance. Let z denote an arbitrary point in RGB space. We say that z is similar
to aif the distance between them is less than a specified threshold, Do. The Euclidean distance between z and a is given byD(z, a) = |z ~ all= [( — a)"@ — a) (67-1)1= [(zp ~ ap)? + (2g - ae) + (Za > a8)’
where the subscripts &, G, and B denote the RGB components of vectors a and
z.The locus of points such that D(z, a) = Dois a solid sphere of radius Dp, as illustrated in Fig. 6.43(a). Points contained within the sphere satisfy the specified
color criterion; points outside the sphere do not. Coding these 1wo sets of pointsin the image with, say, black and white, produces a binary segmented image.
A useful generalization of Eq. (6.7-1) is a distance measure of the formD(z, a) = ((z — a)’C'z — a): (67-2) abeFIGURE 6.43
Three approaches
for enclosing data
regions for RGB
vector
segmentation.
6.7 @ Image Segmentation Based on Color 465@ Figure 6.41(a) was obtained using Eq, (3.6-7) and the mask in Fig. 3.37(c) to
compute the Laplacians of the RGB component images in Fig. 6.38. These results were combined to produce the sharpened full-color result. Figure 6.41(b)
shows a similarly sharpened image based on the HSI components in Fig. 6.39.
This result was generated by combining the Laplacian of the intensity component with the unchanged hue and saturation components. The difference between the RGB and HSI sharpened images is shown in Fig. 6.41(c). The reason
for the discrepancies betwéen the two images is as in Example 6.12. =Image Segmentation Based on ColorSegmentation is a process that partitions an image into regions. Although
segmentation is the topic of Chapter 10, we consider color segmentation
briefly here for the sake of continuity. You will have no difficulty following
the discussion.6.7.1 Segmentation in HSI Color SpaceIf we wish to segment an image based on color, and, in addition, we want to
carry out the process on individual planes, it is natural to think first of the HSI
space because color is conveniently represented in the hue image. Typically,
saturation is used as a masking image in order to isolate further regions of interest in the hue image. The intensity image is used less frequently for segmentation of color images because it carries no color information. The following
example is typical of how segmentation is performed in the HSI color space.@ Suppose that it is of interest to segment the reddish region in the lower left
of the image in Fig. 6.42(a). Although it was generated by pseudocolor methods, this image can be processed (segmented) as a full-color image without loss
of generality. Figures 6.42(b) through (d) are ts HSI component images. Note
by comparing Figs. 6.42(a) and (b) that the region in which we are interested
has relatively high values of hue, indicating that the colors are on the bluemagenta side of red (see Fig. 6.13). Figure 6.42(e) shows a binary mask generated by thresholding the saturation image with a threshold equal to 10% of the
maximum value in that image. Any pixel value greater than the threshold was
set to 1 (white). All others were set to 0 (black).Figure 6,42(f) is the product of the mask with the hue image, and Fig.
6.42(g) is the histogram of the product image (note that the gray scale is in the
range [0, 1]). We see in the histogram that high values (which are the vaJues of
interest) are grouped at the very high end of the gray scale, near 1.0. The result
of thresholding the product image with threshold value of 0.9 resulted in the
binary image shown in Fig. 6.42(h). The spatial location of the white points in
this image identifies the points in the original image that have the reddish hue
of interest. This was far from a perfect segmentation because there are points
in the original image that we certainly would say have a reddish hue, but that
were not identified by this segmentation method. However, it can be determinedEXAMPLE 6.13:
Sharpening with
the Laplacian.EXAMPLE 6.14:
Segmentation in
HSI space.
468 Chapter 6 m Color Image ProcessingEXAMPLE 6.15:
Color image
segmentation in
RGB space.where C is the covariance matrix! of the samples representative of the color
we wish to segment. The locus of points such that D(z, a) = Do describes a
solid 3-D elliptical body [Fig. 6.43(b)] with the important property that its
principal axes are oriented in the direction of maximum data spread. When
C = I, the 3 X 3 identity matrix, Eq. (6.7-2) reduces to Eq. (6.7-1). Segmentation is as described in the preceding paragraph.Because distances are positive and monotonic, we can work with the distance squared instead, thus avoiding square root computations. However,
implementing Eq. (6.7-1) or (6.7-2) is computationally expensive for images
of practical size, even if the square roots are not computed. A compromise is
to use a bounding box, as illustrated in Fig. 6.43(c). In this approach, the box
is centered on a, and its dimensions along each of the color axes is chosen
proportional to the standard deviation of the samples along each of the axis.
Computation of the standard deviations is done only once using sample
color data.Given an arbitrary color point, we segment it by determining whether or
not it is on the surface or inside the box, as with the distance formulations.
However, determining whether a color point is inside or outside a box is much
simpler computationally when compared to a spherical or elliptical enclosure.
Note that the preceding discussion is a generalization of the method introduced in Section 6.5.3 in connection with color slicing,W@ The rectangular region shown Fig. 6.44(a) contains samples of reddish colors we wish to segment out of the color image. This is the same problem we
considered in Example 6.14 using hue, but here we approach the problem
using RGB color vectors. The approach followed was to compute the mean
vector a using the color points contained within the rectangle in Fig. 6.44{a),
and then to compute the standard deviation of the red, green, and blue values
of those samples. A box was centered at a, and its dimensions along each of the
RGB axes were selected as 1.25 times the standard deviation of the data along
the corresponding axis. For example, let og denote the standard deviation of
the red components of the sample points. Then the dimensions of the box
along the R-axis extended from (ag — 1.25cg) to (ag + 1.25eR), where a, denotes the red component of average vector a. The result of coding each point
in the entire color image as white if it was on the surface or inside the box, and
as black otherwise, is shown in Fig. 6.44(b). Note how the segmented region
was generalized from the color samples enclosed by the rectangle. In fact, by
comparing Figs, 6.44(b) and. 6.42(h), we see that segmentation in the RGB
vector space yielded results that are much more accurate, in the sense that
they correspond much more closely with what we would define as “reddish”
points in the original color image. a ‘Computation of the covariance matrix of a set of vector samples is discussed in Section 11.4
4.7 @ image Segmentation Based on Color 469 6.7.3 Color Edge DetectionAs discussed in Chapter 10, edge detection is an important too! for image segmentation. In this section, we are interested in the issue of computing edges on
an individual-image basis versus computing cdges directly in color vector
space. The details of edge-based segmentation are given in Section 10.2.Edge detection by gradient operators was introduced in Section 3.6.4 in
connection with image sharpening. Unfortunately, the gradient discussed in
Section 3.6.4 is not defined for vector quantities. Thus, we know immediately
that computing the gradient on individual imapes and then using the results to
form a color image wil! lead to erroneous results. A simple example will help
illustrate the reason why.  a
bFIGURE 6.44
Segmentation in
RGB space.(a) Original image
with colors of
interest shown
enclosed by a
rectangle,(b) Result of
segmentation in
RGB vector
space. Compare
with Fig. 6.42(h).
470 = Chapter 6 Color Image ProcessingConsider the twa M x M color images (M odd) in Figs. 6.45(d) and (h),
composed of the three component images in Figs. 6.45(a) through (c) and (e)
through (g)}, respectively. If, for example, we compute the gradient image of
each of the component images [see Eq. (3.6-11)] and add the results to form
the two corresponding RGB gradient images, the value of the gradient at point
[CM + 1)/2,(M + 1)/2] would be the same in both cases. Intuitively, we
would expect the gradient at that point to be stronger for the image in Fig.
6.45(d) because the edges of the R, G,and B images are in the same direction
in that image, as opposed to the image in Fig. 6.45(h), in which only two of the
edges are in the same direction, Thus we see from this simple example that
processing the three individual planes to form a composite gradient image can
yield erroneous results. If the problem is one of just detecting edges, then the
individual-component approach usually yields acceptable results. If accuracy
is an issue, however, then obviously we need a new definition of the gradient applicable to vector quantities. We discuss next a method propased by
Di Zenzo [1986] for doing this. *The problem at hand is to define the gradient (magnitude and direction) of
the vector ein Eq. (6.4-2) at any point (x, y). As was just mentioned, the gradient we studied in Section 3.6.4 is applicable to a scalar function f(x, y); 11 is not
applicable to vector functions. The following is one of the various ways in
which we can extend the concept of a gradient to vector functions. Recall that
for a scalar function f(x, y), the gradient is a vector pointing in the direction of
maximum rate of change of fat coordinates (x, y). FIGURE 6.45 (4)-(c) R, G. and B component images and (a) resulting RCIB color image. fe) 48) ROG andl B
component images and {4} resulting ROB colur image.
6.7 m Image Segmentation Based on ColorLet r, g, and b be unit vectors along the R.G, and B axis of RGB color space
(Fig. 6.7), and define the vectors
aR aG_ , aB=—r+—gtox ax ax (67-3)and6R 0G aB
—r+——gt+——b
dy ay” ay(6.7-4)v=Let the quantities g,,, g,y, and g,, be defined in terms of the dot product of
these vectors, as follows:          OR? jaGl?_ |aBl?
By =uu=o p= axl * lax ox (6.7-5)
aR? jaG|* . |aBl?
syyvevy= l/—] + [— — 6.7-6
; By =Vvavy ay He ay ( )
an
R 4
F  dRAR 8G 8G | 8B AB (6.7-7)Bay “WOVE WY Oy ay ax ay Ox dyKeep in mind that 2, G, and B, and consequently the g’s, are functions of x and
y. Using this notation, it can be shown (Di Zenzo [1986]) that the direction of
maximum rate of change of e(x, y) is given by the angle2 x
a(x, y) = 5 ear| ee] (67-8)
aX yyand that the value of the rate of change at (x, y), in the direction of @(x, y), is
given by~.*1 . . aE
F(x, ») = {Fle + By) + (Bax ~ Byy) COS2O(x, y) + 2g, Sin 20(x, »}} (6.7-9)Because tan{a) = tan(a + 7), if is a solution to Eq. (6.7-8), so is 0) + 7/2.
Furthermore, Fy = Fo4.,80 F has to be computed only for values of @ in the
half-open interval [0, 7). The fact that Eg. (6.7-8) provides two values 90°
apart means that this equation associates with each point (x, y) a pair of orthogonal directions. Along one of those directions F is maximum, and it is
minimum along the other. The derivation of these results is rather lengthy,
and we would gain little in terms of the fundamental objective of our current
discussion by detailing it here. Consult the paper by Di Zenzo [1986] for
details. The partial derivatives required for implementing Eqs. (6.7-5)
through (6.7-7) can be computed using, for example, the Sobel operators discussed in Section 3.6.4,471
472 Chapter 6 m Color Image ProcessingEXAMPLE 6.16:
Edge detection in
vector space.@b
cdFIGURE 6.46(a) RGB image.
(b) Gradient
computed in RGB
color vector
space.(c} Gradients
computed on a
per-image basis
and then added.
(d) Difference
between (b)
and (c).Mi Figure 6.46(b) is the gradient of the image in Fig. 6.46(a), obtained using
the vector method just discussed. Figure 6.46(c} shows the image obtained by
computing the gradient of each RGB component image and forming a composite gradient image by adding the corresponding values of the three component images at cach coordinate (x, y). The edge detail of the vector
gradient image is more complete than the detail in the individual-plane gradient image in Fig. 6.46(c); for example, see the detail around the subject’s right
eye. The image in Fig. 6.46(d) shows the difference between the two gradient
images at each point (x, y). It is important to note that both approaches yielded reasonable results. Whether the extra detail in Fig. 6.46(b) is worth the
added computational burden (as opposed to implementation of the Sobel operators, which were used {o generate the gradient of the individual planes)
can only be determined by the requirements of a given problem. Figure 6.47
shows the three component gradient images, which, when added and scaled,
were used to obtain Fig. 6.46(c). a
6.8 M Noise in Color Images 473 FIGURE 6.47 Component gradient images of the color image in Fig. 6.46, (a} Red component, (b} green
component, and (c} blue component. These three images were added and scaled to produce the image inFig. 6.46(c).| 6.8 | Noise in Color ImagesThe noise models discussed in Section 5.2 are applicable to color images. Usually, the noise content of a color image has the same characteristics in each
color channel, but it is possible for color channels to be affected differently by
noise. One possibility is for the electronics of a particular channel to malfunction. However, different noise levels are more likely to be caused by differences
in the relative strength of illumination available to each of the color channels.
For example, use of a red (reject) filter in a CCD camera will reduce the
strength of illumination available to the red sensor. CCD sensors are noisier at
lower levels of illumination. so the resulting red component of an RGB image
would tend to be noisier than the other two component images in this situation.@ In this example we take a brief look at noise in color images and how noise
carries over when converting from one color model to another. Figures 6.48(a)
through (c) show the three color planes of an RGB image corrupted by Gaussian noise, and Fig. 6.48(d) is the composite RGB image. Note that fine grain
noise such as this tends to be less visually noticeable in a color image than it is
in a monochrome image. Figures 6.49(a) through (c) show the result of converting the RGB image in Fig, 6.48(d) to HSI. Compare these results with the
HSI components of the original image (Fig. 6.39) and note how significantly
degraded the hue and saturation components of the noisy image are. This is
due to the nonlinearity of the cos and min operations in Eqs. (6.2-2} and (6.2-3),
respectively. On the other hand, the intensity component in Fig. 6.49(c) is
slightly smoother than any of the three noisy ROB component images. This is
due to the fact that the intensity image is the average of the RGB images. us indicated in Eq. (6.2-4). (Recall the discussion in Scetion 2.6.3 regarding the fact
that image averaging reduces raudom noise.)EXAMPLE 6.17:
Illustration of the
effects of
converting noisy
RGB images to
HSI
474 Chapter6 # Color Image Processingae
#8
FIGURE 6.48
{a}-(c) Red,green, and bluecomponentimages corruptedby additiveGaussian noise ofmean 0 andvariance 800.(d) ResultingRGB image.[Compare (d)Be
FIGURE 6.49 HSI components of the noisy colar image in Fig, O4S8(). fa} Hne. (b) Saturation. (ce) Intensity.with Fig. 6.46(a).]
6.8 & Noise in Color Images 475 FIGURE 6.50 (a) RGB image wiih green plane corrupted by salt-and-pepper noise.
(b) Hue component of HS} image. (c) Saturation component. (d} Intensity
component.In cases when, say. only onc RGB channel is affected by noise, conversion
to HSI spreads the noise to all HSE component images. Figure 6.50 shows an
example, Figure 6,50(a) shows an RGB image whose green image is corrupted
by salt-and-pepper noise, in which the probability of either salt or pepper is
0.05. The HSI component images in Figs. 6.50(b} through (d) show clearly how
the noise spread from the green RGB channel to all the HSI images. Of
course, this is not unexpected because computation of the HSL components
makes use of all RGB components, as shown in Section 6.2.3 Eo   As is true of the processes 5
color images can be cz
476 Chapter 6 m@ Color Image ProcessingEXAMPLE 6.18;
A color image
compression
example.space, depending on the process. For example, noise reduction by using an
averaging filter is the process discussed in Section 6.6.1, which we know
gives the same result in vector space as it does if the component images are
processed independently. Other filters, however, cannot be formulated in
this manner. Examples include the class of order statistics filters discussed
in Section 5.3.2. For instance, to implement a median filter in color vector
space it is necessary to find a scheme for ordering vectors in a way that the
median makes sense. While this was a simple process when dealing with
scalars, the process is considerably more complex when dealing with vectors. A discussion of vector ordering is beyond the scope of our discussion
here, but the book by Plataniotis and Venetsanopoulos [2000] is a good reference on vector ordering and some of the filters based on the ordering
concept.EX color Image CompressionBecause the number of bits required to represent color is typically three to
four times greater than the number employed in the representation of gray
levels, data compression plays a central role in the storage and transmission
of color images. With respect to the RGB, CMY(K), and HS] images of the
previous sections, the data that are the object of any compression are the
components of each color pixel (e.g., the red, green, and blue components of
the pixels in an RGB image); they are the means by which the color information is conveyed. Compression is the process of reducing or eliminating
redundant and/or irrelevant data. Although compression is the topic of
Chapter 8, we illustrate the concept briefly in the following example using a
color image.M@ Figure 6.51(a) shows a 24-bit RGB full-color image of an iris in which 8 bits
each are used to represent the red, green, and blue components. Figure 6.51(b)
was reconstructed from a compressed version of the image in (a) and is, in fact,
a compressed and subsequently decompressed approximation of it. Although
the compressed image is not directly displayable—it must be decompressed
before input to a color monitor—the compressed imiage contains only 1 data
bit (and thus 1 storage bit) for every 230 bits of data in the original image. Assuming that the compressed image could be transmitted over, say, the Internet,
in 1 minute, transmission of the original image would require almost 4 hours.
Of course, the transmitted data would have to be decompressed for viewing,
but the decompression can be done in a matter of seconds. The JPEG 2000
compression algorithm used to generate Fig. 6.51(b) is a recently introduced
standard that is described in detail in Section 8.2.10, Note that the reconstructed approximation image is slightly blurred. This is a characteristic of many
lossy compression techniques; it can be reduced or eliminated by altering the
level of compression. a
Ce mene en SummaryThe material in this chapter is an introduction lo color image processing and cavers topics
selected to provide a solid background in the techniques used in this branch of image processing. Our treatment of color fundamentals and color models was prepared as foundation
material for a field that is wide in technical scope and areas of apphcation. In particular, we
focused on color models that we felt are not only useful in digital image processing bat provide also the tools necessary for further study in tits area of image processing. The discussion of pseudocolor and full-color processing on an individual image basis provides a tic 10
techniques that were covered in some detail in Chapters 3 theengh &.The material on color vector spaces is a departure from methods that we had sted
ied before and highlights some important differcnees between gray-scale and full-color
processing. In terms of techniques, the arcas of dircet calor vector processing a           
      ® Summary 477a
bFIGURE 6.51
Color image
compression.(a) Original RGB
image. (b) Result
of compressing
and decompressing the
image in (a). numerous and inchide processes such as median and:
m Problems 479   6.3 Consider any four valid colors cy, cs, ¢;. and ¢4 with coardinates (x;, v1), (%2. ¥9).
(x3, ¥3) and (x4, y4), in the chromaticity diagram of Fig. 6,5. Derive the necessary
general expression({s) for computing the relative percentages of ¢). cp, ¢,, and C4
composing a given color that is known to lie withm the square whose vertices
are at the coordinates of c), cz, cs, and 4.6.4 In an automated assembly application, four classes of parts are to be color
coded in order to simplify detection. However, only a monochrome TV camera
is available to acquire digital images. Propose a technique for using this camera
to detect the four different colors.6.5  Inasimple RGB image. the R, G,and B component images have the horizontal
intensity profiles shown in the fotlowing diagram. What color would a person
see in the middle column of this image?    N/2 LN}
Position Position Position 6.6 Sketch the RGB components of the following image as they would appear on a
monochrome monitor. Ajl colors are at maximum intensity and saturation. In
working this problem, consider the middle gray border as part of the image. 6.7 How many different shades of gray are there in a color RGB system in which
each RGB image is an 12-bit image?
68 Consider the RGB color cube shown in Fig 6.8, and answer cach of the following:
x(a) Describe how the gray levels vary in the R, (7, and B primary images that
make up the top face of the color cube.
(b) Suppose that we replace every color in the RGB cube by us CMY color,
This new cube is dispiaved on an RGR moniter, Label with a color nant: the
eight vertices of the new cube that you would see on the screen.
478 Ghopter 6 @ Color image ProcessingDetailed solutions (o the
problems marked with a
star can be found in the
book Web site. The site
also contains suggested
projects based on the material in this chapter.morphological filters, image restoration, image compression, and many others. These
processes are not equivalent to color processing carried out on the individual component images of a color image. The references in the following section provide a pointer
to further results in this field.Our treatment of noise in color images also points out that the vector nature of the
problem, along with the fact that color images are routinely transformed from one
working space to another, has implications on the issue of how to reduce noise in these
images. In some cases, noise filtering can be done on a per-image basis, but others, such
as median filtering, require special treatment to reflect the fact that color pixels are
vector quantities, as mentioned in the previous paragraph.Although segmentation is the topic of Chapter 10 and image data compression is
the topic of Chapter 8, we gained the advantage of continuity by introducing them here
in the context of color image processing. As will become evident in subsequent discussions, many of the techniques developed in those chapters are applicable to the discussion in this chapter.References and Further ReadingFor a comprehensive reference on the science of color, see Malacara [2001]. Regarding
the physiology of color, see Gegenfurtner and Sharpe [1999]. ‘These two references,
along with the early books by Walsh [1958] and by Kiver [1965], provide ample supplementary material for the discussion in Section 6.1. For further reading on color models
(Section 6.2), see Fortner and Meyer [1997], Poynton [1996], and Fairchild [1998]. For a
detailed derivation of the HSI model equations in Section 6.2.3 see the paper by Smith
{1978} or consult the book Web site. The topic of pseudocolor (Section 6.3) is closely
tied to the general area of data visualization. Wolff and Yaeger [1993] is a good basic
reference on the use of pseudocolor. The book by Thorell and Smith [1990] also is of interest. For a discussion on the vector representation of color signals (Section 6.4), see
Plataniotis and Venetsanopoulos (2000).References for Section 6.5 are Benson [1985], Robertson [1977], and CIE [1978]. See
also the classic paper by MacAdam [1942]. The material on color image filtering
{Section 6.6) is based on the vector formulation introduced in Section 6.4 and on our
discussion of spatial filtering in Chapter 3. Segmentation of color images (Section 6.7)
has been a topic of much attention during the past ten years. The papers by Liu and Yang
[1994] and by Shafarenko et al. [1998] are representative of work in this field. A special
issue of the JEEE Transactions on Image Processing [1997] also is of interest. The discussion on color edge detection (Section 6.7.3) is from Di Zenzo [1986]. The book by
Plataniotis and Venetsanopoulos [2000] does a good job of summarizing a variety of approaches to the segmentation of color images. The discussion in Section 6.8 is based on
the noise models introduced in Section 5,2, References on image compression (Section
6.9) are listed at the end of Chapter 8. For details of software implementation of many of
the techniques discussed in this chapter, see Gonzalez, Woods, and Eddins {2004}.~Problems6.1 Give the percentages of red (X), green (Y), and blue (Z) light required to generate the point labeled “Day Light" in Fig. 6.5.*6.2 Consider any two valid colors ¢; and ¢c) with coordinates (x, y)) and (x3, v7) in
the chromaticity diagram of Fig. 6.5. Derive the necessary general expression(s)
for computing the relative percentages of colors c; and ¢. composing a given
color that is known to lie on the straight line joining these two colors.
480 Chapter @ Color Image Processing6.9* 6.10611. * 6.126.13*6,146.15(c) What can you say about the colors on the edges of the RGB color cube regarding saturation?(a) Sketch the CMY components of the image in Problem 6.6 as they would appear on a monochrome monitor.(b) If the CMY components sketched in (a) are fed into the red, green, and blue
inputs of a color monitor, respectively, describe the resulting image.Derive the CMY intensity mapping function of Eq. (6.5-6) from its RGB coun
terpart in Eq. (6.5-5).Consider the entire 216 safe-color array shown in Fig, 6.10(a). Label each cell byits (row, column) designation, so that the top left cell is (1,1) and the rightmostbottom cell is (12, 18). At which cells will you find(a) The purest red?(b) The purest yellow?Sketch the HSI components of the image in Problem 6.6 as they would appearon a monochrome monitor. ;Propose a method for generating a color band similar to the one shown in thezoomed section entitled Visible Spectrum in Fig. 6.2. Note that the band starts ata dark purple on the left and proceeds toward pure red on the right. (Hint: Usethe HSI color model.)Propose a method for generating a color version of the image shown diagram
matically in Fig. 6.13(c)}. Give your answer in the form of a flow chart. Assumethat the intensity value is fixed and piven. (Hint: Use the HSI color model.)Consider the following image composed of solid color squares. For discussingyour answer, choose a gray scale consisting of eight shades of gray, 0 through 7,where 0 is black and 7 is white. Suppose that the image is converted to HSI colorspace. In answering the following questions, use specific numbers for the grayshades if using numbers makes sense. Otherwise, the relationships “same as,”“lighter than,” or “darker than” are sufficient. If you cannot assign a specific graylevel or one of these relationships to the image you are discussing, give the reason.(a} Sketch the hue image.{b) Sketch the saturation image.(c) Sketch the intensity image. YellowMagenta Cyan > WhiteGreen  Black
w% Problems 4816.16 The following &-bit images are (left to right) the H, S, and / component images from Fig, 6.16, The numbers indicate gray-level values. Answer the following questions, explaining the basis fer your answer in each. ff it is not
possible to answer a question based on the given information, state why you
cannot do sv.*(a) Give the gray-icvei values of all regions in the hue image.
(b} Give the gray-level value of all regions in the saturation image.
{c) Give the gray-levefvalues of all regions in the intensity image.  (a) (bj6.17 Refer to Fig. 6.27 in answering the following:
(a) Why does the image in Fig. 6.27(f} exhibit predaminantly red tones?
(b) Suggest an automated procedure for coding the water in Fig, 6.27 in a
bright-blue color.
(ec) Suggest an automated procedure for coding the predominantly man-made
components in a bright red color. [Afint; Work with Fig. 6.27(f).]
*6.18 Show that the saturation component of the complement of a color image cannot
. be computed from the saturation component of the input image alone.
6.19 Explain the shape of the hue transformation function for the complement approximation in Fig, 6.33(5) using the HSI color model.
*6.20 Derive the CMY transformations to generate the complement of a colar image.
6.21 Draw the general shape of the transiormation functions used to correct excessive contrast in the RGB color space
#622) Assume thal the monitoy and printer of an imaging system are impericctly calibrated. An image that looks balanced on the monitor appears cyan in print. Deseribe genera transformations that might correct the imbalance,
6.23 Compute the L*a*h* components of the image in Problem 6.6 assuming
X 1} [0588 0179 0.183 1 R”y}=| 029 0.606 0.105 |] G
lz L 0 0.068 1021 || 8   This matrix equation defines the tristimulus values of the cotors generated by
standard National Television System Committee (NTSC) calor 1!V phosphors
viewed onder 65 standard ilumination (Benson (1985]).*6.24. How would vou implement the color equivalent of pray scale histogram match
ing (specification) from Section 2.2.27
482 Copter m Color Image Processing6.256.26Consider the following 1000 x 1000 RGB image, in which the squares are fullysaturated red, green, and blue, and each of the colors is at maximum intensity[e-g.. (1, 0,0) for the red square). An HSI image is generated from this image.(a) Describe the appearance of each HS] component image.{b) The saturation component of the HS! image is smoothed using an averaging
mask of size 250 x 250, Describe the appearance of the result (you may ignore image border effects in the filtering operation).{c) Repeat (b) for the hue image.   Show that Eq. (6.7-2) reduces to Eq. (6.7-1) when C = I, the identity matrix.6.27 *%(a) With reference to the discussion in Section 6.7.2, give a procedure (in flow6.286.29chart form) for determining whether a color vector (point) z is inside a cube
with sides W, centered at an average color vector a. Distance computationsare not allowed.
(b) This process also can be implemented on an image-by-image basis if the box
is lined up with the axes. Show how you would do it.Sketch the surface in RGB space for the points that satisfy the equation
DQ, a) = ((2 - a)? C'"z — a) = Dy
where Dy is a specified nonzero constant. Assume that a = 0 and that8 0 0
c=);0 1 0
0 0 1Refer to Section 6.7.3. One might think that a logical approach for defining the
gradient of an RGB image at any point (x, y) would be to compute the gradient
vector (see Section 3,6.4) of each component image and then form a gradient
vector for the color image by summing the thrge individual gradient vectors,
Unfortunately, this method can at times yield erroneous results. Specifically, it is
possible for a color image with clearly defined edges to have a zero gradient if
this method were used. Give an example of such an image. (Hint: Set one of the
color planes to a constant value to simplify your analysis. )
Wavelets and
Multiresolution
ProcessingAll this time, the guard was looking at her, first through
a telescope, then through a microscope, and then
through an opera glass. °Lewis Carrol, Through the Looking Glass PreviewAlthough the Fourier transform has been the mainstay of transform-based
image processing since the late 1950s, a more recent transformation, called the
wavelet transform. is now making it even easier to compress, transmit, and analyze many images. Unlike the Fourier transform, whose basis functions are sinusoids, wavelet transforms are based on small waves, called wavelets, of
varying frequency and limited duration. This allows them to provide the equivalent of a musical score for an image, revealing not only what notes (or frequencies) to play but also when to play them. Fourier transforms, on the other
hand, provide only the notes or frequency information, temporal information
is lost in the transformation process,In 1987, wavelets were first shown to be the foundation of a powerful new
approach to signal processing and analysis called multiresolution theory (Mallat
[1987]). Multiresolution theory incorporates and unifies techniques from a variety of disciplines, including subband coding from signal processing, quadrature
mirror filtering from digital speech recognition, and pyramidal image processing.
As its name implies, multiresolution theory is concerned with the representation
and analysis of signals (or images) at more than one resolution, The appeal of
such an approach is obvious— features that might go undetected at one.resolution may be easy to detect af another. Although the imaging community’s interest in multiresolution analysis was limited until the late 1980s, it 1s now difficult to
keep up with the number of papers, theses. and books devoted to the subject.483
484 Chapter 7 @ Wavelets and Multiresolution ProcessingLacal histograms are
histograms of the pixels
in a neighborhood (see
Section 3.3.3),FIGURE 7.1An image and its
local histogram
variations.In this chapter, we examine wavelet-based transformations from a multiresolution point of view. Although such transformations can be presented in other
ways, this approach simplifies both their mathematical and physical interpretations. We begin with an overview of imaging techniques that influenced the formulation of multiresolution theory, Our objective is to introduce the theory’s
fundamental concepts within the context of image processing and simultaneously provide a brief historical perspective of the method and its application.
The bulk of the chapter is focused on the development and use of the discrete
wavelet transform. To demonstrate the usefulness of the transform, examples
ranging from image coding to noise removal and edge detection are provided,
In the next chapter, wavelets will be used for image compression, an application
in which they have received considerable attention.BackgroundWhen we look at images, generally we see connected regions of similar texture
and intensity levels that combine to form objects. If the objects are small in
size or low in contrast, we normally examine them at high resolutions; if they
are large in size or high in contrast, a coarse view is all that is required. If both
small and large objects —or low- and high-contrast objects —are present simultaneously, it can be advantageous to study them at several resolutions. This, of
course, is the fundamental motivation for multiresolution processing.From a mathematical viewpoint, images are two-d}mensionat arrays of intensity values with locally varying statistics that result from different combinalions
of abrupt features like edges and contrasting homogeneous regions. As illustrated
in Fig, 7.1—an image that will be examined repeatedly in the remainder of the
7.1 m Background 485section—local histograms can vary significantly from one part of an image to
another, making statistical modeling over the span of an entire image a difficult, or impossible task,7.1.1 Image PyramidsA powerful, yet conceptually simple structure for representing images at more
than one resolution is the image pyramid (Burt and Adelson [1983]). Originally
devised for machine vision and image compression applications, an image
pyramid is a collection of decreasing resolution images arranged in the shape
of a pyramid. As can be seen in Fig. 7.2(a), the base of the pyramid contains a
high-resolution representation of the image being processed; the apex contains a low-resolution approximation. As you move up the pyramid, both size
and resolution decrease. Base level J is of size 2’ x 27 or N X N, where
J = log, N, apex level 0 is of size 1 x 1, and general level is of size 2/ x 2/,
where 0 = j = J. Although the pyramid shown in Fig. 7.2(a) is composed of
J + 1 resolution levels from 2’ x 2/ to 2° x 2°, most image pyramids are truncated to P + 1] levels, where 1 = P= Jandj=J- P,...,J -2,3 -1,/.
That is, we normally limit ourselves to P reduced resolution approximations of
the original image; a 1 X 1 (i¢., single pixel) approximation of a 512 x 512
image, for example, is of little value. The tota] number of pixels in a P + 1 level
pyramid for P > isN7(1 + a + = peep te) ety(ay 4)
Figure 7.2(b} shows a simple system for constructing two intimately related
image pyramids. The Levef j — 1 approximation output provides the images>
1x 1 Zeb Level 0 (apex)
: > Level 1
2x 2 ert!Pe |cvel 2 x   4 Level J (base)Downsampler
(rows and columns)—-| Spreximation —
filterLevel j - 1
approximation 2 Upsampte:(rows and columns)(“Interpolation |
__filter  Prediction~~} — Level j
prediction
residualLevel
input bnage a
bFIGURE 7.2(a) An image
pyramid. (b) A
simple system for
creating
approximation
and prediction
residual pyramids.
486 Chapter 7 m Wavelets and Multiresolution Processingtn general. a prediction
residual can be defined
as the difference
between an image and a
predicted version of the
image. As witl be seen in
Section 8.2.9, prediction
residuals can often be
coded more efficiently
than 2.D intensity arrays.needed to build an approximation pyramid (as described in the preceding
paragraph), while the Level j prediction residual output is used to build a
complementary prediction residual pyramid. Unlike approximation pyramids,
prediction residual pyramids contain only one reduced-resolution approximation of the input image (at the top of the pyramid, level / — P). All other
levels contain prediction residuals, where the level j prediction residual (for
J-P+1=js J)is defined as the difference between the level j approximation (the input to the block diagram) and an estimate of the level j approximation based on the level j - 1 approximation (the approximation output in
the block diagram).As Fig. 7.2(b) suggests, both approximation and prediction residual pyramids are computed in an iterative fashion. Before the first iteration, the image
to be represented in pyramidal form is placed in level J of the approximation
pyramid. The following three-step procedure is then executed P times—for
j=J,F-1,...,andJ — P+ 1 (in that order):Step 1. Compute a reduced-resolution approximation of the Level j input
image [the input on the left side of the block diagram in Fig. 7.2(b)]. This is
done by filtering and downsampling the filtered result by a factor of 2. Both
of these operations are described in the next paragraph. Place the resulting
approximation at level j — 1 of the approximation pyramid.Step 2. Create an estimate of the Level j input image from the reducedresolution approximation generated in step 1. This is done by upsampling
and filtering (see the next paragraph) the generated approximation. The resulting prediction image will have the same dimensions as the Level j input
image.Step 3. Compute the difference between the prediction image of step 2
and the input to step 1, Place this result in level j of the prediction residual
pyramid.At the conclusion of P iterations (i.e. following the iteration in which
f=J—- P+ 1), the level J — P approximation output is placed in the prediction residual pyramid at level J — P. If a prediction residual pyramid is not
needed, this operation—along with steps 2 and 3 and the upsampler, interpolation filter, and summer of Fig. 7.2(b)—-can be omitted. .A variety of approximation and interpolation filters can be incorporated
into the system of Fig. 7.2(b). Typically, the filtering is performed in the spatial
domain (see Section 3.4). Useful approximation filtering techniques include
neighborhood averaging (see Section 3.5.1.), which produces mean pyramids,
lowpass Gaussian filtering (see Sections 4.7.4 and 4.8.3), which produces
Gaussian pyramids; and no filtering, which results in subsampling pyramids.
Any of the interpoiation methods described in Section 2.4.4, including nearest
neighbor, bilinear, and bicubic, can be incorporated into the interpolation filter. Finally, we note that the upsampling and downsampling blocks of Fig.
7.2(b) are used to double and halve the spatial dimensions of the approximation and prediction images that are computed. Given an integer variable » and
1-D sequence of samples f(1). upsampled sequence fy:(n) is defined as
7.1 w Background 487fey [00 Msemm aswhere, as is indicated by the subscript, the upsampling is by a factor of 2. The
complementary operation of downsampling by 2 is defined asfa(n) = f2n) (7.1-2)Upsampling can be thought of as inserting a 0 after every sample in a sequence;
downsampling can be viewed as discarding every other sample. The upsampling
and downsampling blocks in Fig. 7.2(b), which are labeled 21 and 21, respectively,
are annotated to indicate that both the rows and columns of the 2-D inputs on
which they operate are to be up- and downsamplied. Like the separable 2-D DFT
in Section 4.11.1,2-D upsampling and downsampling can be performed by successive passes of the 1-D operations defined in Eqs, (7.1-1) and (7.1-2).M Figure 7.3 shows both an approximation pyramid and a prediction residual
pyramid for the vase of Fig. 7.1. A lowpass Gaussian smoothing filter (see
Section 4.7.4) was used to produce the four-level approximation pyramid in
Fig. 7.3(a). As you can see, the resulting pyramid contains the original
512 X 512 resolution image (at its base) and three low-resolution approximations (of resolution 256 < 256,128 x 128, and 64 X 64). Thus, P is 3 and levels
9, 8,7, and 6 out of a possible log (512) + 1 or 10 levels are present. Note the
reduction in detail that accompanies the lower resolutions of the pyramid. The
level 6 (i.e., 64 x 64) approximation image is suitable for locating the window
stiles {i.e., the window pane framing), for example, but not for finding the stems
of the plant. In general, the lower-resolution levels of a pyramid can be used for
the analysis of large structures or overall image context; the high-resolution images are appropriate for analyzing individual object characteristics. Such a
coarse-to-fine analysis strategy is particularly useful in pattern recognition.A bilinear interpolation filter was used to produce the prediction residual
pyramid in Fig. 7.3(b). In the absence of quantization error, the resulting prediction residual pyramid can be used to generate the complementary approximation pyramid in Fig. 7.3(a), including the original image, without error. To do so,
we begin with the level 6 64 x 64 approximation image (the only approximation image in the prediction residual pyramid), predict the level 7 128 X 128 resolution approximation (by upsampling and filtering), and add the level 7
prediction residual. This process is repeated using successively computed approximation images until the original 512 x 512 image is generated. Note that
the prediction residual histogram in Fig. 7.3(b) is highly peaked around zero; the
approximation histogram in Fig. 7.3(a) is not. Unlike approximation images, prediction residual images can be highly compressed by assigning fewer bits to the
more probable values (see the variable-length codes of Section 8.2.1). Finally, we
note that the prediction residuals in Fig. 7.3(b} are scaled to make small prediction errors more visible; the prediction residual histogram, however, is based on
the original residual values, with leve] 128 representing zero error. ®Jn this chapter, we will be
working with bolh
continuous und discrete
functions and variables.
With the notable
exception of 2-D image
f(x,y) and unless otherwise noted, x, y, Z,-.. ate
continuous variables;Fe a eee
discrete variables. EXAMPLE 7.1:
Approximation
and prediction
residual pyramids.
488 Chapter 7 @ Wavelets and Multiresolution ProcessingbFIGURE 7.3Two image
pyramids and
their histograms:
(a) an
approximation
pyramid;{b) a prediction
residual pyramid.The approximation
pyramid in (2) is called a
Gaussian pyramid
because a Gaussian filter
was used to construct it.
‘The prediction residual
pyramid in (b) is often
calleda Laplacian
pyramid; note the
similarity tn appearance
with the Laplacian filtered images in Chapter 3.   7.4.2 Subband CodingAnother important imaging technique with ties to multiresolution analysis is
subband coding. In subband coding, an image is decomposed into a set of
bandlimited companents. called subbands, The decomposition is performed so
that the subbands can be reassembled to reconstruct the original image without error. Because the decomposition and reconstruction are performed by
means of digital filters, we begin our discussion with a brief introduction to
digital signal processing (DSP) and digital signal fittering.The term “delay” implies Consider the simple digital filter in Fig. 74(a) and note that it is constructeda time-based input from three basic components ~~ wt? delays, multipliers, and adders, Along the
sequence and reflects the : . : : ‘fact that in digital signat top of the filter, unit delays arc connected in series to create K-~ | delayed
filtering, the inputs (ie., right shifted) versions of the input sequence f(71). Detaved sequenceusually a sampicd analog "
signal. F(n — 2), for example, is
7.1 @ Background 489yy — J #0) forn =2
PMD" V0) torn =2 4123As the grayed annotations in Fig. 7.4(a} indicate, input sequence f(n) =
f(a ~ 0) and the K — 1 delayed sequences at the outputs of the unit delays,
denoted f(n — 1), f(n — D,...,f(a ~ K +1), are multiplied by constants
h(0), h(1),..., ACK — 1), respectively, and summed to produce the filtered
output sequenceoo
f(n)= 3X Ak)f(n - k)
k=
= Fn) & hn) ars)
where * denotes convolution. Note that—except for a change in variables—
Eq, (7.1-3) is equivalent to the discrete convolution defined in Eq. (4.4-10) of
Chapter 4. The K multiplication constants in Fig. 7.4(a) and Eq. (7.1-3) are      dQyitay AU  If the coefficients of the
filter in Fig. 7.4(a} are
indexed using values of 1
between Oand K - 1 (as
we have done), the limits
on the sum in Eq. (7.1-3)
can be reduced ta (10K ~ 1 [bike Eq. (44-10)}.-(+) 7 -(*) fin = fomivfusy Alife LoyAant(ad © Beta © Poe eat yita 2)Noathifi A$ 0 Hari Ate)x
~ nO)A(K ~1) Aa)Input sequence f(r) = 5{n)
o
Impulse response h(n} AO)
tt res  -1 -1
-1012... .. K-) -1012..a
beFIGURE 7.4 (a) A digital filler; (b) a unit discrete impulse sequence; and (c) the impulse response of the filter.
7.1 w Background 491Filters A3(”) and A4(m) in Figs. 7.5(c) and (d) are order-reversed versions of
hy(n):h3(n) = hy(-n) (7.1-6)
A,n) = hy(K - 1-1) (7.1-7)Filter 43(7) is a reflection of (7) about the vertical axis; filter A4(”) is a reflected and translated (i.e.) shifted) version of 4,(m). Neglecting translation,
the responses of the two filters are identical. Filter A5(7) in Fig. 7.5(e), which is
defined ash(n) = (-1)"hy() (7.1-8)is called a modulated version of h,(). Because modulation changes the signs
of al! odd-indexed coefficients [i.e., the coefficients for which n is odd in
Fig. 7.5(e)], (1) = —A,(1) and 45(3) = ~A1(3), while A5(0) = h,(0) and
h.{2) = hy(2). Finally, the sequence shown in Fig. 7.5(f) is an order-reversed
version of /,(n) that is also modulated:Ag(n) = (-1)"hy(K ~ 1 — 2) (7.1-9)This sequence is included to illustrate the fact that sign reversal, order reversal, and modulation are sometimes combined in the specification of the relationship between two filters.With this brief introduction to digital signal filtering, consider the two-band
subband coding and decoding system in Fig. 7.6(a). As indicated in the figure,
the system is composed of two filter banks, each containing two FIR filters of
the type shown in Fig. 7.4(a). Note that each of the four FIR filters is depicted  for)   Order reversal is often
called time reversal when
the input sequence is a
sampled analog signal.A filter bank is a calleeion of two er muce fillersa
bFIGURE 7.6{a) A two-band
subband coding
and decoding
system, and (b) its
spectrum splitting
properties.
1K — 1 a)
0 0 | 2 )
fl } | I '
1 l 1
! | 1
1 | . '
I -1 i j |     7372-10 125 45 47 --72-2-10 £2 R45 67 ” -3-2-1O0 1234567n 4 ”
abc
def
FIGURE 7.5 Six functionalty related filter impulse responses: (a) reference response; (b) sign reversat:(c) and (d) order reversal (differing by the delay introduced); (¢) modulation: and (f) order reversal and
modulation.
7.1 m Background 493
(gi(1), a(n + 2m)} = SF — f)8(m), ij = {0,1} (7.1-13)which defines orthonormality for perfect reconstruction filter banks. In addition to Eq. (7.1-13), orthonormal filters can be shown to satisfy the following
two conditions:&i(n) = (-1)"Go(Keven -1- n)
h,(n) = 8: {Keven ~1i- n), is {0, 1}(7.1-14)where the subscript on Keyen is used to indicate that the number of filter coefficients must be divisible by 2 {i.e., an even number). As Eq. (7.1-14) indicates,
synthesis filter g, is related to gy by order reversal and modulation. In addition, both Ag and A; are order-reversed versions of synthesis filters, gp and g;,
respectively. Thus, an orthonormal filter bank can be developed around the
impulse response of a single filter, called the prototype; the remaining filters
can be computed from the specified prototype’s impulse response. For
biorthogonal filter banks, two prototypes are required; the remaining filters
can be computed via Eq. (7.1-10) or (7.1-11). The generation of useful prototype filters, whether orthonormal or biorthogonal, is beyond the scope of this
chapter. We simply use filters that have been presented in the literature and
provide references for further study.Before concluding the section with a 2-D subband coding example, we note
that 1-D orthonormal and biorthogonal filters can be used as 2-D separable
filters for the processing of images. As can be seen in Fig, 7.7, the separable filters are first applied in one dimension (e.g., vertically) and then in the other
(e.g., horizontally) in the manner introduced in Section 2.6.7. Moreover, downsampling is performed in two stages—once before the second filtering operation to reduce the overall number of computations. The resulting filtered   
  
   7a FIGURE 7.7
* hg(n)- a(m,n) Atwodimensional, fourtalons) band filter bank
for subbandimage coding.  & Ay{n) 2 --— d"(m,n)Rows
(along m)   f(m,n) Columnsw g(r) 2i dm.) Columns   2) LannyColumns
492 — Chopter 7 m Wavelets and Multiresolution ProcessingBy real-coefficient, we
mean that the fiher
coefficients are reat (not
complex) numbers.Equations (7.1-10)
Ubrough {7.1-14) are
described in detail in thefilter bank literature (see.for exampie, Velterli and
Kovacevic (1995}}.as a single block in Fig. 7.6(a), with the impulse response of each filter (and the
convolution symbol) written inside it. The analysis filter bank, which includes
filters hg(n) and /,(n), is used to break input sequence f(n) into two halflength sequences f\,(”) and fip(v), the subbands that represent the input. Note
that filters Ag(n) and 4,(7) are half-band filters whose idealized transfer characteristics, Hy and H,, are shown in Fig. 7.6(b). Filter Ag(m) is a lowpass filter
whose output, subband f,,(7), is called an approximation of f(n); filter hy(2) is
a highpass filter whose output, subband fi, (7), is called the high frequency or
detail part of f(n). Synthesis bank filters go(n) and g,(n) combine f\,(m) and
Fap() to produce f(n). The goal in subband coding is to select Ao(n), A;(n),
8o(n), and g, (7) so that fn) = f (2). That is, so that the input and output of the
subband coding and decoding system are identical. When this is accomplished,
the resulting system is said to employ perfect reconstruction filters.There are many two-band, real-coefficient, FIR, perfect reconstruction filter banks described in the fitter-bank literature. In all of them, the synthesis filters are modulated versions of the analysis filters—with one (and only one)
synthesis filter being sign reversed as well. For perfect reconstruction, the impulse responses of the synthesis and analysis filters must be related in one of
the following two ways:Bal) = (—1)"Ar(n)(7.1-10)
gi(m) = (-1)"* p(n)or=(-1)"*th
Bo(7t) = * i(n) (74-1)
gilt) = (—1)"ho(n)
Filters Aig(n), /y(), go(), and gi(") in Eqs. (7.1-10) and (7.1-11) are said to be
cross-modulated because diagonally opposed filters in the block diagram ofFig. 7.6(a) are related by modulation [and sign reversal when the modulation
factor is —(-1)" or (—1)"* +]. Moreover, they can be shown to satisfy the fol
lowing biorthogonality condition:
(hi(2n — k), g(k)) = 6 — fd(n), if = {0,1} (7.1-12)Here, (h;(2n — k), gj(k)} denotes the inner product of h;(2n — k) and gj(k).!
When / is not equal to j, the inner product is 0; when i and ; are equal, the
product is the unit discrete impulse function, 5(1). Biorthogonality will be considered again in Section 7.2.1.Of special interest in subband coding —and in the development of the fast
wavelet transform of Section 7.4—are filters that move beyond biorthogonalityand require *The vector inner product of sequences f(a) and f,(n) is (fi, f2) = Dfile)foln), where the * denotes
the complex conjugate operation. If f\(7) and f>(n) are real, (f. f) = (fo fi).
494 Chapter 7 # Wavelets and Multiresolution ProcessingEXAMPLE 7.2:
A four-band
subband coding of
the vase in Fig. 7.1.TABLE 7.1
Daubechies 8-tap
orthonormal filter
coefficients for
8o(7t) (Daubechies
[1992]).ab
edFIGURE 7.8The impulse
responses of four
8-tap Daubechies
orthonormal
filters. SeeTable 7.1 for the
values of go(#) for
O=nr=7.outputs, denoted a(m, x), d’(m, 2), d4#(m,n), and d?(m,n) in Fig. 7.7, are
called the approximation, vertical detail, horizontal detail, and diagonal detail
subbands of the input image, respectively. These subbands can be split into
four smaller subbands, which can be split again, and so on—a property that
will be described in greater detail in Section 7.4,MM Figure 7.8 shows the impulse responses of four 8-tap orthonormal filters.
The coefficients of prototype synthesis filter gy(#) for 0 = n = 7 [in Fig. 7.8(c)]
are defined in Table 7.1 (Daubechies [1992]). The coefficients of the remaining
orthonormal filters can be computed using Eq. (7.1-14). With the help of Fig.
7.5, note (by visual inspection) the cross modulation of the analysis and synthesis filters in Fig. 7.8. It is relatively easy to show numerically that the filters areae &oln)0 0,23037781
1071484657
2 0.63088076
3 —0.02798376
4 —0.1870348)
5 0.03084 138
6 0.03288301
7 --0,01059740  fig( 2)
1en9 oe eeAyn)
1 -05
-1 n ~ ] wet tt
0 2 4 6 8 Q 2 4 6 8
tr) (2)
7.1 m@ Background 495 both biorthogonal (they satisfy Eq.7.1-12) and orthonormal (they satisfy Eq. 7.113). As a result, the Daubechies 8-tap filters in Fig, 7.8 support error-free reconstruction of the decomposed input.A four-band split of the 512 x 512 image Of a-vase in Fig. 7.1, based on the
filters in Fig, 7.8, is shown in Fig. 7.9. Each quadrant of this image is a subband
of size 256 X 256. Beginning with the upper-left corner and proceeding in a
clockwise manner, the four quadrants contain approximation subband a, horizontal detail subband @”, diagonal detail subband @”, and vertical detail subband a”, respectively. All subbands, except the approximation subband in
Fig. 7.9(a), have been scaled to make their underlying structure more visible.
Note the visual effects of aliasing that are present in Figs. 7.9(b) and (c)— the d#
and d” subbands. The wavy lines in the window area are due to the downsanipling of a barely discernable window screen in Fig. 7.1. Despite the aliasing, the
original image can be reconstructed from the subbands in Fig. 7.9 without
error. The required synthesis filters, xo() and g,(#), are determined from
Table 7.1 and Eq. (7.1-14), and incorporated into a filter bank that roughly
mirrors the system in Fig. 7.7. In the new filter bank, filters A,(1) for i = 40, 1}
are replaced by their g,(7) counterparts, and upsanplers and summers are
added. ®ab
edFIGURE 7.9A four-band split
of the vase inFig. 7.1 using the
subband coding
system of Fig. 7.7.
The four
subbands that
result are the{a) approximation,
(b) horizontal
detail, (c) vertical
detail, and(d) diagonal detail
subbands,Sev Section 4.5.4 for
more on aliasing:
496 Chopter7 m Wavelets and Multiresolution Processing7.1.3 The Haar Transform
The third and final imaging-related operation with ties to multiresolution
analysis that we will look at is the Haar transform (Haar [1910]), Within
the context of this chapter, its importance stems from the fact that its basis
functions (defined below) are the oldest and simplest known orthonormal
wavelets. They will be used in a number of examples in the sections that
follow.With reference to the discussion in Section 2.6.7, the Haar transform can be
expressed in the following matrix formT = HEH? (71-15). where F is an N X N image matrix, H is an N X N Haar transformation
matrix, and T is the resulting N X N transform. The transpose is required
because H is not symmetric; in‘Eq. (2.6-38) of Section 2.6.7, the transformation matrix is assumed to be symmetric. For the Haar transform, H contains
the Haar basis functions, 4,{z). They are defined over the continuous, closed
interval ze [0,1] for k = 0,1,2,...,N ~ 1, where N = 2". To generate H,
we define the integer & such that k = 2" + q~- 1,where0 = p=n-— 1,
q = Oorl for p = 0, and1 = q = 2? for p # 0. Then the Haar basis funcflons areho(z) = hoo(z) = He ze [0,1] (7.1-16)and1 (2 @= 1h sz < @ ~ 0592"
flz) = Apg(2) = s4 -2??  (q — 0.5)/2? = z < g/2?VN 0 otherwise, z € (0, 1] (7.1-17)The ith row of an N X N Haar transformation matrix contains the elements
of h;(z) for z = 0/N,1/N,2/N,...,(N — 1)/N. For instance, if N = 2, the
first row of the 2 X 2 Haar matrix is computed using #)(z) with z = 0/2, 1/2.
From Eq. (7.1-16), Ay(z) is equal to 1/2, independent of z, so the first row of
H, has two identical 1/ V2 elements. The second row is obtained by computing
Ay(z) for z = 0/2,1/2. Because k = 2”? + q- [| when k = 1,p = 0 and
g = 1. Thus, from Eq. (7.1-17), 4\(0) = 2°¢V2 = 1/V2, A, (1/2) = -2°7V2
= —1/V2, and the 2 x 2 Haar matrix is1f1 1
m= Ja ‘| (7.1-18)
7.1 m@ Background 497If N = 4,k,q, and p assume the values and the 4 x 4 transformation matrix, Hy, is(7.1-19)
0 0 v2 -vVOur principal interest in the Haar transform is that the rows of H, can be used
to define the analysis filters, 4p(n) and f,(n), of a 2-tap perfect reconstruction
filter bank (see the previous section), as well as the scaling and wavelet vectors
(defined in Sections 7.2.2 and 7.2.3, respectively) of the simplest and oldest
wavelet transform (see Example 7.10 in Section 7.4). Rather than concluding
the section with the computation of a Haar transform, we close with an example that illustrates the influence of the decomposition methods that have been
considered to this point on the methods that will be developed in the remainder
of the chapter.& Figure 7.10(a) shows a decomposition of the 512 512 image in Fig. 7.1 EXAMPLE 7.3:
that combines the key features of pyramid coding, subband coding, and the Haar functions in
Haar transform (the three techniques we have discussed so far). Called the a discrete wavele!
discrete wavelet transform (and developed later in the chapter), the represen- :
tation is characterized by the following important features:1. With the exception of the subimage in the upper-left corner of Fig. 7.10(a),
the local histograms are very similar. Many of the pixels are close to zero.
Because the subimages (except for the subimage in the upper-left corner)
have been scaled to make their underlying structure more visible, the displayed histograms are peaked at intensity 128 (the zeroes have been
scaled to mid-gray). The large number of zeroes in the decomposition
makes the image an excellent candidate for compression (see Chapter 8).2. In a manner that is similar to the way in which the levels of the prediction
residua] pyramid of Fig. 7.3(b) were used to create approximation images
of differing resolutions, the subimages in Fig. 7.10(a) can be used to construct both coarse and fine resolution approximations of the original
vase image in Fig. 7.1. Figures 7.10(b) through (d), which are of size
498  Chopter7 m Wavelets and Multiresolution Processing&
FIGURE 7.10(a) A discrete
wavelet transform
using Haar H,
basis functions. Its
local histogram
variations are also
shown. (b)—(d)
Several different
approximations
(64 x 64,128 X 128, and
256 X 256) that
can be obtained
from {a}. 64 x 64,128 * 128, and 256 X 256, respectively, were generated from
the subimages in Fig. 7.10{a). A perfect 512 x 512 reconstruction of the
original image is also possible. .3. Like the subband coding decomposition in Fig, 7.9, a simple real-coefficient.
FIR filter bank of the form given in Fig. 7.7 was used to produce Fig. 7. 1(a).
After the generation of a four subband image like that of Fig, 7.9, the
256 X 256 approximation subband was decomposed and replaced by four
128 X 128 subbands (using the same filter bank), and the resulting approximation subband was again decomposed and replaced by four 64 x 64 subbands. This process produced the unique arrangement of subimages thal
7.2 @ Multiresolution Expansionscharacterizes discrete wavelet transforms. The subimages in Fig. 7.10(a)
become smaller in size as you move from the lower-right-hand to upperleft-hand corner of the image.4. Figure 7.10(a) is not the Haar transform of the image in Fig. 7.1. Although
the filter bank coefficients that were used to produce this decomposition
were taken from Haar transformation matrix Hb), a variety of othronormal
and biorthogonal filter bank coefficients can be used in discrete wavelet
transforms. .5. As will be shown in Section 7.4, each subimage in Fig. 7.10(a) represents a
specific band of spatial frequencies in the original image. In addition,
many of the subimages demonstrate directional sensitivity [e.g., the
subimage in the upper-right corner of Fig. 7.10(a) captures horizontal edge
information in the original image].Considering this impressive list of features, it is remarkable that the discrete
wavelet transform of Fig. 7.10(a) was generated using two 2-tap digital filters
with a total of four filter coefficients. &KE® Multiresolution ExpansionsThe previous section introduced three well-known imaging techniques that
play an important role in a mathematical framework called muttiresolution
analysis (MRA). In MRA, a scaling function is used to create a series of approximations of a function or image, each differing by a factor of 2 in resolution from its nearest neighboring approximations. Additional functions, called
wavelets, are then used to encode the difference in information between adjacent approximations.7.2.) Series Expansions
A signal or function f(x) can often be better analyzed as a linear combinationof expansion functions
~fQ) = Dauee(s) (72-1)where & is an integer index of a finite or infinite sum, the a, are real-valued
expansion coefficients, and the ¢,(x) are real-valued expansion functions. If
the expansion is unique — that is, there is only one set of a, for any given f(x)—
the ¢,{x) are called basis functions, and the expansion set, ey is called a
basis for the class of functions that can be so expressed. The expressible functions form a function space that is referred to as the closed span of the expansion set, denotedV= Span{ ¢x(x)} (7.2-2)To say that f(x) « V means that f(x) is in the closed span of {g,(x)} and can
be written in the form of Eq. {7.2-1).499
S500 Chapter 7 @ Wavelets and Multiresolution ProcessingFor any function space V and corresponding expansion set {95(x) } there isa
set of dual functions denoted {%,(x)} that can be used to compute the a, coefficients of Eq. (7.2-1) for any f(x) € V. These coefficients are computed by taking
the integral inner products' of the dual ¢,(x) and function f(x). That is,ay = (6x2), f(x) = | e(x)f (x) dx (7.2-3)where the * denotes the complex conjugate operation. Depending on the orthogonality of the expansion set, this computation assumes one of three possible forms. Problem 7.10 at the end of the chapter illustrates the three cases
using vectors in two-dimensional Euclidean space.Case J: If the expansion functions form an orthonormal basis for V,meaning that .as 0 j#k(@)(2), e409) = 8x = { ink (7.2-4)the basis and its dual are equivalent. That is, p,(x) = ¢(x) and Eq. (7.2-3)
becomesay, = (x(x), f(x)) (7.2-5)The a, are computed as the inner products of the basis functions and f(x).Case 2: If the expansion functions are not orthonormal, but are an orthogonal basis for V, then(9;(2), oR (x)} =O FRR (7.2-6)and the basis functions and their duals are called biorthogonal. The a, are
computed using Eq. (7.2-3), and the biorthogonal basis and its dua] are
such that~ O j#k
(9x), BAX)) = Bie = {i j ok (7.2-7)Case 3: If the expansion set is not a basis for V, but supports the expansion defined in Eq. (7.2-1), it is a spanning set in which there is more than
one set of a, for any f(x) e V. The expansion functions and their duals are
said to be overcomplete or redundant. They form a frame in which"Alf@oP = > Kees), FO)? = BIFOYP (7.28)  ‘The integral inner product of two real or complex-valued functions f(x) and g(x) is (f(x), e()}) =
[rou dx. Hf f(x) is real, f°(x) = f(x) and (f(x), g{x)) - [reo dx,*The norm of f(x), denoted f(x} is defined as the square root of the absolute value of the inner produet of f(x) with itself.
7.2 @ Multiresolution Expansionsfor some A > 0, B < 00, and all f(x) eV. Dividing this equation by the
norm squared of f{x), we see that A and B “frame” the normalized inner
products of the expansion coefficients and the function. Equations similar
to {7.2-3) and (7.2-5) can be used to find the expansion coefficients for
frames. If A = B, the expansion set is called a tight frame and it can be
shown that (Daubechies [1992])f= FZ Dla) Fen (72-9)Except for the A! term, which is a measure of the frame’s redundancy,
this is identical to the expression obtained by substituting Eq. (7.2-5) (for
orthonormal bases) into Eqs. (7.2-1).7.2.2 Scaling Functions
Consider the set of expansion functions composed of integer translations and
binary scalings of the real, square-integrable function g(x); this is the set
{en}. where .9; (x) = 2PeQix — k) (7.2-10)for all j,k eZ and (x) ¢ L7(R).' Here, k determines the position of ,,,(x)
along the x-axis, and j determines the width of ¢; ,(x)—that is, how broad or
narrow it is along the x-axis. The term 2/” controls the amplitude of the function. Because the shape of g; ,(x) changes with j, ¢(x) is called a scaling function.
By choosing (x) properly, {¢;,.(x)} can be made to span L?(R), which is the
set of all measurable, square-integrable functions.If we restrict j in Eq. (7.2-10) to a specific value, say j = jo, the resulting
expansion set, {9),4(x)} is a subset of {o;,2(x)} that spans a subspace of L?(R).
Using the notation of the previous section, we can define that subspace asVi, = Span{ oj...) (7.2-11)
That is, Vj, is the span of g;, x(x) over k. If fx) eV), we can writefix) = Zane a0) (7.212)
More generally, we will denote the subspace spanned over & for any j asV; = Span{ejx(x)} (7.2-13)As will be seen in the following example, increasing j increases the size of V,,
allowing functions with smaller variations or finer detail to be included in the
subspace. This is a consequence of the fact that, as j increases, the , ,(x) that
are used to represent the subspace functions become narrower and separated
by smaller changes in x. “The notation Z?{R), where R is the set of rea! numbers. denotes the set of measurable. square-integrable.
one-dimensional functions; Z is the set of integers.501
502 Chapter 7 mt Wavelets and Multiresolution ProcessingEXAMPLE 7.4;
The Haar scaling
function.ab
cd
efFIGURE 7.11
Some Haar
scaling functions,@ Consider the unit-height, unit-width scaling function (Haar [1910])1 O<x<1
#(%) {i otherwise (72-14)
Figures 7.11{a) through (d) show four of the many expansion functions that
can be generated by substituting this pulse-shaped scaling function into
Eg. (7.2-10). Note that the expansion functions for j = 1 in Figs. 7.11(c} and
(d) are half as wide as those for j = 0 in Figs. 7.11(a) and (b). For a given interval on x, we can define twice as many V, scaling functions as Vy scaling functions (€.g., @1,9 and ¢) , of V; versus gp 9 of Vp for the intervalO = x < 1).
Figure 7.11(e) shows a member of subspace Vj. This function does not beJong to Vy, because the VY) expansion functions in 7.11(a)} and (b) are too
coarse to represent it. Higher-resolution functions like those in 7.11(c) and (d)Puul*) = oC) ¥oi(t) = ex - 1)   wl grolx) = V2 p(2x)         0 I 2 3
7.2 @ Multiresolution Expansions 503are required. They can be used, as shown in (e), to represent the function by
the three-term expansionF(x) = 0.591.0(2) + G1,104) — 0.25¢; 4(x)To conclude the example, Fig. 7.11(f) illustrates the decomposition of
¥o,0(x) as a sum of V, expansion functions. In a similar manner, any Vy expansion function can be decomposed using1 1
0,x(x) = a P1284) +A 1,2%41(%)Thus, if f(x) is an element of Vo, it is also an element of V,. This is because all
Vp expansion functions are contained in V,. Mathematically, we write that Vp is
a subspace of V,, denoted Vy C V,. tiThe simple scaling function in the preceding example obeys the four fundamental requirements of multiresolution analysis (Mallat [1989a]):MRA Requirement 1: The ‘scaling function is orthogonal to its integer
translates.This is easy to see in the case of the Haar function, because whenever it has a
value of 1, its integer translates are 0,so that the product of the two is 0. The
Haar scaling function is said to have compact support, which means that it is
0 everywhere outside a finite interval called the support. In fact, the width of
the support is 1; it is 0 outside the half open interval [0, 1). It should be noted
that the requirement for orthogonal integer translates becomes harder to
satisfy as the width of support of the scaling function becomes larger than 1.MRA Requirement 2: The subspaces spanned by the scaling function at low
scales are nested within those spanned at higher scales,As can be seen in Fig. 7.12, subspaces containing high-resolution functions
must also contain all lower resolution functions. That is,Vico C1 CV CMC VW GYIC CV (7.2-15)Moreover, the subspaces satisfy the intuitive condition that if f(+) ¢ V;, then
f(2x) € V;,;. The fact that the Haar scaling function meets this requirementWo, o¥[FIGURE 7.12
The nested
function spaces
spanned by a
scaling function,
504 Chapter 7 @ Wavelets and Multiresolution ProcessingThe a, are changed to
h(n) because they ave
used later {see Section
7.4) as filter bank
coefficients.should not be taken to indicate that any function with a support width of 1
automatically satisfies the condition. It is left as an exercise for the reader
to show that the equally simple function(x) = t 0.25 = x < 0.75
° 0 elsewhereis not a valid scaling function for a multiresolution analysis (see Problem 7.11).MRA Requirement 3: The only function that is common to all V; is f(x) = 0.If we consider the coarsest possible expansion functions (i.e., j = —°),
the only representable function is the function of no information. That is,Vico = {0} (7.2-16)MRA Requirement 4: Any function can be represented with arbitrary precision.
Though it may not be possible to expand a particular f(x) at an arbitrarily
coarse resolution, as was the case for the function in Fig. 7.11(e), all measurable, square-integrable functions can be represented by the scaling
functions in the limit as j —> 00. That is,Vo = {L°(R)} (7.2-17)
Under these conditions, the expansion functions of subspace V, can be ex
pressed as a weighted sum of the expansion functions of subspace V;,,. Using
Eq. (7.2-12), we letPj bx) = Den Pisin (2)where the index of summation has been changed to 7 for clarity. Substituting
for +1, (x) from Eq. (7.2-10) and changing variable a, to A,{n), this becomesGj a(X) = A (n2 (21x — n)Because o(x) = ¢o,9{x), both j and & can be set to 0 to obtain the simpler nonsubscripted expressiong(x) = Shg(2) V29(2x — n) (7.2-18)The A,() coefficients in this recursive equation are called scaling function coefficients; h, is referred to as a scaling vector. Equation (7.2-18) is fundamenta!
to multiresolution analysis and is called the refinement equation, the MRA
equation, or the dilation equation. It states that the expansion functions of any
subspace can be built from double-resolution copies of themselves — that is,
from expansion functions of the next higher resolution space. The choice of a
reference subspace, Vo, is arbitrary.
7.2 @ Multiresolution Expansions 505M@ The scaling function coefficients for the Haar function of Eq. (7.2-14)
are h,(0) = A,(1) = 1/2, the first row of matrix Hz in Eq. (7.1-18). Thus,
Eq. (7.2-18) yieldsola) = Ja V2e2x)] + Zg[V2e2x - 0]This decomposition was illustrated graphically for gp 9 (x) in Fig. 7.11(f), where
_ the bracketed terms of the preceding expression are seen to be g(x) and
#1,1(%). Additional simplification yields g(x) = ¢(2x)} + (2x — 1). .7.2.3 Wavelet FunctionsGiven a scaling function that meets the MRA requirements of the previous
section, we can define a wavelet function (x) that, together with its integer
translates and binary scalings, spans the difference between any two adjacent
scaling subspaces, V; and V;,;. The situation is illustrated graphically in Fig. 7.13.
We define the set {y,.(x)} of waveletsHe (X) = 2 yp(Qix — k) (7.2-19)for all k  Z that span the W, spaces in the figure. As with scaling functions, we
writeW, = Span{iija()} (7.2-20)
and note that if f(x) e W,,
f(x) = Dagbjxls) (7.2-21)
The scaling and wavelet function subspaces in Fig. 7.13 are related by
Vier = VOW, (7.2-22)where ® denotes the union of spaces (like the union of sets). The orthogonal
complement of V; in V;,; is W;, and all members of V; are orthogonal to the
members of W;. Thus, ee
(Gj. (4), BCX) = 0 (7.2-23)for alt appropriate j,k, /eZ,V2 = V, OW, =%)O Wye WY, EXAMPLE 7.5:
Haar scaling
function
coefficients.FIGURE 7.13
The relationship
between scaling
and wavelet
function spaces.
506 Chapter 7 m Wavelets and Multiresolution ProcessingEXAMPLE 7.6:
The Haar wavelet
function
coefficients.We can now express the space of all measurable, square-integrable functions asL7(R) =YOWOW, Oo... (7,2-24)
or
LR) = V, OW, OW, ... (7.2-25)
oreven
LR) = - DW,9W_,OWOW OWE... (7.2-26)which eliminates the scaling function, and represents a function in terms of
wavelets alone [i.¢., there are only wavelet function spaces in Eq. (7.2-26)].
Note that if f(x) is an element of V;, but not Vo, an expansion using Eq. (7.2-24)
contains an approximation of f(x) using Vo scaling functions. Wavelets from
Wo would encode the difference between this approximation and the actual
function. Equations (7.2-24) through (7.2-26) can be generalized to yieldLR) = Vj, ®W;, ® Wie @ (7.2-27)where jy is an arbitrary starting scale.Since wavelet spaces reside within the space: spanned by the next higher
resolution scaling functions (see Fig. 7.13), any wavelet function—like its scaling function counterpart of Eq. (7.2-18)—can be expressed as a weighted sum
of shifted, double-resolution scaling functions. That is, we can writewx) = Dhyln) V2e(2x — 1) (7.2-28)where the h,{m) are called the wavelet function coefficients and hy is the
wavelet vector. Using the condition that wavelets span the orthogonal complement spaces in Fig. 7.13 and that integer wavelet translates are orthogonal, it
can be shown that h,(n) is related to h,(n) by (see, for example, Burrus,
Gopinath, and Guo [1998})Ay(n) = (-1)"h,(1 — 1) (7.2-29)Note the similarity of this result and Eq. (7.1-14), the relationship governing
the impulse responses of orthonormal subband coding and decoding filters.Mi In the previous example, the Haar scaling vector was defined as
h,{0) = h,) = 1/V2. Using Eq. (7.2-29), the corresponding wavelet
vector is ,(0) = (~1)°A,(1 — 0) = 1/V2 and A,(1) = (-1)'4,(1 - 1)
= —1/V2. Note that these coefficients correspond to the second row of matrix H) in Eq. (7.1-18). Substituting these values into Eq. (7.2-28), we get
7.2 & Multiresolution Expansions 507W(x} = p(2x) — g(2x — 1), which is plotted in Fig. 7.14(a). Thus, the Haar
wavelet function is
1 OSx%<05
vQ)=$-1 05ex<1 (7.2-30)
0 elsewhereUsing Eq. (7.2-19), we can now generate the universe of scaled and translated
Haar wavelets, Two such wavelets, io 2(«) and yy; 9(x), are plotted in Figs. 7.14(b)
and (c), respectively. Note that wavelet y; 9(x) for space W, is narrower than
o,2(x) for Wo; it can be used to represent finer detail.Figure 7.14(d) shows a function of subspace V, that is not in subspace Vo. This
function was considered in an earlier example [see Fig. 7.11(e)]. Although the
function cannot be represented accurately in Vo, Eq. (7.2-22) indicates that it can
be expanded using V, and Wp expansion functions. The resulting expansion isF(x) = fale) + falx)w(x) = boo(x) . $o.2(x) = pla - 2) ab
cd
ef
FIGURE 7.14
Haar waveletfunctions in Wy
and W,. d0(2) = V2 ¥(2x)   fix} & Vo fala) e Wo
7.3 m Wavelet Transforms in One Dimension 509functions form an orthonormal basis or tight frame, which is often the case, the
expansion coefficients are calculated—based on Egs. (7.2-5) and (7.2-9)—ascult) = F644) = [Foe eC@) dx (73-2)
and
dy(k) = (£0). Had) = f flared ds (733)In Eggs. (7.2-5) and (7.2-9), the expansion coefficients (i.e., the a,) are defined
as inner products of the function being expanded and the expansion functions
being used. In Eqs. (7.3-2) and (7.3-3), the expansion functions are the ¢;,,, and
x; the expansion coefficients are the c;, and d;. If the expansion functions
are part of a biorthogonal basis, the ¢ and w terms in these equations must be
replaced by their dual functions, $ and #, respectively.Mi Consider the simple function.Je? 08x81
y 0 otherwiseshown in Fig, 7.15(a). Using Haar wavelets—see Eqs. (7.2-14) and (7.2-30)—
and a starting scale jg = 0, Eqs. (7.3-2) and (7.3-3) can be used to compute the
following expansion coefficients: 1 1
co(0) = [ x*gqq(x) dx = [ dx =~
0 0 31 0s 1 1
d.(0) = [ x Woo(x) dx = [ x dx - [ dx = —=
0 0 05 41 0.25 : 0.5 V3
a0) = f eyo(x)ax = f evide~ [ evidr= ~E
‘0 0 0.25 21 0.75 1
a,(1) = [P00 dx = [ V2 dx - [ weV2 dx = _3V2
0 0.5 0.75 j 32Substituting these values into Eq. (7,3-1), we get the wavelet series expansion v2 2
y= denote) + [-ooot] + [Puree - vate] +% Ww W,
V, = VeeW
¥, = V,eW, ~ HOM enw,Because f is real,n0 conjugates are needed in the
inner products of Eqs.
(7.3-2} and (7.3-3).EXAMPLE 7,7:
The Haar wavelet
series expansion
ofy =x.
508  Chopter 7 m@ Wavelets and Multiresolution Processing
wherefax) = epoxy - Ponate)and
fax) = YB vaa(x) - oats)Here, f,(x) is an approximation of f(x) using Vp scaling functions, while f(x)
is the difference f(x) — f,(x) as a sum of Wo wavelets. The two expansions,
which are shown in Figs. 7.14(e) and (f), divide f(x) in a manner similar to a
lowpass and highpass filter as discussed in connection with Fig. 7.6. The low
frequencies of f(x) are captured in f,(x)—it assumes the average value of
f(x) in cach integer interval—while the high-frequency details are encoded infal). a s.Wavelet Transforms in One DimensionWe can now formally define several closely related wavelet transformations:
the generalized wavelet series expansion, the discrete wavelet transform, and
the continuous wavelet transform. Their counterparts in the Fourier domain
are the Fourier series expansion, the discrete Fourier transform, and the integral Fourier transform, respectively. In Section 7.4, we develop a computationally efficient implementation of the discrete wavelet transform called the fast
wavelet transform.7.3.1 The Wavelet Series ExpansionsWe begin by defining the wavelet series expansion of function f(x) € L?(R) relative to wavelet ¢s(x) and scaling function g(x). In accordance with Eq. (7.2-27),
f(x) can be represented by a scaling function expansion in subspace V,,
(Eq. (7.2-12) defines such an expansion] and some number of wavelet function expansions in subspaces W,,, Wj,.,... [as defined in Eq. (7.2-21)].Thus,f= Dena) +S DaOba (73-1)j=hy kwhere jo is an arbitrary starting scale and the c;,(k}and d,(k) are relabeled a,
from Eqs. (7.2-12) and (7.2-21), respectively. The ¢;,(k) are normally called
approximation and/or scaling coefficients; the dj(k) are referred to as detail
and/or wavelet coefficients. This is because the first sum in Eq, (7.3-1) uses scaling functions to provide an approximation of f(x) at scale jo [unless f(x) € Vj,
so that the sum of the scaling functions is equal to f(x)]. For each higher scale
j = jo in the second sum, a finer resolution function—a sum of wavelets—is
added to the approximation to provide increasing detail. If the expansion
510 Chapter 7 m Wavelets and Multiresolution Processingab
ed
efFIGURE 7.15A wavelet series
expansion ofy = x? using Haar
wavelets.1  0 0.25 05 0.75 1 WAZ112   0 0.25 05 0.75 1  36116
—Vib—uIG The first term in this expansion uses cy(0) to generate a subspace Vy approximation of the function being expanded. This approximation is shown in Fig. 7.15(b)
and is the average value of the original function. The second term uses dp(0) to
refine the approximation by adding a level of detail from subspace Wo. The
added detail and resulting V approximation are shown in Figs. 7.15(c) and
(d), respectively. Another level of detail is added by the subspace W, coefficients d,(0) and d,(1). This additional detail is shown in Fig. 7.15(e), and the
resulting ¥2 approximation is depicted in 7.15(f). Note that the expansion is
now beginning to resemble the original function. As higher scales (greater levels of detail) are added, the approximation becomes a more precise representation of the function, realizing it in the limit as} 7™. B‘3.2 The Discrete Wavelet TransformLike the Fourier series expansion, the wavelet series expansion of the previous
section maps a function of a continuous variable into a sequence of coefficients. If the function being expanded is discrete {i.e., a sequence of numbers),
the resulting coefficients are called the discrete wavelet iransform (DWT). For
example, if f(#) = f(x + nAx)} for some xo, Ax, andn = 0,1.2,....M - 1,
7.3m Wavelet Transforms inOne Dimension 511the wavelet series expansion coefficients for f(x) [defined by Eqs. (7.3-2) and
(7.3-3)] become the forward DWT coefficients for sequence f(n):Wei) = ee Dildo) (735)
WOK) = a DOM jan) for] = f 736)The ¢;,,.() and y; , (#) in these equations are sampled versions of basis functions @,;, ,(x) and y,,, (x). For example, o;, ,(#) = 9),,%(%s + nAx,) for some
x,, Ax,, and n = 0,1,2,...,M — 1. Thus, we employ M equally spaced samples over the support of the basis functions (see Example 7.8 below). In accordance with Eq. (7.3-1), the complementary inverse DWT isHea) = Fag DMolin Deja) + 9q DTM DYal—) (73)J*}oNormally, we Jet jp = 0 and select M to be a power of 2 (ie, M = 2’) so
that the summations in Eqs. (7.3-5) through (7.3-7) are performed over
n=0,1,2,...,.M—1,j =0,1,2,...,/ - 1, and & = 0,1,2,...,2/ — 1. For
Haar wavelets, the discretized scaling and wavelet functions employed in the
transform (i.e., the basis functions) correspond to the rows of the M x M
Haar transformation matrix of Section 7.1.3. The transform itself is composed
of M coefficients, the minimum scale is 0, and the maximum scale is J — 1. For
reasons noted in Section 7.3.1 and illustrated in Example 7.6, the coefficients
defined in Eqs. (7.3-5) and (7.3-6) are usually called approximation and detail
coefficients, respectively.The W,(jo.k) and W,(j,&) in Eqs. (7.3-5) to (7.3-7) correspond to the
¢;,(k) and d,(k) of the wavelet series expansion in the previous section. (This
change of variables is not necessary but paves the way for the standard notation used for the continuous wavelet transforn}of-the next section.) Note that
the integrations in the series expansion have beeri replaced by summations,
and a 1/\/M normalizing factor, reminiscent of the DFT in Section 4.4.1, has
been added to both the forward and inverse expressions. This factor alternately could be incorporated into the forward or inverse alone as 1/M. Finally, it
should be remembered that Eqs. (7.3-5) through (7.3-7) are valid for orthonormal bases and tight frames alone. For biorthogonal bases, the ¢ and w
terms in Eqs. (7.3-5) and (7.3-6) must be replaced by their duals, $ and @,
Tespectively.@ To illustrate the use of Eqs. (7.3-5) through (7.3-7), consider the discrete
function of four points: f(0) = 1, f(1) = 4, f(2) = —3, and f(3) = 0. Because
M=4,/ =2 and, with jg = 0, the summations are performed over
x = 0,1,2,3.7 = 0,1, and & = 0 for} = Oor k = 0,1 for j = 1. We will use
the Haar scaling and wavelet functions and assume that the four samples ofEXAMPLE 7.8:
Computing a onedimensional
discrete wavelet
transform.
512 Guepter 7 @ Wavelets and Multiresolution Processingf(x) are distributed over the support of the basis functions, which is 1 in width.
Substituting the four samples into Eq, (7.3-5), we find that3
W,(0, 0) = 2 D f(a)eo,0()
2 yao
= 50 +451-3-14+0-1] =1because ypo(n) = I for n = 0, 1, 2,3. Note that we have employed uniformly
spaced samples of the Haar scaling function for j = 0 and k = 0. The values
correspond to the first row of Haar transformation matrix Hg of Section 7.1.3.
Continuing with Eq. (7.3-6) and similarly spaced samples of #, , (x), which correspond to rows 2,3, and 4 of Hy, we getW,(0,0) = > [1-1 + 4-1 -3-(-1) + 0-(-b] = 4W,(1,0) = = [1-V2 + 4-(- V4) - 3-0 + 0-0] = -1.5V2Nl MleWl, 1) =F [1-0 + 4-0 ~3-V24+0+(-v42)] = -15V2Thus, the discrete wavelet transform of our simple four-sample function relative to the Haar wavelet and scaling function is fi, 4, -1.5V2, -1. 5V3},
where the transform coefficients have been arranged i in the order in which
they were computed.Equation (7,3-7) lets us reconstruct the original function from its transform.
Iterating through its summation indices, we getf(a) = ; [w,(0, 0) go.oGr) + W, (0, O}s o (7) + W, (1. 0) eyo)
+ WA, 11,1 (7)]forn = 0,1, 2,3. If = 0, for instance,f(0) = 5 [ter + aot = 15v2-(v2 2) - 15V2-0] =As in the forward case, uniformly spaced samples of the scaling and wavelet
functions are used in the computation of the inverse. ®The four-point DWT in the preceding example is an illustration of a twoscale decomposition of f(m)—that is, j = {0,1}. The underlying assumption
was that starting scale jg was zero, but other starting scales are possible. It is
left as an exercise for the reader (see Problem 7.16) to compute the singlescale transform {25V4, ~1.5V2, -L5V2, -1.5 v2}, which results when the
starting scale is 1. Thus, Eqs. (7.3-5) and (7.3-6} define a “family” of transforms
that differ in starting scale j,;
7.3 ® Wavelet Transforms in One Dimension7.3.3 The Continuous Wavelet TransformThe natural extension of the discrete wavelet transform is the continuous
wavelet transform (CWT), which transforms a continuous function into a highly
redundant function of two continuous variables—translation and scale. The resulting transform is easy to interpret and valuable for time-frequency analysis.
Although our interest is in discrete images, the continuous transform is covered here for completeness,The continuous wavelet transform of a continuous, square-integrable function, f(x), relative to a real-valued wavelet, (x), is defined as W,(s,7) = [ f)by (0) dx (7.3-8)
where
ool) = S0(% = *) (73-9)and s and 7 are called scale and translation parameters, respectively. Given
W,(s, 7), f(x) can be obtained using the inverse continuous wavelet transform 1 ff WAX)
IO = [ [Mon 2 dr ds (7.3-10)
where
ad 2
Cy= [era (7.3-11)and V(,) is the Fourier transform of w(x). Equations (7.3-8) through (7.3-11)
define a reversible transformation as tong as thé:so-called admissibility criterion,
Cy < 0°, is satisfied (Grossman and Morlet [1984]). In most cases, this simply means that ¥(0) = 0 and W(yz)->0 as 4 —> 00 fast enough to make
Cy < ©.The preceding equations are reminiscent of their discrete counterparts—
Egs. (7.2-19), (7.3-1), (7.3-3), (7.3-6), and (7.3-7). The following similarities
should be noted:1. The continuous translation parameter, 7, takes the place of the integer
translation parameter, k.2. The continuous scale parameter, s, is inversely related to the binary scale
parameter, 2, This is because s appears in the denominator of
u(x - z)/s) in Eq. (7.3-9). Thus, wavelets used in continuous transforms
are compressed or reduced in width when 0 < s < 1 and dilated or expanded when s > 1. Wavelet scale and our traditional notion of frequency
are inversely related.513
514 Chapter 7 @ Wavelets and Multiresolution ProcessingEXAMPLE 7.9:
A onedimensional
continuous
wavelet
transform.3. The continuous transform is similar to a series expansion [see Eq. (7.3-1)]
or discrete transform [see Eq. (7.3-6)] in which the starting scale
jo = ~089. This—in accordance with Eq. (7.2-26)—eliminates explicit scaling function dependence, so that the function is represented in terms of
wavelets alone.4, Like the discrete transform, the continuous transform can be viewed as a
set of transform coefficients, {W,(s, 1)}, that measure the similarity of f(x)
with a set of basis functions, {he sy}. In the continuous case, however, both
sets are infinite. Because #,,(x) is real valued and ys, (x) = ¥;,(x), each
coefficient from Eq. (7.3-8) is the integral inner product, (f(x), &,,(*)), off(x) and y,,(x).
& The Mexican hat wavelet, _vx) = (% a ‘Ja ~ ate" (7.312)gets its name from its distinctive shape [see Fig. 7.16(a)]. It is proportional to
the second derivative of the Gaussian probability function, has an average
value of 0, and is compactly supported (i.e., dies out rapidly as |x] — 00), Although it satisfies the admissibility requirement for the existence of continuous,
reversible transforms, there is not an associated scaling function. and the computed transform does not result in an orthogonal analysis. Its most distinguishing features are its symmetry and the existence of the explicit expression of
Eg. (7.3-12).The continuous, one-dimensional function in Fig. 7.16(a) is the sum of two
Mexican hat wavelets:F(x) = ty, 10(4) + Ho,s0(2)Its Fourier spectrum, shown in Fig. 7.16(b), reveals the close connection between scaled wavelets and Fourier frequency bands. The spectrum contains
two broad frequency bands (or peaks) that correspond to the two Gaussianlike perturbations of the function.Figure 7.16(c) shows a portion (1 = s = 10 and + = 100) of the CWT of
the function in Fig. 7,16(a) relative to the Mexican hat wavelet. Unlike the
Fourier spectrum in Fig. 7.16(b), it provides both spatial and frequency information. Note, for example, that when s = 1, the transform achieves a maximum at + = 10, which corresponds to the location of the #1. 15 (x) component
of f(x). Because the transform provides an objective measure of the similarity
between f(x) and the wavelets for which it is computed, it is easy to see how it
can be used for feature detection. We simply need wavelets that match the features of interest. Similar observations can be drawn from the intensity plot in
Fig. 7.16(d), where the absolute value of the transform |W,({s, 7)| is displayed
as intensities between black and white, Note that the continuous wavelet
transform turns a 1-D function into a 2-D result. “i
7.4 ® The Fast Wavelet Transform 515 f(x) |Flse)| ab
t OAS p—~- a ed
0s FIGURE 7.16The continuous
wavelet transform
{cand d) and
Fourier spectrum
(b) of a
continuous 1-D
function (a). IOS     The Fast Wavelet TransformThe fast wavelet transform (FWT) is a computationally efficient implementation of the discrete wavelet transform (DWT) that exploits a surprising but
fortunate relationship between the coefficients of the DWT at adjacent scales.
Also called Mallat’s herringbone algorithm (Mallat [!989a, }989b]). the FWT
resembles the two-band subband coding scheme of Section 7.1.2.Consider again the multiresolution refinement equationol) = DAs V2e(2x ~ 0) AT) Famer ery
Scaling x by 2/, translating it by k, and letting #1 + 2k ++ gives
g(Q/x - k) = Sada) V5e (22! Ay ny
© Shiny Vip ly - 2k» a)wtShGn  DkyV 2p (2a C7 AD)a
516 Chapter 7 @ Wavelets and Multiresolution ProcessingThe wavelet series
expansion coefficients
became the DWT
coefficient when f is
discrete. Here, we begin
with the series expansion
coefficients tu simplify
the derivation; we will be
able lo substitute freely
from earlier results (like
the scaling and wavelet
function definitions).Note that scaling vector A, can be thought of as the “weights” used to expand
g(2ix - k) as a sum of scale j + 1 scaling functions. A similar sequence of
operations—beginning with Eq. (7.2-28)—provides an analogous result for
(2x — k). That is,W(2ix — k) = Shy(m — 2k) V2p(2!* 1x — m) (7.4-3)where scaling vector h,(n) in Eq. (7.4-2) corresponds to wavelet vector h,(”)
in Eq. (7.4-3).Now consider Eqs. (7.3-2) and (7.3-3) of Section 7.3.1. They define the
wavelet series expansion coefficients of continuous function f(x). Substituting
Eq. (7.2-19)—the wavelet defining equation —into Eq. (7.3-3), we getdj(k) = freuen — k)dx (7.4-4)which, upon replacing ¢(2/x — k) with the right side of Eq. (7.4-3), becomesaj(k) = / poor] Sim — 2k) V29(2i x - m | dx (7.4-5)Interchanging the sum and integral and rearranging terms then givesdi(k) = Sham ~ 28) | F(x) DP gQitty | (74-6)where the bracketed quantity is c,(k) of Eq. (7.3-2) with jp = j + 1 and
k = m.Tosee this, substitute Eq. (7.2-10) into Eq, (7.3-2) and replace jy and k
with j + 1 and m, respectively. Therefore, we can writedk) = Shy(m ~ 2k)ejai(m) (7.4-7)and note that the detail coefficients at scale j are a function of the approximation coefficients at scale j + 1. Using Eqs. (7.4-2) and (7.3-2) as the starting
point of a similar derivation involving the wavelet series expansion (and
DWT) approximation coefficients, we find similarly thatclk) = Dhl — 2kejelm) (7.4-8)Because the c;(k) and d,(k) coefficients of the wavelet series expansion become the W,(j, k) and W,(j, k) coefficients of the DWT when f(x) is discrete
(see Section 7.3.2), we can write
7.4 m@ The Fast Wavelet Transform 517Wk) = Shylm — 2k)W,(j + 1,m) (7.4-9)
W,Ui,k) = Shg(m ~ 2k)W,f + 1, m) (7.4-10)Equations (7.4-9) and (7.4-10) reveal a remarkable relationship between
the DWT coefficients of adjacent scales. Comparing these results to Eq. (7.1-7),
we see that both W,(j, k) and W,(j, &), the scale j approximation and the detail coefficients, can be computed by convolving W,(j + 1, x), the scale j + 1
approximation coefficients, with the order-reversed scaling and wavelet vectors, h,(—n) and hy(—n), and subsampling the results. Figure 7.17 summarizes
these operations in block diagram form. Note that this diagram is identical to
the analysis portion of the two-band subband coding and decoding system of
Fig. 7.6, with Ao(n) = A,(—1) and h(n) = h,(—n). Therefore, we can writeW,U,K) = hy(-n) *W,G + 1,7) (7.4-11) n=2k,kz0
and(7.4-12) WU. k) = Ag(—n) *® WC + 1,7)» Ine tkkeO
where the convolutions are evaluated at instants n = 2k for k = 0. As will be
shown in Example 7.10, evaluating convolutions at nonnegative, even indices
is equivalent to filtering and downsampling by 2.Equations (7.4-11) and (7.4-12) are the defining equations for the computation of the fast wavelet transform. For a sequence of length M = 2/, the number of mathematical operations involved is on the order of O(M), That is, the
number of multiplications and additions is linear with respect to the length of
the input sequence—because the number of multiplications and additions involved in the convolutions performed by the FWT analysis bank in Fig. 7.17 is
proportional to the length of the sequences being convolved. Thus, the FWT
compares favorably with the FFT algorithm, which requires on the order of
O(M log: M) operations. >To conclude the development of the FWr" we . simply note that the filter
bank in Fig. 7.17 can be “iterated” to create multistage structures for computing
DWT coefficients at two or more successive scales, For example, Fig. 7.18(a)
shows a two-stage filter bank for generating the coefficients at the two highest
scales of the transform. Note that the highest scale coefficients are assumed to
be samples of the function itself. That is, W,(J, 2) = f(n), where J is the highest We + in)Wo, ") If A,Gn — 2k} in
Eq, (7.4-9) is rewritten as
hd -(2k — mi}, we see
that the first minus sign
is responsible for the
order reversal [seeEq. (7.1-6)], the 2k is
responsibie for the
subsampling [see Eq.
(7.1-2)]. and m is the
dummy variable for
convolution [seeEq. (7.1-7)}.FIGURE 7.177
An FWT analysis
bank.
518 Chapter 7 @ Wavelets and Multiresolution Processinga
bFIGURE 7.18(a) A two-stage or
two-scale FWT
analysis bank and
(b) its frequency
splitting
characteristics.r whg(—n) WU - ton)
an) & hy(-n) H 21 -—@W,(J - 2,2)
\ whel—n) H 24 }-—ew, - 2,0)      |A(o)|  0 a/A x/2scale. [In accordance with Section 7.2.2, f(x)eV,;, where V; is the scaling
space in which f(x) resides.] The first filter bank in Fig, 7.18(a) splits the original function into a lowpass, approximation component, which corresponds to
scaling coefficients W,(J — 1, 2): and a highpass, detail component, corresponding to coefficients W,(J — 1,7). This is illustrated graphically in Fig. 7.18(b),
where scaling space V, is split into wavelet subspace W,_, and scaling subspace
Vj-. The spectrum of the original function is split into two half-band components. The second filter bank of Fig. 7.18(a) splits the spectrum and subspace
V,-1, the lower half-band, into quarter-band subspaces W,.. and V,_, with
corresponding DWT coefficients W,{J — 2,n) and W,(/ — 2, 1), respectively.The two-stage filter bank of Fig. 7.18(a) is extended easily to any number of
scales. A third filter bank, for example, would operate on the W,(J ~ 2, n)} coefficients, splitting scaling space V,_, into two eighth-band subspaces W,_3
and V;..3. Normally, we choose 2/ samples of f(x) and employ P filter banks
(as in Fig. 7.17) to generate a P-scale FWT at scales J - 1,/ ~ 2,...,/ -— P.
The highest scale (i.e,, / — 1) coefficients are computed first; the lowest scale
(i.e., J — P) last. If function f(x) is sampled above the Nyquist rate, as is usually the case, its samples are good approximations of the scaling coefficients at
the sampling resolution and can be used as the starting high-resolution scaling
coefficient inputs. In other words, no wavelet or detail coefficients are needed
at the sampling scale. The highest-resolution scaling functions act as unit discrete impulse functions in Eqs. (7.3-S) and (7,3-6), allowing f(#) to be used as
the scaling (approximation) input to the first two-band filter bank (Odegard,
Gopinath, and Burrus [1992]}.
7.4 m The Fast Wavelet Transform 519@ To illustrate the preceding concepts, consider the discrete function f(m) EXAMPLE 7.10:
= {1,4, -3,0} from Example 7.8. As in that example, we will compute the Computing a1-D
transform based on Haar scaling and wavelet functions. Here, however, we will fast waveletnot use the basis functions directly, as was done in the DWT of Example 7.8. .
Instead, we will use the corresponding scaling and wavelet vectors fromExamples 7.5 and 7.6:WV2 n=01hen) = { 0 otherwise (74-13)and
V2 n=0
Ayn) = § -1/V2 n=1 (7.4-14)
0 otherwiseThese are the functions used to build the FWT filter banks; they provide the filter
coefficients. Note that because Haar scaling and wavelet functions are orthonormal, Eq. (7.1-14) can be used to generate the FWT filter coefficients from a single
prototype filter—like 4, (71) in Table 7.2, which corresponds to gq(#) in Eq. (7.1-14):Since the DWT computed in Example 7.8 was composed of elements
{w,(0, 0), W,(0, 0), W,(1, 0), Wy (1, Dv}, we will compute the corresponding
two-scale FWT for scales j = {0,1}. That is, J = 2 (there are 2/ = 2? samples) and P = 2 (we are working with scales J —-1=2-—1-=1 and
J — P=2-2 = Qin that order}. The transform will be computed using the
two-stage filter bank of Fig. 7.18(a), Figure 7.19 shows the sequences that result from the required FWT convolutions and downsamplings. Note that function f(?) itself is the scaling (approximation) input to the leftmost filter bank.
To compute the W,(1,k) coefficients that appear at the end of the upper
branch of Fig. 7.19, for example, we first convolve f(n) with Ay(—7). As explained in Section 3.4.2, this requires flipping oe of the functions about the origin, sliding it past the other, and computing the sum of the point-wise product of
the two functions. For sequences {1,4, ~3,0} and {-1/ V2, 1/ v3}, this
produces{-1/V2, -3/V2, 1/V2, ~3/-V2, 0}where the second term corresponds to index k = 2n = 0. (In Fig. 7.19, underlined values represent negative indices, i.e, 2 < 0.) When downsampled byTABLE 7.2
Orthonormal
Haar filter
coefficients for
hn).
520 Chapter 7 Wavelets and Multiresolution Processing{-1/¥2. -3/ V2. 7/2, - 3/2. 0}{A/V 1/12}  
 W,(1,n) = {-3/¥2, -3/N2}  
     W,Q2,n) = - _ - G4 80} Welln) = {5/N2, ya H{-1/NE 1/2} il Wo(0.0) = {4}    
 {=2.5, 4, -15}& (L/V21/N2}s (2.5, 4,-1.5} 
 
  & {1/¥2,1/V2}: {U/v2, 5/N2, 1/¥2. -3/v2, 0}   W,(0,0) = {1} FIGURE 7.19 Computing a two-scale fast wavelet transform of sequence {1, 4, —3, 0} using Haar scaling and
wavelet vectors,taking the even-indexed points, we get W,(1,k) = {-3/V2, -3/ V3} for
k = {0,1}. Alternatively, we can use Eq. (7.4-12) to compute = hy{—n) & f(r)n=2k.k20 W,(1, A) = hy(—n) % W,(2, 2)
n=2k k=O= Shull — 2k)x(d)k=0,1 1 tVa x{2k) Vi
Here, we have substituted 2k for 1 in the convolution and employed / as a dummy
variable of convolution (i.e., for displacing the two sequences relative to one
another). There are only two terms in the expanded sum because there are only
two nonzero values in the order-reversed wavelet vector A,{—7). Substitutin,
k = 0, we find that W,(1,0) = -3/V2; for k = 1, we get W,(1, 1) = -3/V2.
Thus, the filtered and downsampled sequence is {~3AV2, —3/ v3}, which matches the earlier result. The remaining convolutions and downsamplings are performed in a similar manner. 8x(2k + |
k=O.As one might expect, a fast inverse transform for the reconstruction of f({n)
from the results of the forward transform can be formulated. Called the
inverse fast wavelet transform (FWT~'), it uses the scaling and wavelet vectors
employed in the forward transform, together with the level j approximation
7.4 The Fast Wavelet Transform 521synthesis filter
Win) efal x An)bank,
and detail coefficients, to generate the level j + 1 approximation coefficients.
Noting the similarity between the FWT analysis bank in Fig. 7.17 and the twoband subband analysis portion of Fig. 7.6(a), we can immediately postulate the
required FWT ? synthesis filter bank. Figure 7.20 details its structure, which is
identical to the synthesis portion of the two-band subband coding and decoding system in Fig. 7.6(a). Equation (7.1-14) of Section 7.1.2 defines the relevant
synthesis filters, As noted there, perfect reconstruction (for two-band orthonormal filters) requires g;() = 4,(—n) for i = {0,1}. That is, the synthesis
and analysis filters must be order-reversed versions of one another. Since the
FWT analysis filters (see Fig. 7.17) are ho(n) = hy(—n) and hy(n) = hy(—n),
the required FWT™ synthesis filters are go() = fo(~n) = A,(n) and g,(2)
= h,(—n) = h(n). It should be remembered, however, that it is possible also
to use biorthogonal analysis and synthesis filters, which are not order-reversed
versions of one another. Biorthogonal analysis and synthesis filters are crossmodulated per Eqs. (7.1-10) and (7.1-11).
The FWT™! filter bank in Fig. 7.20 implements the computation WeCi + Ln)  We +L) = AAR W IG R) + AEAWE GK] 7.4.15)where W7! signifies upsampling by 2 [i.e., inserting zeros in W as defined by — Remember that like in
Eq. (7.1-1) so that it is twice its original length]. The upsampled coefficients are iano (see
filtered by convolution with h,(7) and h,(1) and added to generate a higher transformscsn be comscale approximation. In essence, a better approximation of sequence f(m) with Puledata wer specified
greater detai] and resolution is created. As with the forward FWT, the inverse 2! « 2/ image, fos exomfilter bank can be iterated as shown in Fig. 7.21, where a two-scale structure for Plnitere ae i + loge
computing the final two scales of a FWT~! reconstruction is depicted. This co- neuen
efficient combining process can be extended to any number of scales and guar
antees perfect reconstruction of sequence f(/).FIGURE 7.21
WV - lnje— at |- & Aga) A two-stage or
two-scale FWT7!
n) synthesis bank.
W,tJ.n) QRhin)        WF - 2,n) “+ 2 fA *h,{n)
522 Chepter 7 m Wavelets and Multiresolution ProcessingEXAMPLE 7.11:
Computing a 1-D
inverse fast .
wavelet
transform.4
W,,(0, 0) = {4} we 1/B, -1/ NB}W,(0.0) = {1}
tB Computation of the inverse fast wavelet transform mirrors its forward counterpart. Figure 7.22 illustrates the process for the sequence considered in Example
7.10. To begin the calculation, the level 0 approximation and detail coefficients are
upsampled to yield {1,0} and {4,0}, respectively. Convolution with filters
gon) = g(a) = {1/V2,1/V2} and gy(n) = hyn) = (1/-V2, -1/V2}
produces {1/Z, 1/V2,0} and {4/V2, ~4/V2,0}, which when added give
W,(1,n) = {5/V2, -3/V2}. Thus, the level 1 approximation of Fig. 7.22, which
matches the computed approximation in Fig, 7.19, is reconstructed. Continuing in
this manner, f(”) is formed at the right of the second synthesis filter bank. sWe conclude our discussion of the fast wavelet transform by noting that
while the Fourier basis functions (i.e., sinusoids) guarantee the existence of the
FFT, the existence of the FWT depends upon the availability of a scaling function for the wavelets being used, as well as the orthogonality (or biorthogonality) of the scaling function and.corresponding wavelets. Thus, the Mexican hat
wavelet of Eq. (7.3-12), which “does not have a companion scaling function,
cannot be used in the computation of the FWT. In other words, we cannot construct a filter bank like that of Fig. 7.17 for the Mexican hat wavelet; it does not
satisfy the underlying assumptions of the FWT approach.Finally, we note that while time and frequency usually are viewed as different
domains when representing functions, they are inextricably linked. When you
try to analyze a function simultaneously in time and frequency, you run into the
following problem: If you want precise information about time, you must accept
some vagueness about frequency, and vice versa. This is the Heisenberg
uncertainty principle applied to information processing. To illustrate the principle graphically, each basis function used in the representation of a function can
be viewed schematically as a tile in a time-frequency plane. The tile, also called a
Heisenberg cell or Heisenberg’ box, shows the frequency content of the basis
function that it represents and where the basis function resides in time. Basis
functions that are orthonormal are characterized by nonoverlapping tiles.Figure 7.23 shows the time-frequency tiles for (a) an impulse function (i.e.,
conventional time domain) basis, (b) a sinusoidal (FFT) basis, and (c) an FWT{-3/N2, 0, -3/V3, 0}
t  {~1.5, 1.5, -1.5, 1.5, 0}  
 
       Wyn) = {~3/ V2, -3/ 2} 2
{4, 0}(UVB, ~1/82}  (4/2, -4/V2, 0}fea) = Wn)
pues (5/82, -3/°8) ={h4" 3.0) *& {1/42 1/N2}+ 21    (2.5.2.5, -1.5,.-1.5, 0}
{5/¥2, 0, -3/ v2, 0}
{1/v2. 1/ V2, 0} w (1/2. 1/42}{1,0}FIGURE 7.22 Computing a two-scale inverse fast wavelet transform of sequence {1, 4,-15V2, - 1.5V3}
with Haar scaling and wavelet functions.
7.5 ® Wavelet Transforms in Two Dimensions     |
PN
Sg
§ a
5 fo
i- a
2
w
Time Time
abcFIGURE 7.23 Time-frequency tilings for the basis functions associated with (a) sampled
data, (b) the FFT, and (c) the FWT. Note that the horizontal strips of equal height
rectangles in (c) represent FWT scales.basis. Each tile is a rectangular region in Figs. 7.23(a) through (c); the height
and width of the region defines the frequency and time characteristics of the
functions that can be represented using the basis function. Note that the standard time domain basis in Fig. 7.23(a) pinpoints the instants when events occur
but provides no frequency information [the width of each rectangle in Fig. 7.23(a)
should be considered one instant in time]. Thus, to represent a single frequency
sinusoid as an expansion using impulse basis functions, every basis function is
required. The sinusoidal basis in Fig. 7.23(b), on the other hand, pinpoints the
frequencies that are present in events that occur over long periods but provides no time resolution [the height of each rectangle in Fig, 7.23(b) should be
considered a single frequency]. Thus, the single frequency sinusoid that wasrepresented by an infinite number of impulse basis functions can be represented .as an expansion involving one sinusoidal basis function. The time and frequency
resolution of the FWT tiles in Fig. 7.23(c) vary, but the area of each tile (rectangle) is the same. At low frequencies, the tiles are shorter (i.e., have better frequency resolution or less ambiguity regarding frequency) but are wider (which
corresponds to poorer time resolution or more ambiguity regarding time). At
high frequencies, tile width is smaller (so the time resolution is improved) and
tile height is greater (which means the frequency resolution is poorer). Thus,
the FWT basis functions provide a compromise between the two limiting cases
in Fig. 7.23(a) and (b). This fundamental difference between the FFT and FWT
was noted in the introduction to the chapter and is important in the analysis of
nonstationary functions whose frequencies vary in time,Wavelet Transforms in Two DimensionsThe one-dimensional transforms of the previous sections are easily extended to
two-dimensional functions like images. In two dimensions, a two-dimensional
scaling function, g(x,y), and three two-dimensional wavelets, w(x, y),
w(x, y), and w(x, y), are required. Each is the product of two onedimensional functions. Excluding products that produce one-dimensional results,
like p(x)y(x), the four remaining products produce the separabie scaling functionox. ¥) = o(x)ely) (7.5-1}523
524  Qwpter7 m Wavelets and Multiresolution ProcessingNow that we are dealing
with 2-D images, f(x, »)
is a discrete function or
sequence of values and x
and y are discrete
variables, The scaling and
wavelet functions inEq. (7.5-7) and (75-8)
are sampled over their
support (as was done in
the 1-D case inSection 7.3.2).and separable, “directionally sensitive” waveletsw(x, y) = wde(y) (75-2)
w(x, y) = oeW(y) (7.5-3)
w(x, y) = oxy) (7.5-4)These wavelets measure functional variations—intensity variations for images—
along different directions: y’ measures variations along columns (for example, horizontal edges), w” responds to variations along rows (like vertical
edges), and y” corresponds to variations along diagonals. The directional sensitivity is a natural consequence of the separability in Eqs. (7.5-2) to (7.5-4); it
does not increase the computational complexity of the 2-D transform discussed in this section.Given separable two-dimensional scaling and wavelet functions, extension
of the 1-D DWT to two dimensions is straightforward. We first define the
scaled and translated basis functions:Pj mil, Y) = 2 o(2!x — m,2y — n) (7.5-5)
Pi mrlts ¥) = 2? yiQix - my —n), i= {H,V,D} (75-6)where index i identifies the directional wavelets in Eqs. (7.5-2) to (7.5-4).Rather than an exponent, / is a superscript that assumes the values H, V, andD.The discrete wavelet transform of image f(x, y) of size M x N is then
M~1N-1Whi mm) = eS SHY amal 9) (75-7)x=0 y=0
; 1 MaIN-1 Wein) = Tage De Def Wimalt Ye f= (HV. DY (758)
z=0 y=As in the one-dimensional case, jo is an arbitrary starting scale and the
W,(jo, m, n) coefficients define an approximation of ft (x, y) at scale jy. The
W, (j, m, n) coefficients add horizontal, vertical, and diagonal details for scales
J = jo. We normally let jp = 0 and select N = M = 2/ so that j = 0,1,2,.-.,
J —1andm =n =0,1,2,...,2/ — 1. Given the W, and Wi, of Eqs. (7.5-7) and
(75-8), f(x, y) is obtained via the inverse discrete wavelet transform1
fa”) = Tay = > Wello M1, M)Pjo.mnl%> Y)1 ~ igs : A + Vin, 2p 2 Twili m, en y) (7.5-9) Like the 1-D discrete wavelet transform, the 2-D DWT can be implemented
using digital filters and downsamplers. With separable two-dimensional scaling
and wavelet functions, we simply take the 1-D FWT of the rows of f(x, y), followed by the 1-D FWT of the resulting columns. Figure 7.24(a) shows the
7.5 @ Wavelet Transforms in Two Dimensions 525& hy{—m) 24 owt, myn)
RowsColumns
{along n) | &AY(-m) H «+ Wim. n)Rowswk hyf-m) Wali, m,n)Rows      
  
 
    Wj + l,m, n)    Columns Weis) FW Ai + 1m, a)  wojmnye— 21 hymn)Rows(along m) (+) [amin by
7] Columng 
Welj, mm, nt) eS 21 H * h,{m) | (along nbRows : G)- W.OG + lem, #)           Rows om
21 * An)
— Columns
WU, mn)e@ 2 hm)Rowsi5&FIGURE 7.24 The 2-D fast wavelet transform: (a) the analysis filler bank: ¢b) the
resulting decomposition; and (c} the synthesis filter bank
7.5 # Wavelet Transforms in Two Dimensions (now 1/16th of the size of the original image) decomposition results that were
generated in the second filtering pass. Finally, Fig. 7.25(d) is the three-scale
FWT that resulted when the subimage from the upper-left-hand corner of Fig.
7.25(c) was used as the filter bank inpui. Each pass through the filter bank pro:
duced four quarter-size output images that were substituted for the input from
which they were derived. Note the directiotfal nature of the wavelet-based
subimages, Wi}, WY, and W?, at each scale. EsThe decomposition filters used in the preceding example are part of a wellknown family of wavelets called syiets, short for “symmetrical wavelets.” Although they are not perfectly symmetrical, they are designed to have the least
asymmetry and highest number of vanishing moments’ for a given compact
support (Daubechies [1992]). Figures 7,26(e) and ({) show the fourth-order      The kth moment of wavelet yr(x) is m4} fx ye ere Moments impact the smoothness of the
scaling and wavelet functions and our abitity lo represent Chem as polvnumials An arder 4 syimtct hasN vanishing moments. 527abedFIGURE 7.25
Computing a 2-D
three-scale FWT:
(a) the original
image; (b) 2 onescale FWT; (c) a
two-scale FWT,
and (d) a threescale FWT.Recall Us the compact
support of a function is
the interval bf which tie
function has aea-zere,yale,
528 Chapter 7 mw Wavelets and Multiresolution Processingenw
rawgFIGURE 7.26
Fourth-order
symlets: (a)-(b)
decomposition
filters; (c)-(d)
reconstruction
filters; (e) the
one-dimensional
wavelet; (f} the
one-dimensional
scaling function;
and (g) one of
three twodimensionalwavelets, # (x, y).See Table 7.3 for
the values of
4,(n) for
OsSn=7.h(n) = Ag(—n)
12
108
06
04
0.20-02 "0 2 4 6 8Bo(n)} = y(n)1.2
ilAy(n) = Ayn) 0.5OP eo“0.5Bn) = hy)  OS
526 Chapter 7 w Wavelets and Multiresolution ProcessingNote how W,. Wi WY.
and W? are arranged in
Fig, 7.24{b). For each
scule that is computed,
they replace (he previous
scale approximation on
which they were based.EXAMPLE 7.12:
Computing a 2-D
fast wavelet
transform.‘The scating and wavelet
vectors used in this
example are described
later. Our facus here is
on the meehunics of the
transform computation,
which are independent of
the (iter coctficients
employed.process in block diagram form. Note that, like its one-dimensional counterpart
in Fig. 7.17, the 2-D FWT “filters” the scale j + 1 approximation coefficients
to construct the scale j approximation and detail coefficients, In the twodimensional case, however, we get three sets of detail coefficients—the horizontal, vertical, and diagonal details.The single-scale filter bank of Fig. 7.24(a) can be “iterated” (by tying the approximation output to the input of another filter bank) to produce a P scale
transform in which scale j is equal to J — 1,/ — 2,...,J — P. Asin the onedimensional case, image f(x, y) is used as the W,(/, m, n) input. Convolving
its rows with A,(—n) and hy(—n) and downsampling its columns, we get two
subimages whose horizontal resolutions are reduced by a factor of 2. The highpass or detail component characterizes the image's high-frequency information with vertical orientation; the lowpass, approximation component contains
its low-frequency, vertical information. Both subimages are then filtered
columnwise and downsampled to yield four quarter-size output subimages—
W,.W#, Wy, and W). These subimages, which are shown in the middle of
Fig. 7.24(b), are the inner products of f(x, y) and the two-dimensional scaling
and wavelet functions in Eqs. (7.5-1) through (7.5-4), followed by downsampling by two in each dimension. Two iterations of the filtering process produces the two-scale decomposition at the far right of Fig. 7.24(b).Figure 7.24(c) shows the synthesis filter bank that reverses the process just
described. As would be expected, the reconstruction algorithm is similar to the
one-dimensional case-At each iteration, four scale j approximation and detail
subimages are upsampled and convolved with two one-dimensional filters —
one operating on the subimages’ columns and the other on its rows, Addition
of the results yields the scale j + 1 approximation, and the process is repeated
until the original image is reconstructed.®& Figure 7.25(a) is a 128 X 128 computer-generated image consisting of 2-D
sine-like pulses on a black background. The objective of this example is to
illustrate the mechanics involved in computing the 2-D FWT of this image.
Figures 7.25(b) through (d) show three FWTs of the image in Fig. 7.25(a). The
2-D filter bank of Fig. 7.24({a) and the decomposition filters shown in Figs. 7.26(a)
and (b) were used to generate all three results.Figure 7.25(b) shows the one-scale FWT of the image in Fig. 7.25(a). To
compute this transform, the original image was used as the input to the filter
bank of Fig. 7.24{a). The four resulting quarter-size decomposition outputs (i.e.,
the approximation and horizontal, vertical, and djagonal details} were then
arranged in accordance with Fig. 7.24(b) to produce the image in Fig. 7.25(b). A
similar process was used to generate the two-scale FWT in Fig. 7.25(c), but the
input to the filter bank was changed to the quarter-size approximation subimage from the upper-left-hand corer of Fig. 7.25(b). As can be seen in Fig.
7.25(c), that quarter-size subimage was then replaced by the four quarter-size
7.5 w Wavelet Transforms in Two Dimensions 529 1-D symlets (i.e., wavelet and scaling functions). Figures 7.26(a) through (d)
show the corresponding decomposition and reconstruction filters. The coefficients of lowpass reconstruction filter go(m) = h(n) for 0 = n = 7 are
given in Table 7.3. The coefficients of the remaining orthonormal filters are
obtained using Eq. (7.1-14). Figure 7.26(g), a low-resolution graphic depiction
of wavelet w(x, y), is provided as an illustration of how a one-dimensional
scaling and wavelet function can combine to form a separable, two-dimensional
wavelet. ,We conclude this section with two examples that demonstrate the usefulness of wavelets in image processing. As in the Fourier domain, the basic approach is toStep 1. Compute a 2-D wavelet transform of an image.
Step 2. Alter the transform.
Step 3. Compute the inverse transform.Because the DWT’s scaling and wavelet vectors are used as lowpass and highpass filters, most Fourier-based filtering techniques have an equivalent
“wavelet domain” counterpart.~
fe® Figure 7.27 provides a simple iJJustration of the preceding three steps. In
Fig. 7.27(a), the lowest scale approximation component of the discrete wavelet
transform shown in Fig. 7.25(c) has been eliminated by setting its values to
zero. As Fig. 7.27(b) shows, the net effect of computing the inverse wavelet
transform using these modified coefficients is edge enhancement, reminiscent
of the Fourier-based image sharpening results discussed in Section 4.9. Note
how well the transitions between signal and background are delineated, despite the fact that they are relatively soft, sinusoidal transitions, By zeroing the
horizontal details as well—see Figs. 7.27(c) and (d)—we can isolate the vertical edges. aTABLE 7.3
Orthonormal
fourth-order
symlet filter
coefficients for
A,{n).
(Daubechies
[1992].}EXAMPLE 7.13:
Wavelet-based
edge detection.
7.5 & Wavelet Transforms in Two Dimensions 531 Figure 7,28(b) shows the result of performing these operations with fourthorder symlets, two scales (i.e., P = 2}, and a global threshold thal was determined interactively. Note the reduction in nois :
edges. This toss of edge detail is rechiced senificanly im Pig    oO Db
d
fLedFIGURE 7.28
Modifying a DWT
for noise removal:
(a) a noisy CT of a
human head: (b),
(c) and (e) various
reconstructions
after thresholding
the detail
coefficients;(d) and (f) the
information
removed during
the reconstruction
of (c) and {e).
(Original image
courlesy
Vanderbilt
University
Medical Center.)
530 Chapter? Wavelets and Multiresolution Processingab
cdFIGURE 7.27
Modifying a DWT
for edge
detection: (a) and
{c) two-scale
decompositions
with selected
coefficients
deleted; (b) and
(d) the
corresponding
reconstructions.EXAMPLE 7.14:
Wavelet-based
noise removal,;
L
3‘nace=~ = i As a second example, consider the CT image of a human head shown in
Fig. 7,28(a). As can be seen in the background, the image has been uniformly
corrupted with additive white noise. A general wavelet-based procedure for
denoising the image (i.e., suppressing the noise part) is as follows:Step 1. Choose a wavelet (e.g. Haar. symlet,...) and number of levels
(scales). P, for the decomposition. Then compute the FWT of the noisy
image.Step 2. Threshold the detail coefficients. That is, select and apply a threshold to the detail coefficients from scales J — 1 to J — P. This can be accomplished by hard thresholding, which means setting to zero the elements
whose absolute values are lower than the threshold, or by soft thresholding, which involves first selling to zero the elenrents whose absolute values
are lower than the threshold and then scaling the nonzero coefficients toward zero. Soft thresholding eliminates the discontinuity (at the threshold)
that is inherent in hard thresholding. (See Chapter 10 for a discussion of
thresholding.)Step 3. Compute the inverse wetului transform (.¢., perform a wavelet reconstruction) using the original approximation coefficients atlevel J ~ P and the
modified detail coefficients fo: A Lied FP,
532  Geapter 7 m Wavelets and Multiresolution ProcessingBecause only the highest
resolution detail
coefficients were kept
when generatingFig. 7.28(d). the inverse
transform is their conteibution to the image. In
the same way, Fig. 7.28(£)
is the contribution of alt
the detail coefficients.abFIGURE 7.29An (a) coefficient
tree and{b) analysis tree
for the two-scale
FWT analysis
bank of Fig, 7.18.was generated by simply zeroing the highest-resolution detail coefficients
(not thresholding the lower-resolution details) and reconstructing the
image. Here, almost all of the background noise has been eliminated and
the edges are only slightly disturbed. The difference image in Fig. 7.28(d)
shows the information that is Jost in the process. This result was generated
by computing the inverse FWT of the two-scale transform with all but the
highest-resolution detail coefficients zeroed. As can be seen, the resulting
image contains most of the noise in the original image and some of the edge
information. Figures 7.28(e) and (f) are included to show the negative effect
of deleting all the detail coefficients, That is, Fig. 7.28(e) is a reconstruction
of the DWT in which the details at both levels of the two-scale transform
have been zeroed; Fig. 7.28(f) shows the information that is lost. Note the
significant increase in edge information in Fig. 7.28(f) and the corresponding decrease in edge detail in Fig. 7.28(e). Es]Wavelet PacketsThe fast wavelet transform decomposes a function into a sum of scaling and
wavelet functions whose bandwidths are logarithmically related. That is, the
low frequency content (of the function) is represented using (scaling and
wavelet) functions with narrow bandwidths, while the high-frequency content
is represented using functions with wider bandwidths. If you look along the
frequency axis of the time-frequency plane in Fig. 7.23(c), this is immediately
apparent. Each horizontal strip of constant height tiles, which contains the
basis functions for a single FWT scale, increases logarithmically in height as
you move up the frequency axis. If we want greater control over the partitioning of the time-frequency plane (e.g., smaller bands at the higher frequencies),
the FWT must be generalized to yield a more flexible decomposition—called
a wavelet packet (Coifman and Wickerhauser [1992]). The cost of this generalization is an increase in computational complexity from O( M) for the FWT to
O(M log, M) for a wavelet packet.Consider again the two-scale filter bank of Fig. 7.18(a)— but imagine the decomposition as a binary tree. Figure 7.29(a) details the structure of the tree, and
links the appropriate FWT scaling and wavelet coefficients [from Fig, 7.18(a)] to
its nodes, The reot node is assigned the highest-scale approximation coefficients,W,G.n) = feo) WiF~ lon) Welf- bn) Vy W,y
WJ -2.n) Wd - 2.) Vp. Wy-2
7.6 & Wavelet PacketsW)- pp   2h[2 Le W14024 Hey saaWie 1p  
 
     
    
  
  
 wh (-n)f@ev;
2 pe Wy_2,0   Wi-aa |A(w))       Vi.4 \ Wyoy 1fi
'ja— Vy_g et Wy .2 —o le Wyn he Wyo
f ' _ 0 2/8 w/4 a{2 7V, = Vj-3 © Wy-3 B Wy 2 4 B Wyo DOW)-1, 4a@ Wy), 4p ® Wy-1.04@Wy-1,00 (7-6-4)
whose spectrum is shown in Fig. 7.32(b), or Be
Vz = Vy-1 ® Wy-1, 4 ® Wy-1, a B Wy-1.0d (7.6-5)whose spectrum is depicted in Fig. 7.33. Note the difference between this last
spectrum and the full packet spectrum of Fig. 7.32(b), or the three-scale FWT|4(w}  0 a/2  SafBBar f4 7535a
bFIGURE 7.32The (a) filter bank
and (b) spectrum
splitting
characteristics of
a three-scale full
wavelet packet
analysis tree.Recall that ® denotes
the union of spaces {like
the union of sets). The 26
decompositions associated
with Fig-7.3) are
determined by various
combinations of nodes
(spaces) that can he
combined lo represent
the rval nade (space) at
the Lop of the tree.Eqs (7.6-4) and (76-8)
define two of them.FIGURE 7.33The spectrum of
the decomposition in Eq. (7.6-5).
534 Chapter 7 m Wavelets and Multiresolution ProcessingFIGURE 7.31#. three-scale
wavelet packet
analysis Lree.easy to detect valid decompositions. The three-scale analysis tree of Fig. 7.30(b),
for example, makes possible the following three expansion options:Vv, = Vy, OW), (7.6-1)
Vy = Vj-2® W)-2® W,_, (7.6-2)
= Vj-3 BW)_3 8 W,_2 8 Wy; (76-3)They correspond to the one-, two-, and three-scale FWT decompositions of
Section 7.4 and may be obtained from Eq. (7.2-27) of Section 7.2.3 by letting
jo = J — Pfor P = {1,2, 3}. In general, a P-scale FWT analysis tree supports
P unique decompositions.Analysis trees also are an efficient mechanism for representing wavelet packers,
which are nothing more than conventional wavelet transforms in which the details
are filtered iteratively. Thus, the three-scale FWT analysis tree of Fig. 7.30{b)
becomes the three-scale wavelet packet tree of Fig. 7.31, Note the additional subScripting that is introduced. The first subscript of a double-subscripted node
identifies the scale of the FWT parent node from which it descended. The
second —a variable length string of As and Ds— encodes the path from the parent to the node. An A designates approximation filtering, while a D indicates detail filtering. Subspace W, ~ |, p4, for example, is obtained by “filtering” the scale
J — 1 FWT coefficients (i.e., parent W,_, in Fig. 7.31) through an additional detail filter (yielding W;_,p), followed by an approximation filter (giving Wy~1,5,).
Figures 7,.32(a) and (b) are the filter bank and spectrum splitting characteristics
of the analysis tree in Fig. 7.31. Note that the “naturally ordered” outputs of the
filter bank in Fig, 7.32(a) have been reordered based on frequency content in
Fig. 7.32(b) (see Problem 7.25 for more on “frequency ordered” wavelets).The three-scale packet tree in Fig. 7,31 almost triples the number of decompositions (and associated time-frequency tilings) that are available from the threescale FWT tree, Recail that in a normal FWT, we split, filter, and downsample the
lowpass bands alone. This creates a fixed logarithmic (base 2) relationship between the bandwidths of the scaling and wavelet spaces used in the representation of a function [see Figure 7.30(c)]. Thus, while the three-scale FWT analysis
tree of Fig. 7.30(a) offers three possible decompositions — defined by Eqs. (7.6-1)
to (7.6-3) —the wavelet packet tree of Fig. 7.31 supports 26 different decompositions. For instance, V; [and therefore function f(n)] can be expanded asa
ZN vanWren Wi-ipS\N SSN ISWrora Wrosp Wroy as Wroay Wena We von
536 Chapter 7 m Wavelets and Multiresolution ProcessingabFIGURE 7.34The first
decomposition of
a two-dimensional
FWT: (a) the
spectrum and(b) the subspace
analysis tree.spectrum of Fig. 7.30(c). In general, P-scale, one-dimensional wavelet packet
transforms (and associated P + 1-level analysis trees) supportD(P +1) =[D(P)f +1 (7.6-6)unique decompositions, where D(1) = 1. With such a large number of valid
expansions, packet-based transforms provide improved control over partitioning the spectrum of the decomposed function. The cost of this control is an increase in computational complexity [compare the filter bank in Fig. 7.30(a) to
that of Fig. 7.32(a)].Now consider the two-dimensional, four-band filter bank of Fig. 7.24(a). As
was noted in Section 7.5, it splits approximation W,(j + 1, m, 2) into outputs,
WC, m,n), wee, m,n), Wii, m,n), and weg, m,n). As in the onedimensional case, it can be “iterated” to generate P scale transforms for scales
f=J-1,3 —2,...,4 — P, with WJ, m,n) = f(m,n). The spectrum resulting from the first iteration [ive., using j + 1 = J in Fig. 7.24(a)] is shown in
Fig. 7.34(a). Note that it divides the frequency plane into four equal areas. The
low-frequency quarter-band in the center of the plane coincides with transform coefficients W,(J — 1, m, 2) and scaling space V;_,. (This nomenclature
is consistent with the one-dimensional case.) To accommodate the twodimensional nature of the input, however, we now have three (rather than one)
wavelet subspaces. They are denoted W§'_,, WY_,, and W7_, and correspond
to coefficients WH(J — 1,m.n), WY ~ Loma) and WPT - 1.m,n),
respectively. Figure 7.34(b) shows the resulting four-band, single-scale guaternary
FWT analysis tree. Note the superscripts that link the wavelet subspace designations to their transform coefficient counterparts.Figure 7.35 shows a portion of a three-scale, two-dimensional wavelet packet
analysis tree. Like its one-dimensional counterpart in Fig. 7.31, the first subscript of
every node that is a descendant of a conventional FWT detail node is the scale of
that parent detail node, The second subscript —a variable length string of As, Hs, Vs,
and Ds—encodes the path from the parent to the node under consideration. The
node labeled W#_, yp, for example, is obtained by “row/column filtering” thevertical
7.6 ® Wavelet Packets 537iH Vv
Vy_y Wy; Wy;>
W)-yONIN ATW ZIWWK INSci #
Vy-2 W)-2 WrayZI T_T" v D oH ” H H H # H
Vy-3 Wis Wy-s Woes Wreoa Wo-2n Wien Wien | Wieava Wreavia Wo-iw We-ivoFIGURE 7.35 A three-scale, ful] wavelet packet decomposition tree. Only a portion of the tree is provided,scale J - 1 FWT horizontal detail coefficients (ic. parent W}, in Fig. 7.35)
through an additional detail/approximation filter (yielding W#?_, ,), followed by a
detail/detail filter (giving W 3, yp). A P-scale, two-dimensional wavelet packet tree supportsDP +1) = [Dip)}* +1 (76-7)unique expansions, where D({t) = 1. Thus, the three-scale tree of Fig. 7.35 offers 83,522 possible decompositions. The problem of selecting among them is
the subject of the next example.% As noted in the above discussion, a single wavelet packet tree presents numerous decomposition options. In fact, the number of possible decompositions is
often so large that it is impractical, if not impossible, to enumerate or examine
them individuaily. An efficient algorithm for finding optimal decompositions with
respect to application specific criteria is highly desirable. As will be seen, classical
entropy- and energy-based cost functions are applicable in many situations and
are well suited for use in binary and quaternary tree searching algorithms.Consider the problem of reducing the amaunt of data needed to represent
the 400 x 480 fingerprint image in Fig. 7.36(4)..Image compression is discussed in detail in Chapter 8. In this example, we want to select the “best”
three-scale wavelet packet decomposition as a starting point for the compression process. Using three-scale wavelet packet trees, there are 83,522
[see Eg. (7.6-7)] potential decompositions. Figure 7.36(b) shows one of
them—a full wavelet packet, 64-leaf decomposition like the analysis tree of
Fig. 7.35. Note that the leaves of the tree correspond to the subbands of the
8 X 8 array of decomposed subimages in Fig, 7.36(b). The probability that this
particular 64-leaf decomposition is in some way optimal for the purpose of
compression, however, is relatively low. In the absence of a suitable optimality
criterion, we can neither confirm nor deny it.One reasonable criterion for selecting a decomposition for the compression
of the image of Fig. 7.36(a) is the additive cost functionE(f) = Dl Ffen, n)| (7.68)minEXAMPLE 7.15:
Two-dimensional
wavelet packet
decompositions.The 64 Iwaf nudes inFig. 7.35 correspond to
the 8 X 8 array of 64
sSubimages in Fig. 7.36(b),
Despite appearances,
they are nol square. The
distortivn (particularly
noticeable in the
approximation subimnage) is duc to the
Program used to produce
the result.
538 Chapter 7 m Wavelets and Multiresolution Processing abFIGURE 7.36 (a) A scanned fingerprint and (b) ils three-scale, full wavelet packet decomposition. (Original
image courtesy of the National Institute of Standards and Technology.)Other possible energy
measures include the
sum of the squares of
F(x. y).the sum of the
log of the squares, etc.
Problem 7.27 defines one
possible entropy-based
cost function,This function provides one possible measure of the energy content of twodimensional function f. Under this measure, the energy of function f(s, 2) = 0
for all m and » is 0. High values of , on the other hand, are indicative of functions with many nonzero values. Since most transform-based compression
schemes work by truncating or thresholding the small coefficients to zero, a cost
function that maximizes the number of neas-zero values is a reasonable criterion
for selecting a “good” decomposition from a compression point of view.The cost function just described is both computationally simple and easily
adapted to tree optimization routines. The optimization algorithm must use
the function to minimize the “cost” of the leaf nodes in the decomposition
tree. Minimal energy leaf nodes should be favored because they have more
near-zero values, which Jeads to greater compression. Because the cost function of Eq. (7.6-8) is a local measure that uses only the information available at
the node under consideration, an efficient algorithm for finding minimal energy
solutions is easily constructed as follows:For each node of the analysis tree, beginning with the root and proceeding
level by level to the leaves:Step 1. Compute both the energy of the node, denoted Fp (for parent
energy). and the energy of its four offspring--denoted £4, Fy, Ey, and
Ey. For two-dimensional wavelet packet decompositions, the parent is
a two-dimensional array of approxiraation of detail coefficients; the7.6 & Wavelet Packets 539offspring are the filtered approximation, horizontal, vertical, and diagonal details.Step 2. If the combined energy of the offspring is Jess than the energy of the
parent—that is, £4 + Ey + Ey + Ep < Ep—include the offspring in the
analysis tree. If the combined energy of the offspring is greater than or
equal to that of the parent, prune the offspring, keeping only the parent. it
is a leaf of the optimized analysis tree.The preceding algorithm can be used to (1) prune wavelet packet trees or (2)
design procedures for computing optimal trees from scratch. In the latter case.
nonessential siblings— descendants of nodes that would be eliminated in step 2
of the algorithm —would not be computed. Figure 7.37 shows the optimized
decomposition that results from applying the algorithm to the image of
Fig. 7.36(a) with the cost function of Eq. (7.6-8). The corresponding analysis
tree is given in Fig. 7.38. Note that many of the original full packet decomposition’s 64 subbands in Fig. 7.36(b) (and corresponding 64 leaves of the analysis tree in Fig. 7.35) have been eliminated. In addition, the subimages that are
not split (further decomposed) in Fig. 7.37 are relatively smooth and composed of pixels that are middle gray in value. Because all but the approximation subimage of this figure have been scaled so that gray tevel 128 indicates a
zero-valued coefficient, these subimages contain little energy. There would be
no overall decrease in energy realized by splitting them. %The preceding example is based on a real-world problem that was solved
through the use of wavelets. The Federal Bureau of Investigation (FBI) currently maintains a large database of fingerprints and has established a waveletbased national standard for the digitization and compression of fingerprint FIGURE 7.37An optimal
wavelet packet
decomposition for
the fingerprint of
Fig, 7.36(a).
ho{n)
120.8
06
0.4
02o ol
=~0.2
ost?
0.6
04
0.2
0=-0.2(x)
14
121
08
06
0.4
0.20
02
-04 i:on)0 2 4 6 8 10 12 14 161
¥@)12
t
0.8
0.6
04
02
0
0.2 
0 2 4 6 8 10 12 14 16 187.6 @ Wavelet Packets 541 -0.8 -] x
04.2.4 6 8 16 12 14 16 18fe(x)
1510.5
0
-0.5—-1 x
0 2 4 6 8 10 12 14 16 18Oo %
maeBhFIGURE 7.39A member of the
CohenDaubechiesFeauveau
biorthogonal
wavelet family:
(a) and (b)
decomposition
filter coefficients;
(c) and (d)
reconstruction
filter coefficients;
(e)-(h) dual
wavelet and
scaling functions.
See Table 7.3 for
the values of
hio(n) and Ayn)
for0 sn 5 17.
540 Chopter 7 m Wavelets and Multiresolution ProcessingOP 7 WwVierY,es eeVy-4 18 ¥ ¥ v v v
Weer Wa Wer Wri Wie Wao4 voo ow NH eit vo ogy ye get BD UP ve op v ¥ ¥ v
Vie Wing Wee Way Wrag Wren Wray Wren Winn Wee at Win Were Wray Winewn Wea Wea) Waa Wie Winway Wy -vapa
Wri7# ” ¥ #
Wir Wein Www Wie~7I~Na # # ” a “t "
Whiaa Wievan Weeray Wrap Wi-104 Win Ws-1ov WireFIGURE 7.38 The optimal wavelet packet analysis tree for the decomposition in Fig. 7.37.TABLE 7.4
Biorthogonai
CohenDaubechiesFeauveau filter
coefficients
(Cohen,
Daubechies, andFeauveau [1992]).images (FBI [1993]). Using biorthogonal wavelets, the standard achieves a typical
compression ratio of 15:1. The advantages of wavelet-based compression over
the more traditional JPEG approach are examined in the next chapter.The decomposition filters used in Example 7.15, as well as by the FBI, are
part of a well-known family of wavelets called Cohen-Daubechies-Feauveau
biorthogonal wavelets (Cohen, Daubechies, and Feauveau (1992]). Because
the scaling and wavelet functions of the family are symmetrical and have similar lengths, they are among the most widely used biorthogonal wavelets,
Figures 7.39(e) through (h) show the dual scaling and wavelet functions.
Figures 7.39(a) through (d) are the corresponding decomposition and reconstruction filters. The coefficients of the lowpass and highpass decomposition
filters, Ag(7) and A,(n) for 0 = x = 17 are shown in Table 7.4. The corresponding coefficients of the biorthogonal synthesis filters can be computed
using go(7t) = (—1)"*1A,(n) and gy(m) = (-1)" Ao(71) of Eq. (7.1-11). That is,
they are cross-modulated versions of the decomposition filters. Note that zero
padding is employed to make the filters the same length and that Table 7.4 and
Fig, 7.39 define them with respect to the subband coding and decoding system
of Fig. 7.6(a); with respect to the FWT, A,(—n) = ho(n) and hy(—n) = hy(n).            n Aig{n) io(n) y(t)
0 0 0 0.8259 0.4178
1 0.0019 0 10 0.4208 0.0404
2 -0.0019 0 1 0.0941 — 0.0787
3 -0.017 0.0144 12 ~-0.0773 —0.0145
4 0.0119 ~0.0145 13 0.0497 0.0144
5 0.0497 0.0787 14 0.0119 06 —0.0773 0.0404 15 ~0.017 07 ~0.0941 0.4178 16 -0.0019 08 1.4208 —0,7589 17 0.0040 0
@ Problems 543Processing (1993), and a special section on multiresolution representation in the EEE
Transactions on Pattern Analysis and Machine Intelligence [1989].Although the chapter focuses on the fundamentals of wavelets and their application
to image processing, there is considerable interest in the construction of wavetets themselves, The interested reader is referred to the work of Battle [1987] [1988], Daubechies
[1988] [1992], Cohen and Daubechies (1992], Meyer [1990], Mallat [1989], Unser,
Aldroubi, and Eden [1993], and Grichenig and Madych [1992]. This is not an exhaustive list but should serve as a starting point for further reading. See also the general references on subband coding artd filter banks, including Strang and Nguyen [1996] and
Vetterli and Kovacevic [1995], and the references included in the chapter with respect
to the wavelets we used as examples.Problems
7.1 Design a system for decoding the prediction residual pyramid generated by the
encoder of Fig. 7.2(b) and draw its block diagram. Assume there is no quantization error introduced by the encoder.
&7,2 Construct a fully populated approximation pyramid and corresponding prediction residual pyramid for the image12 «13 14_|15 16 17° 18
FOEN=Fi9 29 21 22
3 24 25 2%Use 2 X 2 block neighborhood averaging for the approximation filter in Fig. 7.2(b)
and assume the interpolation filter implements pixe) replication.&73 Given a 2/ x 2! image, does a J + 1-level pyramid reduce or expand the
amount of data required to represent the image? What is the compression or expansion ratio?7.4 Is the two-band subband coding filter bank containing filters ho(rt) = { ~1/Vv2,
-1/V3}, hyn) = {-1/V2, 1/V2Z}, gol) = (1/2, -1/V3}, and g(r)
= {1/V2, - i/v2} orthonormal, biorthogonal, or both?75 Given the sequence f(n) = {0.1, 0.25, 0.5, 1, where n = 0, 1,2, 3, compute:
(a) The sign-reversed sequence. :{b) The order-reversed se§uence.(c) The modulated sequence.(d) The modulated and then order-reversed sequence.(e} The order-reversed and then modulated sequence,(f) Does the result from (d) or (e) correspond to Eq. (7.1-9}?76 Compute the coefficients of the Daubechies synthesis filters Bo(n) and g)(n)
for Example 7.2. Using Eq. (7.1-13) with m = 0 only, show that the filters are
orthonormal. Are the filters orthogonal for m= 17.7 Draw a two-dimensiona] four-band filter bank decoder to reconstruct input
f (mm, n) in Fig, 7.7.7.8 Obtain the Haar transformation matrix for NV = 8.79 (a) Compute the Haar transform of the 2 x 2 imageelt |
542Chopter 7. Wavelets and Multiresolution ProcessingSummaryThe material of this chapter establishes a solid mathematical foundation for understanding and accessing the role of wavelets and multiresolution analysis in image processing, Wavelets and wavelet transforms are relatively new imaging tools that are
being rapidly applied to a wide variety of image processing problems. Because of their
similarity to the Fourier transform, many of the techniques in Chapter 4 have wavelet
domain counterparts. A partial listing of the imaging applications that have been approached from a wavelet point of view includes image matching, registration, segmentation, denoising, restoration, enhancement, compression, morphological filtering, and
computed tomography. Since it is impractical to cover all of these applications in a single chapter, the topics included were chosen for their value in introducing or clarifying
fundamental concepts and preparing the reader for further study in the field. Jn
Chapter 8, we will apply wavelets to the compression of images.References and Further ReadingThere are many good texts on wavelets and their application. Several complement our
treatment and were relied upon during the development of the core sections of the
chapter, The material in Section 7.1.2 on subband coding and digital filtering follows
the book by Vetterlit and Kovacevic [1995], while Sections 7.2 and 7.4 on multiresolution expansions and the fast wavelet transform follow the treatment of these subjects in
Burrus, Gopinath, and Guo [1998]. The remainder of the material in the chapter is
based on the references cited in the text. All of the exampies in the chapter were done
using MATLAB (see Gonzalez et al. (2004)).The history of wavelet analysis is recorded in a book by Hubbard [1998]. The early
predecessors of wavelets were developed simultaneously in different fields and unified
in a paper by Mallat [1987]. It brought a mathematical framework to the field. Much of
the history of wavelets can be traced through the works of Mever [1987] [1990] [1992a,
1992b] [1993], Mallat [1987] [1989a-c] [1998], and Daubechies [1988] [1990] [1992]
[1993] [1996]. The current interest in wavelets was stimulated by many of their publications. The book by Daubechies [1992] is a classic source for the mathematical details of
wavelet theory.The application of wavelets to image processing is addressed in general image processing texts, tike Castleman [1996], and many application specific books, some of
which are conference proceedings. In this latter category, for example, are Rosenfeld
[1984], Prasad and Lyengar [1997}, and Topiwala [1998]. Recent articles that can serve
as starting points for further research into specific imaging applications include Gao
et al. [2007] on corner detection; Olkkonen and Olkkonen [2007] on lattice implementations; Selesnick et al. [2005] and Kokare et al. [2005] on complex wavelets; Thévenaz
and Unser [2000] for image registration; Chang and Kuo [1993] and Unser (!995] on
texture-based classification; Heijmans and Goutsias [2000] on morphological wavelets;
Banham et al. [1994], Wang, Zhang, and Pan [1995], and Banham and Kastaggelos
[1996] on image restoration; Xu et al. [1994] and Chang, Yu, and Vetterli [2000] on
image enhancement; Delaney and Bresler [1995] and Westenberg and Roerdink [2000]
on computed iomography; and Lee, Sun, and Chen [1995], Liang and Kuo [1999], Wang.
Lee, and Toraichi [1999], and You and Bhattacharya [2000] on image description and
matching. One of the most important applications of wavelets is image compression —
see, for example, Brechet et al. [2007]. Demin Wang et al. (2006), Antonini et al. [1992].
Wei ct al. [1998], and the book by Topiwala [1998]. Finally, there have been a number of
special issues devoted to wavelets, including a special issue on wavelet transforms and
multiresolution signal anaysis in the /EEE Transactions on Information Theory [1992].
@ special issue on wavelets and signal processing in the /EEE Transactions on Signal
544 Chepter 7 m Wavelets and Multiresolution Processing(b) The inverse Haar transform is F = H“TH, where T is the Haar transform of
F and H’ js the matrix inverse of H. Show that Hy) = Hi and use it to compute the inverse Haar transform of the result in (a).
7.10 Compute the expansion coefficients of 2-tuple [1,3]' for the following bases
and write the corresponding expansions:
*(a) Basis yy = [1/v2, v2," and g, = i/v3, -1/ v3, on R®, the set of
teal 2-tuples.
(b) Basis gp = [1,0]? and g, = [1.1]’. and its dual, = [t,-1]" and
%, = (0, 1], on R?.
(c) Basis gy = [1, 0)", 9, = [-1/2. VI/2]! andy, = [~1/2,-V3/2]" and
their duals. $, = 2,/3 for? = {0.1.2.J,00 R?.
(Hint: Vector inner products must be used in place of the integral inner products
of Section 7.2.].)
7.11 Show that scaling functiongay = {1 OS Sx < 075
a 0 elsewheredoes not satisfy the second requirement of a multiresolution analysis.7.12 Write an expression for scaling space V3 as a function of scaling function g(x).
Use the Haar scaling function definition of Eq. (7.2-14) to draw the Haar >
scaling functions at translations & = {0.1.2.3}.*7A3 Draw wavelet y2>(x) for the Haar wavelet function. Write an expression for
#22(x) in terms of Haar scaling functions.7.14 Suppose function f(x) is a member of Haar scaling space W,—that is.
f(x)e V5. Use Eq. (7.2-22) lo express V2 as a iunction of scaling space Vy and
any required wavelet spaces. [f f(x) is 0 outside the interval (0, 1}, sketch the
scaling and wavelet functions required for a linear expansion of f(x) based on
your expression. :7.18 Compute the first four terms of the wavelet series expansion of the function
used in Example 7.7 with starting scale jy = 2. Wrile the resulting expansion in
terms of the scaling and wavelet functions invalved. How does your result compare to the example. where the starting scale was jy = 077.16 The DWT in Egs. (7.3-5) and (7.3-6) is a function of starting scale jo.(a) Recompute the one-dimensional DWT of function f(n) = {1.3.0.4} tor
0 = a” S 3in Example 7.8 with jf = 1 (rather than 0).
(b) Use the result from (a) to compute f(2) from the transform values.
7.17) What does the following continuous wavelet Wansform reveal about the onedimensional function upon which it was based? Scale Hime
7.187,197.20(a) The continuous wavelet transform of Problem 7.17 is computer generated.
The function upon which it is based was first sampled at discrete intervals.
What is continuous about the transform-~-or what distinguishes it from the
discrete wavelet transform of the function?*(b) Under what circumstances is the DWT a better choice than the CWT? Are
there times when the CWT is better than the DWT?Draw the FWT filter bank required to compute the transform in Problem 7.16.Label all inputs and oubputs with the appropriate sequences.The computational complexity of an A¢-point fast wavelet transform is O(M).That is, the number of operations is proportional 44, What determines the con
stant of proportionality?7.21 * (a) If the input to the three-scale FWT filter bank of Fig. 7.30(a) is the Haar7,227.23«7.247.25scaling function w(”) = | for st = 0, 1,....7 and 0 elsewhere, what is the resulting transform with respect to Haar wavelets?
(b) What is the transform if the input is the corresponding Haar wavelet function ¢{n) = (1,1, 1.1, -1.-1, -1,-1} fora = 0,1.....77
(c) What input sequence produces transform {0, 0, 0, 8,0, 0,0, 0} with nonzcre
coefficient W,(1.1) = B?
The two-dimensional fast wavelet transform is similar to the pyramidal coding
scheme of Section 7.2.1. How are they similar? Given the three-scale wavelet
transform in Fig, 7.10(a), how would you construct the corresponding approximation pyramid? How many levels would it have?
Compute the two-dimensiona! wavelet transform with respect to Haar wavelets
of the 2 X 2 image in Problem 7.9. Draw the required filter bank and label all
inputs and outputs with the proper arrays. In the Fourier domainFs ~ x0, ¥ ~ Yo) 69 P(g, ve mee MH)and translation does not affect the display of | F(z, v}|. Using the following sequence of images, explain the translation property of wavelet transforms, The
leftmost image contains two 16 X 16 white squares centered on a 64 x 64 gray
background. The second image (from the left) is its single-scale wavelet transform with respect to Haar wavelets. The third. is the wavelet transform of the
original image after shifting it 16 pixels to the right and downward, and the final
(rightmost) image is the wavelet transform of the original image after it has
been shifted one pixel to the right and downward. The following table shows the Haar wavelet and scaling functions for a fourscale fast wavelet transform. Sketch the additional basis functions needed for a
full three-scale packet decomposition. Give the mathematical expression or expressions for determining them. Then order the basis functions according ic fre
quency content and explain the results.t Problems545
546 Gupter 7 @ Wavelets and Multiresolution Processing,7.267.27          A wavelet packet decomposition of the vase from Fig, 7.1 is shown below.{a) Draw the corresponding decomposition analysis tree, labeling all nodes
with the names of the proper scaling and wavelet spaces.(b) Draw and label the decomposition’s frequency spectrum. Using the Haar wavelet, determine the minimum entropy packet decomposition
for the function f(m) = 0.5 for n = 0, 1.2,..., 15. Employ the nonnormalized
Shannon entropy,El f(n)] = SPO In! f(a)!ah  as the minimization criterion. Draw (he opiimal tree fabeling the aecdes with thecomputed entropy values.
Image Compression But life is short and information endless ...
Abbreviation is a necessary evil and the abbreviator’s
business is to make the best of a job which, although
intrinsically bad, is still better than nothing., Aldous Huxley PreviewImage compression, the art and science of reducing the amount of data required
to represent an image, is one of the most useful and commercially successful
technologies in the field of digital image processing. The number of images that
are compressed and decompressed daily is staggering, and the compressions
and decompressions themselves are virtually invisible to the user. Anyone who
owns a digital camera, surfs the web, or watcfigs the latest Hollywood movies
on Digital Video Disks (DVDs) benefits from the algorithms and standards
discussed in this chapter.To better understand the need for compact image representations, consider
the amount of data required to represent a two-hour standard definition (SD)
television movie using 720 X 480 X 24 bit pixel arrays. A digital movie (or
video) is a sequence of video frames in which each frame is a full-color still
image. Because video players must display the frames sequentially at rates
near 30 fps (frames per second), SD digital video data must be accessed atel _, byt
go tames frames x (720 x 490) EAE s bytes =a}
fran 2 pixel 1,104,000 bytes/secand a two-hour movie consists ofsec ; 1"
60° he x 2hrs = 2.24 x 10" bytes 31,104,000547
548 Chapter 8 @ Image Compressionor 224 GB (gigabytes) of data. Twenty-seven 8.5 GB dual-layer DVDs (assuming
conventional 12 cm disks) are needed to store it. To put a two-hour movie on a
single DVD, each frame must be compressed--on average —by a factor of 26.3.
The compression must be even higher for high definition (HD) television, where
image resolutions reach 1920 X 1080 X 24 bits/image.Web page images and high-resolution digital camera photos also are compressed routinely to save storage space and reduce transmission time. For example, residential Internet connections deliver data at speeds ranging from 56 Kbps
{kilobits per second) via conventional phone lines to more than 12 Mbps
(megabits per second) for broadband. The time required to transmit a small
128 X 128 X 24 bit full-color image over this range of speeds is from 7.0 to
0.03 seconds. Compression can reduce transmission time by a factor of 2 to 10
or more. In the same way, the number of uncompressed full-color images that
an 8-megapixel digital camera can store on a 1-GB flash memory card [about
forty-one 24 MB (megabyte) images] can be similarly increased. In addition to
these applications, image compression plays an important role in many other
areas, including televideo conferencing, remote sensing, document and medical
imaging, and facsimile transmission (FAX). An increasing number of applications depend on the efficient manipulation, storage, and transmission of binary,
gray-scale, and color images.In this chapter, we introduce the theory and practice of digital image compression. We examine the most frequently used compression tcchniques and
describe the industry standards that make them useful. The material is introductory in nature and applicable to both still image and video applications. The chapter concludes with an introduction to digital image watermarking, the process of
inserting visible and invisible data (like copyright information) into images.Ee FundamentalsThe term data compression refers to the process of reducing the amount of data
required to represent a given quantity of information. In this definition, data and
information are not the same thing; data are the means by which information is
conveyed. Because various amounts of data can be used to represent the same
amount of information, representations that contain irrelevant or repeated
information are said to contain redundant data. If we let b and b’ denote the number of bits (or information-carrying units) in two representations of the same
information, the relative data redundancy R of the representation with 6 bits isR=1-4 (81-1)Cc > ,where C, commonly called the compression ratio, is defined asb
Cay (8.1-2)If C = 10 (sometimes written 10:1), for instance, the larger representation
has 10 bits of data for every 1 bit of data in the smaller representation.
8.1 = Fundamentals 549The corresponding relative data redundancy of the larger representation is 0.9
(R = 0.9), indicating that 90% of its data is redundant.In the context of digital image compression, b in Eq. (8,.1-2) usually is the
number of bits needed to represent an image as a 2-D array of intensity values.
The 2-D intensity arrays introduced in Section 2.4.2 are the preferred formats
for human viewing and interpretation—and the standard by which all other
representations are judged. When it comes to compact image representation,
however, these formats are far from optimal. Two-dimensional intensity arrays
suffer from three principal types of data redundancies that can be identified
and exploited:1. Coding redundancy. A code is a system of symbols (letters, numbers, bits, and
the like) used to represent a body of information or set of events, Each piece
of information or event is assigned a sequence of code symbols, called a code
word. The number of symbols in each code word is its length. The 8-bit codes
that are used to represent the intensities in most 2-D intensity arrays contain
more bits than are needed to represent the intensities.2. Spatial and temporal redundancy. Because the pixels of most 2-D intensity
arrays are correlated spatially (i.e., each pixel is similar to or dependent on
neighboring pixels), information is unnecessarily replicated in the representations of the correlated pixels. In a video sequence, temporally correlated pixels (i.¢., those similar to or dependent on pixels in nearby frames}
also duplicate information.3. Irrelevant information. Most 2-D intensity arrays contain information that
is ignored by the human visual system and/or extraneous to the intended
use of the image. It is redundant in the sense that it is not used.The computer-generated images in Figs. 8.1(a) through (c) exhibit each of these
fundamental redundancies. As will be seen in the next three sections, compression is achieved when one or more redundancy is reduced or eliminated. ja BecFIGURE 8.1 Computer generated 256 X 256 X 8 bit images with (a) coding redundancy, (b) spatial redundancy.
and (c) irrelevant information. (Each was designed to demonstrate one principal redundancy but may exhibit
others as well.}
550  Gupter 8 Image CompressionEXAMPLE 8.1:
A simple
illustration of
variable-length
coding.TABLE 8.1
Example of
variable-length
coding.8.1.1 Coding RedundancyIn Chapter 3, we developed techniques for image enhancement by histogram
processing, assuming that the intensity values of an image are random quantities, In this section, we use a similar formulation to introduce optimal information coding.Assume that a discrete random variable r, in the interval [0, L — 1] is used
to represent the intensities of an M X N image and that each ry, occurs with
probability p,{r,). As in Section 3.3,= ke = _ PAG) = Fay KE OLZ LAI (8.1-3)where L is the number of intensity values, and n, is the number of times that
the kth intensity appears in the image. If the number of bits used to represent
each value of 7; is /(r,), then the average number of bits required to represent
each pixel isL-1
Lavg = Blended) (8.1-4)That ts, the average length of the code words assigned to the various intensity
values is found by summing the products of the number of bits used to represent each intensity and the probability that the intensity occurs. The total
number of bits required to represent an M X N image is MNL,yg. If the
intensities are represented using a natural m-bit fixed-length code,’ the
right-hand side of Eq. (8.1-4) reduces to m bits. That is, Layg = m when m is
substituted for /(r,). The constant m can be taken outside the summation,
leaving only the sum of the p,{r,) for 0 = k = L — 1, which, of course, equals 1.® The computer-generated image in Fig. 8.1(a) has the intensity distribution
shown in the second column of Table 8.1. If a natural 8-bit binary code (denoted
as code 1 in Table 8.1) is used to represent its 4 possible intensities, Z.,,,—the
average number of bits for code 1—is 8 bits, because /,(r,) = 8 bits for all r,.PAT) Code 1 U(r) Code2 42{ ry)r37 = 87 0.25 01010111
Pig = 128 0.47 10000000
igs = 186 0.25 11000100
toss = 255 0,03 12141111
ry for k # 87, 128, 186,255 0 
   *A natural binary code is one in which each event or piece of information to he encoded (such as intensity value) is assigned one of 2” codes from an m-bit binary counting sequence.
8.) m Fundamentals 551On the other hand, if the scheme designated as code 2 in Table 8.1 is used, the average length of the encoded pixels is, in accordance with Eq. (8.1-4),Layg = 0.25(2) + 0.47(1) + 0.25(3) + 0.03(3) = 1.81 bitsThe total number of bits needed to represent the entire image is MNLivg =
256 X 256 X 1.81 or 118,621. From Eqs. (8.1-2) and (8.1-1), the resulting compression and corresponding relative redundancy are_ 256 X 256x8 8= * 4.42c 118,621 81andR=1 = 0.774442
respectively. Thus 77.4% of the data in the original 8-bit 2-D intensity array is
redundant. .The compression achieved by code 2 results from assigning fewer bits to
the more probable intensity values than to the less probable ones, In the resulting variable-length code, rj», —the image’s most probable intensity — is assigned the 1-bit code word 1 [of length /,(r;23) = 1], while r255 —its least probable
occurring intensity —is assigned the 3-bit code word 001 [of length h( ross) = 3).
Note that the best fixed-length code that can be assigned to the intensities of
the image in Fig. 8,1 (a) is the natural 2-bit counting sequence {00, 01, 10,11},
but the resulting compression is only 8/2 or 4:1—about 10% less than the
4.42:1 compression of the variable-length code. aAs the preceding example shows, coding redundancy is present when the
codes assigned to a set of events (such as intensity values) do not take full advantage of the probabilities of the events. Coding redundancy is almost always
present when the intensities of an image are represented using a natural binary
code. The reason is that most images are composed of objects that have a regular and somewhat predictable morphology (shape) and reflectance, and are
sampled so that the objects being depicted are much Jarger than the picture elements, The natural consequence is that, for most images, certain intensities are
more probable than others (that is, the histograms of most images are not uniform). A natural binary encoding assigns the same number of bits to both the
most and least probable values, failing to minimize Eq. (8.1-4) and resulting in
coding redundancy.8.\.2 Spatial and Temporal Redundancy
Consider the computer-generated collection of constant intensity lines in
Fig. 8.1(b). In the corresponding 2-D intensity array:1. All 256 intensities are equally probable. As Fig. 8.2 shows, the histogram of
the image is uniform.
552 Chapter 8 # Image CompressionFIGURE 8.2 The
intensity histogram
of the image inFig. 8.1(b). ny256 2. Because the intensity of each line was selected randomly, its pixels are independent of one another in the vertical direction.3. Because the pixels along each line are identical, they are maximally correlated (completely dependent on one another) in the horizontal direction.The first observation tells us that the image in Fig. 8.1(b)—when represented
as a conventional 8-bit intensity array—cannot be compressed by variablelength coding alone. Unlike the image of Fig. 8.1(a) (and Example 8.1), whose
histogram was not uniform, a fixed-length 8-bit code in this case minimizes
Eq. (8.1-4). Observations 2 and 3 reveal a significant spatial redundancy that
can be eliminated, for instance, by representing the image in Fig. 8.1(b) as a
sequence of run-lengih pairs, where each run-length pair specifies the start of
a new intensity and the number of consecutive pixels that have that intensity.
A run-length based representation compresses the original 2-D, 8-bit intensity
array by (256 X 256 X 8)/[{256 + 256) x 8} or 128:1. Each 256-pixel line of
the original representation is replaced by a single 8-bit intensity value and
length 256 in the run-length representation.In most images. pixels are correlated spatially (in both x and y) and in time
(when the image is part of a video sequence). Because most pixel intensities
can be predicted reasonably weli from neighboring intensities, the information
carried by a single pixel is small. Much of its visual contribution is redundant in
the sense that it can be inferred from its neighbors. To reduce the redundancy
associated with spatially and temporally correlated pixels, a 2-D intensity array
must be transformed into a more cfficient but usually “non-visual” representation. For example, run-fengths or the differences between adjacent pixels can
be used. Transformations of this type are called mappings. A mapping is said to
be reversible if the pixels of the original 2-D intensity array can be reconstructed without error from the transformed data set; otherwise the mapping is
said to be irreversible. 58.1.3 Irrelevant InformationOne of the simplest ways to compress a set of data is to remove superfluous
data from the set. In the context of digital image compression, information that
is ignored by the human visual systein or is extraneous to the intended use of an
image are obvious candidates for omission. Thus, the computer-generated
image in Fig. 8.1(c}, because it appears to be a homogeneous field of gray, can
8.1 @ Fundamentals 553be represented by its average intensity alone —a single 8-bit value. The original
256 X 256 X 8 bit intensity array is reduced to a single byte; and the resulting
compression is (256 X 256 x 8)/8 or 65,536:1. Of course, the original
256 X 256 X 8 bit image must be recreated to view and/or analyze it—but
there would be little or no perceived decrease in reconstructed image quality.Figure 8.3(a) shows the histogram of the image in Fig. 8.1(c), Note that
there are several intensity values (intensities 125 through 131) actually
present. The human visual'system averages these intensities, perceives only
the average value, and ignores the small changes in intensily that are present in this case. Figure 8.3(b), a histogram equalized version of the image
in Fig. 8.1(c), makes the intensity changes visible and reveals two previously undetected regions of constant intensity—one oriented vertically and
the other horizontally. If the image in Fig. 8.1{c) is represented by its average value alone, this “invisible” structure (i.e, the constant intensity regions) and the random intensity variations surrounding them—real
information —is lost. Whether or not this information should be preserved is
application dependent. If the information is important, as it might be in a
medical application (like digital X-ray archival), it should not be omitted;
otherwise, the information is redundant and can be excluded for the sake of
compression performance. ,We conclude the section by noting that the redundancy examined here is
fundamentally different from the redundancies discussed in Sections 8.1.1 and
8.1.2, Its elimination is possible because the information itself is not essential
for normal visual processing and/or the intended use of the image. Because its
omission results in a loss of quantitative information, ils removal is commonly
referred to as quantization. This terminology is consistent with normal use of
the word, which generally means the mapping of a broad range of input values
to a limited number of output values (see Section 2.4). Because information is
lost, quantization is an irreversible operation.&,1.4 Measuring Image InformationIn the previous sections, we introduced several ways to reduce the amount of
data used to represent an image. The question that naturally arises is this: How nNNumber of pixels
Ww  9 50 100 150 200 250 abFIGURE 8.3(a} Histogram of
the image inFig. 8.1(c) and
(b) a histogram
equalized version
of the image:
554 chapter 8 w Image CompressionConsult the book Web
site for a brief review ofinformation and prebability theary.Equation (%.1-6} is for
zero-memury sources
with J source symbols,
Eq. (8.1-7) uses probability estimates for the£ — Fintensity vatues in
an image.few bits are actually needed to represent the information in an image? That is,
is there a minimum amount of data that is sufficient to describe an image without losing information? information theory provides the mathematical framework to answer this and related questions. Its fundamental premise is that the
generation of information can be modeled as a probabilistic process that can
be measured in a manner that agrees with intuition. In accordance with this
supposition, a random event £ with probability P(£) is said to containI(E) = lot 565 = —log P(E) (8.1-5)units of information. If P(£) = 1 (that is, the event always occurs), /(E) = 0
and no information is attributed to it. Because no uncertainty is associated
with the event, no information would be transferred-by communicating that
the event has occurred [it always occurs if P(E) = 1].The base of the logarithm in Eq. (8.1-5) determines the unit used to measure information. If the base m logarithm is used, the measurement is said
to be in m-ary units. If the base 2 is selected, the unit of information is the
bit. Note that if P(E) = $,/(E) = —log,3, or 1 bit, That is, 1 bit is the
amount of information conveyed when one of two possible equally likely
events occurs. A simple example is flipping a coin and communicating the
result.Given a source of statistically independent random events from a discrete
set of possible events {a .4@,...,@;} with associated probabilities { P(a,),
P(a2),..., P(a;}}, the average information per source output, called the
entropy of the source, isH=~ SP(a) log P(a;) (8.1-6)jvThe a; in this equation are called source symbols. Because they are statistically
independent, the source itself is called a zero-memory source.If an image is considered to be the output of an imaginary zero-memory
“intensity source,” we can use the histogram of the observed image to estimate the symbol probabilities of the source. Then the intensity source's entropy becomes ;. L-1
H= ~ RP) logs pr(%) (8.1-7)where variables L, r;,, and p,{r,) are as defined in Sections 8.1.1 and 3.3, Because the base 2 logarithm is used, Eq. (8.1-7) is the average information per
intensity output of the imaginary intensity source in bits. It is not possible to
code the intensity values of the tmaginary source (and thus the sample image)
with fewer than H bits/pixel.
81 m Fundamentals 555Bi The entropy of the image in Fig. 8.1(a) can be estimated by substituting the
intensity probabilities from Table 8.1 into Eq. (8.1-7):H = ~(0.25 log, 0.25 + 0.47 log, 0.47 + 0.25 log 0.25 + 0.03 log, 0.03]
—[0.25(-2) + 0.47(-1.09) + 0.25(-2) + 0.03(—5.06)]1,6614 bits/pixeli2z.In a similar manner, the entropies of the images in Fig. 8.1(b) and (c) can be
shown to be 8 bits/pixel and 1.566 bits/pixel, respectively. Note that the image
in Fig. 8.1(a) appears (o have the most visual information, but has almost the
lowest computed entropy — 1.66 bits/pixel. The image in Fig. 8.1(b) has almost
five times the entropy of the image in (a), but appears to have about the same
(or less) visual information; and the image in Fig. 8.1(c), which seems to have
little or no information, has almost the same entropy as the image in (a). The
obvious conclusion is that the amount of entropy and thus information in an
image is far from intuitive. . tjShannon’s first theoremRecall that the variable-length code in Example 8.1 was able to represent the
intensities of the image in Fig. 8.1(a) using only 1.81 bits/pixel. Although this is
higher than the 1.6614 bits/pixel entropy estimate from Example 8.2, Shannon's
first theorem —also called the noiseless coding theorem (Shannon [1948])—assures
us that the image in Fig. 8.1(a) can be represented with as few as 1.6614 bits/pixel.
To prove it in a general way, Shannon looked at representing groups of n consecutive source symbols with a single code word (rather than one code word per
source symbo!) and showed thatnoo ~
te. Lavg.n _
lim ES =H (8.1-8)where Lay, ,, is the average number of code symbols required to represent all
n-symbol groups. In the proof, he defined the nth extension of a zero-memory
source to be the hypothetical source that produces n-symbol blocks' using the
symbols of the original source; and computed Lyyg , by applying Eq. (8.1-4) to
the code words used to represent the n-symbol blocks. Equation (8.1-8) tells
us that Lye, ,/4 can be made arbitrarily close to H by encoding infinitely long
extensions of the single-symbol source. That is, it is possible to represent the
output of a zero-memory source with an average of H information units per
source symbol. 'The output of the ath extension is an n-tuple of symbols from the underlying singte-symbol source. 11
was considered a block random variable in which the probability of each v-tuple is the product of the
probabilities of its individual symbols. The entropy of the ath extension is then » times the entropy of
the single-symbo! source from which it is derivedEXAMPLE 8.2:
Image entropy
estimates,
556  Ghoptes 8 @ Image CompressionIf we now return to the idea that an image is a “sample” of the intensity
source that produced it, a block of m source symbols corresponds to a group
of » adjacent pixels. To construct a variable-length code for n-pixel blocks,
the relative frequencies of the blocks must be computed. But the nth extension of a hypothetical intensity source with 256 intensity values has 256" possible n-pixel blocks. Even in the simple case of m = 2, a 65,536 element
histogram and up to 65,536 variable-length code words must be generated.
For m = 3, as many as 16,777,216 code words are needed. So even for small
values of n, computational complexity limits the usefulness of the extension
coding approach in practice.Finally, we note that although Eq. (8.1-7) provides a lower bound on the
compression that can be achieved when coding statistically independent pixels
directly, it breaks down when the pixels of an image are correlated. Blocks of
correlated pixels can be coded with fewer average bits per pixel than the equation predicts, Rather than using source extensions, less correlated descriptors
(like intensity run-lengths) are normally selected and coded without extension. This was the approach used to compress Fig. 8.1(b) in Section 8.1.2. When
the output of a source of information depends on a finite number of preceding
outputs, the source is called a Markov or finite memory source.8.1.5 Fidelity CriteriaIn Section 8.1.3, it was noted that the removal of “irrelevant visual” information involves a loss of real or quantitative image information. Because information is lost, a means of quantifying the nature of the loss is needed. Two
types of criteria can be used for such an assessment: (1) objective fidelity criteria and (2) subjective fidelity criteria.When information loss can be expressed as a mathematical function of the
input and output of a compression process, it is said to be based on an objective fidelity criterion. An example is the root-mean-square (rms) error between two images. Let f(x,y) be an input image and f(x,y) be an approximation of f(x,y)
that results from compressing and subsequently decompressing the input. For
any value of x and y, the error e(x, y) between f(x, y) and f(x, y) ise(xy) = fly) - Flay) (8.1-9)so that the total error between the two images isM-1N-1_
> S[Fan ~ flay]x=0 y=0where the images are of size M x N. The root-mean-square error, €,m., between
f(x,y) and f(x, y) is then the square root of the squared error averaged over the
M X N array, or11 McIN=1 2 yw2
rms = [dy > = [fey — flxy)] | (8.1-10)
x=0 y=
8.1 m@ Fundamentals 557If f(x,y) is considered [by a simple rearrangement of the terms in Eq. (8.1-9)] to
be the sum of the original image f(x, y) and an error or “noise” signal e(x, y), the
mean-square signal-to-noise ratio of the output image, denoted SNR,,., can be
defined as in Section 5.8:M-1N> Si ayy
SNRaw "ae (8.1-11)
> SIF (x,y) — fny))
x=0 y=The rms value of the signal-to-noise ratio, denoted SNR ins, is obtained by taking the square root of Eq. (8.1-11).While objective fidelity criteria offer a simple and convenient way to evaluate information loss, decompressed images are ultimately viewed by humans.
So, measuring image quality by the subjective evajuations of people is often
more appropriate. This can be done by presenting a decompressed image to a
cross section of viewers and averaging their evaluations. The evaluations may
be made using an absolute rating scale or by means of side-by-side comparisons
of f(x, y) and F(x, y). Table 8.2 shows one possible absolute rating scale. Sideby-side comparisons can be done with a scale such as {—-3, —2, ~1, 0, 1, 2, 3} to
Tepresent the subjective evaluations {much worse, worse, slighily worse, the
same, slighily betier, better, much better}, respectively. In either case, the evaluations are based on subjective fidelity criteria.W@ Figure 8.4 shows three different approximations of the image in Fig. 8.1(a). EXAMPLE 8.3: Using Eq. (8.1-10) with Fig. 8.1(a) for f(x,y) and the images in Figs. 8.4(a) Image quality
through (c) as f(x,y), the computed rms errors are 5.17, 15.67, and 14.17 in. COMPATISONS
tensity levels, respectively. In terms of rms error—an objective fidelity criterion —
the three images in Fig. 8.4 are ranked in order of decreasing quality as
{(a), (c), (b)}. PsTABLE 8.2      Description   
   
  
   
    
  
   Value RatingExcellent An image of extremely high quality, as good as you couldRating scale of
the Television
Allocations Study desire.
2 Fine An image of high quality, providing enjoyable viewing. Organization.
Interference is not objectionable. (Frendendal! and
3 Passable An image of acceptable quality. Interference is not Behrend.)
objectionable.
4 Marginal An image of poor quatity; you wish you could improve it.
Interference is somewhat objectionable.
5 Inferior A very poor image, but you could watch it. Objectionableinterference is definitely present.
6 Unusable An image so bad that you could not watch it.
558 Chapter 8 & Image Compression abe
FIGURE 8.4 Three approximations of the image in Fig, 8.1(a).Here, the notationf(x,..- )is used to
denote hoth fix. ei ad
KYA).. ‘Figures 8.4(a) and (b) are typical of images that have been compressed
and subsequently reconstructed. Both retain the essential information of the
original image —tike the spatial and intensity characteristics of its objects.
And their rms errors correspond roughly to perceived quality. Figure 8.4(a),
which is practically as good as the original image, has the lowest rms error,
while Fig. 8.4(b} has more error but noticeable degradation at the boundaries between objects’ This is exactly as one would expect.Figure 8.4(c) is an artificially generated image that demonstrates the limitations of objective fidelity criteria. Note that the image is missing large sections
of several important lines (i.e., visual information), and has small dark squares
(i¢., artifacts) in the upper right quadrant. The visual content of the image is
misleading and certainly not as-accurate as the image in (b), but it has less rms
error— 14,17 versus 15,67 intensity values. A subjective evaluation of the three
images using Table 82 might yield an exce/fent rating for (a), a passable or
marginal rating tor (b), and an inferior of unusable rating for (c). The rms error
measure. on the other hand, ranks (¢) ahead of (b). si:.4 Image Compression Models .
As Fig. 8.5 shows, an image compression system is composed of two distinct
functional components: an encoder and a decoder. The encoder performs compression, and the decoder performs the complementary operation of decompression. Both operations can be performed in software, as is the case in Web
browsers and many commercial image cditing programs, or in a combination
of hardware and firmware, as m commercial DVD players. A codec is a device
or progran) that is capable of both encoding and decoding.
Inpul image f(x....) is fed into the encoder, which creat
representation of the input. This representation ts stored for later use.or transmitted for storage and use at a remote tocation. When the compressed representation 18 presented Lo its complementary decoder. a reconstructed output
image f(v....) is generated. In still image applications. the encoded impart and
decoder output are foxy, vp and flx.¥). respeclivelyiin Video applications, they  A compressed
8.1 @ Fundamentals 559f(xy)
orfy. Compressed data
for storageand transmissiont | *
i Symbol Inverse ‘ f Wey )
decoder mapper fi (x.y.t)Decader are f(x, y, t) and F(x, y, 0), where discrete parameter f specifies time. In general,
Fx, ...) may or may not be an exact replica of f(x,...). If it is, the compression system is called error free, lossless, or information preserving. If not, the
reconstructed output image is distorted and the compression system is referred to as lossy.The encoding or compression processThe encoder of Fig. 8.5 is designed to remove the redundancies described in
Sections 8.1.1-8.1.3 through a series of three independent operations. In the first
stage of the encoding process, a mapper transforms f(x, ...) into a (usually nonvisual) format designed to reduce spatial and temporal redundancy. This operation generally is reversible and may or may not reduce directly the amount of
data required to represent the image. Run-length coding (see Sections 8.1.2 and
8.2.5) is an example of a mapping that normally yields compression in the first
step of the encoding process. The mapping of an image into a set of less correlated transform coefficients (see Section 8.2.8) is an example of the opposite
case (the coefficients must be further processed to achieve compression). In
video applications, the mapper uses previous (and in some cases future) video
frames to facilitate the removal of temporal rédundancy.The quantizer in Fig. 8.5 reduces the accuracy of the mapper’s output in accordance with a pre-established fidelity criterion. The goal is to keep irrelevant
information out of the compressed representation. As noted in Section 8.1.3,
this operation is irreversible. It must be omitted when error-free compression
is desired. In video applications, the bit rate of the encoded output is often
measured (in bits/second) and used to adjust the operation of the quantizer so
that a predetermined average output rate is maintained. Thus, the visual quality of the output can vary from frame to frame as a function of image content.In the third and final stage of the encoding process, the syrtbol coder of Fig. 8.5
generates a fixed- or variable-length code to represent the quantizer output and
maps the output in accordance with the code. In many cases, a variable-length
code is used. The shortest code words are assigned to the most frequently occurring quantizer output values—thus minimizing coding redundancy, This operation is reversible. Upon its completion, the input image has been processed for
the removal of each of the three redundancies described in Sections 8.1.1 to 8.1.3.FIGURE 8.5
Functional block
diagram of a
general image
compression
system,
560 Chapter 8 s Image CompressionFIGURE 8.6 Some
popular image
compression
standards, file
formats, and
containers.
Internationally
sanctioned entries
are shown in
black; al] others
are grayed.The decoding or decompression processThe decoder of Fig. 8.5 contains only two components: a symbol decoder and
an inverse mapper. They perform, in reverse order, the inverse operations of
the encoder’s symbol encoder and mapper. Because quantization results in
irreversible information loss, an inverse quantizer block is not included in the
general decoder model. In video applications, decoded output frames are
maintained in an internal frame store (not shown) and used to reinsert the
temporal redundancy that was removed at the encoder.3.1.7 Image Formats, Containers, and Compression StandardsIn the context of digital imaging, an image file format is a standard way to
organize and store image data. It defines how the data is arranged and the type
of compression—if any—that is used. An #mage container is similar to a file
format but handles multiple types of image data. Image compression standards, on the other hand, defing procedures for compressing and decompressing images —~that is, for reducing the amount of data needed to represent an
image. These standards are the underpinning of the widespread acceptance of
image compression technology.Figure 8.6 lisis the most important image compression standards, file formats, and containers in use today, grouped by the type of image handled. The
entries in black are international standards sanctioned by the International
Standards Organization (ISO), the International Electrotechnical Commission
(IEC), and/or the /nternational Telecommunications Union (ITU-T)—a United
Nations (UN) organization that was once called the Consultative Commtittee of
the International Telephone and Telegraph (CCITT). Two video compression
standards, VC-1 by the Society of Motion Pictures and Television Engineers
(SMPTE) and AVS by the Chinese Ministry of Information Industry (MU), areImage Compression
Standards, Formats, and Containers  I L
Still lmage Video
| DV
| H.261
Binary Continuous Tone H.262
CCITT Group 3 JPEG 1.263
CCITT Group 4 JPEG-LS H.264
JBIG (or JBIGL) JPEG-2000 . MPEG-1
JBIG2 reve MPEG-2
- rw MPEG-4
Tins OH MPEG-4 AVCVest
i Vv
istLuby
8.1 mw Fundamentals 561also included. Note that they are shown in gray, which is used in Fig. 8.6 to denote entries that are not sanctioned by an international standards organization.Tables 8.3 and 8.4 summarize the standards, formats, and containers listed
in Fig. 8.6. Responsible organizations, targeted applications, and key compres~
sion methods are identified. The compression methods themselves are the subject of the next section. In both tables, forward references to the relevant
subsections of Section 8.2 are enclosed in square brackets. Name Organization Bi-Level Still Images
CCITT ITU-T Designed as a facsimile (FAX) method for transmitting
Group 3 binary documents over telephone lines. Supports 1-Dand 2-D run-length [8.2.5] and Huffman [8.2.1] coding.
CCITT ITU-T A simplified and streamlined version of the CCITTGroup 4 Group 3 standard supporting 2-D run-length coding only.
JBIG or ISO/IEC! A Joint Bi-level Image Experts Group standard for
JBIG1 ITU-T progressive, lossless compression of bi-level images.Continuous-tone images of up to 6 bits/pixel can be
coded on a bit-plane basis [8.2.7]. Context sensitive
arithmetic coding [8.2.3] ts used and an initia! low
resolution version of the image can be gradually
enhanced with additional compressed data.JBIG2 ISOAEC/ A follow-on to JBIG! for bi-level images in desktop,
ITU-T Internet, and FAX applications. The compression
method used is content based, with dictionary based
methods [8.2.6] for text and halftone regions, and
Huffman [8.2.1] or arithmetic coding [8.2.3] for other
image content. It can be lossy or lossless.Continuous-Tone Still Imagesae
JPEG ISOAEC/ A Joint Photographic Experts Group standard for images
ITU-T of photographic quality. Its lossy baseline coding system
(most commonly implemented) uses quantized discrete
cosine transforms (DCT) on 8 X 8 image blocks [8.2.8],
Huffman [8.2.1], and run-length [8.2.5] coding. It is one
of the most popular methods for compressing images on
the Internet.JPEG-LS ISO/IEC/ A lossless to near-lossiess standard for continuous tone
ITU-T images based on adaptive prediction [8.2.9], context
modeling [8.2.3], and Golomb coding [8.2.2].JPEG- ISO/EC/ A follow-on to JPEG for increased compression of2000 ITU-T photographic quality images. Arithmetic coding [8.2.3]
and quantized discrete wavelet transforms {DWT}
[8.2.10] are used. The compression can be lossy or lossless.  (Continues)TABLE 8.3
Internationally
sanctioned image
compression
standards. The
numbers in
brackets refer to
sections in this
chapter.
562  Chopter 8 # Image CompressionTABLE 8.3
(Continued) NameDVH.261H.262
H.263H.264MPEG-1MPEG-2MPEG-4MPEG-4
AVEOrganizationIECITU-TITU-T
ITU-TITU-TISO/IECISO/MTECISO/IECISONECDescription Digital Video. A video standard tailored to home and
semiprofessional video production applications and
equipment- like electronic news gathering and camcorders,
Frames are compressed independently for uncomplicated
editing using a DCT-based approach (8.2.8] similar to JPEG.A two-way videoconferencing standard for ISDN
{integrated services digital network) lines. It supports
non-interlaced 352 x 288 and 176 X 144 resolution
images, called CIF (Cormmon Intermediate Format) and
QCIF (Quarter CIF), respectively. A DCT-based
compression approach [8.2.8] similar to JPEG is used,
with frame-to-frame prediction differencing [8.2.9] to
reduce temporal redundancy. A block-based technique is
used to Compensate for motion between frames.See MPEG-2 below.An enhanced version of H.261 designed for ordinary
telephone modems (i.¢., 28,8 Kb/s) with additional
resolutions: SQCIF (Sub-Quarter CIF 128 X 96),4CIF
(704 x 576), and 16CIF (1408 x 512).* An extension of H.261-H.263 for videoconferencing,Internet streaming, and television broadcasting. [1
supports prediction differences within frames [8.2.9],
variable block size integer transforms (rather than the
DCT), and context adaptive arithmetic coding [8.2.3}.A Motion Pictures Expert Group standard for CD-ROM
applications with non-interlaced video at up to 1.5 Mb/s.
It is similar to H.261 but frame predictions can be based
on the previous frame, next frame, or an interpolation of
both. It is supported by almost ail computers and DVD
players.An extension of MPEG-1 designed for DVDs with
transfer rates to 15 Mb/s. Supports interlaced video and
HDTV. It ts the most successful video standard to date.An extension of MPEG-2 that supports variable block
sizes and prediction differencing [8.2.9] within frames.MPEG-4 Part 10 Advanced Video Coding (AVC). Identical
to H.264 above. ~
OrganizationContinuous-Tone Still ImagesBMPGIFPDFPNGTIFFVideo
AVSHDVM-JPEGQuick-TimeVC-1
WMV9MicrosoftCompuServeAdobe SystemsWorld Wide Web
Consortium
(W3C)AldusMIICompany
consortiumVarious
companiesApple ComputerSMPTE
Microsoft8.) m Fundamentals 563Windows Bitmap. A file format used mainly for
simple uncompressed images.Graphic Interchange Format. A file format that
uses lossless LZW coding [8.2.4] for
1- through 8-bit images. It is frequently used
to make small animations and short low
resolution films for the World Wide Web.Portable Document Format. A format for
representing 2-D documents ina device and
resolution independent way. Jt can function as
a container for JPEG, JPEG 2000, CCITT, and
other compressed images, Some PDF versions
have become ISO standards.Portable Network Graphics. A file format that
losstessly compresses full color images with
transparency (up to 48 bits/pixel) by coding
the difference between each pixel’s value and
a predicted value based on past pixels [8.2.9].Tagged Image File Format. A flexible file format
supporting a variety of image compression
standards, including JPEG, JPEG-LS, JPEG2000, JBIG2, and others.Audio-Video Standard. Similar to H.264 but uses
exponential Golomb coding [8.2.2]. Developed
in China.High Definition Video. An extension of DV
for HD television that uses MPEG-2 like
compression, including temporal] redundancy
removal by prediction differencing (8.2.9}.Motion JPEG. A compression format in which
each frame is compressed independently
using JPEG.A media container supporting DV, H.261, H.262,
H.264, MPEG-1, MPEG-2, MPEG-4, and
other video compression formats.The most used video format on the Internet.
Adopted for HD and Blu-ray high-definition
DVDs, It is similar to H.264/AVC, using an
integer DCT with varying block sizes [8.2.8
and 8.2.9] and context dependent variablelength code tables {8.2.1}]— but no predictions
within frames.  TABLE 8.4
Popular image
compression
standards, file
formats, and
containers, not
included in
Table 8.3.
8.2 & Some Basic Compression Methods 565    Original source Source reductionSymbol = Probability Code i 2 3 4
a 04 1 04 1 04 1 04 1 0.6 0
% 0.3 00 0.3 (0 03 00 03 non} a4 i
a 0.1 ou 0.1 O11 0.2 610 0.3 Ol
44 0.1 0106 0.1 ono0={ 01 O11
ay 0.06 01910 ---~— 0.1 0101
ats 0.04 M1011  each reduced source until the original source is reached. The final code appears
at the far left in Fig. 8.8. The average length of this code isLavg = (O4)(1) + (0.3)(2) + (0.1)(3) + (.1)(4) + (0.06)(5) + (0.04)(5)
= 2.2 bits/pixeland the entropy of the source is 2.14 bits/symbol.Huffman’s procedure creates the optimal code for a set of symbols and
probabilities subject to the consiraint that the symbols be coded one at a time.
After the code has been created, coding and/or error-free decoding is accomplished in a simple lookup table manner. The code itself is an instantaneous
uniquely decodable block code, It is called a block code because each source
symbol is mapped into a fixed sequence of code symbols. It is instantaneous
because each code word in a string of code symbols can be decoded without
referencing succeeding symbols. It is uniquely decodable because any string of
code symbols can be decoded in only one way. Thus, any string of Huffman
encoded symbols can be decoded by examining the individual symbols of the
string in a left-to-right manner. For the binary code of Fig. 8.8, a left-to-right
scan of the encoded string 010100111100 reveals that the first valid code word
is 01010, which is the code for symbol a3. The next valid code is 011, which
corresponds to symbo} a,. Continuing in this manner reveals the completely
decoded message to be a3a) a 24745. ‘M@ The 512 x 512 x 8 bit monochrome image in Fig, 8.9(a) has the intensity
histogram shown in Fig. 8.9(b). Because the intensities are not equally probable,    
 tao r
500   
 
 
 Number of pixelsso 100182  FIGURE 8.8
Huffman code
assignment
procedure.EXAMPLE 8.4:
Huffman coding.abFIGURE 8.9 (a)A S12 & 512 8-bit
image, and (b) its
histogram.
564 Chapter 8 Image CompressionWith reference to Tables
8. and 8.4, Huffman
codes are used in© CCITT« JBIG2« JPEG@ MPEG-12.4© =H.261,H.242,6.263, H.204and other compression
standards.FIGURE 8.7
Huffman source
reductions.EXE Some Basic Compression MethodsIn this section, we describe the principal lossy and error-free compression
methods in use today. Our focus is on methods that have proven useful in mainstream binary, continuous-tone still images, and video compression standards.
The standards themselves are used to demonstrate the methods presented.8.2.) Huffman CodingOne of the most popular techniques for removing coding redundancy is due to
Huffman (Huffman [1952]). When coding the symbols of an information
source individually, Huffman coding yields the smallest possible number of
code symbols per source symbol. In terms of Shannon's first theorem (see
Section 8.1.4), the resulting code is optimal for a fixed value of n, subject to the
constraint that the source symbols be coded one at a time. In practice, the
source symbols may be either the intensities of an image or the output of an
intensity mapping operatiori (pixel differences, run lengths, and so on).The first step in Huffman’s approach is to create a series of source reductions
by ordering the probabilities of the symbols under consideration and combining
the lowest probability symbols into a single symbol that replaces them in the next
source reduction, Figure 8.7 illustrates this process for binary coding (K-ary Huffman codes can also be constructed). At the far left, a hypothetical set of source
symbols and their probabilities are ordered from top to bottom in terms of
decreasing probability values. To form the first source reduction, the bottom two
probabilities, 0.06 and 0.04, are combined to form a “compound symbol!” with
probability 0.1. This compound symbol and its associated probability are placed
in the first source reduction column so that the probabilities of the reduced
source also are ordered from the most to the least probable. This process is then
repeated until a reduced source with two symbols (at the far right) is reached.The second step in Huffman’s procedure is to code each reduced source,
starting with the smallest source and working back to the original source. The
minimal length binary code for a two-symbol source, of course, are the symbols
0 and 1. As Fig. 8.8 shows, these symbols are assigned to the two symbols on the
right (the assignment is arbitrary; reversing the order of the 0 and 1 would work
just as well). As the reduced source symbol with probability 0.6 was generated
by combining two symbols in the reduced source to its left, the 0 used to code it
is now assigned to both of these symbols, and a 0 and 1 are arbitrarily appended
to each to distinguish them from each other. This operation is then repeated for         Original source Source teductionSymbol Probability 1 2 3 4a 04 04 04 0.4 p06as 0.3 0.3 6.3 0.3 {oaa 0.1 0.1 027-03a 0.1 0.1 {" 01a, 0.06 -—+ 0.1as 0.04
566 Chapter 8 mt Image Compression‘With reference to
Tables 8.3 and 8.4,
Golomb codes are used in
2 JPEG-LS
« AVScompression.a MATLAB implementation of Huffman’s procedure was used to encode
them with 7,428 bits/pixel— including the Huffman code table that is required
to reconstruct the original 8-bit image intensities. The compressed representation
exceeds the estimated entropy of the image [7.3838 bits/pixel from Eq. (8.1-7)]
by 512? x (7.428 — 7.3838) or 11,587 bits—about 0.6%. The resulting
compression ratio and corresponding relative redundancy are
C = 8/7.428 = 1.077 and R = 1 — (1/1.077) = 0.0715, respectively. Thus
7.15% of the original 8-bit fixed-length intensity representation was removed
as coding redundancy. &When a large number of symbols is to be coded, the construction of an optimal Huffman code is a nontrivial task. For the general case of J source symbols,
J symbol probabilities, / — 2 source reductions, and J — 2 code assignments
are required. When source symbol probabilities can be estimated in advance,
“near optimal” coding can be achieved with pre-computed Huffman codes,
Several popular image compression standards, including the JPEG and MPEG
standards discussed in Sections 8.2.8 and 8.2.9, specify default Huffman coding
tables that have been pre-computed based on experimental data.8.2.2 Golomb CodingIn this section we consider the coding of nonnegative integer inputs with exponentially decaying probability distributions. Inputs of this type can be optimally encoded {in the sense of Shannon's first theorem) using a family of
codes that are computationally simpler than Huffman codes. The codes themselves were first proposed for the representation of nonnegative run lengths
(Golomb [1966]). In the discussion that follows, the notation | x] denotes the
largest integer less than or equal to x, [ x] means the smallest integer greater
than or equal to x, and xmod y:is the remainder of x divided by y.Given a nonnegative integer n and a positive integer divisor m > 0, the
Golomb code of n with respect to m, denoted G,,(n), is a combination of the
unary code of quotient |n/mj and the binary representation of remainder
nmodm. G,,{(#) is constructed as follows:Step 1. Form the unary code of quotient | /m |. (The unary code of an integer q is defined as q 1s followed by a 0.)
Step 2. Let k = [log,m],c = 2 — m,r = nmodm, and compute truncated remainder r’ such that, r truncated tok —1lbits QO=r<ea + ctruncated tok bits  ptherwise (82-1)Step 3. Concatenate the results of steps 1 and 2.To compute G,(9), for example, begin by determining the unary code of
the quotient | 9/4] = [2.25] = 2, which is 110 (the result of step 1}. Then
let k = [log, 4] = 2,¢ = 2? - 4 = 0, and r = 9mod4, which in binary is
100! mod 0100 or 0001, In accordance with Eq. (8.2-1), r' is then r (i-e., 0001)
truncated to 2 bits, which is 01 (the result of step 2). Finally, concatenate 110
from step 1 and 01 from step 2 to get 11001, which is G,(9).
8.2 ® Some Basic Compression MethodsFor the special case of m = 2*,c¢ =Oandr’ = r = nmodmtruncated to k
bits in Eq. (8.2-1) for all n. The divisions required to generate the resulting
Golomb codes become binary shift operations and the computationally simpler codes are called Golomb-Rice or Rice codes (Rice [1975]}. Columns 2, 3,
and 4 of Table 8.5 list the G,, G,, and G4 codes of the first ten nonnegative integers. Because mm is a power of 2 in each case (i.e. 1 = 29.2 = 2' and 4 = 27),
they are the first three Golomb-Rice codes as well. Moreover, G; is the
unary code of the nonnegative integers because [n/1 |] = nandnmod1 = 0
for all n.Keeping in mind that Golomb codes can only be used to represent nonnegative integers and that there are many Golomb codes to choose from, a key
step in their effective application is the selection of divisor m. When the integers to be represented are geometrically distributed with probability mass
function (PMF)*P(n) = (1 ~— p)p” (8.2-2)for some 0 < p < 1, Golomb cades can be shown to be optimal —in the sense
that G,,(n) provides the shortest average code length of all uniquely decipherable codes— when (Gallager and Voorhis [1975])m= Ee + 2)
loga(1/p)Figure 8.10(a) plots Eq. (8.2-2) for three values of p and illustrates graphically
the symbol probabilities that Golomb codes handle well (that is, code efficiently). As is shown in the figure, small integers are much more probable than
large ones.Because the probabilities of the intensities in an image [see, for example,
the histogram of Fig. 8.9(b)] are unlikely to match the probabilities specified in
Eq. (8.2-2) and shown in Fig, 8.10(a), Golomb codes are seldom used for the
coding of intensities. When intensity differences are to be coded, however, theste(82-3)    n Gy(n) Gala) Ga(n) Gop (n)
0 0 00 000 01 10 01 001 1002 110 100 010 1013 1116 101 011 110004 11110 1100 1000 110015 111110 1101 1001 110106 1111110 11100 1010 110117 11111110 13201 101) 1110000
8 111111110 111100 11000 1110001
9 1111114110 111101 11001 1110010   ‘A probability mass function (PMF) is a function that defines the probability that a discrete random variable is exactly equal to some value. A PMF differs from a PDF in that a PDF's values are not probabilities; rather, the integral of a PDF over a specified interval is a probability.567‘The discrete probability
distribution defined by
the PMF in Eg. {8.2-2) is
called the geometic
probabitity distribution.
Its continuous countespart is the exponential
distribution,‘The graphical
representation of a PMF
is a histogram.TABLE 8.5
Several Golomb
codes for the
integers 0 - 9.
8.2 m Some Basic Compression Methods 569Probability0.15  o
-4 -3 -2 -1 0 1 2 3 4 This is due to the fact that the probabilities of the intensities of the image in
Fig. 8.9(a) are much different than the probabilities defined in Eq. (8.2-2). Ina
similar manner, Huffman codes can produce ‘data expansion when used to
encode symbols whose probabilities are different from those for which the
code was computed. In practice, the further you depart from the input probability assumptions for which a code is designed, the greater the risk of poor
compression performance and data expansion, aTo conclude our coverage of Golomb codes, we note that Column 5 of
Table 8.5 contains the first 10 codes of the zeroth order exponentialGolomb code, denoted Goep(”). Exponential-Golomb codes are useful for
the encoding of run lengths, because both short and long runs are encoded
efficiently. An order-k exponential-Golomb code GS,(n) is computed as
follows: ~.“teStep 1. Find an integer i = 0 such that
i-] i
Dw sn < Dak (8.2-5)}
j=0 j=0and form the unary code of i. If kK = 0,1 = | log,(m + 1)J and the code is
also known as the Elias gamma code.
Step 2. Truncate the binary representation ofind
n~ Sirk (8.2-6)
j=0tok + i least significant bits.
Step 3. Concatenate the results of steps 1 and 2.abFIGURE 8.11(a) The
probability
distribution of
the ‘mage inFig. 8.1(c) after
subtracting the
mean intensity
from each pixel,
and (b) a mapped
version of {a}
using Eq. (8.2-4).‘When C is less than 1 in
Eq. (8.1-2), there is data
expansion,
568  Chopter8 w Image CompressionabeFIGURE 8.10(a) Three onesided geometric
distributions from
Eg. (8.2-2); (b) a
two-sided
exponentially
decaying
distribution; and
{c) a reordered
version of(b) usingEq. (8.2-4).EXAMPLE 8.5:
Golomb-Rice
coding,Probability probabilities of the resulting “difference values” (see Section 8.2.9)—with
the notable exception of the negative differences—often resemble those of
Eq. {8.2-2) and Fig. 8.10(a). To handle negative differences in Golomb coding, which can only represent nonnegative integers, a mapping like*2n nz0
M(n) = (3 -1 n<0 (82-4)typically is used. Using this mapping, for example, the two-sided PMF shown
in Fig. 8.10(b} can be transformed into the one-sided PMF in Fig. 8.10(c). Its
integers are reordered, alternating the negative and positive integers so that
the negative integers.are mapped into the odd positive integer positions. If
P(n) is two-sided and centered at zero, P(M(m)) will be one-sided. The
mapped integers, M(n), can then be efficiently encoded using an appropriate
Golomb-Rice code (Weinberger et al. [1996]).@ Consider again the image from Fig. 8.1(c) and note that its histogram —see
Fig. 8.3(a)—is similar to the two-sided distribution in Fig. 8.10(b) above. If we
let n be some nonnegative integer intensity in the image, where 0 = 1 = 255,
and yu be the mean intensity, P(n — 4) is the two-sided distribution shown in
Fig. 8.11(a). This plot was generated by normalizing the histogram in Fig. 8.3(a)
by the total number of pixels in the image and shifting the normalized values
to the left by 128 (which in effect subtracts the mean intensity from the
image). In accordance with Eq. (8.2-4), P(M(# — )) is then the one-sided
distribution shown in Fig. 8.11(b). If the reordered intensity values are
Golomb coded using a MATLAB implementation of code G, in column 2 of
Table 8.5, the encoded representation is 4.5 times smailer than the original
image (i.e, C = 4.5). The G, code realizes 4.5/5.1 or 88% of the theoretical
compression possible with variable-length coding>(Based on the entropy calculated in Example 8.2, the maximum possible compression ratio through
variable-length coding is C = 8/1.566 ~ 5.1.) Moreover, Golomb coding
achieves 96% of the compression provided by a MATLAB implementation
of Huffman’s approach—and doesn’t require the computation of a custom
Huffman coding table.Now consider the image in Fig. 8.9(a). If its intensities are Golomb coded
using the same G, code as above, C = 0.0922, That is, there is data expansion.
8.2 & Some Basic Compression Methods Encoding sequence ———~
a a a3 a3 a4 0.08 5 0.072 0.0688 —as , as af a /, ay
4 0.06752 4 +ay a ay a ay| a a %0.0624 0.056   ProbabilityInitial Subinterval[0.0, 0.2)Source Symbol{0.2,0.4)
[0.4,0.8)
[0.8, 1.0) [0,0.2) is expanded to the full height of the figure and its end points labeled by
the values of the narrowed range. The narrowed range is then subdivided in
accordance with the origina] source symbol probabilities and the process continues with the next message symbol. In this manner, symbol a, narrows the
subinterval to [0.04, 0.08), a3 further narrows it to [0.056, 0.072), and so on. The
final message symbol, which must be reserved as a special end-of-message indicator, narrows the range to (0.06752, 0.0688). Of course, any number within
this subinterval—for example, 0.068—can baused to represent the message.In the arithmetically-coded message of Fig, 8.12, three decimal digits are used
to represent the five-symbol message. This translates into 0.6 decimal digits per
source symbol and compares favorably with the entropy of the source, which,
from Eq. (8.1-6), is 0.58 decimal digits per source symbol. As the length of the sequence being coded increases, the resulting arithmetic code approaches the
bound established by Shannon’s first theorem. In practice, two factors cause coding performance to fall short of the bound: (1) the addition of the end-of-message
indicator that is needed to separate one message from another; and (2) the use
of finite precision arithmetic. Practical implementations of arithmetic coding
address the latter problem by introducing a scaling strategy and a rounding strategy (Langdon and Rissanen [1981]). The scaling strategy renormalizes each
subinterval to the (0,1) range before subdividing it in accordance with the symbol
probabilities, The rounding strategy guarantees that the truncations associated
with finite precision arithmetic do not prevent the coding subintervals from being
represented accurately.FIGURE 8.12571Arithmetic codingprocedure.TABLE 8.6Arithmetic codingexample.
570 Chapter 8 @ Image CompressionWith reference to Tables
8.3 and 8.4, arithmetic
cuding is used in« JBIG) JBIG2@ JPEG-2000@ H264e MPEG-4 AVC
and other compression
standards.To find Gexp(8), for example, we let i = | log, 9| or 3 in step 1 because k = 0.
Equation (8.2-5) is then satisfied becauseSoi0 < 8 < S20
j=0j=0S22 <8 < D2j=0 j=029+ 2'+2? 28 < 29+ 21427423
7=8<15The unary code of 3 is 1110 and Eq. (8.2-6) of step 2 yields3-1 2 .
8 — Sito =8- Svs - 2° 4+ 2! + 27)=8-7=1 = 0001
j=0 j=0which when truncated to its 3 + 0 least significant bits becomes 001. The concatenation of the results from steps 1 and 2 then yields 1140001, Note that this is
the entry in column 4 of Table 8.5 for n = 8. Finally, we note that like the Huffman codes of the last section, the Golomb codes of Table 8.5 are variable-length,
instantaneous uniquely decodable block codes.8.2.3 Arithmetic CodingUnlike the variable-length codes of the previous two sections, arithmetic coding generates nonblock codes. In arithmetic coding, which can be traced to the
work of Elias (see Abramson [1963]), a one-to-one correspondence between
source symbols and code words does not exist. Instead, an entire sequence of
source symbols (or message) is assigned a single arithmetic code word. The
code word itself defines an interval of rea] numbers between 0 and 1. As the
number of symbols in the message increases, the interval used to represent it
becomes smaller and the number of information units (say, bits) required to
represent the interval becomes larger. Each symbol of the message reduces
the size of the interval in accordance with its probability of occurrence. Because the technique does not require, as does Huffman’s approach, that each
source symbol translate into an integral number of code symbols (that is, that
the symbols be coded one at a time), it achieves (but only in theory) the bound
established by Shannon’s first theorem of Section 8.1.4.Figure 8.12 illustrates the basic arithmetic coding process. Here, a five-symbol
sequence or message, @)@2€34,@4, trom a four-symbol source is coded. At the
start of the coding process, the message is assumed to occupy the entire balf-open
interval [0, 1). As Table 8.6 shows, this interval is subdivided initially into four regions based on the probabilities of each source symbol. Symbol a;, for example, is
associated with subinterval {0, 0.2). Because it is the first symbol of the message
being coded, the message interval is initially narrowed to [0, 0.2). Thus in Fig. 8.12
572 Chapter 8 mw Image Compressiona
bed
FIGURE 8.13
(a} An adaptive,
context-based
arithmetic coding
approach (often
used for binary
source symbols).
(b)-(d) Three
possible context
models.Adaptive context dependent probability estimatesWith accurate input symbo! probability models, that is, models that provide the
true probabilities of the symbols being coded, arithmetic coders are near optimal in the sense of minimizing the average number of code symbols required
to represent the symbols being coded. Like in both Huffman and Golomb coding, however, inaccurate probability models can lead to non-optimal results. A
simple way to improve the accuracy of the probabilities employed is to use an
adaptive, context dependent probability model. Adaptive probability models
update symbol probabilities as symbols are coded or become known. Thus, the
probabilities adapt to the local statistics of the symbols being coded. Context
dependent models provide probabilities that are based on a predefined neighborhood of pixels—called the context— around the symbols being coded. Normally, a causal context—one limited to symbols that have already been
coded—is used. Both the Q-coder (Pennebaker et al. [1988]) and MQ-coder
(ISO/IEC [2000]}, two well-known arithmetic coding techniques that have
been incorporated into the JBIG, JPEG-2000, and other important image
compression standards, use probability models that are both adaptive and context dependent. The Q-coder dynamically updates symbol probabilities during
the interval renormalizations that are part of the arithmetic coding process.
Adaptive context dependent models also have been used in Golomb coding —
for example, in the JPEG-LS compression standard.Figure 8.13(a} diagrams the steps involved in adaptive, context-dependent
arithmetic coding of binury source symbols. Arithmetic coding often is used
when binary symbols are to be coded. As each symbol (or bit) begins the coding
process, its context is formed in the Context determination block of Fig. 8.13(a).
Figures 8.13(b) through (d) show three possible contexts that can be used:
(1) the immediately preceding symbol, (2) a group of preceding symbols, and
(3) some number of preceding symbols plus symbols on the previous scan line.
For the three cases shown, the Probability estimation block must manage 2!
(or 2), 28 (or 256), and 2° (or 32) contexts and their associated probabilities.
For instance, if the context in Fig. 8.13(b) is used, conditional probabilitiesUpdate probability
for current contextInput i Probability
symbols determination Symbol estimation Symbol 
  
 Code
bits Arithmetic
coding and probability
context
Context Context Context
Sanne)
‘elope |     Symbo] being coded Symbol being coded Symbol being coded
8,2 ® Some Basic Compression Methods 573P(0|a = 0) (the probability that the symbol being coded is a 0 given that the
preceding symbol is a 0), P(tla = 0), P(Ola = 1), and P(1|a = 1) must be
tracked. The appropriate probabilities are then passed to the Arithmetic coding
block as a function of ihe current context and drive the generation of the arithmetically coded output sequence in accordance with the process illustrated in
Fig. 8.12. The probabilities associated with the context involved in the current
coding step are then updated to reflect the fact that another symbo! within that
context has been processed.Finally, we note that a variety of arithmetic coding techniques are protected
by United States patents (and may in addition be protected in other jurisdictions). Because of these patents and the possibility of unfavorable monetary
judgments for their infringement, most implementations of the JPEG compression standard, which contains options for both Huffman and arithmetic
coding, typically support Huffman coding alone.8.2.4 LZW CodingThe techniques covered in the previous sections are focused on the removal
of coding redundancy. In this section, we consider an error-free compression
approach that also addresses spatial redundancies in an image. The technique,
called Lempel-Ziv-Welch (LZW) coding, assigns fixed-length code words to
variable length sequences of source symbols. Recall from Section 8.1.4 that
Shannon used the idea of coding sequences of source symbols, rather than individual source symbols, in the proof of his first theorem. A key feature of LZW
coding is that it requires no a priori knowledge of the probability of occurrence of
the symbols to be encoded. Despite the fact that until recently it was protected
under a United States patent, LZ W compression has been integrated into a variety of mainstream imaging file formats, including GIF, TIFF, and PDF. The PNG
format was created to get around LZW licensing requirements.W@ Consider again the 512 x 512, 8-bit image from Fig. 8.9(a). Using Adobe
Photoshop, an uncompressed TIFF version of this image requires 286,740
bytes of disk space —262,144 bytes for the 513 x’ 512 8-bit pixels plus 24,596
bytes of overhead. Using TIFF’s LZW compression option, however, the resulting file is 224,420 bytes. The compression ratio is C = 1.28. Recall that for
the Huffman encoded representation of Fig. 8.9(a) in Example 8.4,C = 1,077.
The additional compression realized by the LZ'W approach is due the removal
of some of the image’s spatial redundancy. aLZW coding is conceptually very simple (Welch [1984]). At the onset of the
coding process, a codebook or dictionary containing the source symbols to be
coded is constructed. For 8-bit monochrome images, the first 256 words of the
dictionary are assigned to intensities 0, 1,2, ..., 255. As the encoder sequentially examines image pixels, intensity sequences that are not in the dictionary
are placed in algorithmically determined (e.g., the next unused) locations. If
the first two pixels of the image are white, for instance, sequence “255-255”
might be assigned to location 256, the address following the locations reserved
for intensity levels 0 through 255. The next time that two consecutive whiteWith reference to
Tables 8.3 and 8.4, LZW
coding is ysed in theformats, but not im any of
the internationally
sanctioned compression
standards,EXAMPLE 8.6:
LZW coding
Fig. 8.9{a).
574 Chapter 8 m Image CompressionEXAMPLE 8.7:
LZW coding.pixels are encountered, code word 256, the address of the location containing
sequence 255~255, is used to represent them. If a 9-bit, 512-word dictionary is
employed in the coding process, the original (8 + 8) bits that were used to represent the two pixels are replaced by a single 9-bit code word. Clearly, the size
of the dictionary is an important system parameter. If it is too small, the detection of matching intensity-level sequences will be less likely; if it is too large,
the size of the code words will adversely affect compression performance.@ Consider the following 4 x 4, 8-bit image of a vertical edge:39 39s«126—Ss«126
39-39 «126 126
39°39 126 126
3939 126 126Table 8.7 details the steps invalved in coding its 16 pixels. A 512-word dictionary with the following starting content is assumed:   Locations 256 through 511 initially are unused.The image is encoded by processing its pixels in a left-to-right, top-to-bottom
manner. Each successive intensity value is concatenated with a variable—
column 1 of Table 8.7—called the “currently recognized sequence.” As can be
seen, this variable is initially null or empty. The dictionary is searched for each concatenated sequence and if found, as was the case in the first row of the table, is
replaced by the newly concatenated and recognized (i.e., located in the dictionary)
sequence. This was done in column 1 of row 2. No output codes are generated,
nor is the dictionary altered. If the concatenated sequence is not found, however,
the address of the currently recognized sequence is output as the next encoded
value, the concatenated but unrecognized sequence is added to the dictionary,
and the currently recognized sequence is initialized to the current pixel value.
This occurred in row 2 of the table. The last two cotumns detail the intensity sequences that are added to the dictionary when scanning the entire 4 x 4
image. Nine additional code words are defined. At the conclusion of coding,
the dictionary contains 265 code words and the LZW algorithm has successfully identified several repeating intensity sequences— leveraging them to reduce
the original 128-bit image to 90 bits (i.e., 10 9-bit codes). The encoded output is
obtained by reading the third column from top to bottom. The resulting compression ratio is 1.42:1. z
§.2 m Some Basic Compression Methods 575      
    Currently Dictionary
Recognized Pixel Being Encoded Location
Sequence Processed Output (Code Word) Dictionary Entry     
  
    39-39   39 126 39 257 39-126
126 126 126 258 126-126
126 39 126 259 126-39
39 39
39-39 126 256 260 39-39-126
126 126
126-126 39 258 261 126-126-39
39 39
39-39 126
39-39-126 126 260 262 39-39-126-126
126 39
126-39 39 259 263 126-39-39
39 126 .
39-126 126 257 264 39-126-126
126 126 . A unique feature of the LZW coding just demonstrated is that the coding
dictionary or code book is created while the data are being encoded. Remarkably, an LZW decoder builds an identical decompression dictionary as it decodes simultaneously the encoded data stream. It is left as an exercise to the
Treader {see Problem 8.20) to decode the output of the preceding example and
reconstruct the code book. Although not needed in this example, most practical applications require a strategy for handling dictionary overflow. A simple
solution is to flush or reinitialize the dictionary when it becomes full and continue coding with a new initialized dictionary.A_more complex option is to
monitor compression performance and flush the dictionary when it becomes
poor or unacceptable. Alternatively, the least used dictionary entries can be
tracked and replaced when necessary.8.2.5 Run-Length CodingAs was noted in Section 8.1.2, images with repeating intensities along their
tows (or columns) can often be compressed by representing runs of identical
intensities as run-length pairs, where each run-length pair specifies the start of
a new intensity and the number of consecutive pixels that have that intensity. The technique, referred to as run-length encoding (RLE), was developed
in the 1950s and became, along with its 2-D extensions, the standard compression approach in facsimile (FAX) coding. Compression is achieved by
eliminating a simple form of spatial redundancy— groups of identical intensities. When there are few (or no) runs of identical pixels, run-length encoding
results in data expansion.TABLE 8.7
LZW coding
example.With reference to Tables
8.3 and 8.4, the coding of
run-lengths is used inCCITT
JBIG2
JPEG
M-JPEG
MPEG-1,2,4
BMPund other compression
standards and file formats
576 Chapter 8 a Image CompressionEXAMPLE 8.8:
RLE in the BMP
file format.Note that duc to differences in overhead, the
uncompressed BMP file
is smaller than Ihe uncompressed TIFF file in
Example 8,7.TABLE 8.8BMP absolute
coding mode
options, In this
mode, the first
byte of the BMP
pair is 0.@ The BMP file format uses a form of run-length encoding in which image
data is represented in two different modes: encoded and absolute —and either
mode can occur anywhere in the image. In encoded mode, a two byte RLE
representation is used. The first byte specifies the number of consecutive pixels that have the color index contained in the second byte. The 8-bit color
index selects the run’s intensity (color or gray value) from a table of 256 possible intensities,In absolute mode, the first byte is 0 and the second byte signals one of four
possible conditions, as shown in Table 8.8. When the second byte is 0 or 1, the
end of a line or the end of the image has been reached. If it is 2, the next two
bytes contain unsigned horizontal and vertical offsets to a new spatial position
(and pixel) in the image. If the second byte is between 3 and 255, it specifies
the number of uncompressed pixels that follow—with each subsequent byte
containing the color index of one pixei. The total number of bytes must be
aligned on a 16-bit word boundary. ,An uncompressed BMF file ésaved using Photoshop) of the 512 x 512 x 8
bit image shown in Fig. 8.9(a} requires 263,244 bytes of memory. Compressed
using BMP’s RLE option, the file expands to 267,706 bytes—and the compression ratio is C = 0,98. There are not enough equal intensity runs to make runlength compression effective; a small amount of expansion occurs. For the
image in Fig, 8.1(c), however, the BMP RLE option results in a compression
ratioC = 1.35. RRun-length encoding is particularly effective when compressing binary images, Because there are only two possible intensities (black and white), adjacent
pixels are more likely to be identical. In addition, each image row can be represented by a sequence of tengths only—rather than length-intensity pairs as was
used in Example 8.8. The basic ‘idea is to code each contiguous group (i.e., run)
of 0s or 1s encountered in a left to right scan of a row by its length and to establish a convention for determining the value of the run. The most common conventions are (1) to specify the value of the first run of each row, or (2) to assume
that each row begins with a white run, whose run length may in fact be zero.Although run-length encoding is in itself an effective method of compressing binary images, additional compression can be achieved by variable-length
coding the run lengths themselves. The black and white run tengths can be
coded separately using variable-length codes that are specifically tailored to
their own statistics. For example, letting symbol a; represent a black run of
length j, we can estimate the probability that symbol a; was emitted by an
imaginary black run-length source by dividing the number of black run lengths    Second Byte Value Condition
it) End of line
1 End of image
2 Mave to a new position
3-255 Specify pixels individually
8.2 m Some Basic Compression Methods 579Start new
coding linePut ag before
the first pixelDetect a,Detect Bb,
Detect 52No    
    
           
 No  
   Detect a, 
      
  
   
     
 Vertical mode
codingHorizontal
mode coding  Pass mode
codingPut ao
under 6,[|  
   
 
      ~. .
Put ay OB a2" Pula ona, No1 Yes FIGURE 8.14
CCITT 2-D
READ coding
procedure. The
notation [a,))|
denotes the
absolute value of
the distance
between changing
elements 2,and b,.
8.2 & Some Basic Compression Methods 577of length 7 in the entire image by the total number of black runs. An estimate
of the entropy of this black run-length source, denoted Hp, follows by substituting these probabilities into Eq. (8.1-6). A similar argument holds for the entropy of the white runs, denoted H,. The approximate run-length entropy of
the image is thenHg = Ly + L, (8.2-7)where the variables Ly and L, denote the average values of black and white
run lengths, respectively. Equation (8.2-7) provides an estimate of the average
number of bits per pixel required to code the run Jengths in a binary image
using a variable-length code.Two of the oldest and most widely used image compression standards are
the CCITT Group 3 and 4 standards for binary image compression. Although they have been used in a variety of computer applications, they were
originally designed as facsimile (FAX) coding methods for transmitting documents over telephone networks. The Group 3 standard uses a 1-D runlength coding technique in which the last K — 1 lines of each group of K
lines (for K = 2 or 4} can be optionally coded in a 2-D manner, The Group 4
standard is a simplified or streamlined version of the Group 3 standard in
which only 2-D coding is allowed. Both standards use the same 2-D coding
approach, which is two-dimensional in the sense that information from the
previous line is used to encode the current line. Both 1-D and 2-D coding are
discussed next.One-dimensional CCITT compressionIn the 1-D CCITT Group 3 compression standard, each line of an image’ is
encoded as a series of variable-length Huffman code words that represent
the run lengths of alternating white and black runs in a left-to-right scan of
the line. The compression method employed is commonly referred to as
Modified Huffman (MH) coding. The code words themselves are of two
types, which the standard refers to as termirmgting codes and makeup codes.
If run length r is less than 63, a terminating code -from Table A.1 in Appendix A is used to represent it. Note that the standard specifies different terminating codes for black and white runs, If r > 63, two codes are used—a
makeup code for quotient |r/64] and terminating code for remainder
rmod64. Makeup codes are listed in Table A.2 and may or may not depend
on the intensity (black or white) of the run being coded. If | r/64] < 1792,
separate black and white run makeup codes are specified; otherwise, makeup
codes are independent of run intensity. The standard requires that each line
begin with a white run-length code word, which may in fact be 00110101, the
code for a white run of length zero. Finally, a unique end-of-line (EOL) code
word 000000000001 is used to terminate each line, as well as to signal the
first line of each new image. The end of a sequence of images ts indicated by
six consecutive EOLs. *)n the standard. images are referred 10 as pages and sequences of images are called documents.Recall from Section 8.2.2
tbat the notation | 4)
denotes the kirgest
integer Jess {han or
equal to x.
578 Chapter 8 m Image CompressionTwo-dimensional CCITT compressionThe 2-D compression approach adopted for both the CCITT Group 3 and 4
standards is a line-by-line method in which the position of each black-to-white
or white-to-black run transition is coded with respect to the position of a
reference element ag that is situated on the current coding line. The previously
coded line is called the reference line; the reference line for the first line of
each new image is an imaginary white line. The 2-D coding technique that is
used is called Relative Element Address Designate (READ) coding. In the
Group 3 standard, one or three READ coded lines are allowed between successive MH coded lines and the technique is called Modified READ (MR)
coding. In the Group 4 standard, a greater number of READ coded lines are ailowed and the method is called Modified Modified READ (MMR) coding. As
was previously noted, the coding is two-dimensional in the sense that information from the previous line is used to encode the current line. Two-dimensional
transforms are not involved.Figure 8.14 shows the basic 2-D coding process for a single scan line. Note
that the initial steps of the procedure are directed at locating several key
changing elements: ay, ,, a2, b;, and b,. A changing element is defined by the
standard as a pixel whose value is different from that of the previous pixel on
the same line. The most important changing element is ag (the reference element), which is either set to the location of an imaginary white changing element to the left of the first pixel of each new coding line or determined from
the previous coding mode, Coding modes are discussed in the following paragraph. After ay is located, a, is identified as the location of the next changing
element to the right of a) on the current coding line, a, as the next changing
element to the right of a; on the coding line, b, as the changing element of
the opposite value (of ao) and to the right of ap on the reference (or previous) line, and 62 as the next changing element to the right of 5, on the reference line. If any of these changing elements are not detected, they are set to
the location of an imaginary pixel to the right of the last pixel on the appropriate line. Figure 8.15 provides two illustrations of the general relationships
between the various changing elements.After identification of the current reference element and associated changing elements, two simple tests are performed to select one of three possible
coding modes: pass mode, vertical mode, or horizontal mode. The initial test,
which corresponds to the first branch point in the flowchart in Fig. 8.14, compares the location of b; to that of a. The second test. which corresponds to the
second branch point in Fig. 8.14, computes the distance (in pixels) between the
locations of a, and 6; and compares it against 3. Depending on the outcome of
these tests, one of the three outlined coding block’ of Fig. 8.14 is entered and
the appropriate coding procedure is executed. A new reference element is then
established, as per the flowchart, in preparation for the next coding iteration.Table 8.9 defines the specific codes utilized for each of the three possible
coding modes. In pass mode, which specifically excludes the case in which &, is
directly above a), only the pass mode code word (001 is needed. As Fig. 8.15(a}
shows, this mode identifies white or black reference line runs that do not overlap
580 — Chepter 8 mf Image CompressionTABLE 8.9
CCITT twodimensional code
table.EXAMPLE 8.9:
CCITT vertical
mode coding
example.a
bFIGURE 8.15
CCITT (a) pass
mode and(b) horizontal]
and vertical mode
coding
parameters. Mode Code WordPass 0001
Horizontal 001 + M(aga,) + M(a;a2)
Verticala, below by 1a, one to the right of b, 011@; two to the right of 5, 000011@; three to the right of 5; 0000021a, one to the left of 6, O10a; two to the left of d; 000010a, three to the left of 5; 0000010
Extension 0000001 xxx  the current white or black coding line runs. In horizontal coding mode, the distances from ap to a, and a, to a;must be coded in accordance with the termination and makeup codes of Tables A.1 and A.2 of Appendix A and then
appended to the horizontal mode code word 001. This is indicated in Table 8.9
by the notation 001 + M{aga,) + M(a,a@2), where aja, and aa denote the distances from ap to a; and a, to a2, respectively. Finally, in vertical coding mode,
one of six special variable-length codes is assigned to the distance between a,
and 5. Figure 8.15(b) illustrates the parameters involved in both horizontal
and vertical mode coding. The extension mode code word at the bottom of
Table 8.9 is used to enter an optional facsimile coding mode. For example, the
0000001111 code is used to initiate an uncompressed mode of transmission.@ Although Fig. 8.15(b) is annotated with the parameters for both horizontal
and vertical mode coding (to facilitate the discussion above), the depicted pattern of black and white pixels is a case for vertical mode coding, That is, because 5, is to the right of aj, the first (or pass mode) test in Fig. 8.14 fails. The
second test, which determines whether the vertical or horizontal coding mode[LU cee
IEOECG SECOCoding line a Pass mode  |
im                        Vertical modeReference line             cr |
i    Na4
oOHorizontal mode
8.2 ® Some Basic Compression Methodsis entered, indicates that vertical mode coding should be used, because the distance from a, to dy is Jess than 3, In accordance with Table 8.9, the appropriate
code word is 000010, implying that a, is two pixels left of b,. In preparation for
the next coding iteration, ap is moved to the location of ay. ie@ Figure 8.16(a) is a 300 dpi scan of a7 X 9.25 inch baok page displayed at
about 1/3 scale. Note that about half of the page contains text, around 9% is occupied by a halftone image,and the rest is white space. A section of the page is
enlarged in Fig. 8.16(b). Keep in mind that we are dealing with a binary image:
the illusion of gray tones is created, as was described in Section 4.5.4, by the
halftoning process used in printing. If the binary pixels of the image in Fig. 8.16(a)
are stored in groups of 8 pixels per byte, the 1952 X 2697 bit scanned image,
commonly called a document, requires 658,068 bytes. An uncompressed PDF file
of the document (created in Photoshop) requires 663,445 bytes. CCITT Group
3 compression reduces the file to 123,497 bytes—resulting in a compression
ratio C = 5.37; CCITT Group 4 compression reduces the file to 110,456 bytes,
increasing the compression ratio to about 6. Fy8.2.6 Symbol-Based CodingIn symbol- or token-based coding, an image is represented as a collection of
frequently occurring sub-images. called symbols, Each such symbol is stored in
a symbol dictionary and the image is coded as a set of triplets {(41, 4.4),
(x2, 2, #2),--. }, where each (x,, y;) pair specifies the location of a symbel inAF tense riompatones 7D leveis eeapertivets581EXAMPLE 8.10:
CCITT
compression
example.Do not confuse the PDF
used here, which stands
for Portable Document
Format, with the PDF
used in previous seclions
ang chaptets for probabitity density function.With reference to Tables
8.3 and 8.4. symbal-based
coding is used in= JBIG2compression.abFIGURE 8.16A binary scan ofa book page:(a) scaled to show
the general page
content; {b) scaled
to show the
binary pixels used
in dithering.
582  Chopter 8 # Image CompressionabcFIGURE 8.17(a) A bi-level
document,(b) symbol
dictionary, and
(c) the triplets
used to locate the
symbois in the
document,the image and token f; is the address of the symbol or sub-image in the dictionary.
That is, each triplet represents an instance of a dictionary symbol in the
image. Storing repeated symbols only once can compress images significantly —
particularly in document storage and retrieval applications, where the symbols are often character bitmaps that are repeated many times.Consider the simple bilevel image in Fig. 8.17(a). It contains the single word,
banana, which is composed of three unique symbois: a 6, three a’s, and two n’s.
Assuming that the b is the first symbol identified in the coding process, its 9 x 7
bitmap is stored in location 0 of the symbol dictionary. As Fig, 8.17(b) shows, the
token identifying the 6 bitmap is 0. Thus, the first triplet in the encoded image's
representation [see Fig. 8.17(c)} is (0,2, 0)—- indicating that the upper-left corner
(an arbitrary convention) of the rectangular bitmap representing the b symbol is
to be placed at location (0,2) in the decoded image. After the bitmaps for the a
and n symbols have been identified and added to the dictionary, the remainder
of the image can be encoded with five additional triplets. As long as the six
triplets required to locate the Symbols in the image, together with the three
bitmaps required to define them, are smaller than the original image, compression occurs. In this case, the starting image has 9 X 51 x 1 or 459 bits and,
assuming that each triplet is composed of 3 bytes, the compressed representation has (6 X 3 8} + [(9 X 7) + (6 X 7) + (6 X 6)] or 285 bits; the resulting compression ratio C = 1.61. To decode the symbol-based representation
in Fig. 8.17(c), you simply read the bitmaps of the symbols specified in the
triplets from the symbol dictionary and place them at the spatial coordinates
specified in each triplet.Symbol-based compression was proposed in the early 1970s (Ascher and
Nagy [1974]), but has become practical only recently. Advances in symbol
matching algorithms (see Chapter 12) and increased CPU computer processing speeds have made it possible both to select dictionary symbols and to find
where they occur in an image in a timely manner. And like many other compression methods, symbol-based decoding is significantly faster than encoding.
Finally, we note that both the symbol bitmaps that are stored in the dictionary
and the triplets used to reference them can themselves be encoded to further
improve compression performance. If—as in Fig. 8.17—only exact symbol
matches are allowed, the resulting compression is Jossless; if small differences
are permitted, some level of reconstruction error will be present. Token | Symbol Triplet
| bee (0,2.0)
9 G, 10,1)
Po ESPaErSrs:) (3, 18.2)
(3, 26,1)
we
G.42.1)
> |
L                    ABS
tices
8.2 % Some Basic Compression Methods 583JBIG2 compressionJBIG2 is an international standard for bilevel image compression. By segmenting
an image into overlapping and/or non-overlapping regions of text, halftone, and
generic content, compression techniques that are specifically optimized for each
type of content are employed:© Text regions are composed of characters that are ideally suited for a symbo!based coding approach, Typically, each symbol will correspond to a character bitmap—a subimage representing a character of text. There is normally
only one character bitmap (or subimage) in the symbol dictionary for each
upper- and lowercase character of the font being used. For example, there
would be one “a” bitmap in the dictionary, one “A” bitmap, one “b” bitmap,
and so on.In lossy JBIG2 compression, often called perceptually lossless or
visually lossless, we neglect differences between dictionary bitmaps (i.e.,
the reference character bitmaps or character templates) and specific instances of the corresponding characters in the image. In lossless compression, the differences are stored and used in conjunction with the triplets
encoding each character (by the decoder) to produce the actual image
bitmaps. All bitmaps are encoded either arithmetically or using MMR (see
Section 8.2.5); the triplets used to access dictionary entries are either
arithmetically or Huffman encoded.* Halftone regions are similar to text regions in that they are composed of
patterns arranged in a regular grid. The symbols that are stored in the dictionary, however, are not character bitmaps but periodic patterns that represent intensilies (e.g., of a photograph) that have been dithered to
produce bilevel images for printing.© Generic regions contain non-text, non-halftone information, like line art
and noise, and are compressed using cither arithmetic or MMR coding.As is true of many image compression standards, JBIG2 defines decoder behavior. It does not explicitly define a standard encoder, but is flexible enough
to allow various encoder designs. Although the design of the encoder is left unspecified, it is nevertheless important, because it determines the level of compression that is achieved. After all, the encoder must segment the image into
regions, choose the text and halftone symbols that are stored in the dictionaries, and decide when those symbols are essentially the same as, or different
from, potential instances of the symbols in the image. The decoder simply uses
that information to recreate the original image.B Consider again the bilevel image in Fig. 8.16(a). Figure 8.18(a) shows a reconstructed section of the image after lossless JBIG2 encoding (by a commercially available document compression application). It is an exact replica of
the original image. Note that the ds in the reconstructed text vary slightly, despite the fact that they were generated from the same d entry in the dictionary.
The differences between that d and the ds in the image were used to refine the
output of the dictionary. The standard defines an algorithm for accomplishingEXAMPLE 8.11:
JBIG2
compression
example.
586 Chapter 8 Image CompressionPROP
mT,BhFIGURE 8.19(a) A 256-bit
monochrome
image. (b)-(h)
The four most
significant binary
and Gray-coded
bit planes of the
image in (a).
584 Chapter 8 # Image CompressionabeFIGURE 8.18
JBIG2
compression
comparison:(a) lossless
compression and
reconstruction;
(b) perceptually
lossless; and{c) the scaled
difference
between the two.With reference to
Tables 8,3 and 8.4,
bit-plane coding is used
in the« JBIGte IPEG-2000compression standards,(AAR Vk Ute AeA rd Oajust described just described
esulting coeffic esulting coeffic
nt arrays. nt arrays.
retained coeffi retained coeffi
2 we disrepar :n we disresar this during the decoding of the encoded dictionary bitmaps. For the purposes
of our discussion, you can think of it as adding the difference between a dictionary bitmap and a specific instance of the corresponding character in the
image to the bitmap read from-the dictionary.Figure 8.18(b) is another reconstruction of the area in (a) after perceptually lossless JBIG2 compression. Note that the ds in this figure are identical.
They have been copied directly from the symbol dictionary. The reconstruction is called perceptually lossless because the text is readable and the font is
even the same. The small differences—shown in Fig. 8.18(c)— between the «ds
in the original image and the d in the dictionary are considered unimportant
because they do not affect readability, Remember that we are dealing with
bilevel images, so there are only three intensities in Fig. 8.18(c). Intensity 128
indicates areas where there is no difference between the corresponding pixels
of the images in Figs. 8.18(a) and (b); intensities 0 (black} and 255 (white) indicate pixels of opposite intensities in the two images—for example, a black
pixel in one image that is white in the other, and vice versa.The lossless JBIG2 compression that was used to generate Fig. 8.18(1) reduces the original 663,445 byte uncompressed PDF image to 32,705 bytes; the
compression ratio is C = 20.3. Perceptually lossless JB1G2 compression reduces the image to 23,913 bytes, increasing the compression ratio to about
27.7. These compressions are 4 to 5 times greater than the CCITT Group 3 and
4 results from Example 8.10. :#.2.7 Bit-Plane Coding
The run-length and symbol-based techniques of the previous sections can be
applied to images with more than two intensities by processing their bit planes
individually. The technique. called bit-plane coding, is based on the concept of
decomposing a multilevel (monochrome or color) image into a series of binary
images (see Section 3.2.4) and compressing each binary image via one v! sew
eral well-known binary compression methods. In this section, we describe the
two most popular decomposition approaches.The intensities of an m-bit monochrome image can be represented in the
form of the base-2 polynomial>yy eH Ayer FL #2! + aay” (8.2-S}
8.2 & Some Basic Compression Methods 585Based on this property, a simple method of decomposing the image into a collection of binary images is to separate the m coefficients of the polynomial into
m 1-bit bit planes. As noted in Section 3.2.4, the lowest order bit plane (the
plane corresponding to the least significant bit) is generated by collecting the ay
bits of each pixel, while the highest order bil plane contains the a,,,_, bits or coefficients. In general, each bit plane is constructed by setting its pixels equal to the
values of the appropriate bits or polynomial coefficients from each pixel in the
original image. The inherenf disadvantage of this decomposition approach is that
small changes in intensity can have a significant impact on the complexity of the
bit planes. If a pixel of intensity 127 (01111111) is adjacent to a pixel of intensity
128 (10000000), for instance, every bit plane will contain a corresponding 0 to 1
(or 1 to 0) transition. For example, because the most significant bits of the binary
codes for 127 and 128 are different, the highest bit plane will contain a zero-valued
pixel next toa pixel of value 1, creating a 0 to 1 (or f to 0) transition at that point.An alternative decomposition approach (which reduces the effect of
small intensity variations) is to first represent the image by an m-bit Gray
code. The m-bit Gray code g,,_) ... 82818 that corresponds to the polynomial!
in Eq. (8.2-8) can be computed fromR= 4, 8a,, Of' S=m-2
Bm-1 = @m-1 (82-9)Here, ® denotes the exclusive OR operation. This code has the unique property that successive code words differ in only one bit position. Thus, smail
changes in intensity are less likely to affect all m bit planes. For instance, when
intensity levels 127 and 128 are adjacent, only the highest order bit plane will
contain a 0 to 1 transition, because the Gray codes that correspond to 127 and
128 are 01000000 and 11000000, respectively.®% Figures 8.19 and 8.20 show the eight binary and Gray-coded bit planes of
the 8-bit monochrome image of the child in Fig. 8.19(a). Note that the highorder bit planes are far Jess complex than their low-order counterparts. That is,
they contain large uniform areas of significarttly less detail, busyness, or randomness. In addition, the Gray-coded bit planes are less complex than the corresponding binary bit planes, Both observations are reflected in the JBIG2
coding results of Table 8.10, Note. for instance, that the as and gs results are      Coefficient Binary Code Gray Code Compression
m (PDF bits) (PDF bits) . Ratio
7 6,999 6,999 1.00
6 12,791 11,024 1.16
5 40,104 36,914 1.09
4 55,911 47.415 1.18
3 78,915 67,787 1.16
2 101,535 92,630 1,10
1 107,909 105.286 1.03
0 99,783 7.909 0.92EXAMPLE 8.12:
Bit-plane coding.TABLE 8.10
JBIG2 tosstess
coding results for
the binary and
Gray-coded bit
planes ofFig. 8.19(a). These
results include the
overhead of each
bit plane's PDF
representation,
§.2 « Some Basic Compression Methods 587ean
“OL&hFIGURE 8.20
(a}-(h) The four
least significant
binary (left
column) and
Gray-coded
(right column)
bit planes of
the image in
Fig. 8.19(a).Ba| ef,
590 = Chopter 8 m Image CompressionTo compote the WHT
of an NX N input
image f(x, y), rather
than a subimage, change
ato N in Eq. (82-16),FIGURE 8.22
Walsh-Hadamard
basis functions for= 4, The origin
of each block is at
its top left.and1
S(x,y, Uy v) = —ePmturr orn (8.2-15)
nwhere j = V—I. These are the transformation kernels defined in Eqs. (2.6-34)
and (2.6-35) of Chapter 2 with M = N = n. Substituting these kernels into
Egs. (8.2-10) and (8.2-11) yields a simplified version of the discrete Fourier
transform pair introduced in Section 4.5.5,A computationally simpler transformation that is also useful in transform
coding, called the Walsh-Hadamard transform (WHT), is derived from the
functionally identical kernels1S heonteoapc0))
r(x. y, u,v) = s(x, y, u,v) = nob (8.2-16)where n = 2”. The summation in the exponent of this ¢ expression is performed
in modulo 2 arithmetic and b,(z) is the kth bit (from right to left) in the binary
representation of z. If m = 3 and z = 6(110in binary), for example, by(z) = 0,
b,(z) = 1, and b)(z) = 1. The p; (it) in Eq. (8.2-16) are computed using:Pol) = By, - (8)
Pylit) = by fee) + by -o(u)
p(t) = Byte) + By s(t) (8.2-17)Pmt) = Dylie) + byl2e)
where the sums, as noted previously, are performed in modulo 2 arithmetic.
Similar expressions apply to p;(v).
Unlike the kernels of the DFT, which are sums of sines and cosines [see
Eqs. (8.2-14) and (8.2-15)], the Walsh-Hadamard kernels consist of alternatingplus and minus Is arranged in a checkerboard pattern. Figure 8.22 shows the
kernel for n = 4, Each block consists of 4 x 4 = 16 elements (subsquares).uuoe at
=e
me
===:aox
8.2 m Some Basic Compression Methods 589Transform selection
Block transform coding systems based on a variety of discrete 2-D transforms
have been constructed and/or studied extensively. The choice of a particular
transform in a given application depends on the amount of reconstruction
error that can be tolerated and the computational resources available. Compression is achieved during the quantization of the transformed coefficients
(not-during the transformation step).With reference to the “discussion in Section 2.6.7, consider a subimage
a(x, y) of size n X n whose forward, discrete transform, J(u, v), can be expressed in terms of the general relationn=-iIn-]
T(u, v) = & 2st y)r(x, y, u,v) (8.2-10)
Asfor u,v = 0,1,2,...,n — 1. Given T(u, v), g(x, y) similarly can be obtained
using the generalized inverse discrete transformA-in-1
a(x y) = S ST (u,v) s(x, y, u,v) (8.2-11)4u=00=0for x, y = 0,1,2,...,2 — 1. In these equations, r(x, y, u,v) and s(x, y, u, v)
are called the forward and inverse transformation kernels, respectively. For
reasons that will become clear later in the section, they also are referred to
as basis functions or basis images. The T(u, v) for u,v = 0,1,2,...,42 - Lin
Eq. (8.2-10) are called transform coefficients; they can be viewed as the expansion coefficients —see Section 7.2.1—of a series expansion of g(2, y) with
respect to basis functions s(x, y, u, v).
As explained in Section 2.6.7, the kernel in Eq. (8.2-10) is separable ifr(x, yu, v) = Ox, Wray, v) (8.2-12)In addition, the kernel is symmetric if r, is fungtionally equal to r). In this case,
Eq. (8.2-12) can be expressed inthe form ~ ”r(x, yu. v) = r(x, wry, 2) (8.2-13)Identical comments apply to the inverse kernel if r(x, y, u, v) is replaced by
s(x, y, u, v) in Eqs. (8.2-12) and (8.2-13). It is not difficult to show that a 2-D
transform with a separable kernel can be computed using row-column or
column-row passes of the corresponding 1-D transform, in the manner explained in Section 4.11.1.The forward and inverse transformation kernels in Eqs, (8.2-10) and (8.2-11)
determine the type of transform that is computed and the overall computational complexity and reconstruction error of the block transform coding system in
which they are employed. The best known transformation kernel pair isr(x, yu, v) = en Palurtoyyn (8.2-14)‘We use g(x, y) to differ.
entiale a subimage from
the input image f(x, y).
‘Thus, the summation limits become n rather than
Mand N.
8.2 & Some Basic Compression MethodsWhite denotes +1 and black denotes —1. To obtain the top left block, we let
u = v = Oand plot values of r(x, y, 0,0) for x, y = 0,1, 2, 3. All values in this
case are +1, The second block on the top row is a plot of values of r(x, y, 0, 1)
for x, y = 0, 1, 2, 3, and so on. As already noted, the importance of the WalshHadamard transform is its simplicity of implementation —all kernel values are
+lor -1.One of the transformations used most frequently for image compression is
the discrete cosine transform (DCT). It is obtained by substituting the following (equal) kernels into Eqs. (8.2-10) and (8.2-11)r(x, y, u,v) = s(x, yu, v)a{u)a(v) cs] (2x cine) co] OF Der (8.2-18)i wherefi foru = 0
Yn
2
i
and similarly for a(v). Figure 8.23 shows r(x, y, u,v) for the case n = 4. The
computation follows the same format as explained for Fig. 8.22, with the dif
ference that the values of r are not integers. In Fig. 8.23. the lighter intensity
values correspond to larger values of r.a(u) = (8.2-19)forw = 1,2,...,n-1M@ Figures 8.24(a) through (c) show three approximations of the 512 x 512
monochrome image in Fig. 8.9(a). These pictures were obtained by dividing
the original image into subimages of size 8 X 8, representing each subimage
using one of the transforms just described (i... the DFT. WHT, or DCT
transform), truncating 50% of the resulting caéfficients, and taking the inverse
transform of the truncated coefficient arrays. 591To compute the DCT of
an N X N input image
(3, v), rather than a
subimage, change n 10
N in Eqs. (8.2-18) and
(8.2-19).EXAMPLE 8,13;
Block transform
coding with the
DFT, WHT, and
DCT.FIGURE 8.23
Discrete-cosine
basis functions for
n = 4, The origin
of each block is at
its top left.
588  Chopter8 # Image CompressionWith reference to Tables
83 and 8.4, block trans:
form coding is used in
@ JPEG
@ MJPEG
@ MPEG-1.2.4
@ =H261,H.262,
H.263, and H.264
«© DVandHDV
«© VC1
and other compression
standards.In this section, we testrict
‘our allention to square
subimages (the most
commonly used). itis
assumed that the input
image is padded, if
necessary, So that bothM and N are multiples
of mt.a
bFIGURE 8.21A block
transform coding
system:(a) encoder:(b) decoder.significantly larger than the ag and g, compressions; and that both gs and g,
are smaller than their as and a, counterparts. This trend continues throughout
the table, with the single exception of ay. Gray-coding provides a compression
advantage of about 1.06:1 on average. Combined together, the Gray-coded
files compress the original monochrome image by 678,676/475,964 or 1.43:1;
the non-Gray-coded files compress the image by 678,676/503,916 or 1.35:1.
Finaily, we note that the two least significant bits in Fig. 8.20 have little apparent structure. Because this is typical of most 8-bit monochrome images, bitplane coding is usually restricted to images of 6 bits/pixel or less. JBIG1, the
predecessor to JBIG2, imposes such a limit. a8.2.8 Block Transform CodingIn this section, we consider a compression technique that divides an image into
small non-overlapping blocks of equal size (e.g., 8 X 8) and processes the
blocks independently using a 2-D transform. In dlock-transform coding, a reversible, linear transform (suchas the Fourier transform) is used to map each
block or subimage into a set of transform coefficients, which are then quantized
and coded. For most images, a significant number of the coefficients have small
magnitudes and can be coarsely quantized (or discarded entirely) with little
image distortion. A variety of transformations, including the discrete Fourier
transform (DFT) of Chapter 4, can be used to transform the image data.
Figure 8.21 shows a typical block transform coding system. The decoder implements the inverse sequence of steps (with the exception of the quantization
function) of the encoder, which performs four relatively straightforward operations: subimage decomposition, transformation, quantization, and coding. An
M X N input image is subdivided first into subimages of size n x n, which are
then transformed to generate MN/n? subimage transform arrays, each of size
n X n, The goal of the transformation process is to decorrelate the pixels of
each subimage, or to pack as much information as possible into the smallest
number of transform coefficients. The quantization stage then selectively eliminates or more coarsely quantizes the coefficients that carry the least amount
of information in a predefined sense (several methods are discussed later in
the section). These coefficients have the smallest impact on reconstructed
subimage quality. The encoding process terminates by coding (normaily using
a variable-length code) the quantized coefficients. Any or all of the transform
encoding steps can be adapted to local image content, called adaptive transform coding, or fixed for all subimages, called nonadaptive transform coding.   Input Construct cee
image nXn Forward Quantizers Symbol Compressed
(MN) subimages transform | | encoder image
Compressed Symbol Inverse a Merge | _. Decompressed
image decoder transform | subimages image
592 Chapter 8 #@ Image Compression abcdef
“FIGURE 8.24 Approximations of Fig. 8.9(a) using the (a) Fourier, (b) Walsh-Hadamard, and (c) cosine
transforms, together with the corresponding scaled error images in (d)-{f)In each case, the 32 relained coefficients were selected on the basis of maximum magnitude. Note that in all cases, the 32 discarded coefficients had little
visual impact on the quality of the reconstructed image. Their elimination,
however, was accompanied by some mean-square error. which can be seen in
the scaled error images of Figs. 8.24(d) through (f). The actual rms errors were
2.32, 1.78, and t.13 intensities, respectively. EeThe smal! differenves in mean-square reconstruction error noted in the preceding example are related directly to the energy or information packing properties of the transforms employed. In accordance with Eq. (8.2. 01).an aX 7subimage g(x, ¥) cam be expressed as a function of its 2-D wansform f(a. ¢):relant
gy, ¥) = > S7 (ovys(xy yu 8) (8.2-20}
Pied ttt)
© QoQ Aa }. Because the inverse kernel str vir. e) toand vot an the values of -20)) depends oaly on the indice you
teeun be viewed as defining 
  set of basiy fusions ar beavis mw Fuergt
8.2 m Some Basic Compression Methodsimages for the series defined by Eq. (8.2-20). This interpretation becomes
clearer if the notation used in Eq. (8.2-20) is modified to obtainaclaclG=> ZT v) Sun (8.2-21)x=0 v=0where G is ann X n matrix containing the pixels of g(x, y) and5(0,0,u,v) ~ sO,ljuv)  - — s(O,n — "1,4, 2)
5(1, 0, u,v) : cee :
Sy. = ;
s(n -1,0,u,v) s(n - 1, 1juv) + s(n In lu, v)
(8.2-22)Then G, the matrix containing the pixels of the input subimage, is explicitly defined as a linear combination of n* matrices of size n X n; that is, the S,,,, for
u,v = 0,1,2,...,4 — lin Eq. (8.2-22). These matrices in fact are the basis images (or functions) of the series expansion in Eq. (8.2-20); the associated
T(u, v) are the expansion coefficients. Figures 8.22 and 8.23 illustrate graphically the WHT and DCT basis images for the case of n = 4.If we now define a transform coefficient masking function0 if T(u, v) satisfies a specified truncation criterion
X(u, 0) = {° otherwise , (82-23)for u,v = 0,1,2,...,n — 1, an approximation of G can be obtained from the
truncated expansionG= 3 Sx, mre 8.9 (8.2-24)u=0v=0where X(u, v) is constructed to eliminate the basis images that make the
smallest contribution to the total sum in Eq. (8.2-21). The mean-square errorbetween subimage G and approximation G then is{ie - ér}n~la~t n-la-)
t DY Su 8.0 -— YS DAC, vy r) Suv=Ov=0 w=0osla-la-l= YJ Soh wn[1 — Hur]a=0u=0ems  ‘l}nod ‘ (8.2-25)> Sr, V)Syof1 — X(u, 2)]u=0 0=0   593
8.2 t& Some Basic Compression Methods 595 
 
 
      
   Discontinuity
™ |
| |
Boundary |~——-n |
wo points
{——— +2n  |
|
|
|
+b+ content. When the DFT transform coefficients are truncated or quantized, the
Gibbs phenomenon’ causes the boundary points to take on erroneous values,
which appear in an image as blocking artifact. That is, the boundaries between adjacent subimages become visible because the boundary pixels of the subimages
assume the mean values of discontinuities formed at the boundary points [see
Fig. 8.25(a)]. The DCT of Fig. 8.25(b) reduces this effect, because its implicit
2n-point periodicity does not inherently produce boundary discontinuities.Subimage size selectionAnother significant factor affecting transform coding error and computational
complexity is subimage size. In most applications, images are subdivided so
that the correlation (redundancy) between adjacent subimages is reduced to
some acceptable level and so that n is an integer power of 2 where, as before, 1
is the subimage dimension. The latter condition simplifies the computation of
the subimage transforms (see the base-2 successive doubling method discussed in Section 4.11.3). In general, both theleyel of compression and computational complexity increase as the subimage size increases. The most
popular subimage sizes are 8 X 8 and 16 X 16.@ Figure 8.26 illustrates graphically the impact of subimage size on transform
coding reconstruction error. The data plotted were obtained by dividing the
monochrome image of Fig. 8.9(a) into subimages of size n Xn, for
n = 2, 4,8, 16,..., 256,512, computing the transform of each subimage, truncating 75% of the resulting coefficients, and taking the inverse transform of the
truncated arrays. Note that the Hadamard and cosine curves flatten as the size of
the subimage becomes greater than 8 X 8, whereas the Fourier reconstruction"This phenomenon, described in most electrical engineering texts on circuit analysis, occurs because the
Fourier transform fails 10 converge uniformly al discontinuities. Al discontinuitics, Fourier expansions
ake the mean values of the points of discontinuity.a
bFIGURE 8.25 The
periodicity
implicit in the 1-D
(a) DFT and(b) DCT.EXAMPLE 8.14:
Effects ofsubimage size on
transform coding.
594 Chapter 8 m Image CompressionIn Example 8.13, 50% of
a DFT, WHT, and DCT
block transform coded
image’s coefficients were
discarded (using 8 x 8
blocks}. After decoding,
the DCT-based result
had the smallest rms
error, indicating that with
Tespect to rms error the
least amouna of information was discarded.where |G — Gllis the norm of matrix {G - G6) and oF(,») is the variance of
the coefficient at transform location (u, v). The final simplification is based on
the orthonormal nature of the basis images and the assumption that the pixels
of G are generated by a random process with zero mean and known covariance. The total mean-square approximation error thus is the sum of the variances of the discarded transform coefficients; that is, the coefficients for which
X(u, v) = 0, so that [3 ~ X(u, v)| in Eq. (8.2-25) is 1. Transformations that redistribute or pack the most information into the fewest coefficients provide
the best subimage approximations and, consequently, the smallest reconstruction errors. Finally, under the assumptions that led to Eq. (8.2-25), the meansquare error of the MN/n® subimages of an M x N image are identical. Thus
the mean-square error (being a measure of average error) of the MX N
image equals the mean-square error of a single subimage.The earlier example showed that the information packing ability of the DCT
is superior to that of the DFT and WHT. Although this condition usually holds
for most images, the Karhunen-Loéve transform (see Chapter 11), not the
DCT, is the optimal transform in an information packing sense. This is due to
the fact that the KLT minimizes the mean-square error in Eq. (8.2-25) for any
input image and any number of retained coefficients (Kramer and Mathews
[1956]).t However, because the KLT is data dependent, obtaining the KLT
basis images for each subimage, in general, is a nontrivial computational task.
For this reason, the KLT is used infrequently in practice for image compression.
Instead, a transform, such as the DFT. WHT, or DCT, whose basis images are
fixed (input independent), normally is used. Of the possible input independent
transforms, the nonsinusojdal transforms (such as the WHT transform) are the
simplest to implement. The sinusoidal transforms (such as the DFT or DCT)
more closely approximate the information packing ability of the optimal KLT.Hence, most transform coding systems are based on the DCT, which provides
a good compromise between information packing ability and computational
complexity. In fact, the properties of the DCT have proved to be of such practical value that the DCT has become an international standard for transform coding systems, Compared to the other input independent transforms, it has the
advantages of having been implemented in a single integrated circuit, packing
the most information into the fewest coefficients? (for most images), and minimizing the block-like appearance, called blocking artifact, that results when the
boundaries between subimages become visible. This last property is particularly
important in comparisons with the other sinusoidal transforms. As Fig. 8.25(a)
shows, the implicit 7-point periodicity (see Section 4.6.3) of the DFT gives rise to
boundary discontinuities that result in substantial high-frequency transform ‘An additional condition for optimality is that the masking function of Eg. (8.2-23) selects the KL coefficients of maximum variance.‘Ahmed et al. (1974] first noticed that the KLT basis images of a first-order Markov image source closely resemble the DCT's basis images. As the correlation between adjacenl pixels approaches one, the
input dependent KL basis images hecome identical to the input independent DCT basis images
596 Chapter 8 mw Image CompressionFIGURE 8.26
Reconstruction
error versus
subimage size. abed   656= 5.5o2 55S2 4.53B& 4$E35B 3m425oe on oO?qh. L aint
TE Te 8) FT HSE S12Subimage sizeerror continues to decrease in this region, As » further increases, the Fourier reconstruction error crosses the Walsh-Hadamard curve and approaches the cosine
result. This result is consistent with the theoretical and experimental findings reported by Netravali and Limb [1980] and by Pratt [1991] for a 2-D Markov image
source.All three curves intersect when 2 X 2 subimages are used. In this case, only
one of the four coefficients (25%) of each transformed array was retained. The
coefficient in all cases was the dc component, so the inverse transform simply
replaced the four subimage pixels by their average value [see Eq. (4.6-21)}.
This condition is evident in Fig. 8.27(b), which shows a zoomed portion of
the 2 x 2 DCT result. Note that the blocking artifact that is prevalent in
this result decreases as the subimage size increases to 4 X 4 and 8 X 8 in
Figs. 8.27(c) and (d). Figure 8278) shows a zoomed portion of the original
image for reference, *Bit allocationThe reconstruction error associated with the truncated series expansion of
Eq. (8.2-24) is a function of the number and relative importance of the FIGURE 8.27 Approximations of Fig. 8.27(a) using 25% of the DCT coefficients and (b) 2 x 2 subimages, (c)
4 X 4 subimages, and (d) 8 x 8 subimages. The original image in (a) js a zoomed section of Fig. 8,9(a).
8.2 & Some Basic Compression Methods 597transform coefficients that are discarded, as well as the precision that is
used to represent the retained coefficients. In most transform cading systems, the retained coefficients are selected [that is, the masking function of
Eq. (8.2-23) is constructed] on the basis of maximum variance, called zonal
coding, or on the basis of maximum magnitude. called threshold coding. The
overall process of truncating, quantizing, and coding the coefficients of a
transformed subimage is commonly called bit allocation.@ Figures 8.28(a) and (c) show two approximations of Fig. 8.9(a) in which
87.5% of the DCT coefficients of each 8 X 8 subimage were discarded. The
first result was obtained via threshold coding by keeping the eight largest
transform coefficients, and the second image was generated by using a zonal
coding approach. In the latter case, each DCT coefficient was considered a
random variable whose distribution could be computed over the ensemble of
all transformed subimages. The 8 distributions of largest variance (12.5% of
the 64 coefficients in the transformed 8 < 8 subimage) were located and used
to determine the coordinates, 4 and v, of the coefficients, 7 (1, v), that were retained for all subimages, Note that the threshold coding difference image of
Fig. 8.28(b) contains less error than the zonal coding result in Fig. 8.28(d), Both
images have been scaled to make the errors more visible. The corresponding
rms errors are 4.5 and 6.5 intensities, respectively. cd EXAMPLE 8.15:
Bit allocation.ab
edFIGURE 8.28
Approximations
of Fig. 8.9(a) using
12.5% of the& x 8DCT
coefficients:
(a}—(b) threshold
coding results;
(c)-- (d) zonal
coding results. The
difference images
are scaled by 4
598 Chapter 8 @ Image Compressionab
edFIGURE 8.29A typical(a) zonal mask,
(b) zonal bit
allocation,(c) threshold
mask, and(d) thresholded
coefficient
ordering
sequence. Shading
highlights the
coefficients that
are retained.Zonal coding implementation Zonal coding is based on the information
theory concept of viewing information as uncertainty. Therefore the transform
coefficients of maximum variance carry the most image information and
should be retained in the coding process. The variances themselves can be calculated directly from the ensemble of MN/n? transformed subimage arrays, as
in the preceding example, or based on an assumed image model (say,a Markov
autocorrelation function). In either case, the zonal sampling process can be
viewed, in accordance with Eq. (8.2-24), as multiplying each T(u, v) by the corresponding element in a zonal mask, which is constructed by placing a I in the
Jocations of maximum variance and a 0 in all other locations. Coefficients of
maximum variance usually are located around the origin of an image transform, resulting in the typical zonal mask shown in Fig. 8.29(a).The coefficients retained during the zonal sampling process must be quantized and caded,so zonal masks are sometimes depicted showing the number of
bits used to code each coefficient [Fig. 8.29(b)}. In most cases, the coefficients are
allocated the same number of bits, or some fixed number of bits is distributed
among them unequally. In the first case, the coefficients generally are normalized by their standard deviations and uniformly quantized. In the second case, a
quantizer, such as an optimal Lloyd-Max quantizer (see Optimal quantizers in
Section 8.2.9), is designed for each coefficient. To construct the required quantizers, the zeroth or dc coefficient normally is modeled by a Rayleigh density
function, whereas the remaining coefficients are modeled by a Laplacian or   
                              6/ 0/6 [8 )7 6 4! 0
tojolofoj|7i/e[s{a]3|2]i]e
OldO/oO;Ol10 16 s{4 3[3/4 1°
olololotollaj4a/3]3]2]iloj;o
ajo]a 00 ofol|s|3/3i2|i|rjolo|
Lolo ofajolalolj2}2}1}1f/1]ofo]o
t { =
010/}/0/0;0 of o Qo 1/1 o;}Go];]a)/0)0
olelofololojofo|iojojofojolofola
4 = wah. 1 1 J
we '
} jo [010 (Out {6 14} 15 | 27 [28
o[ojolo|!2)4| 7 [13] 16/26 | 29| 4 |
H } . a relies
olo]o]o]} 3} 8 fiz}iz}os) 30} a1 | 43
4
alole]o}iojitjis|24| sr: 40] a4] s3
ote ee a
10 | 0} 04 O |) 10/19; 23192 | 99 | 45 | 82 | 4
0} a} 0! o || 20] 22) 33] 38} 46; 51 | 55 | 60
eo “+t
oto} o[olia:!34; 37] 47 | so] 56 | 59| 6Oye) | 50 |Jo}ofore [35136 [as | a9 57 | ss! 62 [6s
8.2 & Some Basic Compression Methods 599Gaussian density.’ The number of quantization levels (and thus the number of
bits) allotted to each quantizer is made proportional to log, Hy, »)- Thus the retained coefficients in Eq. (8.2-24)— which (in the context of the current discussion) are selected on the basis of maximum variance—are assigned bits in
proportion to the logarithm of the coefficient variances.Threshold coding implementation Zonal coding usually is implemented by
“using a single fixed mask for all subimages. Threshold coding, however, is inherently adaptive in the sense that the location of the transform coefficients
retained for each subimage vary from one subimage to another. In fact,
threshold coding is the adaptive transform coding approach most often used
in practice because of its computational simplicity. The underlying concept is
that, for any subimage, the transform coefficients of largest magnitude make
the most significant contribution to reconstructed subimage quality, as
demonstrated in the last example. Because the locations of the maximum coefficients vary from one subimage to another, the elements of X(u, v)T(u, v)
normally are reordered (in a predefined manner) to form a 1-D, run-length
coded sequence. Figure 8.29(c) shows a typical threshold mask for one subimage of a hypothetical image. This mask provides a convenient way to visualize
the threshold coding process for the corresponding subimage, as well as to
mathematically describe the process using Eq. (8.2-24). When the mask is applied [via Eq. (8.2-24)] to the subimage for which it was derived, and the resulting n X n array is reordered to form an n?-element coefficient sequence
in accordance with the zigzag ordering pattern of Fig. 8.29(d), the reordered
1-D sequence contains several long runs of 0s [the zigzag pattern becomes evident by starting at 0 in Fig. 8.29(d) and following the numbers in sequence].
These runs normally are run-length coded. The nonzero or retained coefficients, corresponding to the mask locations that contain a 1, are represented
using a variable-length code.There are three basic ways to threshold a transformed subimage or, stated
differently, to create a subimage threshold masking function of the form given
in Eq. (8.2-23): (1) A single global threshold éan‘be applied to all subimages;
(2) a different threshold can be used for each subimage; or (3) the threshold
can be varied as a function of the location of each coefficient within the subimage. In the first approach, the level of compression differs from image to
image, depending on the number of coefficients that exceed the global threshold. In the second, called N-largest coding, the same number of coefficients is
discarded for each subimage. As a result, the code rate is constant and known
in advance. The third technique, like the first, results in a variable code rate,
but offers the advantage that thresholding and quantization can be combined ‘As each coefficient is a Jinear combination of the pixels in its subimage [see Eq. (8.2-10)), the centrallimit theorem suggests that, as subimage size increases, the coefficients tend to become Gaussian. This
result does nol apply to the de coefficient, however, because nonnegative images always have positivede coefficients.The N in “W-largest coding” is not an image dimension, but refers to
the number of coefficienls that are kept.
600 Chapter 8 Image Compressionby replacing X({u, v)T(u, v) in Eq. (8.2-24) withF(u,v) = roune| 3 2) | (8.2-26) Z(u, v)where T(u, v) is a thresholded and quantized approximation of 7(u, v), and
Z(u, v) is an element of the transform normalization arrayZ(0, 0) ZO). ZO - 1)
Z(1,0) :
Z= : (8.2-27)
Zn -1,0) Zn—11) ... Za-1n-1)Before a normalized (threshdlded and quantized) subimage transform,
T(u, v), can be inverse transformed to obtain an approximation of subimage
g(x, y), it must be multiplied by Z(u, v). The resulting denormalized array, denoted Tu, v) is an approximation of T(z, v):T(u,v) = T(u, v)Z(u, v) (8.2-28)The inverse transforni of T(, v) yields the decompressed subimage approximation.Figure 8.30(a) depicts Eq. (8.2-26) graphically for the case in which Z(w, v)
is assigned a particular value c. Note that 7(u, v) assumes integer value k if
and only ifke - 5 = T(u,v) < ke + 5 (8.2-29)If Z(u, v) > 2T(u, v), then T (u,v) = 0 and the transform coefficient is completely truncated or discarded. When T(u, v) is represented with a variable-length
code that increases in length as the magnitude of & increases, the number of bits
used to represent T(u, v) is controlled by the value of c. Thus the elements of Z ab T(u.v) ‘ 6 T ~ rFIGURE 8.30 36] 1] 10/16] so] | a
| 12} 2] 14] 19 | 26 | 58 ; 60 | 55
4 
 
 
  
  
 
  
  
 
  coding(a) A threshold
quantization $7} {99 | 56 |    curve [see Eq. 187! ‘0 62 |
(8.2-29)].(b) A waft
typical i 109; i103) 7
normalization 124135155; 64 | 81 10s! 13 92 |
matrix. Semen ae nes   i221 r2ol1o1!
8.2 % Some Basic Compression Methods 601can be scaled to achieve a variety of compression levels, Figure 8,30(b) shows a
typical normalization array. This array, which has been used extensively in the
JPEG standardization efforts (see the next section), weighs each coefficient of a
transformed subimage according to heuristically determined perceptual or psychovisual importance,Wt Figures 8.31(a) through (f) show six threshold-coded approximations of the
- monochrome jmage in Fig™8.9(a). All images were generated using an 8 x 8
DCT and the normalization array of Fig. 8.30(b}. The first resuit, which provides
a compression ratio of about 12 to 1 (i.e.,C = 12), was obtained by direct application of that normalization array. The remaining results, which compress the
original image by 19, 30, 49, 85, and 182 to 1, were generated after multiplying
(scaling) the normalization arrays by 2,4, 8, 16, and 32, respectively. The corresponding rms errors are 3.83. 4.93, 6,62. 9.35, 13.94, and 22.46 intensity levels. ®JPEGOne of the most popular and comprehensive continuous tone, still frame compression standards is the JPEG standard. It defines three different coding systems: (1) a lossy baseline coding system, which is based on the DCT and is
adequate for most compression applications: {2) an extended coding system forEXAMPLE 8.16:
IJustration of
threshold coding. abc
defFIGURE 8.31 Approximations of Fig. 8.9fa) using the OCT and normalization array of Fie. &.30tb): (al %(b) 22, (c) 42. (d) BZ, (ce) 16%. and (1) 32%.
602 Chapter 8 m Image CompressionEXAMPLE 8.17:
JPEG baseline
coding and
decoding,greater compression, higher precision, or progressive reconstruction applications;
and (3) a lossless independent coding system for reversible compression. To be
JPEG compatible, a product or system must include support for the baseline system. No particular file format, spatial resolution, or color space model is specified.In the baseline system, often called the sequential baseline system, the input
and output data precision is limited to 8 bits, whereas the quantized DCT values are restricted to 11 bits. The compression itself is performed in three sequential steps: DCT computation, quantization, and variable-length code
assignment. The image is first subdivided into pixel blocks of size 8 X 8, which
are processed left to right, top to bottom. As each 8 8 block or subimage is
encountered, its 64 pixels are level-shifted by subtracting the quantity 2*~',
where 2* is the maximum number of intensity levels. The 2-D discrete cosine
transform of the block is then computed, quantized in accordance with
Eq. (8.2-26), and reordered, using the zigzag pattern of Fig. 8.29(d), to form a
1-D sequence of quantized coefficients.Because the one-dimensionally reordered array generated under the zigzag
pattern of Fig. 8.29(d) is arranged qualitatively according to increasing spatial
frequency, the JPEG coding procedure is designed to take advantage of the
long runs of zeros that normally result from the reordering. In particular, the
nonzero AC" coefficients are coded using a variable-length code that defines
the coefficient values and number of preceding zeros, The DC coefficient is
difference coded relative to the DC coefficient of the previous subimage. Tables A.3, A.4, and A‘S in Appendix A provide the default JPEG Huffman
codes for the luminance component of a color image or intensity of a monochrome image. The JPEG recommended luminance quantization array is
given in Fig. 8.30(b) and can be scaled to provide a variety of compression
levels. The scaling of this array allows users to select the “quality” of JPEG
compressions. Although default coding tables and quantization arrays are
provided for both color and monochrome processing, the user is free to construct
custom tables and/or arrays, which may in fact be adapted to the characteristics of the image(s) being compressed.% Consider compression and reconstruction of the following 8 x 8 subimage
with the JPEG baseline standard:§2 55 61 66 70 61 64 7363 59 66 90 109 85 69 7262 59 68 113 144 104 66 7363 58 71 122 154 106 | 70 6967 61 68 104 126 88 638 70WD 65 60 70 77 63 58 7585 71 64 59 55 61 65 8387 79 69 68 65 76 7B O4 “La the standard, the erm AC denotes ali transform coefhicieats with the exception of the zeroth or DC
coefficient.
8.2 ® Some Basic Compression Methods 603The original image consists of 256 or 2° possible intensities, so the coding
process begins by level shifting the pixels of the original subimage by —27 or
~128 intensity levels. The resulting shifted array is-7% -73 -67 -62 -S8 -67 -64 ~SS-65 -69 -62 -38 -19 ~-43 -S9 —-S56-6 ~-69 -60 15 16 —24 -62 —55-65 -70 -5S7 -~6 26 -22 -S8 ~59-61 -67 -60 ~24 -2 -40 ~-60 -58-49 -63 ~-68 —S8 ~-51 -65 -70 —-S53~43 ~57 ~-64 -69 -73 -67 -63 ~-45-41 -49 -59 -60 -63 -S2 -50 —34which, when transformed in accordance with the forward DCT of Eqs. (8.2-10}
and (8.2-18) for n = 8, becomes—-415  -29 -62 25 55 +20 -1 37 -21 -€ “9 11 -7 -6 6-46 8& 77 = ~25 «30 10 7 -§-0 13 35 -15 -9 ‘6 0 31t -8 +13 -2 -1 1 ~4 1-10 1 3 -3 -1 0 2 -1-4 -i 2 -1 2 -3 1 -2-1 ~1 -1 -2 -1 -1 0 71
If the JPEG recommended normalization array of Fig. 8.30(b) is used to quantize the transformed array, the scaled and truncated {that is, normalized in ac
cordance with Eq. (8.2-26)] coefficients are—26 ~3 -6 2 2 0 0 0
1 -2 -4 0 0 0 0 9
-3 1 5 -1 -1 BO: ia] ie)
-4 1 2 -1 a 0 0 0
I 0 0 0 0 0 8 0
0 6 9 0 0 0 0 0
0 0 0 0 g 0 0 0
0 0 0 0 0 0 0 0
where, for instance, the DC coefficient is computed as
+ 70,0)
T(0,0) = rune] 320 |—415
= coun = -26Note that the transformation and normalization process produces a large
number of zero-valued coefficients. When the coefficients are reordered in
8.2 m Some Basic Compression Methods 605Here the regenerated array of quantized coefficients is26 ~3 -6 2 2 0 0 0
] ~2 -4 0 0 0 0 0
-3 1 5 -1 -1 0 0 0
-4 1 2 ~1 0 0 0 0
1 0 0 0 0 0 0 0Q 0 8 Q 0 0 0 00 0 0 0 0 0 0 00 0 0 0 0 0 it) 0After denormalization in accordance with Eq. (8.2-28), the array becomes-416 -33 -60 32 48 0 0 012 -24 -56 0 0 0 0 0
~42 13 80-24 -40 0 0 0
-S6 17 44 ~29 0 0 0 018 0 0 +0 0 0 0 0a 0 0 0 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 0 0 0 tt]where, for example, the DC coefficient is computed as
TOO, 0) = T(0, 0)Z(0, 0) = (-—26)(16) = -416The completely reconstructed subimage is obtained by taking the inverse DCT of
the denormalized array in accordance with Eqs. (8.2-11) and (8.2-18) to obtain-70 -64 -61 -64 ~-69 -66 -S58 ~50
-72 -73 61 -39 -30 -40 -54 ~59
-68 -78 —-S8 -9 13 -12 -48 ~64
-59 +77) --57 0 22-13 «-S?: =——-60
-54 -75 -64 -23 -13 -B4: -63 ~56
-52. -71 -72 ~-54 -34 ~71 -71 S54
-45  -5S9 -70 -68 -67 -67 -61 —S0
-35 -47 -61 -66 -60 ~48 -44 —44and level shifting each inverse transformed pixel by 27 (or +128) to yield58 64 67 64 59 62 70 78
56 55 67 89 98 88 74 69
60 50 70 #119 «141 116 80 64
69 51 710 «128149115 77 68
74 53 64 105 115 84 65 72
76 5? 56 74 75 57 57 74
83 69 59 60 61 61 67 8
93 81 67 62 69 80 84 84
604 Chapter 8 a Image Compressionaccordance with the zigzag ordering pattern of Fig. 8.29(d), the resulting 1-D
coefficient sequence is[-26 -3 1-3 -2 -62-41 -41150200-1200000-—1 —1 EOB]where the EOB symbol denotes the end-of-block condition. A special EOB
Huffman code word (see category 0 and run-length 0 in Table A.5) is provided to
indicate that the remainder of the coefficients in a reordered sequence are zeros,The construction of the default JPEG code for the reordered coefficient sequence begins with the computation of the difference between the current DC
coefficient and that of the previously encoded subimage. Assuming the DC coefficient of the transformed and quantized subimage to its immediate left was
17, the resulting DPCM difference is [~26 — (—17)] or ~9, which lies in DC
difference category 4 of Table A.3, In accordance with the default Huffman
difference code of Table A.4, the proper base code for a category 4 difference
is 101 (a 3-bit code), while the total length of a completely encoded category 4
coefficient is 7 bits. The remaining 4 bits must be generated from the least significant bits (LSBs) of the difference value. For a general DC difference category (say, category K), an additional K bits are needed and computed as either
the K LSBs of the positive difference or the K LSBs of the negative difference
minus 1. For a difference of —9, the appropriate LSBs are (0111) — 1 or 0110,
and the complete DPCM coded DC code word is 1010110.The nonzero AC coefficients of the reordered array are coded similarly
from Tables A.3 and A.5. The principal difference is that each default AC
Huffman code word depends on the number of zero-valued coefficients preceding the nonzero coefficient to be coded, as well as the magnitude category
of the nonzero coefficient. (See the column labeled Run/Category in Table
A.5.) Thus the first nonzero AC coefficient of the reordered array (3) is
coded as 0100. The first 2 bits of this code indicate that the coefficient was in
magnitude category 2 and preceded by no zero-valued coefficients (see Table
A.3); the last 2 bits are generated by the same process used to arrive at the
LSBs of the DC difference code. Continuing in this manner, the compietely
coded (reordered) array is1010110 0100 001 0100 0101 10000t 0110 100011 001 100011 001
001 100101 11106110 110110 0110 11110100 000 1010where the spaces have been inserted solely for readability. Although it was not
needed in this example, the default JPEG code contains a special code word for
a run of 15 zeros followed by a zero (see category 0 and run-length F in Table
A.5). The total number of bits in the completely coded reordered array (and
thus the number of bits required to represent the entire 8 x 8, 8-bit subimage
of this example) is 92. The resulting compression ratio is $12/92, or about 5.6:1.To decompress a JPEG compressed subimage, the decoder must first recreate the normalized transform coefficients that led to the compressed bit stream.
Because a Huffman-coded binary sequence is instantaneous and uniquely
decodable, this step is easily accomplished in a simple lookup table manner.
8.2 ® Some Basic Compression Methods607 abc
d e-fFIGURE 8.32 Two JPEG approximations of Fig. 8.9(a). Each row contains a result after compre ssion andreconstruction, the scaled difference between the result and the original image, and a zoomed portion of thereconstructed image.which is encoded using a variable-length code (by the symbo! encader) to generate the next element of the compressed data stream. The decoder in Fig. 8.33(b)
reconstructs e(2) from the received variable-length code words and performs the
inverse operationflr) = etn) + fd) (8.2-31)to decompress or recreate the original input sequence,Various local, global. and adaptive meihods (sce the later subsection entitled Lossy predictive coding) can be used to generate f(v7). In many cases, the
prediction is formed as a linear combination of m previous sampies, That ts.a ne al
fla) = rou} Saji (a - a| (8.2-32)redwhere m is the order of the linear predictor, round is a function used 10 denote
the rounding or nearest integer operation, and the a, for / = 1,2..... mare
prediction coefficients. If the input sequence in Fig. 8.33¢a) is considered To be
606 Chapter 8 m Image CompressionEXAMPLE 8.18:Iilustration of
JPEG coding,With reference to
Tables 8.3 and &.4,
Predictive coding is
used inJBIG2JPEGJPEG-LS
MPEG-12,4
H.261, 1.262,
H.263, and H.264
HDVVC-Land other compression
standards and file
formats,Any differences between the original and reconstructed subimage are a result
of the lossy nature of the JPEG compression and decompression process. In
this example, the errors range from —14 to +J{ and are distributed as follows:-6 -9 ~6 2 11 -1 -6 -57 4 -1 1 1 -3 -5 32 9 -2 -6 ~3) -12 = -14 9
-6 7 0 -4 —5 -9 -7 ]
-7 8 4 -i 4 3 -23 8 4 -4 I2 2 5 -1 -6 —2 5-6 —2 2 6 —4 -4 -6 10The root-mean-square error of the overall compression and reconstruction
process is approximately 5.8 intensity levels. a
moONW@ Figures 8.32(a) and (d) show two JPEG approximations of the monochrome image in Fig. 8.9(a). The first result provides a compression of 25:1;
the second compresses the original image by 52:1. The differences between
the original image and the recunstructed images in Figs. 8.30(a)} and (d) are
shown in Figs. 8.30(b) and (e), respectively. The corresponding rms errors are
5.4 and 10,7 intensities. The errors are clearly visible in the zoomed images in
Figs. 8.32(c) and (f). These images show a magnified section of Figs. 8.32(a)
and (d), respectively. Note that the JPEG blocking artifact increases with
compression. . a§.2.9 Predictive CodingWe now turn to a simpler compression approach that achieves good compression without significant computational overhead and can be either error-free
or lossy. The approach, commonly referred to as predictive coding, is based on
eliminating the reduadancies of closely spaced pixels—in space and/or time —
by extracting and coding only the new information in each pixel. The new information of a pixel is defined as the difference between the actual and
predicted value of the pixel.Lossless predictive codingFigure 8.33 shows the basic components of a lossless predictive coding system.
The system consists of an encoder and a decoder, tach containing an identical
predictor. As successive samples of discrete time input signal, f(n). are introduced to the encoder, the predictor generates the anticipated value of each
sample based on a specified number of past sampies, The output of the predictor is then rounded to the nearest integer, denoted f(/2), and used to form the
difference or prediction errore(n) = f(n) — F(n) (8.2-30)
608 Chapter 8 @ Image Compressiona
bFIGURE 8.33A lossless
predictive coding
model:(a) encoder;(b) decoder,EXAMPLE 8.19:
Predictive coding
and spatial
redundancy.Input fl (yo etn) Symbol Compressed
sequence | encoder sequencePredictor Nearest ahi
integer fin)Compressed Symbol | &")
sequence decoder    F(a) Decompressed
sequencein| ree |samples of an image, the f (7) in Eqs. (8.2-30) through (8. 2-32) are pixels—and
the st samples used to predict'the value of each pixel come from the current
scan line (called 1-D linear predictive coding), from the current and previous
scan lines (called 2-D linear predictive coding), or from the current image and
previous images in a sequence of images (called 3-D linear predictive coding).
Thus, for 1-D linear predictive image coding, Eq. (8.2-32) can be written asf(xy) = round] Safle. yr a| (8.2-33)
iztwhere each sample is now expressed explicitly as a function of the input
image’s spatial coordinates, x and y. Note that Eq. (8.2-33) indicates that the
1-D linear prediction is a function of the previous pixels on the current line
alone. In 2-D predictive coding, the prediction is a function of the previous pixels in a left-to-right. top-to-bottom scan of an image. In the 3-D case, it is based
on these pixels and the previous pixels of preceding frames. Equation (8.2-33)
cannot be evaluated for the first » pixels of each line, so those pixels must be
coded by using other means (such as a Huffman code) and considered as an
overhead of the predictive coding process. Similar comments apply to the
higher-dimensional cases.Consider encoding the monochrome image of Fig. 8.34(a) using the simple
first-order (ie., 72 = 1) linear predictor from Eq. (8.2-33}fix. y) = round[af(x. y - 1)] (8.2-34)This equation is a simplification of Eq. (8.2-33) with m: = | and the subscript of
lone prediction coefficient a, dropped as unnecessary. A predictor of this general form is called a previous pixel predictor, and the corresponding predictive
coding procedure is known as differential coding or previous pixel coding.
Figure 8.34(c) shows the prediction error image, e(x, y) = f(x,y) ~ f(x, 3)
that results from Eq. (8.2-34) with a = 1. The scaling of this image is such that
intensity 128 represents a prediction error of zero. while ail nonzero positive
8.2 ~~ Some Basic Compression Methods    
 
     
  
    D5 pany aera
= TOT T TT |
~~ 2O2
£45 Shi dev. = 38.00
z Frarapy © 2.38
= 1 i
B05
E
a
Z 000 50 100 150 200° 250 3u0Intensity= 20 -poe
& Veh i T i |
= Lor |
lar Sis RT|
lap Emopy = 30 [7]
= 10 4
& O85 oak ~
B04A- 4
E 02h 4
Zz Os LJAY]
- 300-200-100 G00
Prediction error200° 300 and negative prediction errors (under and over estimates) are displayed as
lighter and darker shades of gray, respectively. The mean value of the prediction
image is 128.26. Because intensity 128 corresponds to a prediction error of 0,
the average prediction error is only 0.26 bits.Figures 8.34(b) and (d) show the intensity histogram of the image in Fig. 8.34(a)
and the histogram of prediction error e(x, y), respectively. Note that the standard
deviation of the prediction error in Fig. 8.34(d) i mich smailer than the standard
deviation of the intensities in the original image. Moreover. the entropy of the prediction error—as estimated using Eq. (8.1-7)~is significantly iess than the estimated entropy of the original image (3.99 bits/pixcl as opposed to 7.25
bits/pixel). This decrease in entropy reflects removal of a great deai of spatial redundancy, despite the fact that for k-bil images, {k + 1)-bit numbers are needed
to represent accurately prediction error sequence e{x, y). In general, the maximum compression of a predictive coding approach can be estimated by dividing
the average number of bits used to represent each pixel in the original image by an
estimate of the entropy of the prediction error, In this example, any variable-length
coding procedure can be used to code ¢(v. y), but the resulting compression will be
limited to about 8/3.99 or 2:1. B The preceding example illustrates that the compression achieved in predictive coding is related directly to the entropy reduction that results from mapping609ab
edFIGURE 8.34{a) A view of the
Earth (rom an
orbiting space
shuule. (6) The
intensity
histogram of
(a). {c} The
prediction error
image resulting
from Eq. (8.2-34)
(a) A histogram
of the prediction
error,(Original image
courtesy of
NASA.)ule that the vayiable
length encuded predic.
Vian error as the
compressed Mae
610 Chapter 8 @ Image CompressionEXAMPLE 8.20:
Predictive coding
and temporal
redundancy.ab
cdFIGURE 8.35(a) and (b) Two
views of Earth
from an orbiting
space shuttle
video. {c) The
prediction error
image resulting
from Eq. (8.2-36).
(d) A histogram
of the prediction
error.(Original images
courtesy of
NASA.}an input image into a prediction error sequence ~ often called a prediction residual. Because spatial redundancy is removed by the prediction and differencing
process, the probability density function of the prediction residual is, in general,
highly peaked at zero and characterized by a relatively small (in comparison to
the input intensity distribution) variance. In fact, it is often modeled by a zero
mean uncorrelated Laplacian PDF
rile!
e)=—ye-e & 8.2-35
pe) V0, ( )where o, is the standard deviation af e.W The image in Fig, 8.34(a) is a portion of a frame of NASA video in which
the Earth is moving from lett to right with respect tq a stationary camera altached to the space shuttle. It is repeated in Fig. 8.35(b) — along with its immediately preceding frame in Figs 8.35{a), Using the first-order linear predictorFox yO = round] af (x, yer 1)| (8.2-36)   
  
  Sid. dev. =
Entropy 4 er of pixels (< 10,000)
u Snead nnae
|
:pee ee Load 2
WW) SO BO dP 2 a SUPrediction error
8.2 ® Some Basic Compression Methods 611with a = 1, the intensities of the pixels in Fig. 8.35(b) can be predicted from
the corresponding pixels in (a). Figure 8.34(c) is the resulting prediction residual image, e(x, y,7) = f(x, y.1) — f(x, y, 1). Figure 8.34(d) is the histogram of
e(x, y, 1). Note that there is very little prediction error. The standard deviation
of the error is much smaller than in the previous example —3.76 bits/pixel as
opposed to 15.58 bits/pixel. In addition, the entropy of the prediction error
[computed using Eq. (8.1-7)) has decreased from 3.99 to 2.59 bits/pixel. By
variable-length coding the,resulting prediction residual, the original image is
compressed by approximately 8/2.59 or 3.1:1—a 50% improvement over the
2:1 compression obtained using the spatially-oriented previous pixel predictor
in Example 8.19, aMotion compensated prediction residualsAs you saw in Example 8.20, successive frames in a video sequence often are
very similar. Coding their differences can reduce temporal redundancy and
provide significant compression. However, when a sequence of frames contains rapidly moving objects— or involves camera zoom and pan, sudden scene
changes, or fade-ins and fade-outs—the similarity between neighboring
frames is reduced and compression is affected negatively. That is, like most
compression techniques (see Example 8.5), temporally-based predictive coding works best with certain kinds of inputs— namely,a sequence of images with
significant temporal redundancy. When used on images with little temporal redundancy, data expansion can occur. Video compression systems avoid the
problem of data expansion in two ways:1, By tracking object movement and compensating for it during the prediction and differencing process,2. By switching to an alternate coding method when there is insufficient
interframe correlation (similarity between frames) to make predictive
coding advantageous.The first of these —called motion compensation —is the subject of the remainder of this section. Before proceeding, howeve?, we note that when there is insufficient interframe correlation to make predictive coding effective, the
second problem is typically addressed using a block-oriented 2-D transform,
like JPEG’s DCT-based coding {see Section 8.2.8). Frames compressed in this
way (ie., without a prediction residual) are called intraframes or Independent
frames (I-frames). They can be decoded without access to other frames in the
video to which they belong. I-frames usually resemble JPEG encoded images
and are ideal starting points for the generation of prediction residuals. Moreover, they provide a high degree of random access, ease of editing, and resistance to the propagation of transmission error. As a result, all standards require
the periodic insertion of J-frames into the compressed video codestream.
Figure 8.36 illustrates the basics of motion compensated predictive coding.
Each video frame is divided into non-overlapping rectangular regions — typically
of size4 x 4to16 X 16—called macroblocks, (Only one macroblock is shown in
Fig. 8.36.) The “movement” of each macroblock with respect to its “most likely”
position in the previous {or subsequent) video frame, called the reference frame,Recall again that the
variable-length encaded
Prediction error is the
compressed image.
612 Chapter 8 m@ Image CompressionFIGURE 8.36
Macroblock
motion
specification.The “most likely” position is the one that minimizes an error measure
‘between the reference
macroblock and macrobiock being encoded. The
two blocks do not have
to be representations of
the same object, but they
must minimize the error
measure.Motion vectorMotion wetor is encoded in a motion vector. The vector describes the motion by defining the
horizontal and vertical displacement from the “most likely” position. The displacements typically are specified to the nearest pixel, i pixel, or Fi pixel precision.
If sub-pixel precision is used, thé predictions must be interpolated [e.g., using bilinear interpolation (see Section 2.4.4)] from a combination of pixels in the
reference frame. An encoded frame that is based on the previous frame (a forward prediction in Fig, 8.36) is cailed a Predictive frame (P-frame); one that is
based on the subsequent frame (a backward prediction in Fig. 8.36) is called a
Bidirectional frame (B-frame). B-frames require the compressed codestream
to be reordered so that frames are presented to the decoder in the proper decoding sequence —rather than the natural display order.As you might expect, motion estimation is the key component of motion
compensation. During motion estimation, the motion of objects is measured
and encoded into motion vectors. The search for the “best” motion vector requires that a criterion of optimality be defined. For example, motion vectors
may be selected on the basis of maximum correlation or minimum error be~
tween macroblock pixels and the predicted pixels (or interpolated pixels for
sub-pixel motion vectors) from the chosen reference frame. One of the most
commonly used error measures is mean absolute distortion (MAD)moo1 ; , a . .
MAD(x, y) = pin DDS téy +p) pa tit dxy+jt dy)
ist jot
(8.2-37)where x and y are the coordinates of the upper-left pixel of the m x n macroblock being coded, dx and dy are displacements from the reference frame as
shown in Fig. 8.36, and p is an array of predicted macroblock pixel values. For
sub-pixel motion vector estimation, p is interpolated from pixels in the reference frame. Typicaily, dx and dy musi fall within a limited search region (see
Fig. 8.36) around each macroblock, Values from +8 to +64 pixels are common,
and the horizontal search area often is slightly larger than the vertical area. A
more computationally efficient error measure, called the sum of absolute distortions (SAD), omits the 1/mn factor in Eg. (8.2-37).Given a selection criterion Jike that of Eq. (8.2-37). motion estimation ts
performed by searching for the dy and dy that minimize Ae) D(x. y) over the
8.2 & Some Basic Compression Methods 613allowed range of motion vector displacements — including sub-pixel displacements. This process often is called block matching. An exhaustive search guarantees the best possible result, but is computationally expensive, because
every possible motion must be tested over the entire displacement range. For
16 X 16 macroblocks and a +32 pixel displacement range (not out of the
question for action films and sporting events), 4225 16 16 MAD calculations
must be performed for each macroblock in a frame when integer displacement
precision is used. If 5 or 3 pixel precision is desired, the number of calculations
is multiplied by a factor of 4 or 16, respectively. Fast search algorithms can reduce the computational burden but may or may not yield optimal motion vectors. A number of fast block-based motion estimation algorithms have been
proposed and studied in the literature (see, for example, Furht et al. [1997] or
Mitchell et al. [1997]).Mf Figures 8.37(a) and (b) were taken from the same NASA video sequence
used in Examples 8.19 and 8.20. Figure 8.37(b) is identical to Figs. 8.34(a) and
8.35(b); Fig. 8.37(a) is the corresponding section of a frame occurring thirteen
frames earlier. Figure 8.37(c) is the difference between the two frames, scaled
to the full intensity range. Note that the difference is 0 in the area of the stationary (with respect to the camera) space shuttle, but there are significant differences in the remainder of the image due to the relative motion of the Earth.
The standard deviation of the prediction residual in Fig. 8.37(c) is 12.73 intensity levels; its entropy [using Eq. (8.1-7)] is 4.17 bits/pixel. The maximum compression achievable when variable-length coding the prediction residual is
C = 8/4.17 = 1.92.Figure 8.37(d) shows a motion compensated prediction residual with a
much lower standard deviation (5.62 as opposed to 12,73 intensity levels) and
slightly lower entropy (3.04 vs. 4.17 bits/pixel). The entropy was computed
using Eq. (8.1-7). If the prediction residual in Fig. 8.37(d) is variable-length
coded, the resulting compression ratio is C = 8/3.04 = 2.63. To generate this
prediction residual, we divided Fig, 8.37(b) into non-overlapping 16 x 16
macroblocks and compared each macroblock ‘Against every 16 X 16 region in
Fig. 8.37(a)—the reference frame—that fell within £16 pixels of the mac*
roblock’s position in (b). We used Eq. (8.2-37) to determine the best match
by selecting displacement (dx, dy) with the lowest MAD. The resulting displacements are the x and y components of the motion vectors shown in
Fig. 8.37(e). The white dots in the figure are the heads of the motion vectors; they indicate the upper-left-hand corner of the coded macroblocks. As
you can see from the pattern of the vectors, the predominant motion in the
image is from left to right. In the lower portion of the image, which corresponds to the area of the space shuttle in the original image, there is no motion and therefore no motion vectors displayed. Macroblocks in this area are
predicted from similarly located (i.e., the corresponding) macroblocks in the
reference frame. Because the motion vectors in Fig. 8.37(e) are highly correlated, they can be variable-length coded to reduce their storage and transmission requirements, .EXAMPLE 821;
Motion
compensated
prediction.
614 Chapter 8 ® Image Compression ab
cdeFIGURE 8.37 (a) and (b) Two views of Earth that are thirteen frames apart in an orbiling space shuttle video.
{c) A prediction error image without motion compensation. (d) The prediction residual with motion
compensation. (¢) The motion vectors associated with (d). The white dots in (d} represent the arrow heads of
the motion vectors that are depicted. (Original images courtesy of NASA.)The visual differonce between Figs, 8.3%c) and
8.38(a) is due ta scaling
The image 1 Fig. $344)
has been scaled to match
Figs. 8.38(b)-(d).Figure 8.38 illustrates the increased prediction accuracy that is possible with
sub-pixe! motion compensation. Figure 8.38(a) is repeated from Fig. $.37{c)
and included as a point of reference: it shows the prediction error that results
without motion compensation. The images tn Pigs. 8.38(b), (c), and (d) are motian compensated prediction residuals. They are based on the same two frames
that were used i in Example 8.21 and computed with macroblock displacements
fo 1,4. and! 3 pixel resolution (i.¢., precision), respectively. Macroblacks of size
8 * 8 were used; displacements were limited to +8 pixels.The most significant visual difference between the prediction residuals in
Fig. 8.38 is the number and size of intensity peaks and vi alloys the:r darkest
and lightest areas of intensity. The} , pixel residual in Fi s the “flattest”
of the four images, with the fewest eXCURsiK is to black or white, As would be expected. it has the narrowest histogram. The standard deviations of the prediction residuals in Figs. 8.38(a) through (d) decrease as motion vector precision
increases ~from 12.7 to 44. 4 and 3.8 pivets respectivels ropes of the
8.2 & Some Basic Compression Methods residuals, as determined using Eq. (8.1-7), are 4.17, 3.34. 3.35, and 3.34 bits/pixel,
respectively. Thus, the motion compensated residuals contain about the same
amount of information, despite the fact that the residuals in Figs. 8.38(c) and (d)
use additional bits to accommodate 5 and J 5 1 pixel interpolation. Finally, we note
that there is an obvious strip of increased prediction error on the left side of
each motion compensated residual. This is due to the left-to-right motion of the
Earth, which introduces new or previously unseen areas of the Earth’s terrain
into the left side of each image. Because these areas are absent from the previous frames, they cannot be accurately predicted. regardless of the precision used
to compute motion vectors,Motion estimation ts a computationally demanding Sask. Fortunately. only the encoder must estimate macroblock motion. Given the motion vectors ofthe macroblocks, the decoder simply accesses the areas of the referenceframes that were used in the encoder to form the prediction residua
cause of this, motion estimation is not included in most video comp
standards. Compression standards focas on the decoder placing constraints
on macroblock dimensions. Motion vector precision. horizontal and vertical     615ab
cdFIGURE 8.38
Sub-pixel motion
compensated
prediction
residuals:fa) without
motion
compensation:
(b) single pixel
precision,(ec): 3 Pixel
precision; and
(d) j pixel
precision (All
prediction errors
have been scaled
to the full
intensity range
and then
mulliplied by 2 to
increase their
visibility.)
616 Chapter 8 # Image CompressionTABLE 8.11displacement ranges, and the like. Table 8.11 gives the key predictive coding
parameters of some the most important video compression standards. Note
that most of the standards use an 8 X 8 DCT for I-frame encoding, but
specify a larger area {i.e., 16 X 16 macroblock) for motion compensation. In
addition, even the P- and B-frame prediction residuals are transform coded
due to the effectiveness of DCT coefficient quantization. Finally, we note
that the H.264 and MPEG-4 AVC standards support intraframe predictive
coding (in I-frames) to reduce spatial redundancy.Figure 8,39 shows a typical motion compensated video encoder. It exploits redundancies within and between adjacent video frames, motion uniformity between frames, and the psychovisual properties of the human visual
system. We can think of the input to the encoder as sequential macroblocks
of video. For color video, each macroblock is composed of a luminance block
and two chrominance blocks. Because the eye has far less spatial acuity for
color than for luminance,.the chrominance blocks often are sampled at half
the horizontal and vertical resglution of the luminance block. The grayed elements in the figure parallel the transformation, quantization, and variablelength coding operations of a JPEG encoder. The principal difference is the
input, which may be a conventional macroblock of image data (for I-frames) Predictive coding in video compression standards.
. H.264
H.262 VOL MPEG4  ParameterH.261 MPEG-L MPEG-2 H.263 MPEG-4 WMV-9 AVC       Motion
vector
precisionsizesTransformInterframe
predictions1-frame
intrapredictionsMacroblock16x16 16x 16 16 x 16 16 x 16 16 x 16 16 Xx 16 16 x 16
16x 8 8x8 8x8 &8x8 16 x8
8 x 16
8x8
8x4
4x8
4x4
8x8 gx 8 8x8 8x8 8x 8 &x8s 4& 4
Der DCT DCT pcr pcr 8x4 8x8
4x8 Integer
4x4
. Integer
DCT
P PB PB PB PB PB PB
No No No No No No Yes1 "a ‘A ‘b va 4 Ys
618 Chapter 8 # Image CompressionFrame 002i Frame 0959eer Frame 1224       Frame 1595 Frame 1609FIGURE 8.40 Fifteen frames from an 1829-frame, |-minute NASA video, The original video ts in HD full color.
(Courtesy of NASA.}Lossy predictive codingIn this section. we add a quantizer to the lossless predictive coding made] introduecd earlier and examine the trade-off between reconstruction accuracy and
comp ession performance within Ue contest of sprtial predictors. As Fiz. 84]    shows, fie cpus liver replaces the 4 OF tre errormfree
8.2 * Some Basic Compression Methods   Rate
controller   
        
   
   
  
   
    
  
 Difference
macroblock
Image Mapper = Variable-length Encoded
macroblock (e.g:, DCT) ‘Quarnians coding Rofler macroblock
Inverse
quantizer
Prediction macroblock
Encoded
motionVasiable-lengih
codingmacroblock vector   Motion estimator and
compensator w/frame delay    or the difference between a conventional macroblock and a prediction of it
based on previous and/or subsequent video frames (for P- and B-frames).
The encoder includes an inverse quantizer and inverse mapper (e.g., inverse
DCT) so that its predictions match those of the complementary decoder.
Also, it is designed to produce compressed bit streams that match the capacity of the intended video channel. To accomplish this, the quantization parameters are adjusted by a rate controller as a function of the occupancy of an
output buffer. As the buffer becomes fuller, the quantization is made coarser,
so that fewer bits stream into the buffer.We conclude our discussion of motion compensated predictive coding with
an example illustrating the kind of compression that is possible with modern
video compression methods. Figure 8.40 shows fifteen frames of a 1 minute HD
(1280 x 720) full-color NASA video, parts oWwhiich have been used throughout this section. Although the images shown are monochrome, the video is a sequence of 1,829 full-color frames. Note that there are a variety of scenes, a great
deal of motion, and multiple fade effects. For example, the video opens with a
150 frame fade-in from black, which includes frames 21 and 44 in Fig. 8.40, and
concludes with a fade sequence containing frames 1595, 1609, and 1652 in
Fig. 8.40, followed by a final fade to black. There are also several abrupt scene
changes, like the change involving frames 1303 and 1304 in Fig. 8.40.An H.264 compressed version of the NASA video stored as a Quicktime
file (see Tabie 8.4) requires 44.56 MB of storage —plus another 1.39 MB for
the associated audio, The video quality is excellent. About 5 GB of data
would be needed to store the video frames as uncompressed full-color images. It should be noted that the video contains sequences involving both rotation and scale change (e.g., the sequence including frames 959, 1023, and
1088 in Fig. 840). The discussion in this section, however. has been limited to
translation alone. is617FIGURE 8.39A typical motion
compensated
video encoder.Quantization as defined
earlier in the chapter is
irreversible. The “inverse
quantizer” in Fig. 8.39
does not prevent information loss,EXAMPLE 8.22;
Video
compression
example.ConseSee the hook Web site for
the NASA video segment
used in this section.
8.2 % Some Basic Compression Methods 619eet i ¢) ee Symbol Compressed
4 fin) | sequence
9)
f(r)Compressed Symbol e(n) f(r) Decompressed
sequence decoder + + | * sequenceencoder, is inserted between the symbol encoder and the point at which the prediction error is formed. It maps the prediction error into a limited range of outputs, denoted ¢(n), which establish the amount of compression and distortion
that occurs. :In order to accommodate the insertion of the quantization step, the errorfree encoder of Fig. 8.33(a) must be altered so that the predictions generated
by the encoder and decoder are equivalent. As Fig. 8.41(a) shows, this is accomplished by placing the lossy encoder’s predictor within a feedback loop,
where its input, denoted f(7), is generated as a function of past predictions
and the corresponding quantized errors. That is,F(n) = e(n) + fn) (8.2-38)where f(n) is as defined earlier. This closed loop configuration prevents error
buildup at the decoder’s output. Note in Fig. 8.41(b) that the output of the decoder is given also by Eq, (8.2-38). .fin)    x.
of otWt Delta modulation (DM) is a simple but well-known form of lossy predictive
coding in which the predictor and quantizer are defined asf() = af(n - 1) (82-39)
and
P = +f fore(n) > 0
ein) = \* otherwise (8.2-40)where a is a prediction coefficient (normally less than 1) and ¢ is a positive
constant. The output of the quantizer, é(”), can be represented by a single bit
[Fig. 8.42(a)], so the symbol encoder of Fig. 8.41(a) can utilize a 1-bit fixedlength code. The resulting DM code rate is 1 bit/pixel.Figure 8.42(c) illustrates the mechanics of the delta modulation process,
where the calculations needed to compress and reconstruct input sequencea
bFIGURE 8.41
A lossless
predictive
coding model:(a) encoder;
{b) decoder.EXAMPLE 8.23:
Delta modulation.
8.2 @ Some Basic Compression MethodsNo quantization error, and the quantizer is designed to minimize its own error.
That is, the predictor and quantizer are designed independently of each other.Optimal predictorsIn many predictive coding applications, the predictor is chosen to minimize the
encoder’s mean-square prediction error’E{é} = E{[FO) - foo}} (8.2-41)
subject to the constraint that
F(n) = e(n) + fn) ® e(n) + fF) = FD (8.2-42)
and
Fin) = Lastn - i) (82-43)That is, the optimization criterion is minimal mean-square prediction error, the
quantization error is assumed to be negligible [e() ~ e(n)] and the prediction is
constrained to a linear combination of m previous samples. These restrictions are
not essential, but they simplify the analysis considerably and, at the same time, decrease the computational complexity of the predictor. The resulting predictive
coding approach is referred to as differential pulse code modulation (DPCM).Under these conditions, the optimal predictor design problem is reduced to
the relatively straightforward exercise of selecting the m prediction coefficients that minimize the expressionm 2
E{e(n)} = e{[ so — Safin - 0] \ (8.2-44)
i=lDifferentiating Eq. (8.2-44) with respect to each coefficient, equating the detivatives to zero, and solving the resulting set of simultaneous equations under
the assumption that f(7) has mean zero and variance o” yields@-R'r (8.2-45)
where R™ is the inverse of the m X m autocorrelation matrix
E{f(a— 1} f(n— 19} E{f(n — 1)f( — 2)} - Ef{f(a — f(a — m)}
E{f(n — 2) fin ~ 1)} a
R= : :Elf(a — m) fr ~ WD} EL fla — mfr — Dh ELF(n — myfn ~ my}
(8.2-46) The notation F{-} denotes the statistical expectation operator.#In general, the optimal predictor for a non-Gaussian sequence is a nonlinear function of the samples
used ta form the estimate.621
620 Chapter 8 m Image Compression‘ab(3
FIGURE 8.42
An example ofdelta modulation,e(n)   
  Granular noise
7"a)
Slope overload        Input Encoder Decoder Error
Lon fim) fim) etm) eens) fs) few) fen) Ft
0 14 _— _ — 14.0 ~~ 14.0 7 0.0
1 15 140 10. 65205 140 * 205 -3.5
2 14 20.5 -6.5 «6.5 140 | 205 14.0 0.0
3 15 14.0 10° * 65 20.5 14.0 20.5 -5.5
14 29 20.5 8&5 6.5 27.0 20.5 27.0 2.0
15 37 27.0 10.0 65 33.5 27.0 33.5 3.5
16 47 33.8 13.5 6.5 40.0 33.5 40.0 | 7.0
17 62 400 22.0 65 46.5 40.0 46.5 18.5
18 75 465 28.5 65 53.0 465 53.0 22.0
19 7 53.0 24.0 65 59.6 | 53.0 596 | 17.5
a ee re ee{14, 15, 14, 15, 13, 15, 15, 14, 20, 26, 27, 28, 27, 27, 29, 37, 47, 62, 75, 77, 78,
79, 80, 81, 81, 82, 82} with a = Land g = 6.5 are tabulated. The process begins with the error-free transfer of the first input sample to the decoder. With
the initial condition f(0) = f(0} = 14 established at both the encoder and
decoder, the remaining outputs can be computed by repeatedly evaluating
Eqs. (8.2-39), (8.2-30), (8.2-40), and (8.2-38). Thus, when » = 1, for example,
FQ) = (D4) = 14, e(1) = 15 = 14 = 1,21) = +6.5 (because e(1) > 0),
f(1) = 6.4 + 14 = 20.5, and the resulting reconstruction error is (15 — 20.5),
or ~5.5.Figure 8.42(b) shows graphically the tabulated data in Fig. 8.42(c). Both the
input and completely decoded output [ f() and f(n)}] are shown. Note that in
the rapidly changing area from n = 14 to 19, where ¢ was too small to represent the input’s largest changes, a distortion known as sfope overload occurs.
Moreover, when ¢ was too large to represent the input’s smallest changes, as in
the relatively smooth region from n = 0 to # = 7, granular noise appears. In
images, these two phenomena lead to blurred object edges and grainy or noisy
surfaces (that is, distorted smooth areas). fiThe distortions noted in the preceding example are common to all forms of
lossy predictive coding. The severity of these distortions depends on a complex set
of interactions between the quantization and prediction methods employed. Despite these interactions, the predictor normally is designed with the assumption of
622Chapter 8 @ Image Compressionand r and @ are the m-element vectorsE{finjf(n — 1)} aEUR 2 | ond @ =| (8.2-47)E(f(n)f(n — m)} OnThus for any input sequence, the coefficients that minimize Eq. (8.2-44) can be
determined via a series of elementary matrix operations. Moreover, the coefficients depend only on the autocorrelations of the samples in the original sequence. The variance of the prediction error that results from the use of these
optimal coefficients isop = 0 ~ alr = 0? - SEvreoter - dha; (8.2-48)
Although the mechanics of evaluating Eq. (8.2-45) are quite simple, computation of the autocorrelations needed to form R and tr is so difficult in practice
that focal predictions (those in which the prediction coefficients are computed
for each input sequence) are almost never used. In most cases, a set of global
coefficients is computed by assuming a simple input model and substituting
the corresponding autocorrelations into Eqs. (8.2-46) and (8.2-47). For instance,
when a 2-D Markov itnage source (see Section 8.1.4) with separable autocorrelation functionE(f(x fla ~ iy — )} = oleh (8.2-49)
and generalized fourth-order lmear predictorFly) = afOny 1 + af(x ~ Ly - 1)
+ asf(x — ly) + asf(x - ly + 1) (8.2-50)
are assumed, the resulting optimal coefficients (Jain [1989]) are
GQ) = Py 22 =~ ~PyPh A= Py Ae = 0 (8.2-51)
where p;, and p, are the horizontal and vertical correlation coefficients, respectively. of the image under consideration.Finally, the sum of the prediction coefficients in Eq. (8.2-43) normally is required to be less than or equal to one. That is,a; <1 . (8.2-52)ia]This restriction is made to ensure that the output of the predictor falls within the
allowed range of the input and to reduce the impact of transmission noise
[which generally is seen as horizontal streaks in reconstructed images when the
input to Fig. 8.41(a) is an image]. Reducing the DPCM decoder’s susceptibility
to input noise is important, because a single error (under the right circumstances) can propagate to all future outputs. That is, the decoder’s output may
8.2 & Some Basic Compression Methods 623become unstable. By further restricting Eq. (8.2-52) to be strictly less than 1 confines the impact of an input error to a small number of outputs.® Consider the prediction error that results from DPCM coding the monochrome image of Fig. 8.9(a) under the assumption of zero quantization errorand with each of four predictors:
Fay) = O97F (x,y — 1), (8.2-53)
Fs y) = O5f(x, 9 — 1) + O5f - Ly) (8.2-54)f(xy) = O.75f(x, y — 1) + O.75f(x -— 1,9) -— OSfe = Ly — 1) (82-55)* — JO97f(x,y - 1) if Ah = Av
fle y) = ene - 1,¥) otherwise (82-56)where AA = [f(x —1,y)- f(x-1.y—- 1] and Av = f(x,» - 1) f(x — 1, y - 1)| denote the horizontal and vertical gradients at point (x, y).
Equations (8.2-53) through (8.2-56) define a relatively robust set of a; that
provide satisfactory performance over a wide range of images. The adaptive
predictor of Eq. (8.2-56) is designed 10 improve edge rendition by computing a
local measure of the directional properties of an image (Af and Av) and selecting a predictor specifically tailored to the measured behavior.Figures 8.43(a) through (d) show the prediction error images that result
from using the predictors of Eqs. (8.2-53) through (8.2-56). Note that the EXAMPLE 8.24:
Comparison of
prediction
techniques.ab
cdFIGURE 8.43A comparison of
four linear
prediction
techniques.
624 Chapter 8 @ Image CompressionFIGURE 8.44
A typical
quantization
function,visually perceptible error decreases as the order of the predictor increases,
The standard deviations of the prediction errors follow a similar pattern. They
are 11.1,9.8,9.1, and 9.7 intensity levels, respectively. aOptimal quantizationThe staircase quantization function ¢ = q(s) in Fig. 8.44 ts an odd function of s
[that is, g(—s) = —q(s)] that can be described completely by the L/2 values of
5; and ¢; shown in the first quadrant of the graph. These break points define
function discontinuities and are called the decision and reconstruction levels of
the quantizer. As a matter of convention, s is considered to be mapped to 4; if
it lies in the half-open interval (s;, s;41].The quantizer design problem is to select the best s; and 7; for a particular optimization criterion and input probability density function p(s). If the optimization criterion, which could be either a statistical or psychovisual measure,’ is the
minimization of the mean-square quantization error (that is, E{(s; — 1°} } and
p(s) is an even function, the conditions for minimal error (Max (1960]) are (s —4)p(s)ds f= 1.2... s (8.2-57)0 i=0‘ fj + feat _ L= ce i (8.2-58)Loo (= >2Output
lun be----
    
  Sp Sen yay-t  ‘Predictors that use more than three or four previous pixels provide Jittle compression gain for the
added predictor complexity (Habibi [1971]}.‘See Netravali {1977} and Limb and Rubinstein [1978] for more on psychovisual measures.sine. tune ses
8.2 & Some Basic Compression Methods 625and
Sj = 8 ty = (8.2-59)Equation (8.2-57) indicates that the reconstruction levels are the centroids of
the areas under p(s) over the specified decision intervals, whereas Eq. (8.2-58)
indicates that the decision levels are halfway between the reconstruction levels, Equation (8.2-59) is a consequence of the fact that g is an odd function. For. any L, the s; and ¢, that sattsfy Eqs. (8.2-57) through (8.2-59) are optimal in the
mean-square error sense; the corresponding quantizer is called an L-level
Lloyd-Max quantizer.Table 8.12 lists the 2-, 4-, and 8-level Lloyd-Max decision and reconstruction levels for a unit variance Laplacian probability density function [see
Eq. (8.2-35)]. Because obtaining an explicit or closed-form solution to Eqs.
(8.2-57) through (8.2-59) for most nontrivial p(s) is difficult, these values
were generated numerically (Paez and Glisson [1972]). The three quantizers
shown provide fixed output rates of 1, 2, and 3 bits/pixel, respectively. As
Table 8.12 was constructed for a unit variance distribution, the reconstruction and decision levels for the case of o # 1 are obtained by multiplying
the tabulated values by the standard deviation of the probability density
function under consideration. The final row of the table lists the step size,
6, that simultaneously satisfies Eqs. (8.2-57) through (8.5-59) and the additional constraint thatt, — G-) = 3; 7 5.) = 6 (8.2-60)If a symbol encoder that utilizes a variable-length code is used in the general
lossy predictive encoder of Fig. 8.41(a), an optimum uniform quantizer of
step size @ will provide a lower code rate (for a Laplacian PDF) than a
fixed-length coded Lloyd-Max quantizer with the same output fidelity
(O’Neil [1971]).Although the Lloyd-Max and optimum uniform quantizers are not adaptive, much can be gained from adjusting the quantization levels based on the
local behavior of an image. In theory, slowly changing regions can be finely
quantized, while the rapidly changing areas are quantized more coarsely. This
approach simultaneously reduces both granular noise and slope overload,
while requiring only a minimal increase in code rate. The trade-off is increased quantizer complexity.    Levels 2 4 8
U 5 4 Sy 4 St 4
1 oO 0.707 1,102 0.395 0.504 0.222
2 oo 1.810 1.181 0.785
3 2.285 1.576
4 00 2.994
6 1.414 1.087 O.731  TABLE 8.12
Lieyd-Max
quantizers for a
Laplacian
probability
density function
of unit variance.
8.2 * Some Basic Compression Methodsimages of acceptable error. When the transforming wavelet has a companion scaling function, the transformation can be implemented as a sequence
of digital filtering operations, with the number of filter taps equal to the
number of nonzero wavelet and scaling vector coefficients. The ability of
the wavelet to pack information into a small number of transform coefficients determines its compression and reconstruction performance.
The most widely used expansion functions for wavelet-based compression
. are the Daubechies wavelets and biorthogonal wavelets. The latter allow useful analysis properties, like the number of zera moments (see Section 7.5), to
be incorporated into the decomposition filters, while important synthesis
properties, like smoothness of reconstruction, are built into the reconstruction filters.@ Figure 8.46 contains four discrete wavelet transforms of Fig. 8.9(a). Haar
wavelets, the simplest and only discontinuous wavelets considered in this example, were used as the expansion or basis functions in Fig, 8.46(a). Daubechies
wavelets, among the most popular imaging wavelets, were used in Fig. 8.46(b), 627In digital filtering. each
tiller tap taultiptics a
filter coethcient by a
delayed version of the
signal being filtered EXAMPLE 8.25:
Wavelet bases in
wavelet coding.abcdFIGURE 8.46
Three-scale
wavelet
transforms of
Fig. 8.9(a) with
respect to(a) Haar wavelets.
(b) Daubechies
wavelets,(c} symlets, and
(d) CohenDaubechies
Feauveau
biorthogonal
wavelets.
626 Chapter 8 m Image CompressionWith reference 10 Tables
8.3 and 8.4, wavelet coding is used in the@ JPEG-2000compression standard.a
bFIGURE 8.45A wavelet coding
system:(a) encoder;(b) decoder.8.2.10 Wavelet CodingAs with the transform coding techniques of Section 8.2.8, wavelet coding is
based on the idea that the coefficients of a transform that decorrelates the pixels of an image can be coded more efficiently than the original pixels themselves. If the basis functions of the transform—in this case wavelets—pack
most of the important visual information into a small number of coefficients,
the remaining coefficients can be quantized coarsely or truncated to zero with
little image distortion.Figure 8.45 shows a typical wavelet coding system. To encode a 2’ x 2/
image, an analyzing wavelet, , and minimum decomposition level, J — P, are
selected and used to compute the discrete wavelet transform of the image. If
the wavelet has a complementary scaling function y, the fast wavelet transform (see Sections 7.4 and 7.5) can be used. In either case, the computed transform converts a large portion of the original image to horizontal, vertical, and
diagonal decomposition coefficients with zero mean and Laplacian-like probabilities. Recall the image of Fig. 7.1 and the dramatically simpler statistics of
its wavelet transform in Fig. 7.10(a). Because many of the computed coefficients carry little visual information, they can be quantized and coded to minimize intercoefficient and coding redundancy. Moreover, the quantization can
be adapted to exploit any positional correlation across the P decomposition
levels, One or more lossless coding methods, like run-length, Huffman, arithmetic, and bit-plane coding, can be incorporated into the final symbol coding
step. Decoding is accomplished by inverting the encoding operations ~with
the exception of quantization, which cannot be reversed exactly.The principal difference between the wavelet-based system of Fig. 8.45 and
the transform coding system of Fig. 8.21 is the omission of the subimage processing stages of the transform coder. Because wavelet transforms are both computationally efficient and inherently local (i.e., their basis functions are limited in
duration), subdivision of the original image is unnecessary. As you will see later in
this section, the removal of the subdivision step eliminates the blocking artifact
that characterizes DCT-based approximations at high compression ratios.Wavelet selectionThe wavelets chosen as the basis of the forward and inverse transforms in
Fig. 8.45 affect al! aspects of wavelet coding system design and performance. They impact directly the computational complexity of the transforms and, less directly, the system’s ability to compress and reconstructInput Wavelet Quantizer Symbol! Compressed
image transform encoder imageCompressed .} Symbol Inverse __.. Decompressed
image decoder ry wavelet transform imageos]
628 hopter8 m Image Compressionand symlets, which are an extension of the Daubechies wavelets with increased
symmetry, were used in Fig, 8.46(c). The Cohen-Daubechies-Feauveau wavelets
that were employed in Fig. 8.46(d) are included to illustrate the capabilities ofDWT detail coefficients
are discussed in Section
73.2.corresponding to coefficient value 0,As you can see in Table 8.13, the number of operations involved in the computation of the transforms in Fig. 8.46 increases from 4 to 28 multiplications and
additions per coefficient (for each decomposition level) as you move from Fig.
8.46(a) to (d). All four transforms were computed using a fast wavelet transform (i.e., filter bank) formulation. Note that as the computational complexity
(i.e., the number of filter taps) increases, the information packing performance
does as well. When Haar wavelets are employed and the detail coefficients
below 1,5 are truncated to zero, 33.8% of the total transform is zeroed. With the
more complex biorthogonal wavelets, the number of zetoed coefficients rises to
42.1%, increasing the potential ¢ompression by almost 10%. aDecomposition level selectionAnother factor affecting wavelet coding computational complexity and reconstruction error is the number of transform decomposition levels. Because a
P-scale fast wavelet transform involves P-filter bank iterations, the number of
operations in the computation of the forward and inverse transforms increases
with the number of decomposition levels. Moreover, quantizing the increasingly lower-scale coefficients that result with more decomposition levels
affects increasingly larger areas of the reconstructed image. In many applications, like searching image databases or transmitting images for progressive
reconstruction, the resolution of the stored or transmitted images and the
scale of the lowest useful approximations normally determine the number oftransform levels.EXAMPLE 8.26:
Decomposition
levels in wavelet
coding.biorthogonal wavelets. As in previous results of this type, all detail coefficients
were scaled to make the underlying structure more visible —with intensity 128@ Table 8.14 illustrates the effect of decomposition level selection on the coding of Fig. 8.9(a) using biorthogonal wavelets and a fixed global threshold of
25. As in the previous wavelet coding example, only detail coefficients are
truncated. The table lists both the percentage of zeroed coefficients and theresulting rms reconstruction errors from Eq. (8.1-10). Note that the initial
decompositions are responsible for the majority of the data compression. There
is little change in the number of truncated coefficients above three decompo
sition levels.x TABLE 8.13
Wavelet transform
filter taps and
zeroed coefficients
when truncating
the transforms in
Fig. 8.46 below 1.5.WaveletHaar (see Ex. 7.10)
Daubechies (see Fig, 7.8)
Symlet (see Fig. 7.26)
Biorthogonal (see Fig. 7.39){Scaling + Wavelet)Filter Taps
Zeroed Coefficients33,8%
40.9%
41.2%
42.1% 2+28+88+8
7411
8.2 m Some Basic Compression Methods 629Decomposition Level
{Scales or Filter Approximation Truncated Reconstruction
Bank Iterations) Coefficieat Image Coefficients (%) Error (rms)256 X 256128 x 128
64 X 64
32 x 32
“16 x 16Quantizer designThe most important factor affecting wavelet coding compression and reconstruction error is coefficient quantization. Although the most widely used
quantizers are uniform, the effectiveness of the quantization can be improved
significantly by (1) introducing a larger quantization interval around zero,
called a dead zone, or (2) adapting the size of the quantization interval from
scale to scale. In either case, the selected quantization intervals must be transmitted to the decoder with the encoded image bit stream. The intervals themselves may be determined heuristically or computed automatically based on
the image being compressed. For example, a global coefficient threshold could
be computed as the median of the absolute values of the first-level detail coefficients or as a function of the number of zeroes that are truncated and the
amount of energy that is retained in the reconstructed image.®@ Figure 8.47 illustrates the impact of dead zone interval size on the percentage of truncated detail coefficients for a three-scale biorthogonal
wavelet-based encoding of Fig. 8.9(a). As the size of the dead zone increases,
the number of truncated coefficients does as well. Above the knee of the
curve (i.e., beyond 5), there is little gain. This is due to the fact that the histogram of the detail coefficients is highly peaked around zero (see, for example, Fig. 7.10}. anThe rms reconstruction errors corresponding to the dead zone thresholds in
Fig. 8.47 increase from 0 to 1.94 intensity Jevels at a threshold of 5 and to 3,83
intensity levels for a threshold of 18, where the number of zeroes reaches
93.85%. If every detail coefficient were eliminated, that percentage would increase to about 97.92% (about 4%), but the reconstruction error would grow
to 12.3 intensity levels. «JPEG-2000JPEG-2000 extends the popular JPEG standard to provide increased flexibility
in both the compression of continuous-tone still images and access to the compressed data. For example, portions of a JPEG-2000 compressed image can
be extracted for retransmission, storage, display, and/or editing. The standard is based on the wavelet coding techniques just described. Coefficient
quantization is adapted to individual scales and subbands and the quantized TABLE 8.14
Decomposition
level impact on
wavelet coding
the 512 x $12
image ofFig, 8.9(a).One measure of the
energy of 2 digital signal
is (he sum of the squared
samples,EXAMPLE 8.27:
Dead zoneinterval selection
in wavelet coding.
630 Chapter 8 38 Image CompressionFIGURE 8.47 The
impact of dead
zone interval
selection on
wavelet coding.Ssiz is used in the
standard to denote
intensily resolution.‘The irreversible
component transform
is the component
transform sed for
lossy compression. The
component tragsfornt
itself is not irreversible,
A diflerent component
transform is used for
reversible compression\ 97.918%        100 4
90 36
80 432
= 70 {2s ~
a . RMSE $
8 ° ie
60 o 24 02
= 50 o 42 Fz
‘ 3
5 a os
3 40 dis £2
Fa 35>
2 o
S 30 . 12
£
20 08
10 04
0 1 fo 1 i] 4
0 2 4 6 8 Ww 2 FW 6 1Dead zone threshold
coefficients are arithmetically coded on a bit-plane basis (see Sections 8.2.3
and 8.2.7). Using the notation of the standard, an image is encoded as follows
(ISOMEC [2000]).The first step of the encoding process is to DC level shift the samples of the
Ssiz-bit unsigned image to be coded by subtracting 25"*~', If the image has
more than one component—like the red, green, and blue planes of a color
image —each component is shifted individually. If there are exactly three components, they may be optionally decorrelated using a reversible or nonreversible linear combination of the components. The irreversible component
transform of the standard, for example, is¥% (x, y} = 0.299Jg (x, y) + 0.5872, (x, v) + 0.1144 (x, y)
¥ (4. y) = -0.16875Iy (x, ») — 0.331264, (x, y) + 05h (x,y)  (8.2-61)
% (x, y) = O.Sly(x, y) — 0.418691, (x, y) ~ 0.081314 (x, y)where fp, 7), and /; are the level-shifted input components and ¥, ¥, and ¥ are
the corresponding decorrelated components. If theinput components are the
red, green, and blue planes of a coior image, Eq. (8.2-61) approximates the
R’G’B' to Y'C,C, color video transform (Poynton [1996]).' The goal of the transformation is to improve compression efficiency; transformed components } and
¥ are difference images whose histograms are highly peaked around zero. *R'G’ B’ is a gamma corrected, nonlinear version of a linear CIE (International Commission on [liumination) R&B colorimetry value. Y" is luminance and C, and C, are color differences (ie. scaled
8 - Yo and R'- ¥' vatues}.
§.2 & Some Basic Compression Methods 631After the image has been level shifted and optionally decorrelated, its components can be divided into #/es. Tiles are rectangular arrays of pixels that are
processed independently. Because an image can have more than one component (e.g., it could be made up of three color components), the tiling process
creates tile components. Each tile component can be reconstructed independently, providing a simple mechanism for accessing and/or manipulating a
limited region of a coded image. For example, an image having a 16:9 aspect
ratio could be subdivided into tiles so that one of its tiles is a subimage with a
4:3 aspect ratio, That tile could then be reconstructed without accessing the
other tiles in the compressed image. If the image is not subdivided into tiles, it
is a single tile.The 1-D discrete wavelet transform of the rows and columns of each tile
component is then computed. For error-free compression, the transform is
based on a biorthogonal, 5-3 coefficient scaling and wavelet vector (Le Gall
and Tabatabai [1988]). A rounding procedure is defined for non-integer-valued
transform coefficients. In lossy applications, a 9-7 coefficient scaling-wavelet
vector (Antonini, Barlaud, Mathieu, and Daubechies [1992]) is employed. In either case, the transform is computed using the fast wavelet transform of Section
7.4 or via a complementary fifting-based approach (Mallat [1999]). For example,
in lossy applications, the coefficients used to construct the 9-7 FWT analysis
filter bank are given in Table 8.15. The complementary lifting-based implementation involves six sequential “lifting” and “scaling” operations:Y(Qn+1) = X(2Qn+1)+a[X(2n)+XQn+2)), ip - 35 +1 <i +3¥(2n) = X(Qn)+B[¥Qn-1)+YQn+H], ip - 25 2n <i, +2
¥(2n +1) = ¥(2n+1)+ y[¥(2n)+ ¥(2n+2)], ip 1S 22+ 1 <i +1¥(2n) = ¥(2n)+8[Y(2n-1)+ ¥(2n+)], ig S On <i
¥(2nt 1)=—-K+¥(Q2n+1), ip S2n +1 <i,
¥(2n) = ¥(2n)/K, . fps 2n <i, (8.2-62)Here, X is the tile component being transformed, Y is the resulting transform,
and i and i, define the position of the tile component within a component.
That is, they are the indices of the first sample of the tile-component row or
column being transformed and the one immediately following the last sample.
Variable n assumes values based on ig, i}, and which of the six operations isHighpass Wavelet Lowpass Scaling Filter Tap Coefficient Coefficient0 ~1.115087052456994 0.60294901 82363579
+1 0.5912717631 142470 02668641 184428723
42 0.05754352622849957 —0.07822326652898785
+3 —0.091271 76311424948 — 0.016864 11844287495| #4 0 0.0267487574 1080976 Lifting-based
implementations are
another way lo compule
wavelet transforms. The
coefficients zsed in the
approach are directly
related to the FWT fitter
bank coefficients.TABLE 8,15
Impulse responses
of the low- and
highpass analysis
filters for an
irreversible 9-7
wavelet
transform.
632  Chopter 8 m Image CompressionThese Jifting-based
coefficients are specified
in the standard.Recail from Chapter 7
that the DWT
decomposes an image
into a set of band-limited
components called
subbands,FIGURE 8.48
JPEG 2000
two-scale wavelet
transform
tile-component
coefficient
notation and
analysis gain,being performed. If < ig or n = i), X(n) is obtained by symmetrically extending X. For example, X(ig — 1) = X(iy + 1). X(ig ~ 2) = XU + 2),
X(i;) = Xi; ~ 2), and X(i; + 1) = XC, — 3). At the conclusion of the lifting and scaling operations, the even-indexed values of Y are equivalent to the
FWT lowpass filtered output; the odd-indexed values of Y correspond to
the highpass FWT filtered result, Lifling parameters a, B,y, and § are
—1.586134342, —0.052980118, 0.882911075, and 0.433506852, respectively.
Scaling factor K is 1.230174105S.The transformation just described produces four subbands —a low-resolution
approximation of the tile component and the component's horizontal, vertical,
and diagonal frequency characteristics. Repeating the transformation N;,
times, with subsequent iterations restricted to the previous decamposition's
approximation coefficients, produces an N,-scale wavelet transform. Adjacent scales are related spatially by powers of 2 and the lowest scale contains
the only explicitly defined approximation of the original tile component. As
can be surmised from Fig, 8.48, where the notation of the JPEG-2000 standard
is summarized for the case of N; = 2, a general N,-scale transform contains
3N, + 1 subbands whose coefficients are denoted a,, for b = N,LL,
NLAL,...,1HL,1LH,1HH. The standard does not specify the number of
scales to be computed.When each of the tile components has been processed, the total number of
transform coefficients is equal to the number of samples in the original
image —but the important visual information is concentrated in a few coefticients. To reduce the number of bits needed to represent the transform. coefticient a,(u, v) of subband d is quantized to value gp(u. 7) usinga,(u. v)| |
gpl, v) = selene) oor 2 |8.2-63
A, ( )oop ey
; |
ay r(tt V) | Arye (ue 2) |Qs {Ue YY
8.2 = Some Basic Compression Metheds 635 * ? VeFIGURE 8.49 Four JPEG-2000 approximations of Fig. 8.9{a). Each row contains a result afler compression
and reconstruction. the scaled difference between the result and the original image, and a zoomed portion of
the reconstructed image. (Compare the results in rows ] and 2 with the JPEG resulfs in Pig
8.2 & Some Basic Compression Methods 633where the quantiztion step size A, isA, = reals + 2) (8.2-64)
R, is the nominal dynamic range of subband 6, and «, and 4, are the number
of bits allotted to the exponent and mantissa of the subband’s coefficients. The
nominal dynamic range of subband b is the sum of the number of bits used to
represent the original image and the analysis gain bits for subband b. Subband
analysis gain bits follow the simple pattern shown in Fig. 8.48. For example,
there are two analysis gain bits for subband b = 1HH.For error-free compression, 4, = 0, Ry = &, and A, = 1. For irreversible
compression, no particular quantization step size is specified in the standard.
Instead, the number of exponent and mantissa bits must be provided to the decoder on a subband basis, cailed expounded quantization, or for the N, LL subband only, called derived quantization. In the latter case, the remaining
subbands are quantized using extrapolated N,LL subband parameters. Letting €y and jug be the number of bits allocated to the N; LL subband, the extrapolated parameters for subband 6 areBe = Bo
(8.2-65)i&) = €9 + ny ~ Nz,where n, denotes the number of subband decomposition levels from the original image tile component to subband b.In the final steps of the encoding process, the coefficients of each transformed tile-component’s subbands are arranged into rectangular blocks called
code blocks, which are coded individually, one bit plane at a time. Starting
from the most significant bit plane with a nonzero element, each bit plane is
processed in three passes. Each bit (in a bit plane) is coded in only one of the
three passes, which are called significance propagation, magnitude refinement,
and cleanup. The outputs are then arithmetically coded and grouped with similar passes from other code blocks to form /ayers. A layer is an arbitrary number of groupings of coding passes from each code block. The resulting layers
finally are partitioned into packets, providing an additional method of extracting 2 spatial region of interest from the total code stream. Packets are the fundamental unit of the encoded code stream.JPEG-2000 decoders simply invert the operations described previously.
After reconstructing the subbands of the tile-components from the arithmetically coded JPEG-2000 packets, a user-selected number of the subbands is decoded. Ajthough the encoder may have encoded M, bit planes
for a particular subband, the user—due to the embedded nature of the
code stream~—may choose to decode only N, bit planes. This amounts to
quantizing the coefficients of the code block using a step size of 240" %>- A,
Any nondecoded bits are set to zero and the resulting coefficients, denotedDo not confuse thestandard’s definition of
nominal dynamic range
with the closely related
definition in Chapter 2.
634 Chopter8 @ Image CompressionQuantization as defined
earlier in the chaptec is
irreversible. The term
“inverse quantized” does
nat Mean that there is RO
information loss, This
Process is lossy except
for the case of reversible
JPEG -2000 compression,
where w, = 0, Ry = &,
and A, = 1.EXAMPLE 8.28:
A comparison of
JPEG-2000
wavelet-based
coding and JPEG
DCT-based
compression.9,(u, v), are inverse quantized using(Gy(u, v) + DMM) AL Glu, v) > 0
Ry, (u,v) = § (Gu, v) — rr 2Mo NOM) A, Gatun, 0) < 0 (8.2-66)
0 Alu, v) =0where R,,(u,v) denotes an inverse-quantized transform coefficient and
Np (u, v) is the number of decoded bit planes for 9,(u, v). Reconstruction parameter r is chosen by the decoder to produce the best visual or objective quality
of reconstruction. Generally 0 = r < 1, with a common value being r = 1/2.
The inverse-quantized coefficients then are inverse-transformed by column
and by row using an FWT™ filter bank whose coefficients are obtained from
Table 8.15 and Eq. (7.1-11), or via the following lifting-based operations:X (2n)= K - ¥(2n), oe ig -3 S 2n <i, 4+3
X(QQn+1)=(-1/K)*Y¥Qn +1), ig-2S2n-1< 5,42
X(2n)= X(2n)— 8X (2n-1) + X(2n+1)], ig 3 < In < it3
X(2n-+1)= X(2n +1) -y[X(2n)+ XQn+2)], ip -2S Wt 1 < 442
X(2n)= X(2n)~ B[X 2n-1)+ XQn41)], ig 1S 2n <i41
X(2n+1)=X(2n +1) - a[ X(2n) + XQn + 2)], ig Ss Wnt1 <i,
(8.2-67)where parameters a, 8, y, 5, and K are as defined for Eq. (8.2-62). Inversequantized coefficient row or column element Y() is symmetrically extended
when necessary. The final decoding steps are the assembly of the component
tiles, inverse component transformation (if required), and DC level shifting.
For irreversible coding, the inverse component transformation isZo(x, y) = Yo(x, y) + 1402K(x, y)
A(x, y) = Yo(x, y) ~ 0344 13K (x, y) — 0.71414¥% (x,y) (82-68)
B(x. y) = Yo(x, y) + 1.772K (a, y)and the transformed pixels are shifted by +29%"!.@ Figure 8.49 shows four JPEG-2000 approximations of the monochrome
image in Figure 8.9(a). Successive rows of the figure illustrate increasing levels
of compression— including C = 25, 52,75, and 105. The images in column 1
are decompressed JPEG-2000 encodings. The differences between these images and the original image [Fig. 8.9(a)] are shown in the second column, and
the third column contains a zoomed portion of the reconstructions in column 1,
Because the compression ratios for the first two rows are virtually identical to
the compression ratios in Example 8.18, these results can be compared —both
qualitatively and quantitatively —to the JPEG transform-based results in Figs.
8.32(a) through (£).
636  Chogter 8 #8 Image CompressionA visual comparison of the error images in rows 1 and 2 of Fig. 8.49 with the
corresponding images in Figs. 8.32(b) and (e) reveals a noticeable decrease of
error in the JPEG-2000 results —3.86 and 5.77 intensity levels as opposed to
5.4 and 10.7 intensity levels for the JPEG results. The computed errors favor
the wavelet-based results at both compression levels. Besides decreasing reconstruction error, wavelet coding dramatically increases (in a subjective
sense) image quality. Note that the blocking artifact that dominated the JPEG
results [see Figs. 8.32(c) and (f)] is not present in Fig. 8.49. Finally, we note that
the compression achieved in rows 3 and 4 of Fig. 8.49 is not practical with
JPEG. JPEG-2000 provides useable images that are compressed by more than
100:1 —with the most objectionable degradation being increased image blur.#Ell Digital Image WatermarkingThe methods and standards of Section 8.2 make the’ distribution of images
(whether in photographs or videos) on digital media and over the Internet
practical. Unfortunately, the images so distributed can be copied repeatedly
and without error, putting the rights of their owners at risk. Even when encrypted for distribution, images are unprotected after decryption. One way to
discourage illegal duplication is to insert one or more items of information,
collectively called a watermark, into potentially vulnerable images in such a
way that the watermarks are inseparable from the images themselves. As integral parts of the watermarked images, they protect the rights of their owners in
a variety of ways, including:1. Copyright identification. Watermarks can provide information that serves
as proof of ownership when the rights of the owner have been infringed.
2. User identification or fingerprinting. The identity of legal users can be encoded in watermarks and used to identify sources of illegal copies.
Authenticity determination. The presence of a watermark can guarantee
that an image has not been altered— assuming the watermark is designed
to be destroyed by any modification of the image.
4, Automated monitoring, Watermarks can be monitored by systems that
track when and where images are used (e.g., programs that search the Web
for images placed on Web pages). Monitoring is useful for royalty collection and/or the focation of illegal users.
Copy protection. Watermarks can specify rules of image usage and copying (e.g., to DVD players).35.In this section, we provide a brief overview of digital image watermarking —
the process of inserting data into an image in such a way that it can be used to
make an assertion about the image. The methods described have little in common with the compression techniques presented in the previous sections —
although they do involve the coding of information. In fact, watermarking and
compression are in some ways opposites. While the objective in compression is
to reduce the amount of data used to represent images, the goal in watermarking
is to add information and thus data (i.e., watermarks) to them. As will be seen in
638 Chapter 8 @ Image Compressionab
cdFIGURE 8.51 A
simple invisible
watermark:(a) watermarked
image; (b) the
extracted
watermark;(c) the
watermarked
image after high
quality JPEG
compression and
decompression;
and (d) the
extracted
watermarkfrom (c).Unlike the visible watermark of the previous example, invisible watermarks
cannot be seen with the naked eye. They are imperceptible — but can be recovered with an appropriate decoding algorithm. Invisibility is assured by inserting them as visually redundant information—as information that the human
visual system ignores or cannot perceive (see Section 8.1.3). Figure 8.51(a)
provides a simple example. Because the least significant bits of an 8-bit image
have virtually no effect on our perception of the image, the watermark from
Fig. 8.50(a) was inserted or “hidden” in its two least significant bits, Using the
notation introduced above, we lethe = (4) + a (83-2)and use unsigned integer arithmetic to perform the calculations. Dividing and
multiplying by 4 sets the two least significant bits of f to 0, dividing w by 64
shifts its two most significant kits into the two least significant bit positions,
and adding the two results generates the LSB watermarked image, Note that
the embedded watermark is not visibie in Fig. 8.51(a). By zeroing the most significant 6 bits of this image and scaling the remaining values to the full intensity
range. however, the watermark can be extracted as in Fig, 8.51(b).
mi-transparent” watermark, This is evident in both Fig. 8.50(b)and the difference image in (c). Fyaigital Image hrgurt 8.50
Pr ocessin S A simple visible
watermark:
(a) watermark:
(b} the
watermarked
image, and (c) the
difference
between the
watermarked
image and the
original (nonwatermarked}
image.
8.3 % Digital lmage Watermarking 639An important property of invisible watermarks is their resistance to both
accidental and intentional attempts to remove them. Fragile invisible watermarks are destroyed by any modification of the images in which they are embedded. In some applications, like image authentication, this is a desirable
characteristic. As Figs. 8.51(c) and (d) show, the LSB watermarked image in
Fig, 8.51(a) contains a fragile invisible watermark. If the image in (a) is compressed and decompressed using lossy JPEG, the watermark is destroyed.
Figure 8.51(c) is the result‘after compressing and decompressing Fig, 8.51 (a);
the rms error is 2.1 bits. If we try to extract the watermark from this image
using the same method as in (b), the result is unintelligible [see Fig. 8.51(d)].
Although lossy compression and decompression preserved the important visual
information in the image, the fragile watermark was destroyed.Robust invisible watermarks are designed to survive image modification,
whether the so called attacks are inadvertent or intentional. Common inadvertent attacks include lossy compression, linear and non-linear filtering, cropping, rotation, resampling, and the like. Intentional attacks range from printing
and rescanning to adding additional watermarks and/or noise. Of course, it is
unnecessary to withstand attacks that leave the image itself unusable,Figure 8.52 shows the basic components of a typical image watermarking
system. The encoder in Fig, 8.52(a) inserts watermark w, into image f;, producing watermarked image f,; the complementary decoder in (b) extracts and
validates the presence of w; in watermarked input f,,, or unmarked input f.. If
w; is visible, the decoder is not needed. If it is invisible, the decoder may or
may not require a copy of f; and 2; [shown in gray in Fig. 8.52(b)} to do its job.
If f, and/or w, are used, the watermarking system is known as a private or
restricted-key system; if not, it is a public or unrestricted-key system. Because
the decoder must process both marked and unmarked images, wy is used in
Fig. 8.52(b) to denote the absence of a mark. Finally, we note that to determine
the presence of w; in an image, the decoder must correlate extracted watermark w, with w, and compare the result to a predefined threshold. The threshold sets the degree of similarity that is acceptable for a “match.”ste Image fi Mark Fe, Marked a
Bi insertion image b
fre, FIGURE 8.52
| A typical image
Watermark watermarking
system:(a) encoder;
(b) decoder  p DecisioniSome Creark detected
or slat}Image fash 7 t es
«marked or ———_ et in porrnmeenrenen on a Water yr
unmarked) ——l
640 Chapter 8 % Image CompressionEXAMPLE 8.30: if? Mark insertion and extraction can be performed in the spatial domain, as inA DCT-based the previous examples, or in the transform domain. Figures 8.53(a) and (c)
invisible robust show two watermarked versions of the image in Fig. 8.9(a) using the DCTwatermark. . ‘ .based watermarking approach outlined below (Cox et al. [1997]):Step 1. Compute the 2-D DCT of the image to be watermarked.
Step 2. Locate its K largest coetficients, cy, ¢3,...,¢x. by magnitude. abedFIGURE 8.53 (a} and (c) Two watermarked versions of Fig. 8.9€a): fb) and (d) the ditt
intensity) between the watermarked versions and the anmarked image. These two mages show the intensity
contribution (although sealed dramatically) of the pseudo-random watermarks on the eriginal upagetes {seuled mn
642 Chapter 8 @ Image CompressionWe discuss the
correlation covtficent
in detail in Section
12.21Step 3. Compute watermark &), @),...,x using
@~=¢,-c; for 1sisKk (8.3-4)Recali that watermarks are a sequence of pseudo-random numbers.Step 4. Measure the similarity of 6), @,...,@« {from step 2) and
@, @2,...,@« (from step 3 of the watermarking procedure) using a metric
such as the correlation coefficientK _
AC) ~ @)(@; — @)
et 1sisk (8.3-5)   K _. K
Sa — 6P + Sw; - BP
i=] i=lwhere © and @ are the means of the two K-element watermarks.Step 5. Compare the measured similarity, y, to a predefined threshold, T,
and make a binary detection decision_ ji ify=T
D= {} otherwise (83-6)In other words, D = 1 indicates that watermark w).w2,...,@x is present
(with respect to the specified threshold, 7); D = 0 indicates that it was not.Using this procedure, the original watermarked image in Fig. 8.53(a) -measured
against itself— yields a correlation coefficient of 0.9999, i.e., y = 0.9999. It is
an unmistakable match. In a similar manner, the image in Fig. 8.53(b), when
measured against the image in Fig. 8.53(a), results in a y of 0.0417—it could
not be mistaken for the watermarked image in Fig, 8.53(a) because the correlation coefficient is so low. aTo conclude the section, we note that the DCT-based watermarking approach of the previous example is fairly resistant to watermark attacks, partly
because it is a private or restricted-key method. Restricted-key methods are
always more resilient than their unrestricted-key counterparts. Using the watermarked image in Fig. 8.53(a), Fig. 8.54 illustrates the ability of the method
to withstand a variety of common attacks. As can be seen in the figure, watermark detection is quite good over the range of attacks that were implemented —
the resulting correlation coefficients (shown under each image in the figure)
vary from 0.3113 to 0.9945, When subjected to a high quality but lossy {resulting in an rms error of 7 intensities} JPEG compression and decompression,
y = 0.9945. Even when the compression and reconstructed yields an rms error
of 10 intensity levels, y = 0.7395 —and the usability of this imaye has been significantly degraded. Significant smoothing by spatial filtering and the addition
of Gaussian noise do not reduce the correlation coefficient below 0.8230.
However, histogram equalization reduces y to 0.5210; and rotation has the
largest effect—reducing y to 0.3313. All atlacks, except for the lossy JPEG
8.3 ®% Digital Image Watermarking 641Step 3. Create a watermark by generating a K-element pseudo-random se- A pseudorandom
- + . . ., Bumber sequence
quence of numbers, w;, #9, ...,@x, taken from a Gaussian distribution with — Sppreximstes themean » = 0 and variance 0” = 1. properties of random
numbers. I is not irulyStep 4. Embed the watermark from step 3 into the K largest DCT coeffi- tandem boca i
lepends on acients from step 2 using the following equation predetermined initialvalue,
=o: taw) 1sisk (8.3-3).for a specified constant a > 0 (that controls the extent to which «; alters Fo the images in ,
Ig. 8.3.0 = O70 anc;). Replace the original c; with the computed c} from Eq. (8.3-3). K = 1000,
Step 5. Compute the inverse DCT of the result from step 4.By employing watermarks made from pseudo-random numbers and spreading
them across an image’s perceptually significant frequency components, a can
be made small, reducing watermark visibility. At the same time, watermark
security is kept high because (1) the watermarks are composed of pseudorandom numbers with no obvious structure, (2) the watermarks are embedded
in multiple frequency components with spatial impact over the entire 2-D
image (so their location is not obvious) and (3) attacks against them tend to
degrade the image as well (i.e., the image’s most important frequency compohents must be altered to affect the watermarks).Figures 8,53(b) and (d) make the changes in image intensity that result
from the pseudo-random numbers.that are embedded in the DCT coefficients
of the watermarked images in Figs. 8.53(a) and (c) visible. Obviously, the pseudorandom numbers must have an effect—even if too small to see—on the watermarked images. To display the effect, the images in Figs. 8.53(a) and (c) were
subtracted from the unmarked image in Fig. 8.9(a) and scaled in intensity to
the range [0, 255]. Figures 8.53(b) and (d) are the resulting images; they show
the 2-D spatial contributions of the pseudo-random numbers. Because they
have been scaled, however, you cannot simply add these images to the image
in Fig. 8.9(a) and get the watermarked images in Figs. 8.53(a) and (c). As can
be seen in Figs, 8.53(a) and (c), their actual intensity perturbations are small to
negligible. woTo determine whether a particular image is a copy of a previously watermarked image with watermark ),@2,...,a.% and DCT coefficients
Cy, €2,+.., Cx, We use the following procedure:Step 1. Compute the 2-D DCT of the image in question.Step 2. Extract the K DCT coefficients (in the positions corresponding
to ¢1,¢2,...,cx of step 2 in the watermarking procedure) and denote the
coefficients as é;, @,...,€x. If the image in question is the previously
watermarked image (without modification). ¢, = c} for 1 = i = K. If it is
a modified copy of the watermarked image (i.e., it has undergone some sort
of attack), ¢; = cj for 1 = ¢ = K (the é; will be approximations of the cj).
Otherwise, the image in question will be an unmarked image or an image
with a completely different watermark--and the ¢ will bear no
resemblance to the original ¢;.
= Summary 643 y = 0.8230 y = 0.5210 y = 03113abcdefFIGURE 6.54 Attacks on the watermarked image in Fig. 8.53(a): (a) lossy JPEG compression and
decompression with an rms error of 7 intensity levels: (b) lossy JREG compression and decompression with
an rms error of 10 intensity levels (note the blocking artifact): (c) smoothing by spatial filtering; (d) the
addition of Gaussian noise; (e) histogram equalization; and (4) rotation. Each image is a modified version of
the watermarked image in Fig. 8.53(a). After modification. (hey retain their watermarks lo varying degrees,
as indicated by the correlation coefficients below eath image.compression and reconstruction in (a), have significantly reduced the usability
of the original watermarked imageSummaryThe principal objectives of this chapter were to present the theoretic foundation of
digital image compression, to describe the most commonly used compression methods, and to introduce the related area of digital image watermarking. Allhough the
level of the presentation is intoductory in nature. dhe references provide an entry into
the extensive body of literature dealing with the topics discussed. Ax evidenced by the
international standards listed in Tables 8.3 and 8.4. compression plavs @ key rok: in
646 Chapter 8 mm Image Compression*8.78.88.98.108.11
8.12
8.138.14
* 8.15
8.168.17*8.18Prove that, for a zero-memory source with g symbols, the maximum value of the
entropy is log g, which is achieved if and only if all source symbols are equiprobable. [Hinr: Consider the quantity log q - H(z) and note the inequality
Inn # ¢- 1]{a) How many unique Huffman codes are there for a four-symbol source?(b) Construct them.Consider the simple 4 X 8, 8-bit image:21 21 95 95 169 169 243 243
21 21 95 95 169 169 243 243
21.21 «95 95 169 169 243 243
21 2t 95 95 169 169 243 243{a) Compute the entropy of the image.(b) Compress the image using Huffman coding.{c) Compute the compression achieved and the effectiveness of the Huffman
coding. sx (d) Consider Huffman encoding pairs of pixels rather than individual pixels.That is, consider the image to be produced by the second extension of the
zero-memory source that produced the original image. What is the entropy
of the image when looked at as pairs of pixels?{e) Consider coding the differences between adjacent pixels. What is the entropy of the new difference image? What does this tell us about compressing
the image?{f) Explain the entropy differences in (a), (d) and {e).Using the Huffman code in. Fig. 8.8, decode the encoded string0101001000001 1101011.Compute Golomb code G{n} for 0 = n = 20.Write a general procedure for decoding Golomb code G,,(1).Why is it not possible to compute the Huffman code of the nonnegative integers,n = 0, with the geometric probability mass function of Eq. (8.2-2)?Compute exponential Golomb code Gi,p(2) for 0 sn = 15.Write a general procedure for decoding exponential Golomb code Gh p(t).Plot the optimal Golomb coding parameter m as a function of p forO < p < |in Eq. (8.2-3).Given a foursymboi source {a,b,c,d} with source probabilities{0.1, 0.4, 0,3, 0.2}, arithmetically encode the sequence abcda.  The arithmetic decoding process is the reverse of the encoding procedure. Decode the message 0.32256 given the coding model
Symbol Probabilitya 0.2e 0.3i O01to) 0,2u 0.1 i! 0.4
644 Chepter 8 m Image Compressiondocument image storage and transmission, the Internet, and commercial video distribution (e.g., DVDs). It is one of the few areas of image processing that has received a
sufficiently broad commercial appeal to warrant the adoption of widely accepted standards, And image watermarking is becoming increasingly important as more and more
images are distributed in compressed digital form.References and Further ReadingThe introductory material of the chapter, which is generally confined to Section 8.1,
is basic to image compression and may be found in one form or another in most of
the general image processing books cited at the end of Chapter 1. For additional information on the human visual system, see Netravali and Limb [1980], as well as
Huang [1966], Schreiber and Knapp [1958], and the references cited at the end of
Chapter 2. For more on information theory, see the book Web site or Abramson. [1963], Blahut [1987], and Berger [1971]. Shannon’s classic paper, “A Mathematical
Theory of Communication” [1948], lays the foundation for the area and is another excellent reference. Subjective fidelity. criteria are discussed in Frendendall and
Behrend [1960]. osThroughout the chapter, a variety of compression standards are used in examples.
Most of them were implemented using Adobe Photoshop (with freely available compression plug-ins) and/or MATLAB, which is described in Gonzalez et al. [2004]. Compression standards, as a rule, are lengthy and complex; we have not attempted to cover
any of them in their entirety. For more information on a particular standard, see the published documents of the appropriate standards organization—the International Standards Organization, International Electrotechnical Commission, and/or the International
Telecommunications Union. Additional references on standards include Hunter and
Robinson [1980], Ang et al. [1991], Fox [1991], Pennebaker and Mitchell [1992], Bhatt ct
al. [1997], Sikora [1997], Bhaskaran and Konstantinos (1997], Ngan et al. [1999], Weinberger et al. 2000], Symes [2001], Mitchell et al. [1997], and Manjunath et al. [2001].The lossy and error-free compression techniques described in Section 8.2 and watermarking techniques in Section 8.3 are, for the most part, based on the original papers
cited in the text. The algorithms covered are representative of the work in this area, but
are by no means exhaustive. The material on LZW coding has its origins in the work of
Ziv and Lempel {1977, 1978]. The material on arithmetic coding follows the development in Witten, Neal, and Cleary [1987]. One of the more important implementations
of arithmetic coding is summarized in Pennebaker et al. [1988]. For a good discussion of
lossless predictive coding, see the tutorial by Rabbani and Jones [1991]. The adaptive
predictor of Eq. (8.2-56) is from Graham [1958]. For more on motion compensation,
see S. Solari [1997], which also contains an introduction to general video compression
and compression standards, and Mitchell et al. [1997]. The DCT-based watermarking
technique in Section 8.3 is based on the paper by Cox et al. [1997]. For more on watermarking, see the books by Cox et al. [2001] and Parhi and Nishitani [1999]. See also the
paper by S. Mohanty [1999].Many survey articles have been devoted to the field of image compression. Noteworthy are Netravati and Limb [1980], A. K. Jain [1981], a special issue on picture communication systems in the [EEE Transactions on Communications [1981], a special issue on
the encoding of graphics in the Proceedings of IEEE [1980], a special issue on visual
communication systems in the Proceedings of the JEEE [1985], a special issue on image
sequence compression in the JEEE Transactions on Image Processing [1994]. and a special
issue on vector quantization in the [EEE Transactions on Image Processing |1996], 1%
addition, most issues of the JEEF Transactions on Image Processing [EEE Transactions
on Circuits and Systems for Video Technology, and [EEE Transactions on Multimedia include articles on video and still image compression, motion compensation, and watermarking. See, for example, Robinson [2006], Chandler and Hemami [2005], Yan and
Cosman [2003], Boulgouris et al. [2001], Martin and Bell [2001], Chen and Wilson [2000],
Hartenstein et al. [2000], Yang and Ramchandran [2000], Meyer et al. [2000], S. Mitra et al.
(1998), Mukherjee and Mitra (2003, Xu et al. [2005], Rane and Sapiro [2001], Hu et al.
[2006}, Pi et al. [2006], Dugelay et al. [2006], and Kamstra and Heijmans [2005} as a starting point for further reading and references.Problems8.1 (a) Can variable-length coding procedures be used to compress a histogramequalized image with 2” intensity levels? Explain.(b) Can such an image contain spatial or temporal redundancies that could be
exploited for data compression?8.2 One variation of run-length coding involves (1) coding only the runs of 0’s or 1's
(not both) and (2) assigning a special code to the start of each line to reduce the
effect of transmission errors. One possible code pair is (x,, 7,), where x, and 7,
represent the kth run’s startirig coordinate and run length, respectively. The code
(0,0) is used to signal each new tine.(a) Derive a general expression for the maximum average runs per scan line required to guarantee data compression when run-length coding a 2" X 2” binary image.(b} Compute the maximum allowable value for n = 8.8.3 Consider an 8-pixel line of intensity data, {255, 118, 127, 182, 18, 178, 82, 55}. If
it is uniformly quantized with 4-bit accuracy, compute the rms error and tms
signal-to-noise ratios for the quantized data.*8.4 = Although quantization results in information loss, it is sometimes invisible to the
eye. For example, when 8-bi1 pixels are uniformly quantized to fewer bits/pixel,
false contouring often occurs. It can be reduced or eliminated using improved
gray-scale (IGS) quantization. A sum— initially set to zero—is formed from the
current 8-bit intensity value and the four least significant bits of the previously
generated sum. If the four most significant bits of the intensity value are 1111),
however, 0000; is added instead. The four most significant bits of the resulting
sum are used as the coded pixel value.(a) Construct the IGS code for the intensity data. {108, 139,135,244,
172, 178, 56, 97}.(b) Campute the rms error and rms signa)-to-noise ratios for the decoded [GS
data.85 A 1024 X 1024 8-bit image with 4.2 bits/pixel entropy [computed from its histogram using Eq. (8.1-7)] is to be Huffman coded.(a) What is the maximum compression that can be expected?(b) Will it be obtained?(c) Ifa greater level of lossless compression is required, what else can be done?*8.6 The base e unit of information is commonly called a mat, and the base-10 information unit is called a Hartley, Compute the conversion factors needed to relate
these units to the base-2 unit of information (the bit).@ Problems 645
8.198.208.21
8.228.238.248.258.26
8.278.288.298.30Use the LZW coding algorithm of Section 8.2.4 to encode the 7-bit ASCII string
“AAAAAAAAAAA”.
Devise an algorithm for decoding the LZW encoded output of Example 8.7.
Since the dictionary that was used during the encoding is not available, the code
book must be reproduced as the output is decoded.
Decode the BMP encoded sequence {127, 0, 5, 25, 29, 40, 103, 52, 75, 82}.
(a) Construct the entire 5-bit Gray code,
(b) Create a general procedure for converting a Gray-coded number to its binary equivalent and use it to decode 0111010100111.
Use the CCITT Group 4 compression algorithm to code the second line of the
following two-line segment:
0110011100111111110000!
11111000)11000111112000
Assume that the initial reference element ap is located on the first pixel of the
second line segment.
{a) List all the members of JPEG DC coefficient difference category 3.
(b) Compute their default Huffman codes using Table 4.4.
How many computations are required to find the optimal motion vector of an
8 x 8 macroblock using the MAD optimality criterion, single pixel precision,
and a maximum allowable displacement of 16 pixels? What would it become for
5 pixel precision?
What are the advantages of using B-frames for motion compensation?
Draw the block diagram of the companion motion compensated video decoder
for the encoder in Fig. 8,39.
An image whose autocorrelation function is of the form of Eq. (8.2-49) with
Pr = Dis to be DPCM coded using a second-order predictor.
(a) Form the autocorrelation matrix R and vector r.
(b) Find the optimal prediction coefficients.
(c) Compute the variance of the prediction error that would result from using
the optimal coefficients. ..
Derive the Lloyd-Max decision and reconstéuction levels for L = 8 and the uniform probability density function.1
p(s} = 4 2A
6 otherwise-ASZssAA radiologist from a well-known research hospital recently attended a medical
conference at which a system that could transmit 4096 x 4096 12-bit digitized
X-ray images over standard T) (1.544 Mb/s) phone lines was exhibited. The system transmitted the images in a compressed form using a progressive technique
in which a reasonably good approximation of the X-ray was first reconstructed
at the viewing station and then refined gradually to produce an error-free display. The transmission of the data needed to generate the first approximation
took approximately 5 or 6s. Refinements were made every 5 or 6s (on the average) for the next 1 min, with the first and last refinements having the most and
least significant impact on the reconstructed X-ray, respectively. The physician® Problems647
Morphological Image
ProcessingIn form and feature, face and limb,
| grew so like my brother
That folks got taking me for him
And each for one another.
Henry Sambrooke Leigh, Carols of Cockayne, The Twins PreviewThe word morphology commonly denotes a branch of biology that deals with
the form and structure of animals and plants. We use the same word here in the
context of mathematical morphology as a tool for extracting image components that are useful in the representation and description of region shape,
such as boundaries, skeletons, and the convex hull. We are interested also in
morphological techniques for pre- or postpr&cessing, such as morphological
filtering, thinning, and pruning.In the folowing sections we develop and illustrate severa] important
concepts in mathematical morphology. Many of the ideas introduced here
can be formulated in 1erms of n-dimensional Euclidean space, E”. However, our interest initially is on binary images whose components are elements of Z? (see Section 2.4.2), We discuss extensions to gray-scale images
in Section 9.6.The material in this chapter begins a transition from a focus on purely
image processing methods, whose input and output are images, to processes in
which the inputs are images, but the outputs are attributes extracied from
those images, in the sense defined in Section 1.1. Tools such as morphology and
related concepts are a cornerstone of the mathematical foundation that is utilized for extracting “meaning” from an image. Other approaches are developed and applied in the remaining chapters of the book.649
648 Chapter 8 m Image Compression8.318.328.33
8.34
8.35was favorably impressed with the system, because she could begin her diagnosis
by using the first approximation of the X-ray and complete it as the error-free
reconstruction of the X-ray was being generated. Upon returning to her office,
she submitted a purchase request to the hospital administrator. Unfortunately,
the hospital was on a relatively tight budget, which recently had been stretched
thinner by the hiring of an aspiring young electrical engineering graduate. To appease the radiologist, the administrator gave the young engineer the task of designing such a system. (He thought it might be cheaper to design and build a
similar system in-house. The hospital currently owned some of the elements of
such a system, but the transmission of the raw X-ray data took more than 2
min.) The administrator asked the engineer to have an initial block diagram by
the afternoon staff meeting, With little time and only a copy of Digital Image
Processing from his recent schoo] days in hand, the engineer was able to devise
conceptually a system to satisfy the transmission and associated compression requirements. Construct a conceptual block diagram of such a system, specifying
the compression techniques you would recommend. *Show that the lifting-based ‘Wavelet transform defined by Eq. (8,2-62) is equivalent to the traditional FWT filter bank implementation using the coefficients in
Table 8.15. Define the filter coefficients in terms of a, 8, y, 6, and K.Compute the quantization step sizes of the subbands for a JPEG-2000 encoded
image in which derived quantization is used and 8 bits are allotted to the mantissa and exponent of the 2L.Z subband.How would you add a visible watermark to an image in the frequency domain?
Design an invisible watermarking system based on the discrete Fourier transform.
Design an invisible watermarking system based on the discrete wavelet transform.
650 Chapter? w Morphological Image ProcessingYou will find it helpful to
Teview Sections 2.4.2 and
2.6.4 before pruceeding.The se! reflection operalion is analogous to the
Nipping {rotating) operation performed in spatial
convolution (Section
3.4.2).abcFIGURE 9.1{a) A set, (b) its
reflection, and
(c) its translation
by z.9.1 | PreliminariesThe language of mathematical morphology is set theory. As such, morphology offers a unified and powerful approach to numerous image processing
problems. Sets in mathematical morphology represent objects in an image.
For example, the set of all white pixels in a binary image is a complete morphological description of the image. In binary images, the sets in question are
members of the 2-D integer space Z? (see Section 2.4.2), where each element
of a set is a tuple (2-D vector) whose coordinates are the (x, y) coordinates
of a white (or black, depending on convention) pixel in the image. Grayscale digital images of the form discussed in the previous chapters can be
represented as sets whose components are in Z>. In this case, two components of each element of the set refer to the coordinates of a pixel, and the
third corresponds to its discrete intensity value. Sets in higher dimensional
spaces can contain other image attributes, such as color and time varying
components. ayIn addition to the basic set definitions in Section 2.6.4, the concepts of set
reflection and translation are used extensively in morphology. The reflection of
a set B, denoted B, is defined asB= {wlw=—-b, for be B} (9.1-1)If Bis the set of pixels {2-D points) representing an object in an image, then Bis
simply the set of points in B whose (x, y) coordinates have been replaced by
(-x, —y). Figures 9.1(a) and (b) show a simple set and its reflection.’|
|
| (8); "When working with graphics, such as the sets in Fig, 9.1, we use shading to indicate points (pixels) that
are members of the set under consideration, Whea working with binary images, the sets of interest are
pixels corresponding to objects. We show these in white, and all other pixels in black. The terms
foreground and background are used often to denote the sets of pixels in an image defined to be objects
and non-objects, respectively.
9.1 @ Preliminaries 65]The translation of a set B by point z = (z), Z2), denoted (B),, is defined as
(B), = {cle=b+z, for be BY (91-2)If B is the set of pixels representing an object in an image, then (B), is theset of points in B whose (x,y) coordinates have been replaced by(x + 23, y + 2). Figure 9.1(c) illustrates this concept using the set B from
. Fig. 9.1(a). .Set reflection and translation are employed extensively in morphology to
formulate operations based on so-called structuring elements (SEs): small
sets or subimages used to probe an image under study for properties of interest. The first row of Fig. 9.2 shows several examples of structuring elements where each shaded square denotes a member of the SE. When it does
not matter whether a location in a given structuring element is or is not a
member of the SE set, that location is marked with an “Xx” to denote a “don’t
care” condition, as defined later in Section 9.5.4. In addition to a definition
of which elements are members of the SE, the origin of a structuring element
also must be specified. The origins of the various SEs in Fig. 9.2 are indicated
by a black dot (although placing the center of an SE at its center of gravity is
common, the choice of origin is problem dependent in general), When the
SE is symmetric and no dot is shown, the assumption is that the origin is at
the center of symmetry.When working wiih images, we require that structuring elements be rectangular arrays. This is accomplished by appending the smallest possible
number of background elements (shown nonshaded in Fig. 9.2) necessary to
form a rectangular array. The first and last SEs in the second row of Fig. 9.2
illustrate the procedure. The other SEs in that row already are in rectangular form.As an introduction to how structuring elements are used in morphology,
consider Fig. 9.3. Figures 9.3(a) and (b) show a simple set and a structuring element. As mentioned in the previous paragraph, a computer implementation
requires that set A be converted also to a ré}tangular array by adding background elements. The background border is made large enough to accommodate the entire structuring element when its origin is on the border of the                 FIGURE 9.2 First
row: Examples of
structuring
elements. Second
row: Structuring
elements
converted ta
rectangular
arrays. The dots
denote the centers
of the SEs.
9.2 @ Erosion and Dilation9.2.1 Erosion
With A and B as sets in Z?, the erosion of A by B, denoted A 9 B, is defined asA@B = {2|(B),c A} (9.2-1)In words, this equation indicates that the erosion of A by B is the set of all
points z such that B, translated by z, is contained in A. In the following discussion, set B is assumed to be a structuring element. Equation (9.2-1) is the
mathematical formulation of the example in Fig. 9.3(e), discussed at the end of
the last section. Because the statement that B has to be contained in A is
equivalent to B not sharing any common elements with the background, we
can express erosion in the following equivalent form:A@B = {z|(B), NAS = BY (9.2-2)where, as defined in Section 2.6.4, A° is the complement of A and @ is the
empty set. :Figure 9.4 shows an example of erosion. The elements of A and B are
shown shaded and the background is white. The solid boundary in Fig. 9.4(c)
is the limit beyond which further displacements of the origin of B would
cause the structuring element to cease being completely contained in A.
Thus, the locus of points (locations of the origin of B) within (and including) this boundary, constitutes the erosion of A by 8. We show the erosion
shaded in Fig. 9,4(c). Keep in mind that that erosion is simply the sez of abe
dieFIGURE 9.4 (a) Set A. (b) Square structuring element, B. (c) Erosion of A by B, shown
shaded. (d) Elongated structuring element. (e) Erosion of A by B using this element.
The dotted border in (c} and (e)} is the boundary of set A. shown only for reference.653
652  Chopter9 m Morphological Image ProcessingJn future itlustrauions, we
add enough background
points to form rectangular
arrays, but let the padding
be implicit wher the
meaning is clear in order
to simplify the figures.                                     ab
cde
FIGURE 9.3 (a) A set (each shaded square is a member of the set). (b) A structuring
element. (c) The set padded with background elements to form a rectangular array and
provide a background border. (d) Structuring element as a rectangular array. (e) Set
processed by the structuring elemettt.original set (this is analogous to padding for spatial correlation and convolution, as discussed in Section 3.4.2). In this case, the structuring element is of
size 3 X 3 with the origin in the center, so a one-element border that encompasses the entire set is sufficient, as Fig..9.3(c) shows. As in Fig. 9.2, the structuring element is filled with the smallest possible number of background
elements necessary to make it into a rectangular array [Fig. 9.3(d)}.Suppose that we define an operation on set A using structuring element B,
as follows: Create a new set by running 8 over A so that the origin of B visits
every element of A. At each location of the origin of B, if B is completely contained in A, mark that location as a member of the new set (shown shaded),
else mark it as not being a member of the new set (shown not shaded).
Figure 9.3(e) shows the result of this operation. We see that, when the origin of
Bis on a border element of A, part of B ceases to be contained in A, thus eliminating the location on which B is centered as a possible member for the new
set. The net result is that the boundary of the set is eroded, as Fig. 9.3(¢) shows.
When we use terminology such as “the structuring element is contained in the
set,” we mean specifically that the elements of A and 8 fully overlap. In other
words, although we showed A and B as arrays containing both shaded and
nonshaded elements, only the shaded elements of both sets are considered in
determining whether or not Bis contained in A. These concepts form the basis
of the material in the next section, so it is important that you understand the
ideas in Fig. 9.3 fully before proceeding. .EEX@ Erosion and DilationWe begin the discussion of morphology by studying two operations: erosion
and dilation. These operations are fundamental to morphological processing,
In fact, many of the morphological algorithms discussed in this chapter arebased on these two primitive operations.
654 = Chapter % Morphological image ProcessingEXAMPLE 9.1:
Using erosion to
remove image
components.ab
cdFIGURE 9.5 Using
erosion to remove
image components. (a) A486 X 486 binary
image of a wirebond mask.
(b)-{d) Image
eroded using
square structuring
elements of sizes
11 X 11,15 * 15,
and 45 x 45,
respectively. The
elements of the
SEs were all Is.values of z that satisfy Eq, (9.2-1) or (9.2-2). The boundary of set A is
shown dashed in Figs. 9.4(c) and (e) only as a reference: it is not part of the
erosion operation. Figure 9.4(d) shows an elongated structuring element,
and Fig. 9.4(e) shows the erosion of A by this element. Note that the original set was eroded to a line.Equations (9.2-1) and (9.2-2} are not the only definitions of erosion (see
Problems 9.9 and 9.10 for two additional, equivalent definitions.) However,
these equations have the distinct advantage over other formulations in that
they are more intuitive when the structuring clement B is viewed as a spatial
mask (see Section 3.4.1).#% Suppose that we wish to remove the lines connecting the center region to
the border pads in Fig. 9.5(a). Eroding the image with a square structuring
element of size 11 X 11 whose components are ail ls removed most of the
lines, as Fig. 9.5(b) shows. The reason the two vertical lines in the center were
thinned but not removed éontpletely is that their width is greater than 11
pixels. Changing the SE size to 15 x J5 and eroding the original image again
did remove all the connecting lines, as Fig. 9.5(c) shows (an alternate approach would have been to erode the image in Fig. 9.5(b) again using the
same 11 X 11 SE). Increasing the size of the structuring clement even more
would eliminate larger components. For example. the border pads can be removed with a structuring element of size 45 X 45, as Fig. 9.5(d) shows,4 Fave
eke
9.2 m Erosion and DilationWe see from this example that erosion shrinks or thins objects in a binary image. In fact, we can view erosion as a morphological filtering operation
in which image details smaller than the structuring element are filtered (removed) from the image. In Fig. 9.5, erosion performed the function of a
“line filter.” We return to the concept of a morphological filter in Sections
9.3 and 9.6.3. RB9.2.2 Dilation
With A and Bas sets in Z’, the dilation of A by B, denoted A @ B, is defined asA@B = {2\(B),.A # O} (9.2-3)This equation is based on reflecting B about its origin, and shifting this reflection
by z (see Fig. 9.1), The dilation of A by B then is the set of ail displacements,
Z, such that B and A overlap by at least one element. Based on this interpretation, Eq. (9.2-3)} can be written equivalently asA@B = {z|[(B), AJC A} (9.2-4)As before, we assume that 8 is a structuring element and A is the set (image
objects) to be dilated.Equations (9.2-3) and (9.2-4) are not the only definitions of dilation currently in use (see Problems 9.11 and 9.12 for two different, yet equivalent,
definitions). However, the preceding definitions have a distinct advantage
over other formulations in that they are more intuitive when the structuring
element B is viewed as a convolution mask. The basic process of flipping
(rotating) B about its origin and then successively displacing it so that it
slides over set (image) A is analogous to spatial convolution, as introduced
in Section 3.4.2. Keep in mind, however, that dilation is based on set operations and therefore is a nonlinear operation, whereas convolution is a linear
operation. eeUnlike erosion, which is a shrinking or thinning operation, dilation
“grows” or “thickens” objects in a binary image. The specific manner and extent of this thickening is controlled by the shape of the structuring element
used. Figure 9.6(a) shows the same set used in Fig. 9.4, and Fig. 9.6(b) shows a
structuring element (in this case B = B because the SE is symmetric about its
origin). The dashed line in Fig. 9.6(c) shows the original set for reference, and
the solid line shows the limit beyond which any further displacements of the
origin of B by z would cause the intersection of B and A to be empty. Therefore, all points on and inside this boundary constitute the dilation of A by B.
Figure 9.6(d) shows a structuring element designed to achieve more dilation
vertically than horizontally, and Fig. 9.6(e) shows the dilation achieved with
this element.655
656 Chapter @# Morphological Image Processingabc
dieFIGURE 9.6(a) Set A.(b) Square
structuring element (the dot denotes the origin).
(c) Dilation of A
by B, shown
shaded.(d) Elongated
structuring element. (e) Dilation
of A using this
element. The’
dotted border in
(c) and (e) is theboundary of set A,shown only for
referenceEXAMPLE 9.2:
An illustration of
dilation.ac
bFIGURE 9,7(a) Sample text of
poor resolution
with broken
characters (see
magnified view).
(b) Structuring
element.(c) Dilation of (a)
by (b). Broken
segments were
joined.   d
df4
d [*] aja
B=8
A
d/4
d
B=8 re & One of the simplest applications of dilation is for bridging gaps. Figure 9.7(a)
shows the same image with broken characters that we studied in Fig. 4.49 in
connection with lowpass filtering. The maximum length of the breaks is
known to be two pixels. Figure 9.7(b) shows a structuring element that can be
used for repairing the gaps (note that instead of shauing, we used Is to denote
the elements of the SE and Os for the background; this is because the SE is
now being treated as a subimage and not as a graphic). Figure 9.7(c) shows
the result of dilating the original image with this structuring element. The
gaps were bridged. One immediate advantage of the morphological approach
over the lowpass filtering method we used to bridge the gaps in Fig. 4.49 isHistorically, certain camputer
programs were written using
only two digits rather than
four to define the applicable
year, Accordingly, the
company's software may
recognize a date using "00"as 1900 rather than the year
2000.
658 Chapter 9 m@ Morphological image ProcessingThe opening of set A by structuring element B, denoted A ° B, is defined asAoB=(AGB)OB (9.3-1)Thus, the opening A by B is the erosion of A by B, followed by a dilation of
the result by B.Similarly, the closing of set A by structuring element B, denoted A ¥B, is
defined asA¥B=(A@B)OB (9.3-2)which says that the closing of A by B is simply the dilation of A by B, followed
by the erosion of the result by B.. The opening operation has a simple geometric interpretation (Fig. 9.8).
Suppose that we view the structuring element B as a (flat) “rolling ball.” The
boundary of Ao B is then established by the points in B that reach the
Farthest into the boundary of A as B is rolled around the inside of this boundary. This geometric fitting property of the opening operation leads to a settheoretic formulation, which states that the opening of A by B is obtained by
taking the union of all translates of B that fit into A. That is, opening can be expressed as a fitting process such that~ Ac B=(){(B)\(B),CA (9.3-3)where U {-} denotes the union of all the sets inside the braces.Closing has a similar geometric interpretation, except that now we roll B on
the outside of the boundary (Fig. 9.9). As discussed below, opening and closing
are duals of each other, so having to roll the ball on the outside is not unexpected. Geometrically, a point w is an element of A ¥B if and only if
(B), A # © for any translate of (8), that contains w, Figure 9.9 illustrates
the basic geometrical properties of closing.AcB = U[(B) (8), € A)  abcdFIGURE 9.8 {a) Structuring element B “rotling” along the inner boundary of A {the dot
indicates the origin of B). (b) Structuring element. (c) The heavy line is the outer
boundary of the opening. (d) Complete opening (shaded). We did not shade A in (a)
for clarity.
9.3 m Opening and Closing 657that the morphological method resulted directly in a binary image. Lowpass
filtering, on the other hand, started with a binary image and produced a grayscale image, which would require a pass with a thresholding function to convert it back to binary form. .9.2.2 Duality
Erosion and dilation are duals of each other with respect to set complementation and reflection. That is>(AQBY = A@B (9.2-5)
and
(A@®BY = AOB (9.2-6)Equation (9.2-5) indicates that erosion of A by B is the complement of the dilation of A® by B, and vice versa. The duality property is useful particularly
when the structuring element is symmetric with respect to its origin (as often is
the case), so that B = B. Then, we can obtain the erosion of an image by B
simply by dilating its background (i.e., dilating A°) with the same structuring
element and complementing the result. Similar comments apply to Eq. (9.2-6).We proceed to prove formalty the validity of Eq. (9.2-5) in order to illustrate a typical approach for establishing the validity of morphological expressions. Starting with the definition of erosion, it follows that(A By = {z\(B), CA}
If set (B), is contained in A, then (B), A’ = ©, in which case the preceding
expression becomes ;
(A@ By = {2|(B),.N A =o}
But the complement of the set of z's that satisfy (8), A’ = © is the set of z’s
such that (B). A # @. Therefore,
(Ae By = [2\(B). Fa o}=A@Bwhere the last step follows from Eq. (9.2-3). This concludes the proof. A similar line of reasoning can be used to prove Eq. (9.2-6) (see Problem 9.13).EXY Opening and ClosingAs you have seen, dilation expands the components of an image and erosion
shrinks them. In this section we discuss two other important morphological
operations; opening and closing. Opening generally smoothes the contour of
an object, breaks narrow isthmuses, and eliminates thin protrusions. Closing
also tends to smooth sections of contours but, as opposed to opening, it generally fuses narrow breaks and long thin gulfs, eliminates small holes, and fills
gaps in the contour.
9.3 m Opening and Closing 659 eb:FIGURE 9.9 (a) Structuring element 3 “roiling” on the outer boundary of set A. (b) The
heavy line is the outer boundary of the closing. (c) Complete closing (shaded). We did
not shade A in (a) for clarity,@ Figure 9.10 further illustrates the opening and closing operations. Figure
9.10(a) shows a set A, and Fig. 9.10(b) shows various positions of a disk structuring element during the erosion process, When completed, this process resulted in the disjoint figure in Fig. 9.10(c), Note the elimination of the bridge
between the two main sections. Its width was thin in relation to the diameter of A*B=(AQB)@B toncecocce eee 0 Neecccnoc tees A+B=(A@BOBEXAMPLE 9.3:
Asimple
illustration of
morphological
opening and
closing.sar.
monhiFIGURE 9.10
Morphological
opening and
closing. The
structuring
element is the
small circle shown
in various
positions in{b). The SE was
not shaded here
for clarity. The
dark dot is the
center of the
structuring
element.
662 Chapter 9 m Morphological Image Processingab
ed
é
fFIGURE 9.12.(a) Set A. (b) A
window, W, and
the local background of D with
frespect toW.(W — D).(c) Complement
of A. (d)} Erosion
of A by D.(e) Erosion of A‘
by (W — D).(f} Intersection of
({d} and (e), showing the location of
the origin of D, as
desired. The dots
indicate the
origins of C, D,
and £.EXH The Hit-or-Miss TransformationThe morphological hit-or-miss transform is a basic tool for shape detection.
We introduce this concept with the aid of Fig. 9.12, which shows a set A consisting of three shapes (subsets), denoted C, D, and E. The shading in Figs. 9.12(a)
through (c) indicates the original sets, whereas the shading in Figs. 9.12(d) and
(e) indicates the result of morphological operations. The objective is to find
the location of one of the shapes, say, D.     (ASD)n ars
660 Chapter 9 m Morphological Image ProcessingEXAMPLE 9.4:
Use of opening
and closing for
morphologicat
filtering.the structuring element; that is, the structuring element could not be completely contained in this part of the set, thus violating the conditions of Eq. (9.2-1).
The same was true of the two rightmost members of the object. Protruding elements where the disk did not fit were eliminated. Figure 9.10(d) shows the
process of dilating the eroded set, and Fig. 9.10(e) shows the final result of
opening. Note that outward pointing corners were rounded, whereas inward
pointing corners were not affected,Similarly, Figs. 9.10(f) through (i) show the results of closing A with the
same structuring element. We note that the inward pointing corners were
rounded, whereas the outward pointing corners remained unchanged. The
leftmost intrusion on the boundary of A was reduced in size significantly, because the disk did not fit there. Note also the smoothing that resulted in parts
of the object from both opening and closing the set A with a circular structuring element. 2As in the case with dilatién and erosion, opening and closing are duals of
each other with respect to set complementation and reflection. That is,(A « BY = (Ae B) (93-4)
and
(A ° BY = (AS ¥B) (93-5)We leave the proof of this result as an exercise (Problem 9.14).
The opening operation satisfies the following properties:(a) A © Bisa subset (subimage) of A.
(b) If C is a subset of D, then C o B isasubset of Do B,
{c) (A? B)o B= AoB.Similarly, the closing operation satisfies the following properties:{a) A isa subset (subimage) of A ¥ B.
{b) If Cis a subset of D, thenC ¥ Bis a subset of D ¥B.
(ce) (A ¥B) ¥B= A¥B.Note from condition (c) in both cases that multiple openings or closings of a
set have no effect after the operator has been applied once.@ Morphological operations can be used to construct filters similar in concept
to the spatial filters discussed in Chapter 3. The binary image in Fig. 9.11{a)
shows a section of a fingerprint corrupted by noise*Here the noise manifests
itself as random light elements on a dark background and as dark elements on
the light components of the fingerprint. The objective is to eliminate the noise
and its effects on the print while distorting it as little as possible, A morphological filter consisting of opening followed by closing can be used to accomplish this objective.Figure 9.11(b} shows the structuring element used. The rest of Fig. 9.11
shows a step-by-step sequence of the filtering operation. Figure 9,11(c) is the
9.3 @ Opening and Closing 661          _pcnenae
(A°B)@B [(A°B)@B)OB=(A* BBresult of eroding A with the structuring element. The background noise was
completely eliminated in the erosion stage of opening because in this case ail
noise components are smaller than the structuring element. The size of the
noise elements (dark spots) contained within the fingerprint actually increased
in size. The reason is that these elements are ifner boundaries that increase in
size as the objeos is eroded. This enlargement is countered by performing dilation on Fig. 9.11({c). Figure 9.11(d) shows the result. The noise components contained in the fingerprint were reduced in size or deleted completely.The two operations just described constitute the opening of A by B. We note
in Fig. 9.11(d) that the net effect of opening was to eliminate virtually all noise
components in both the background and the fingerprint itself. However, new
gaps between the fingerprint ridges were created. To counter this undesirable
effect, we perform a dilation on the opentng. as shown in Fig. 9.11(¢). Most of
the breaks were restored, bul the ridges were thickened, a condition that can beremedied by erosion. The result, shown in Fig, 9.11(£), constitutes the closing ofthe opening of Fig. 9.11(d). 'Phis final result is remarkably clean of noise specks,
but it has the disadvantage that some of the print ridges were not fully repaired,
and thus contain breaks. This is not totally unexpected, because no conditions
were built into the procedure for maintaining connectivity (we di this
again in Example 9.8 and demonstrate ways to address it in Section 11.1.7).     a b
ad°
efFIGURE 9.11(a) Noisy image.
(b) Structuring
element.(c} Eroded image.
(d) Opening of A.
(e) Dilation of the
opening.(f) Closing of the
opening.
(Original image
courtesy of the
National Institute
of Standards and
Technology.)
9.4 & The Hit-or-Miss TransformationLet the origin of each shape be located at its center of gravity. Let D be enclosed by a smal! window, W. The local background of D with respect to W is
defined as the set difference (W — D), as shown in Fig. 9.12(b). Figure 9.12(c)
shows the complement of A, which is needed later. Figure 9.12(d) shows the
erosion of A by D (ihe dashed lines are included for reference). Recall that
the erosion of A by D is the set of locations of the origin of D, such that D is
completely contained in A. Interpreted another way, A © D may be viewed
geometrically as the set ofsall locations of the origin of D at which D found a
match (hit) in A. Keep in mind that in Fig. 9.12 A consists only of the three
disjoint sets C, D, and E.Figure 9.12(e) shows the erosion of the complement of A by the local background set (W — D). The outer shaded region in Fig. 9.12(e) is part of the erosion. We note from Figs. 9.12(d) and (e) that the set of locations for which D
exacily fits inside A is the intersection of the erosion of A by D and the erosion
of A‘ by (W — D) as shown in Fig. 9.12(f). This intersection is precisely the location sought. In other words, if B denotes the set composed of D and its background, the match (or set of matches) of B in A, denoted A © B, isA@B = (A@D)N[A OW ~ D)] (9.4-1)We can generalize the notation somewhat by letting B = (B,, By), where
B, is the set formed from elements of 8 associated with an object and B, is the
set of elements of B associated with the corresponding background. From the
preceding discussion, B, = D and B, = (W - D). With this notation, Eq.
(9.4-1) becomesA®B={(AOB)N(A GB) (9.4-2)Thus, set A ® B contains all the (origin) points at which, simultaneously, B,
found a match (“hit”) in A and B, found a match in A’. By using the definition
of set differences given in Eq. (2.6-19) and the dual relationship between erosion and dilation given in Eq. (9.2-5), we can write Eq. (9.4-2) asA@B = (AGB, -tA@h,) (9.4-3)However, Eq. (9.4-2) is considerably more intuitive. We refer to any of the preceding three equations as the morphological hit-or-miss transform.The reason for using a structuring element B; associated with objects and
an element B, associated with the background is based on an assumed definition that two or more objects are distinct only if they form disjoint (disconnected) sets. This is guaranteed by requiring that each object have at least a
one-pixel-thick background around it. In some applications, we may be interested in detecting certain patterns (combinations) of 1s and Qs within a set, in
which case a background is not required. In such instances, the hit-or-miss
transform reduces to simple erosion. As indicated previously, erosion is still a
set of matches, but without the additional requirement of a background match
for detecting individual objects. This simplified pattern detection scheme is
used in some of the algorithms developed in the following section.663
664  Chopter 9 m Morphological Image ProcessingFrom this point on, we do
not show border padding
explicitly.KES Some Basic Morphological AlgorithmsWith the preceding discussion as foundation, we are now ready to consider
some practical uses of morphology. When dealing with binary images, one of
the principa] applications of morphology is in extracting image components
that are useful in the representation and description of shape. In particular,
we consider morphological algorithms for extracting boundaries, connected
components, the convex hull, and the skeleton of a region. We also develop
several methods (for region filling, thinning, thickening, and pruning) that
are used frequently in conjunction with these algorithms as pre- or postprocessing steps. We make extensive use in this section of “mini-images,”
designed to clarify the mechanics of each morphological process as we introduce it. These images are shown graphically with 1s shaded and 0s in
white.3.5.1 Boundary Extraction *The boundary of a set A, denoted by £(A), can be obtained by first eroding
A by B and then performing the set difference between A and its erosion.
That is,B(A) = A- (ASB) (9.5-1)where B is a suitable structuring element.Figure 9.13 illustrates the mechanics of boundary extraction. It shows a
simple binary object, a structuring element B, and the result of using Eq.
(9.5-1). Although the structuring element in Fig. 9.13(b) is among the most
frequently used, it is by no means unique. For example, using a 5 X 5 structuring element of 1s would result in a boundary between 2 and 3 pixels
thick.                       B(A)
ab
edFIGURE 9.13 (a} Set A. (b) Structuring element B. (c) A eroded by B. (d) Boundary,
given by the set difference between A and its erosion
666 — Chopter 9 # Morphological Image Processingoe Be.
som
hnFIGURE 9.15 Hole
filling. (a) Set A
{shown shaded).
(b) Complement
of A.{c) Structuring
element B.(d) Initial point
inside the
boundary.
(e)-(b) Various
steps ofEq. (95-2). °(i) Final result
[union of (a)
and (h)].EXAMPLE 9.6:
Morphological
hole filling.                                                              ® Figure 9.16(a) shows an image composed of white circles with black inner
spots. An image such as this might result fram thresholding into two levels a
scene containing polished spheres (c.g., ball bearings). The dark spots inside
the spheres could be the result of reflections. The objective is to eliminate the
reflections by hole filling. Figure 9.16(a} shows ane point selected inside one of
the spheres, and Fig. 9.16(b) shows the result of filling that component. Finally, abcFIGURE 9.16 (a) Binary image (the white dot inside one of the regions is the starting
point for the hole-filling algorithm). (b} Result of filling that region. (c) Result of filling
all holes.
9.5 & Some Basic Morphological Algorithms @ Figure 9.14 further illustrates the use of Eq. (9.5-1) with a3 X 3 structuring
element of 1s. As for all binary images in this chapter, binary 1s are shown in
white and Os in black, so the elements of the structuring element, which are 1s,
also are treated as white. Because of the size of the structuring element used,
the boundary in Fig. 9.14(b) is one pixel thick. os9.5.2 Hole FillingA hole may be defined as a background region surrounded by a connected
border of foreground pixels. In this section, we develop an algorithm based on
set dilation, complementation, and intersection for filling holes in an image.
Let A denote a set whose elements are 8-connected boundaries, each boundary enclosing a background region (i.c..a hole}, Given a point in each hole, the
objective is to fill all the holes with Is.We begin by forming an array, Xq, of Os (the same size as the array containing A), except at the locations in Xy correspopding to the given point in cach
hole, which we set to 1. Then, the following procedure fills all the holes with 1s:X= (HB BINA k= 1,23... (9.5-2)
where 8 is the symmetric structuring element in Fig. 9.15(c). The algorithm terminates at iteration step & if X, = X;,.,. ‘The set X;, then contains all the filled
holes. The set union of X; and A contains all the filled holes and their boundaries.The dilation in Eq. (9.5-2} would fill the entire area if left unchecked. However,
the intersection at each step with A® Jimits the result to inside the region of interest. This is our first example of how a morphological process can he conditioned to
meet a desired property. In the current application, it is appropriately called
conditional dilation, The rest of Fig. 9.15 illustrates further the mechanics of
Eq.(9.5-2). Although this example only has one hole. the concept clearly applies to
any finite number of holes, assuming that a point inside cach hole repion is given665abFIGURE 9.14(a) A simple
binary image, with
Is represented in
white. (b) Result
of usingEq. (95-4) with
the structuring
element inFig. 9.13(b).EXAMPLE 9.5:
Boundary
extraction by
morphological
processing.
9.5 ® Some Basic Morphological Algorithms 667Fig. 9.16(c) shows the result of filling all the spheres. Because it must be known
whether black points are background points or sphere inner points, fully automating this procedure requires that additional “intelligence” be built into
the algorithm. We give a fully automatic approach in Section 9.5.9 based on
morphological reconstruction. (See also Problem 9.23.) iw9.5.3 Extraction of Connected Components.
The concepts of connectivity and connected components were introduced in
Section 2.5.2. Extraction of connected components from a binary image is central to many automated image analysis applications, Let A be a set containing
one or more connected components, and form an array Xo (of the same size as
the array containing A) whose elements are Os (background values), except at
each location known to correspond to a point in each connected component in
A, which we set to 1 (foreground value). The objective is to start with X and
find all the connected components. The following iterative procedure accomplishes this objective:X= (%-)@B)NA k =1,2,3,... (9.5-3)where B is a suitable structuring element (as in Fig. 9.17). The procedure terminates when X;, = X,-1, with X, containing all the connected components                                                              a
bed
eftg
FIGURE 9.17 Extracting connected components, (a) Structuring element. (b) Array
containing a set with one connected component. (c) Initial array containing a 1 in the
region of the connected component. (d)-(g) Various steps in the iteration of Eq. (9.5-3).
668See Problem 9.24 for an
algorithm that does not
Tequire that a point in
each connected component de known a prioriEXAMPLE 9.7:
Using connected
components to
detect foreign
objects in
packaged food.a
b
edFIGURE 9.18(a) X-ray image
of chicken filet
with bone fragments.(b) Threshoided
image. (c) Image
eroded with a5 X § structuring
element of 1s.(d) Number of
pixels in the
connected components of {c).
{Image courtesy of
NTBElektronische
Geraete GmbH,
Diepholz,
Germany,
www.ntbxray.com.) Chopter $m Morphological Image Processingof the input image. Note the similarity in Eqs. (9.5-3) and (9.5-2), the only difference being the use of A as opposed to A‘. This is not surprising, because
here we are looking for foreground points, while the objective in Section 9.5.2
was to find background points.Figure 9.17 illustrates the mechanics of Eq. (9.5-3), with convergence being
achieved for k = 6, Note that the shape of the structuring element used is
based on 8-connectivity between pixels. If we had used the SE in Fig. 9.15,
which is based on 4-connectivity, the leftmost element of the connected component toward the bottom of the image would not have been detected because
it is 8-connected to the rest of the figure. As in the hole-filling algorithm,
Eq. (9.5-3) is applicable to any finite number of connected components contained in A, assuming that a point is known in cach.@ Connected components are used frequently for. automated inspection.
Figure 9.18(a) shows an X-ray-image of a chicken breast that contains bone
fragments. It is of considerable interest to be able to detect such objects in
processed food before packaging and/or shipping. In this particular case, the
density of the bones is such that their nominal intensity values are different
from the background. This makes extraction of the bones from the background    Connected — No. of pixels in
component counseled camp
tH VW
02 9
03 9
04 39
US 13306 ]
0? 1
ae w43
49 7
ire) Mi
it i}
2 9
138 9
}4 ee
iS 85
670 Chapter 9 #1 Morphological Image Processinga
béd
efs
hFIGURE 9.19(a) Structuring
elements. (b) Set
A. {c)-(f) Results
of convergence
with the
structuring
elements shown
in (a). (g} Convex
hull. (h) Convex
huli showing the
contribution of
each structuring
element.                                                                                                       | HB!
LZ Bg
I ee gt
a 4.) lil 8
+ mal
tp 4
CeEeHee   in A ifthe 3 X 3 region of A under the structuring element mask at that location matches the pattern of the mask. For a particular mask, a pattern match
occurs when the center of the 3 X 3 region in A is 0, and the three pixels under
the shaded mask elements are 1. The values of the other pixels in the 3 X 3 region do not matter. Also, with respect to the notation in Fig. 9.19(a), B’ is a
clockwise rotation of B‘! by 90°.Figure 9,19(b) shows a set A for which the convex hull is sought. Starting
with X$ = A resulted in the set in Fig, 9.19(c) after four iterations of Eq. (9.5-4).
Then, letting X3 = A and again using Eq. (9.54) resulted in the set in
Fig, 9.19(d) (convergence was achieved in only two steps in this case). The next
two results were obtained in the same way. Finaliy, forming the union of the
sets in Figs. 9.19(c), (d). (e), and (f) resulted in the convex hull shown in
Fig. 9.19(g). The contribution of each structuring element is highlighted in the
composite set shown in Fig. 9.19(h).One obvious shortcoming of the procedure just outlined is that the convex hull can grow beyond the minimum dimensions required to guarantee
9.5 © Some Basic Morphological Algorithmsa simple matter by using a single threshold (thresholding was introduced in
Section 3.1 and is discussed in more detail in Section 10.3). The result is the binary image in Fig. 9.18(b).The most significant feature in this figure is the fact that the points that remain are clustered into objects (bones), rather than being isolated, irrelevant
points. We can make sure that only objects of “significant” size remain by eroding the thresholded image. In this example, we define as significant any object
that remains after erosion with a 5 x 5 structuring element of 1s. The result of
erosion is shown in Fig. 9.18(c). The next step is to analyze the size of the objects that remain. We label (identify) these objects by extracting the connected
components in the image. The table in Fig. 9.18(d) lists the results of the extraction. There are a total of 15 connected components, with four of them being
dominant in size. This is enough to determine that significant undesirable objects are contained in the original image. If needed, further characterization
(such as shape) is possible using the techniques discussed in Chapter 11. a9.5.4 Convex Hull
A set A is said to be convex if the straight line segment joining any two points
in A lies entirely within A, The convex hull H of an arbitrary set S is the smallest convex set containing 5. The set difference H — S is called the convex deficiency of S. As discussed in more detail in Sections 11.1.6 and 11.3.2, the
convex hull and convex deficiency are useful for object description. Here, we
present a simple morphological algorithm for obtaining the convex hull, C(A),
of a set A.Let Bi,i = 1,2,3,4, represent the four structuring elements in Fig, 9.19(a).
The procedure consists of implementing the equation:= (Xp, @B)UA i= 1,2,3,4 and k= 1,2.3... (95-4)with X{, = A. When the procedure converges, (ie e., when Xi = X%_;), we Jet
Di = Xj. Then the convex hull of A is x
4
c(A4) = Up! (9.5-5)
71In other words, the method consists of iteratively applying the hit-or-miss
transform to A with B'; when no further changes occur, we perform the union
with A and call the result D?. The procedure is repeated with B? (applied to A)
until no further changes occur, and so on. The union of the four resulting Ds
constitutes the convex hull of A. Note that we are using the simplified implementation of the hit-or-miss transform in which no background match is required, as discussed at the end of Section 9.4.Figure 9.19 illustrates the procedure given in Eqs. (9.5-4) and (9.5-5).
Figure 9.19(a) shows the structuring elements used to extract the convex hull.
The origin of each element is at its center. The X entries indicate “don’t care”
conditions. This means that a structuring element is said to have found a match669
9.5 = Some Basic Morphological Algorithms 671convexity. One simple approach to reduce this effect is to limit growth so
that it does not extend past the vertical and horizontal dimensions of the
original set of points. Imposing this limitation on the example in Fig. 9.19 resulted in the image shown in Fig. 9.20, Boundaries of greater complexity can
be used to limit growth even further in images with more detail. For example, we could use the maximum dimensions of the original set of points along
the vertical, horizontal, and diagonal directions. The price paid for refinements such as this is addittonal complexity and increased computational requirements of the algorithm.9.5.8 ThinningThe thinning of a set A by a structuring element B, denoted A @ B, can be defined in terms of the hit-or-miss transform:A@ B= A-(A@B)
. = AN(A@ BY (9.5-6)As in the previous section, we are interested only in pattern matching with the
structuring elements, so no background operation is required in the hit-or-miss
transform. A more useful expression for thinning A symmetrically is based on
a sequence of structuring elements:{B} = {B', B, B,.:., B”} (9.5-7)where B’ is a rotated version of B'”!. Using this concept, we now define thinning by a sequence of structuring elements asA® {B} = ((...((A® B!)® B’),..) @ B") (9.5-8)The process is to thin A by one pass with B’, then thin the result with one pass
of B’, and so on, until A is thinned with one pass‘of B”. The entire process is
repeated until no further changes occur. Each individual thinning pass is performed using Eq. (9.5-6).                  FIGURE 9.20
Result of limiting
growth of the
convex hull
algorithm to the
maximum
dimensions of the
original set of
points along the
vertical and
horizonta)
directions.
672Chapter 9 @ Morphological Image Processingws oerSe Si 0"
aeFigure 9.21{a) shows a set of structuring elements commonly used for
thinning, and Fig. 9.21(b) shows a set A to be thinned by using the procedure just discussed. Figure 9.21(c) shows the result of thinning after one
pass of A with B', and Figs. 9.21(d) through (k) show the results of passes
with the other structuring elements. Convergence was achieved after the
second pass of B°. Figure 9.21(1) shows the thinned result. Finally, Fig.
9.21(m) shows the thinned set converted to m-connectivity (see Section
2.5.2) to eliminate multiple paths.9.5.6 Thickening
Thickening is the morphological dual of thinning and is defined by the expressionA@B = AU(A@B) (9.5-9)Origin                                             xa =       cr
+
i             mane ae
Ag, converted to
ybconnectivity.          FIGURE 9.2} (a) Sequence of rotated structuring elements used for thinning, (b) Set A.
(c) Result of thinning with the first element. (d)-{i} Results of thinning with the next
seven elements {there was no change between the seventh ood eighth elements).
G) Result of using the first four elements again. (1) Result a!.-r convergence. (m)}
Conversion to m-connectivity,
674 Chopter 9 8 Morphological Image Processingab
edFIGURE 9.23(a) Set A.{b) Various
positions of
maximum disks
with centers on
the skeleton of A.
(c) Another
maximum disk on
a different
segment of the
skeleton of A.(d) Complete
skeleton.  The skeleton of A can be expressed in terms of erosions and openings. That is,
it can be shown (Serra [1982]) thatK .
S(A) = SLA) (9.5-11)
&=0withS,{A) = (AGKB) — (AGKB)° B (9.5-12)where B is a structuring element, and (A © kB) indicates k successiv- erosions
of A:(AGKB) = ((.. (AG BOB)S...)OB) (9.5-13)k times, and K is the last iterative step before A erodes to an empty set. In
other words,
K = max{k|(A@kB) # @} (9.5-14)The formulation given in Eqs. (9.5-11) and (9.5-12) states that S(A) can be
obtained as the union of the skeleton subsets S,(A)}. Also, it can be shown that
A can be reconstructed from these subsets by using the equationK
A = J(5,(A) kB) (9.5-15)
k=0where (5,(A} ® kB) denotes k successive dilations of §,(A); that is,
(S,{A) ® KB) = ((...((S,{A) ® B) ® BY @ ...)B B) (9.5-16)
9.5 w@ Some Basic Morphological Algorithmswhere B is a structuring element suitable for thickening, As in thinning, thickening can be defined as a sequential operation:AQ@{B} = ((...(A© B')O B’)...) OB") (9.5-10)The structuring elements used for thickening have the same form as those
shown in Fig. 9.21(a), but with all 1s and Os interchanged. However, a separate
algorithm for thickening ig seldom used in practice. Instead, the usual procedure is to thin the background of the set in question and then complement the
result, In other words, to thicken a set A, we form C = A‘, thin C, and then
form C*. Figure 9.22 illustrates this procedure.Depending on the nature of A, this procedure can result in disconnected
points, as Fig. 9.22(d) shows. Hence thickening by this method usually is followed by postprocessing to remove disconnected points. Note from Fig. 9.22(c)
that the thinned background forms a boundary for the thickening process.
This useful feature is not present in the direct implementation of thickening
using Eq. (9.5-10), and it is one of the principal reasons for using background
thinning to accomplish thickening.9.5.7 SkeletonsAs Fig. 9.23 shows, the notion of a skeleton, S(A), of a set A is intuitively simple. We deduce from this figure that(a) If z is a point of $(A) and (D), is the largest disk centered at z and contained in A, one cannot find a larger disk (not necessarily centered at z)
containing (D), and included in A. The disk (D), is called a maximum
disk.{b) The disk (D), touches the boundary of A at two or more different places.     7
irH
“4
mi1!  Coo                            ab
cd 4
& L shad wt
FIGURE 9.22 (a) Set A. (b) Complement of A. (c) Result of thinning the complement
of A. (d) Thickened set obtained by complementing (c). {€) Fina] result, with no
disconnected points.         673
9.5 m Some Basic Morphological Algorithms@ Figure 9.24 illustrates the concepts just discussed. The first column
shows the original set {at the top) and two erosions by the structuring element B. Note that one more erosion of A would yield the empty set, so
K = 2 in this case. The second column shows the opening of the sets in the
first column by B. These results are easily expiained by the fitting characterization of the opening operation discussed in connection with Fig. 9.8.
The third column'simply contains the set differences between the first and
second columns. .The fourth column contains two partial skeletons and the final result (at
the bottom of the column). The final skeleton not only is thicker than it
needs to be but, more important, it is not connected. This result is not unexpected, as nothing in the preceding formulation of the morphological skeleton guarantees connectivity. Morphology produces an elegant formulation in
terms of erosions and openings of the given set. However, heuristic formulations such as the algorithm developed in Section 11.1.7 are needed if, as is
usually the case, the skeleton must be maximally thin, connected, and minimally eroded.S,(A)@ kBK
US,(A) BKB
kad                                                                                                               675EXAMPLE 9.8:
Computing the
skeleton of a
simple figure.FIGURE 9.24
Implementation
of Eqs. (9.5-11)
through {9.5-15).
The original set is
at the top left, and
its morphological
skeleton is at the
bottom of the
fourth column.
‘The reconstructed
Set is at the
botiom of the
sixth column.
9.5 ® Some Basic Morphological Algorithms    aaa     B, B, B’, B* (rotated 90°)bat is me
I tit) B®, B, B’, BS (rotated 90°)                       TTI
7                                                       branches removed. To do so first requires forming a set % containing all end
points in X, [Fig. 9.25(e)]:8% = UK © B*) (9.5-18)
k=twhere the B* are the same end-point detector slrown in Figs. 9.25(b) and (c).The next step is dilation of the end points three times, using set A as a delimiter:%& = (%8H)NA (9.5-19)where A is a3 X 3 structuring element of 1s and the intersection with A is
applied after each step. As in the case of region filling and extraction of connected components, this type of conditional dilation prevents the creation
of 1-valued elements outside the region of interest, as evidericed by the result shown in Fig. 9.25(f). Finally, the union of X, and X, yields the desired
result,X= XUX, (9.5-20)in Fig. 9.25(g).
In more complex scenarios, use of Eg. (9.5-19) sometimes picks up the
“tips” of some parasitic branches. This condition can occur when the end677onead
fsFIGURE 9.25{a) Original
image. (b) and(c) Structuring
elements used for
deleting end
points. (d} Resultof three cycles of
thinning. (e) Endpoints of (d).(f) Dilation of endpoints conditioned on (a).(g) Pruned image.Equation (9.5-19) is the
basis for morphologicat
reconstruction by dijatian, as explained in the
neal section.
676 Guapter 9 mt Morphological Image Processing‘We may define an end
Posnt as the center point
of a3 X 3 region that
satisfies any of the
arrangements inFigs. 9.25(b) or (c).The fifth column shows S)(A), 5$,(4)@8, and (S,(A)®2B) =
(S,(A) ® B) @ B. Finally, the last column shows reconstruction of set A, which,
according to Eq. (9.5-15), is the union of the dilated skeleton subsets shown in the
fifth column. a9.5.8 PruningPruning methods are an essential complement to thinning and skeletonizing
algorithms because these procedures tend to leave parasitic components that
need to be “cleaned up” by postprocessing. We begin the discussion with a
pruning problem and then develop a morphological solution based on the material introduced in the preceding sections. Thus, we take this opportunity to illustrate how to go about solving a problem by combining several of the
techniques discussed up to this point.A common approach in the automated recognition of hand-printed characters is to analyze the shapé of the skeleton of each character. These skeletons
often are characterized by “sptirs” (parasitic components). Spurs are caused
during erosion by non uniformities in the strokes composing the characters.
We develop a morphological technique for handling this problem, starting
with the assumption that the length of a parasitic component does not exceed
a specified number of pixels.Figure 9.25(a) shows the skeleton of a hand-printed “a.” The parasitic component on the leftmost part of the character is illustrative of what we are interested in removing. The solution is based on suppressing a parasitic branch
by successively eliminating its end point. Of course, this also shortens (or eliminates) other branches in the character but, in the absence of other structural
information, the assumption in this example is that any branch with three or
less pixels is to be eliminated. Thinning of an input set A with a sequence of
structuring elements designed'to detect only end points achieves the desired
result, That is, letX, = A@{B} (9.5-17)where {B} denotes the structuring element sequence shown in Figs. 9.25(b)
and (c) [see Eq. (9.5-7) regarding siructuring-element sequences]. The sequence of structuring elements consists of two different structures, each of
which is rotated 90° for a total of eight elements. The X in Fig. 9.25(b) signifies a “don’t care” condition, in the sense that it does not matter whether
the pixei in that location has a value of 0 or 1. Numerous results reported in
the literature on morphology are based on the use of a single structuring element, similar to the one in Fig. 9.25(b), but having “don’t care” conditions
along the entire first column. This is incorrect. For example, this element
would identify the point located in the eighth row, fourth column of Fig.
9.25(a) as an end point, thus eliminating it and breaking connectivity in the
stroke.Applying Eq. (9.5-17) to A three times yields the set X; in Fig. 9.25(d). The
next step is to “restore” the character to its original form, but with the parasitic
678Chepter 9 mt Morphological Image Processingpoints of these branches are near the skeleton. Although Eq. (9.5-17) may
eliminate them, they can be picked up again during dilation because they are
valid points in A. Unless entire parasitic elements are picked up again (a rare
case if these elements are short with respect to valid strokes), detecting and
eliminating them is easy because they are disconnected regions.A natural thought at this juncture is that there must be easier ways to solve
this problem. For example, we could just keep track of all deleted points and
simply reconnect the appropriate points to all end points left after application
of Eq. (9.5-17). This option is valid, but the advantage of the formulation just
presented is that the use of simple morphological constructs solved the entire
problem. In practical situations when a set of such tools is available, the advantage is that no new algorithms have to be written. We simply combine the
necessary morphological functions into a sequence of operations,9.5.9 Morphological Reconstruction
The morphological concepts discussed thus far involve an image and a structuring element. In this section, we discuss a powerful morphological transformation called morphological reconstruction that involves two images and a
structuring element. One image, the marker, contains the starting points for
the transformation. The other image, the mask, constrains the transformation.
The structuring element is used to define connectivity.’Geodesic dilation and erosion
Central to morphological reconstruction are the concepts of geodesic dilation
and geodesic erosion, Let F denote the marker image and G the mask image.
It is assumed in this discussion that both are binary images and that FCG.
The geodesic dilation of size 1 of the marker image with respect to the mask,
denoted by Di!)(F), is defined-asDOF) = (F@B)NG (9.5-21)
where M denotes the set intersection (here M may be interpreted as a logicalAND because the set intersection and logical AND operations are the same
for binary sets). The geodesic dilation of size n of F with respect to G is de
fined as
DYMF) = DY De-"(F)] (9.5-22)with D''(F) = F. In this recursive expression, the set intersection in Eq. (9.5-21)
is performed at each step.t Note that the intersection operator guarantees that’ ‘In much of the literature on morphological reconstruction, the structuring element is tacitly assumed Lo
be isotropic and typically is called an efemeniary isetrapic structuring element. In the coutext of this
chapter, an example of such an SE is simply a3 X 3 array of Ls with the origin at the center.*Although it is more intuitive to develop morphological-reconstruction methods using recursive formuJavions {as we du here), their practical implementation typically is based on more computationally etficient algorithms (see, for example. Vincent [1993] and Soille [2003]. All image-based examples in this
secli on were generated using such algorithms.
9.5 m Some Basic Morphological Algorithms 679                                                       mi Mask, Gmask G will limit the growth (dilation) of marker F. Figure 9.26 shows a simple example of a geodesic dilation of size 1. The steps in the figure are a direct
implementation of Eq. (9.5-21).Similarly, the geodesic erosion of size 1 of marker F with respect to mask G
is defined asEW(F) = (F © B) U G (9.5-23)where U denotes set union (or OR operation). The geodesic erosion of size n
of F with respect to G is defined asEF) = EM Ee OF)| (9.5-24)with EQ ') = F. The set union operation in Eq. (9.5-23) is performed at each
iterative step, and guarantees that geodesic erosion of an image remains
greater than or equal to its mask image. As expected from the forms in Eqs.
(9.5-21) and (9.5-23), geodesic dilation and erosion are duals with respect to
set complementation (see Problem 9. 29). Figure 9.27 shows a simple example
of geodesic erosion of size 1. The steps in the igure are a direct implementa
                                                         tion of Eq. (9.5-23). >
aeaee f
“4 L. r
ree ty
Hef
- 4. PT
mn, Pre
H i CHE Marker, F L aa TT il
Loot Marker eroded by & Geodesic erosion, EXP)
r Cob
He It
Pee           A
|
k_i Mask, GFIGURE 9.26
Iitustration of
geodesic dilation.FIGURE 9.27
Illustration of
geodesic erosion.
9.5 & Sume Basic Morphological AlgorithmsReconstruction by dilation and erosion are duals with respect to set complementation (see Problem 9.30),Sample applicationsMorphological reconstruction has a broad spectrum of practical applications,
each determined by the selection of the marker and mask images, by the structuring elements used, and by combinations of the primitive operations defined
in the preceding discussion, The following examples illustrate the usefulness of
these concepts.Opening by reconstruction: In a morphological opening, erosion removes
small objects and the subsequent dilation atiempts to restore the shape of objects that remain. However, the accuracy of this restoration is highly dependent
on the similarity of the shapes of the objects and the structuring element used.
Opening by reconstruction restores exactly the shapes of the objects that remain
after erosion. The opening by reconstruction of size » of an image F is defined as
the reconstruction by dilation of F from the erosion of size # of F; that is,OW) = REFS nB)| (95-27)where (F © nB) indicates n erosions of F by B, as explained in Section 9.5.7.
Note that F is used as the mask in this application. A similar expression can be
written for closing by reconstruction (see Table 9.1).Figure 9.29 shows an example of opening by reconstruction. In this illustration, we are interested in extracting from Fig. 9.29(a} the characters that
contain long, vertical strokes, Opening by reconstruction requires at least
one erosion, so we perform that step first. Figure 9.29(b) shows the erosion 4.
cadFIGURE 9.29 (a) Text image of size 918 X 2018 pixels. The approximate average beight
of the tall characters is 50 pixels. (b} Erosion of (a) with a structuring element of size
51 X 1 pixels. (c) Opening of (a) with ihe same structuring element. shown for
reference. (d) Result of opening by reconstruction.681
680 Chapter 9 m Morphological Image Processingabed
ef 8hFIGURE 9.28
Illustration of
morphological
reconstruction by
dilation. F, G, B
and DW(F) are
from Fig. 9.26.Geodesic dilation and erosion of finite images always converge after a finite
number of iterative step because propagation or shrinking of the marker
image is constrained by the mask.Morphological reconstruction by dilation and by erosion
Based on the preceding concepts, morphological reconstruction by dilation of amask image G from a marker image F, denoted R8(F), is defined as the geodesic dilation of F with respect to G, iterated until stability is achieved; that is,R&(F) = DY(F) (95-25)with k such that D®(F) = D&*YCF),Figure 9.28 illustrates reconstruction by dilation. Figure 9.28(a) continues
the process begun in Fig. 9.26, that is, the next step in reconstruction after obtaining DS) (F) is to dilate this result and then AND it with the mask G to yieldDF), as Fig. 9.28(b) shows “Dilation of pe PF) and masking with G then
yields D@(F), and so on. This procedure is repeated until stability is
reached. St we carried this example one more step, we would find that
D2{F) = DEF). so ithe morphologically reconstructed image by dilation is
given by RAF) = pe (F), as indicated in Eq. (9.5-25). Note that the reconstructed image in this case is identical to the mask because F contained a single l-valued pixel (this is analogous to convolution of an image with an
impulse, which simply copies the image at the location of the impulse, as explained in Section 3.4.2).In a similar manner, the merphological reconstruction by erosion of a mask
image G from a marker image F, denoted RE(F), is defined as the geodesic
erosion of F with respect to G, iterated until stability; that is,RE(F) = EP CF) (9.5-26)with & such that EF) = ESF). As an exercise, you should generate a
figure similar to Fig. 9.28 for morphological reconstruction by erosion. r is a sO ie en Oa na OU Oe im
rt
mi                                                                     of UB) ditared by 8 or)
682  Chepter9 w Morphological Image ProcessingabcdefgFIGURE 9.30
Illustration of
hole filling on a
simple image.of Fig. 9.29(a) with a structuring element of length proportional to the average height of the tall characters (51 pixels) and width of one pixel. For the
purpose of comparison, we computed the opening of the image using the
same structuring element. Figure 9.29(c) shows the result. Finally, Fig. 9.29(d)
is the opening by reconstruction (of size 1) of F [ie., ovr) given in Eq.
(9.5-27), This result shows that characters containing long vertical strokes
were restored accurately; all other characters were removed.Filling holes: In Section 9.5.2, we developed an algorithm for filling holes
based on knowing a starting point in each hole in the image. Here, we develop
a fully automated procedure based on morphological reconstruction. Let
I{x, y) denote a binary image and suppose that we form a marker image F that
is 0 everywhere, except at the image border, where it is set to 1 — /; that is,1-— its, if (x, y) is on the border of J
Fon y) = {; : i. » other (95-28)
Then
H = [RR(F)| (9.5-29)is a binary image equal to / with all holes filled.Let us consider the individual components of Eq. (9.5-29) to see how this
expression in fact leads to all holes in an image being filled. Figure 9.30{a)
shows a simple image / containing one hole, and Fig. 9.30(b) shows its complement. Note that because the complement of / sets all foreground (1-valued)
pixels to background (Q-valued) pixels, and vice versa, this operation in effect
builds a “wall” of Os around the hole. Because /* is used as an AND mask, all
we are doing here is protecting all foreground pixels (including the wall
around the hole) from changing during iteration of the procedure. Figure
9.30(c) is array F formed according to Eq. (9.5-28) and Fig. 9.30(d) is F dilated
with a3 x 3 SE whose elements are ail 1s. Note that marker F has a border of
1s (except at locations where / is 1), so the dilation of F of the marker points
starts at the border and proceeds inward. Figure 9.30(e) shows the geodesic dilation of F using 7° as the mask. As was just indicated, we see that all locations
in this result corresponding to foreground pixels from J are 0, and that this is
true now for the hole pixels as well. Another iteration will yield the same result which, when complemented as required by Eq. (9.5-29), gives the result in
Fig. 9.30(f). As desired, the hole is now filled and the rest of image 7 was unchanged. The operation HM I‘ yields an image containing 1-valued pixels in
the locations corresponding to the holes in /, as Fig. 9.30(g) shows.
9.5 & Some Basic Morphological Algorithmsan = ponents or broken connection paths There is no pot
tion past the level of detail required to identify those 1Segmentation of nontrivial images is one of the mos
processing, Segmentation accuracy determines the ev
of computerized analysis procedures For this reason, ¢
be taken to improve ihe probability of rugged segment
such ah industrial inspection applications, at least some
the environments possihic at times. The experienced i
denigner invariably pays considerable attention to suctHit theseole nay my  Vis pv@babti.!
an epe Figure 9.31 shows a more practical example. Figure 9,31(b) shows the complement of the text image in Fig. 9.31 (a), and Fig. 9.31(c) is the marker image,
F, generated using Eg. (9.5-28). This image has a border of 1s, except at locations corresponding to 1s in the border of the original image. Finally, Fig. 9.31(d)
shows the image with ail the holes filled.Border clearing: The extraction of objects from an image for subsequent
shape analysis is a fundamental task in automated image processing. An algorithm for removing objects that touch (.e., are connected to) the border is a
useful tool because {]) it can be used to screen images so that only complete
objects remain for further processing, or (2) it can be used as a signal that partial objects are present in the field of view. As a final illustration of the concepts introduced in this section, we develop a bordey-clearing procedure based
on morphological reconstruction. In this application. we use the original image
as the mask and the following marker image:
5 Hxoy) if (x) is on the border of 7
F(x y) = 4 otherwise >, (9.5-30)
The border-Eclearing algorithm first computes the morphological reconstruction RP(F) (which simply extracts the objects touching the border) and then
computes the difference
X=] ~ RPP)to obtain an image, X. with no objects touching the border.(9.5-31) 683ab
edFIGURE 9.31(a) Text image of
size 918 X 2018
pixels. (b) Complement of (a} for
use as a mask
image. (c) Marker
image. (d) Result
of hole-filling
using Eq, (9.5-29),ab
FIGURE 9.32
Border — clearing.(a) Marker image.
{b) Image with no
objects touching
the berder. The
onginal tnage ts
2.204),
EquationClosing A¥B = (A@B)OB
Hit-or-miss A@B=(AOB)N(AGB)
transform = (A@B) - (A@B,)
Boundary B(A) = A - (AGB)
extraction
Hole filling X= (Xp) BBN AY
k = 1,2,3,...
Connected Xe = (X-1 BBY NA;
componenis x = 1,2,3,... .
Convexhull X%, = (X4.,@ BU A;
i= 1,2,3,4
k= 1,2,3,...;
Xo = A; and
D = Xvonv
Thinning A@B=A—({(A@B)
= AN(A®@ BY
A®@ {B} =
(C... (A&B!) @ B’)...)@B")
{B} = (B!, BY B’,..., B"}
Thickening AO B= AU(A@B)AQ@{B} =
((...(A@B') © B?...)@ B")K
Skeletons S(A) = S,(A)
ka  K
SCA) = Uta OKB)
~ [(A@KB) & B)}Reconstruction of A:K
A = (J(Si(A) ® kB)
*=0{The Roman numerals refer to the
structuring elements in Fig. 9.33.)9.5 m Some Basic Morphological Algorithms 
  
 CommentsSmoothes contours, fuses
narrow breaks and long thin
gulfs, and eliminates small
holes. (1)The set of points (coordinates)
at which, simultaneously, B,
found a match (“hit”) in A
and 8, found a match in A° 
   
   
   
  
 
 
   
  
 
  Set of points on the boundary
of set A. (1)Fills holes in A; Xp = array of
Qs with a 1 in each hole. (II) Finds connected components
in A; Xp = array of Os witha
1 in each connected
component. (J)" Finds the convex hull C(A) ofset A, where “cony” indicates
convergence in the sense that
X= X47. (I)Thins set A. The first two
equations give the basic definition of thinning. The last
equations denote thinning
by a sequence of structuring
elements. This method is
normally used in practice. (IV)Thickens set A. (See preceding
comments on sequences of
structuring elements.) Uses 1V
with Os and 1s reversed.Finds the skeleton S(A) of set
A. The last equation indicates
that A can be reconstructed
from its skeleton subsets
S,(A). In all three equations,
K is the value of the iterative
step after which the set A
erades to the empty set. The
notation (A © kB) denotes the
kth iteration of successive
erosions of A by B. {I)  (Continued)685TABLE 9,1
(Continued)
684 Chapter 9 # Morphological Image ProcessingFIGURE 9.33 Five
basic types of
structuring
elements used for
binary morphology. The origin of
each element is at
its center and the
X’s indicate
“don't care”
values,TABLE 9.1
Summary of
morphological
operations and
their properties.          *[x]
HE Bii=1,2,3,4 [KET B'f=1,2,...,8
<l*[x] (rotate 90°) EEE] (rotate 45°)Ur IV
bry]. Eq .
FIST] Bi i= 1,2,3,4 Fe Bi i=S,6,7,8
xT] (rotate 90°) |_|"] (rotate 90°)gp eeVvAs an example, consider the text image again. Figure 9.32(a) in the previous
page shows the reconstruction R?(F) obtained using a3 X 3 structuring element of all 1s (note the objéect&touching the boundary on the right side), and
Fig. 9.32(b) shows image X, computed using Eq. (9.5-31). If the task at hand
were automated character recognition, having an image in which no characters
touch the border is most useful because the problem of having to recognize
partial characters (a difficult task at best) is avoided.9.5.38 Summary of Morphological Operations on Binary ImagesTable 9.1 summarizes the morphological results developed in the preceding
sections, and Fig. 9.33 summarizes the basic types of structuring elements used
in the various morphological processes discussed thus far. The Roman numerals in the third column of Table 9.1 refer to the structuring elements in Fig. 9.33. Comments
(The Roman numerals refer to the
Operation Equation structuring elements in Fig. 9.33.)
Translation (B), = {wlw = b + z, Translates the origin
for be B} of B to point z.
Reflection B= {wlw = —b, for be B} Reflects all elements of
B about the origin of this set.
Complement A’ = {w|we A} Set of points not in A.
Difference A- B= {wlwe A, we B} Set of points that belong to A
= ANB but not to B.
Dilation A@B = {2|(B)NA +O} “Expands” the boundary
of A. (1)
Erosion AGB= {<|(B), c A} “Contracts” the boundary of
A. (I)
Opening AcB=(AOB)OB Smoothes contours, breaks narrow isthmuses, andeliminates smal} islands and
sharp peaks. {I)  (Continued)
686 Chapter? Morphological Image ProcessingTABLE 9.1
(Continued)Comments
(The Roman numerals refer to thestructuring elements in Fig. 9.33.)  Equation
Pruning X, = A@{B}
8
X, = U(X, @ BY)
k=l
X3=(X%,BH)NA
Xy= X UX;
Geodesic DF) =(F@B)NG
dilation of
size 1
Geodesic D@ wr) = DY [DoF];
dilation of D(F) = F
i Gc
size n eo
~ ~
Geodesic EK F) = (FOB)UG
erosion of
size 1
Geodesic EMF) = EXEL (F)};
erosion of EY F)=F
size n GMorphological RE(F) = DY(F)
reconstruction
by dilationMorphological RE(F) = ES)(F)
reconstruction
by erosionOpening by
reconstructionClosing by
reconstruction CR(F) = RE[(F @nB)}OW (F) = REF @ nB)]
Hole fillingH = [RACY]Border clearing X¥ = / ~ RP(F)X, is the result of pruning set A.
The number of times that the
first equation is applied to
obtain X, must be specified.
Structuring elements V are used
for the first two equations, In
the third equation H denotes
structuring element I.F and G are called the marker
and mask images, respectively.& is such that
DOF) = DEF)k is such that
EGF) = EGMOF)(F O nB) indicates n
erosions of F by 8.{F © nB) indicates n
dilations of F by B.H is equal to the input
image /, but with all holes
filled. See Eq. (9.5-28) for
the definition of the marker
image F.X is equal to the input
image /, but with all objects
thaftouch (are connected
to) the boundary removed.
See Eq. (9.5-30) for the
definition of the marker
image F.
9.6 m Gray-Scale Morphology 687| 9.6 | Gray-Scale MorphologyIn this section, we extend to gray-scale images the basic operations of dilation,
erosion, opening, and closing. We then use these operations to develop several
basic gray-scale morphological algorithms.Throughout the discussion that follows, we deal with digital functions of the
form f(x, y) and b(x, y), where f(x, y) is a gray-scale image and b(a, y) is a. structuring element. The agsumption is that these functions are discrete in the
sense introduced in Section 2.4.2. That is, if Z denotes the set of real integers,
then the coordinates (x, y) are integers from the Cartesian product Z? and f
and b are functions that assign an intensity value (a real number from the set
of real numbers, R) to each distinct pair of coordinates (x, y). If the intensity
levels are integers also, then Z replaces R.Structuring elements in gray-scale morphology perform the same basic
functions as their binary counterparts: They are used as “probes” to examine a
given image for specific properties. Structuring elements in gray-scale morphology belong to one of two categories: nonflat and flat. Figure 9.34 shows an
example of each. Figure 9.34(a) is a hemispherical gray-scale SE shown as an
image, and Fig. 9.34(c) is a horizontal intensity profile through its center.
Figure 9.34(b) shows a flat structuring element in the shape of a disk and
Fig. 9.34(d) is its corresponding intensity profile (the shape of this profile explains the origin of the word “flat”). The elements in Fig. 9.34 are shown as
continuous quantities for clarity; their computer implementation is based on
digital approximations (¢.g., see the rightmost disk SE in Fig. 9.2). Due to a
number of difficulties discussed later in this section, gray-scale SEs are used
infrequently in practice. Finally, we mention that, as in the binary case, the origin of structuring elements must be clearly identified. Unless mentioned otherwise, all the examples in this section are based on symmetrical, flat
structuring elements of unit height whose origins are at the center. The
reflection of an SE in gray-scale morphology is as defined in Section 9.1,
and we denote it in the following discussion by b(x, y) = b(~x — y).>abcdFIGURE 9.34
Nonflat and fiat
structuring
Nonflat SE. Fiat SE elements, and
corresponding
horizontal
intensity profiles
through their
center. All
examples in this
section are based
on flat SEs. Intensity profile Intensity profile
9.6 m Gray-Scale Morphology 689 abc
FIGURE 9.35 (a) A gray-scale X-ray image of size 448 x 425 pixels. (b) Erosion using a
flat disk SE with a radius of two pixels, (c} Dilation using the same SE, (Original image
courtesy of Lixi, Inc.)image. The reason is that the black dots were originally larger than the white
dots with respect to the size of the SE. Finally, note that the background of the
dilated image is slightly lighter than that of Fig. 9.35(a). i:Nonflat SEs have gray-scale values that vary over their domain of definition. The erosion of image f by nonflat structuring element, ba, is defined as[f Ody |x, y) = cin, {fC + sy +1) — byls.9} (9.6-3)Here, we actually subtract values from f to determine the erosion at any point.
This means that, unlike Eq, (9.6-1), erosion using a nonflat SE is not bounded in general by the values of f, which can present problems in interpreting results, Gray-scale SEs are seidom used in practice because of this, in addition to
potential difficulties in selecting meaningful elements for by, and the added
computational burden when compared with Eq. (9.6-1).In a similar manner, dilation using a nonflat SE is defined as[f ® dy |(x. ») = max {fx = sy — 9) + b(s,0)} (9.6-4)The same comments made in the previous paragraph are applicable to dilation
with nonflat SEs. When all the elements of by are constant (i.e., the SE is flat),
Egs. (9.6-3) and (9.6-4) reduce to Eqs. (9.6-1) and (9.6-2). respectively, within a
scalar constant equal to the amplitude of the SE.AS in the binary case, erosion and dilation are duals with respect to function
complementation and reflection; that is,(f 2d)(r. 9) = (BAM ay)where f° = ~ f(x, y)) and bs b(—x, — 3). The same expression holds for non
flat structuring elements, Except as needed for clarity, we simplify the notation
in the following discussion by omitting the arguments of ail functions, in which
case the preceding equation is written as(FObY » (Ff Bb) (9.6-5)
688 Chapter? # Morphological Image ProcessingEXAMPLE 9.9:
Illustration of
gray-scale erosion
and dilation.9.6.1 Erosion and DilationThe erosion of f by a flat structuring element b at any location (x, y) is defined
as the minimum value of the image in the region coincident with b when the
origin of b is at (x, y). In equation form, the erosion at (x, y) of an image f by
a structuring element d is given by[Feb], ») = min {f(x + y+ 9} (9-6-1)where, in a manner similar to the correlation procedure discussed in Section
3.4.2, x and y are incremented through ali values required so that the origin of
b visits every pixel in f. That is, to find the erosion of f by 5, we place the origin of the structuring element at every pixel location in the image. The erosion
at any location is determined by selecting the minimum value of f from all the
values of f contained in the region coincident with 5. For example, if b is a
square structuring element of size 3 Xx 3, obtaining the erosion at a point requires finding the minimum.of the nine values of f contained in the 3 x 3 region defined by 6 when its origin is at that point.Similarly, the dilation of f by a flat structuring element b at any location (x, y)
is defined as the maximum value of the image in the window outlined by 6 when
the origin of b is at (x, y). That is,[f@d]x,y) = max {fx - 8, y ~ Of (9.6-2)
where we used the fact stated earlier that b = b(—x, —y). The explanation of
this equation is identical to the explanation in the previous paragraph, but
using the maximum, rather than the minimum, operation and keeping in mind
that the structuring element is reflected about its origin, which we take into account by using {~s, ~¢) in the argument of the function. This is analogous to
spatial convolution, as explained in Section 3.4.2.M Because gray-scale erosion with a flat SE computes the minimum intensity
value of f in every neighborhood of (x, y) coincident with b, we expect in general that an eroded gray-scale image will be darker than the original, that the
sizes (with respect to the size of the SE) of bright features will be reduced, and
that the sizes of dark features will be increased. Figure 9.35(b) shows the erosion of Fig. 9.35(a) using a disk SE of unit height and a radius of two pixels. The
effects just mentioned are clearly visible in the eroded image. For instance,
note how the intensities of the small bright dots were reduced, making them
barely visible in Fig. 9.35(b), while the dark featuyes grew in thickness. The
general background of the eroded image is slightly darker than the background of the original image. Similarly, Fig. 9.35(c) shows the result of dilation
with the same SE. The effects are the opposite of those obtained with erosion.
The bright features were thickened and the intensities of the dark features
were reduced. Note in particular how the thin black connecting wires in the
left, middle, and right, bottom of Fig. 9.35(a) are barely visible in Fig. 9.35(c).
The sizes of the dark dots were reduced as a result of dilation but, unlike the
eroded small white dots in Fig. 9.35(b), they still are easily visible in the dilated
690 Ghapter9 m Morphological Image ProcessingAlthough we deal with
flat SEs in the examples
in the remainder of this
section, the concepts discussed are applicable
also to nonflat structuring elements.Sometimes opening and
closing are illustrated by
rolling a circle on the
under and upper sides of
acurve, In 3-D, the citcle becomes a sphere
and the resulting procedures are called teifinybatt algorithms.Similarly,Uf @b)° = (f°) (9.6-6)
Erosion and dilation by themselves are not particularly useful in gray-scale
image processing. As with their binary counterparts, these operations become
powerful when used in combination to derive higher-level algorithms, as the
material in the following sections demonstrates,9.6.2 Opening and Closing
The expressions for opening and closing gray-scale images have the same form
as their binary counterparts. The opening of image f by structuring element 5,
denoted f © 3, is
feob=(fOb) eb (9.6-7)
As before, opening is simply the erosion of f by b, followed by a dilation of the
result with 5, Similarly, the closing of f by b, denoted f - b, is
f¥b=(f@b)Ob
The opening and closing for gray-scale images are duals with respect to complementation and SE reflection:Uf ¥by = feeb(9.6-8)(9.6-9)and
(f ° by = fi ¥6 (9.6-10)Because f* = —f(x, y), Eq. (9.6-9) can be written also as —(f ¥6) = (-f ¢ é)
and similarly for Eq. (9.6-10).Opening and closing of images have a simple geometric interpretation. Suppose that an image function f(x, y) is viewed as a 3-D surface; that is, its intensity values are interpreted as height values over the xy-plane, as in Fig. 2.18(a).
Then the opening of f by b can be interpreted geometrically as pushing the
structuring element up from below against the undersurface of f. At each location of the origin of b, the opening is the highest value reached by any part
of 6 as it pushes up against the undersurface of f. The complete opening is
then the set of all such values obtained by having the origin of 5 visit every
(x, y) coordinate of f.Figure 9.36 iflustrates the concept in one dimension. Suppose that the
curve in Fig. 9.36(a) is the intensity profile along a single row of an image.
Figure 9.36(b) shows a flat structuring element in several positions, pushed up
against the bottom of the curve. The solid curve in Fig. 9.36(c) is the complete
opening, Because the structuring element is too large to fit completely inside
the upward peaks of the curve, the tops of the peaks are clipped by the opening, with the amount removed being proportional to how far the structuring
element was able to reach into the peak. In general, openings are used to remove small, bright details, while leaving the overall intensity levels and larger
bright features relatively undisturbed.
9.6 & Gray-Scale Morphology 691   + Intensity profile   Figure 9.36(d) is a graphical illustration of closing. Observe that the structuring element is pushed down on top of the curve while being translated to all
locations. The closing, shown in Fig. 9.36(e), is constructed by finding the towest points reached by any part of the structuring element as it slides against the
upper side of the curve.The gray-scale opening operation satisfies the following properties:(a) fo b-lf(b) If fitfy, then (f; © b) (fy © b)@) Febseb=febThe notation er is used to indicate that the domain of ¢ is a subset of the do
main ofr, and also that e(x, y) = r(x, y) for any (x, y) in the domain of e.
Similarly, the closing operation satisfies the following properties:(a) ff eb
(b) If f; tf, then (f, ¥b)A1(f2 ¥5)
{c) (f ¥5) ¥b = f ¥bThe usefulness of these properties is similar to that of their binary counterparts.B Figure 9.37 extends to 2-D the 1-D concepts illustrated in Fig, 9.36. Figure
9.37(a) is the same image we used in Example 9.9, and Fig. 9.37(b) is the opening
obtained using a disk structuring element of unit height and radius of 3 pixels. As
expected, the intensity of all bright features decreased, depending on the sizes of
the features relative to the size of the SE. Comparing this figure with Fig. 9.35(b),
we see that, unlike the result of erosion, opening had negligible effect on the dark
features of the image, and the effect on the background was negligible. Similarly,
Fig, 9.37(c) shows the closing of the image with a disk of radius 5 (the small roundoCRoTHFIGURE 9.36
Opening and closing in one dimension. (a) Original
1-D signal. (b) Fiat
structuring
element pushed up
underneath the
signal,{c) Opening.(d) Flat structuring
element pushed
down along the top
of the signal.(e) Closing,EXAMPLE 9.10:
Illustration of
gray-scale
opening and
closing.
692  Chopter9 @ Morphological Image Processing    wBE
FIGURE 9.37 (a) A gray-scale X-ray image of size 448 x 425 pixels. (b) Opening using
a disk SE with a radius of 3 pixels. (c) Closing using an SE of radius 5.black dots are larger than the smail white dots, so a larger disk was needed to
achieve results comparable to the opening). In this image, the bright details and
background were relatively unaffected, but the dark features were attenuated,
with the degree of attenuation being dependent on the relative sizes of the features with respect to the SE. ri9.6.3 Some Basic Gray-Scale Morphological AlgorithmsNumerous morphological techniques are based on the gray-scale morphological concepts introduced thus far. We illustrate some of these algorithms in the
following discussion,Morphological smoothingBecause opening suppresses bright details smaller than the specified SE, and
closing suppresses dark details, they are used often in combination as
morphological filters for image smoothing and noise removal. Consider
Fig. 9.38(a), which shows an image of the Cygnus Loop supernova taken in
the X-ray band (see Fig. 1.7 for details about this image). For purposes of the
present discussion, suppose that the central light region is the object of interest and that the smaller components are noise. The objective is to remove the
noise. Figure 9.38(b) shows the result of opening the original image with a flat
disk of radius 2 and then closing the opening with an SE of the same size.
Figures 9.38(c) and (d) show the resulls of the same operation using SEs of
radii 3 and 5, respectively. As expected. this sequence shows progressive removal of small components as a function of SE size. In the last result, we see
that the object of interest has been extracted. The noise components on the
lower side of the image could not be removed completely because of their
density.The results in Fig. 9.38 are based on opcning the original image and then
closing the opening. A procedure used sometimes is to perform alternating sequential filtering. in which the opening—ciosing sequence starts with the original image, but subsequent steps perform the opening and closing on the results
9.6 # Gray-Scale Morphology 693 of the previous step, This type of filtering is useful in automated image analysis,in which results at each step are compared against a specified metric, Generally, this approach produces more blurring for the same size SE than the
method illustrated in Fig. 9.38.Morphological gradientDilation and erosion can be used in combinatjon with image subtraction to obtain the morphological gradient of an image, @enoted by g, where§ = Bb) — (fed)The dilation thickens regions in an image and the erosion shrinks them. Their
difference emphasizes (he boundaries between regions. Homogenous arcas
are not affected (as long as the SF. is relatively smal) so the subtraction operation tends to eliminate them. The net result is an image in which the edges are
enhanced and the contribution of the homogeneous areas are suppressed. thus
producing a “derivative-like” (gradient) effect.Figure 9.39 shows an example, Figure 9.39(a) is a head CT scan. and the
next two figures are the opening and closing with a3 * 2SE of all fs. Note the
thickening and shrinking just mentioned. Figure 9.39(d) is the morphological
gradient obtained using Eq. (¥-6-11}. in which the boundaries between regions
are clearly delineated. as expected of 4 2-1 derivative image(9.6-11)ab
cdFIGURE 9.38(a) 566 X 566
image of the
Cygnus Loop
supernova, taken
in the X-ray band
by NASA's
Hubble Telescope.
(b)-(d) Results of
performing
opening and
closing sequences
on the original
image with disk
structuring
elements of radii,
l,3.and 5,
respectively.
{Original image
courtesy ofNASA.)See Seetion 3.6.4 for a
cletinition of the invage
gritient
694 Chapter 9 Morphological Image Processingab
edFIGURE 9.39{a) $12 x 512
image of a head
CT scan.(b) Dilation.(c) Erosion.(d) Morphological
gradient, computed as the
difference between (b) and (c).
{Original image
courtesy of Dr.
David R. Pickens,
Vanderbilt
University.) Top-hat and bottom-hat transformationsCombining image subtraction with openings and closings results in so-called
top-hat and bottom-hat transformations. The top-hai transformation of a grayscale image f is defined as f minus its opening:fyalf) = fF ~ fe 2) (9.6-12)Similarly, the botrom-hat transformation of f is defined as the closing of f
minus f:Brault) = (f ¥b) > f (Y.6-13}One of the principal applications of these transformations is in removing objects from an image by using a structuring element in the opening or closing
operation that does not fit the objects to be removed. The difference operation
then yields an image ia which only the removed components remain. The tophat transform is used for light objects on a dark background. and the bottom
hat transform is used for the converse. For Uiis reason, the names white top-hat
and black top-hat, respectively. are used frequently when referring to these
two transformations .An important use of lop-hat iransformations is in correcting the effects of
nonuniform illumination. As we will see in the next chapler. proper (uniform)
illumination plays a centrat role in the process of exiracting chyeets from the
background. This process, called segenenietion. is ane of the tirst steps per
formed in automated image it is. A camatonty ised on ap
proach is to threshold the input   
  AEM entaL    10s
9.6 & Gray-Scale MorphologyTo illustrate, consider Fig. 9.40(a), which shows a 600 x 600 image of grains
of rice. This image was obtained under nonuniform lighting, as evidenced by the
darker area in the bottom. rightmost part of the image. Figure 9.40(b) shows (he
result of thresholding using Oisu’s method, an optimal thresholding method
discussed in Section 10.3.3. The net result of nonuniform ilumination was to
cause segmentation errors in the dark area (several grains of rice were not extracted from the background}, as well as in the top left part of the image, where
parts of the background were misclassified. Figure 9.40(c) shows the opening of
the image with a disk of radius 4(), This SE was large enough so that it would not
fit in any of the objects. As a result, the objects were eliminated, ieaving only an
approximation of the background. The shading pattern is clear in this image. By
subtracting this image from the original (i-¢., performing a top-hat transformation), the background should become more uniform. This is indeed the case, as
Fig. 9.40(d) shows. The background is not perfectly uniform, but the differences
beiween light and dark extremes are less, and this was enough to yield a correct695 eae
FIGURE 9.40 Using the top-hat transformation for shadiig correction. (a) Original image of size
600 x 600 pixels. (b) Threshulded image. {c) Tinage opened using a disk SE of radius 40. (a) ‘Top-hat
transformation (the image minus its opening}. ie} Vlresholded top-hat image
696 Chopter9 @ Morphological Image Processingsegmentation result in which all rice grains were detected, as Fig. 9.40(e) shows.
This image was obtained using Otsu’s methad, as before.GranulometryIn terms of image processing, granulomerry is a field that deals with determining
the size distribution of particles in an image. In practice, particles seldom are
neatly separated, which makes particle counting by identifying individual particles a difficult task. Morphology can be used to estimate particle size distribution
indirectly, without having to identify and measure every particle in the image.The approach is simple in principle. With particles having regular shapes
that are lighter than the background, the method consists of applying openings
with SEs of increasing size. The basic idea is that opening operations of a particular size should have the most effect on regions of the input image that contain particles of similar size. For each opening, the su of the pixel values in
the opening is computed. This sym, sometimes called the surface area, decreases as a function of increasing SE'size because, as we noted earlier, openings decrease the intensity of light features. This procedure yields a 1-D array of such
numbers, with each clement in the array being equal to the sum of the pixels in
the opening for the size SE corresponding to that location in the array. ‘fo emphasize changes between successive openings, we compute the difference between adjacent elements of the 1-D array. To visualize the results, the
differences are plotted. The peaks in the plot are an indication of the predominant size distributions of the particles in the image.As an example, consider Fig, 9.4] (a) which is an image of wood dowel plugs
of two dominant sizes. The wood grain in the dowels are likely to introduce
variations in the openings, so smoothing is a sensible pre-processing step.
Figure 9.41(b) shows the image smoothed using the morphological smoothing abedetFIGURE 9.41 (a) 531 > 67S image of wes! dowels. (>) Smoathed image, fe}-(f) Openings
of (b)} with disks of raclii . qual to 10.20. 25. and 4d prxcis. respectively, (Onginal imagecourtesy of Dr. Steve Eddins, Phe MathWorks. Inu.)
9.6 m Gray-Scale Morphology 697filter discussed earlier, with a disk of radius 5. Figures 9.41(c) through (f) show
examples of image openings with disks of radii 10, 20, 25, and 30. Note in
Fig. 9.41(d) that the intensity contribution due to the small dowels has been almost eliminated. In Fig. 9.41(e) the contribution of the large dowels has been significantly reduced, and in Fig, 9.41(f) even more so. (Observe in Fig. 9.41(e) that
the large dowel near the top right of the image is much darker than the others because of its smaller size. This would be useful information if we had been attempting to detect defective dowels.)Figure 9.42 shows a plot of the difference array. As mentioned previously,
we expect significant differences (peaks in the plot) around radii at which the
SE is large enough to encompass a set of particles of approximately the same
diameter. The result in Fig. 9.42 has two distinct peaks, clearly indicating the
presence of two dominant object sizes in the image.Textural segmentationFigure 9.43(a) shows a noisy image of dark blobs superimposed on a light background. The image has two textural regions: a region composed on large blobs
on the right and a region on the left composed of smaller blobs. The objective is
to find a boundary between the two regions based on their textural content (we
discuss texture in Section 11.3.3). As noted earlier, the process of subdividing an
image into regions is called segmentation, which is the topic of Chapter 10.The objects of interest are darker than the background, and we know that if
we close the image with a structuring element larger than the smal] blobs,
these blobs will be removed. The result in Fig. 9.43(b), obtained by closing the
input image using a disk with a radius of 30 pixels, shows that indeed this is the
case (the radius of the blobs is approximately 25 pixels). So, at this point, we
have an image with large, dark blobs on a light background. If we oper this
image with a structuring element that is large relative to the separation between these blobs, the net result should be an image in which the light patches
between the blobs are removed, leaving the dark blobs and now equally dark
patches between these blobs. Figure 9.43(c) shows the result, obtained using a
disk of radius 60. 7Performing a morphological gradient on this image with, say,a 3 X 3 SE of
1s, will give us the boundary between the two regions. Figure 9.43(d) shows the
boundary obtained from the morphological gradient operation superimposedFIGURE 9.42T TT v7 7 Differences in
surface area as a
function of SE
disk radius, 7. The
two peaks are
indicative of two
dominant particlei sizes in the image.6
25 x ID raunOSDifferences in surface area
698 Chapter 9 #& Morphological Image Processingab
edFIGURE 9.43
Textura]
segmentation.
(a) A 600 x 600
image consisting
of two types of
blobs. (b) Image
with small blobs
removed by
closing (a).(c) Image with
light patches
between large
blobs removed by
opening (b).(d) Original
image with
boundary
between the two
regions in (c)
superimposed.
The boundary was
obtained using a
morphological
gradient
operation.Mis understood that
these expressions are
functions af (x. ¥). We
omit thy coordinates to
simplify the notation. on the original image. All pixels to the right of this boundary are said to belong
to the texture region characterized by large blobs, and conversely for the pixels on the left of the boundary. You will find it instructive to work through this
example in more detail using the graphical analogy for opening and closing illustrated in Fig. 9.36.9,8.4 Gray-Scale Morphological Reconstruction
Gray-scale morphological reconstruction is defined basically in the same manner introduced in Section 9.5.9 for binary images. Let f and g denote the
marker and mask images, respectively. We assume that both are gray-scale images of the same size and that f = g. The geadesic dilation of size | of f with
respect to g is defined asDUP = (F Bd) ag (9.6-14)
where A denotes the point-wise minimum operator. This equation indicates
that the geodesic dilation of size | is obtained by first computing the dilation
of f by & and then selecting the minimum between the resuli and g at every
point (x,y). The dilation is given by Eq. (9.6-2) if A is a flat SE or by Eq. (9.6-4)
if itis not. The geodesic dilation of size a of f with respect to g is defined asDMP DEBEDYS CCP) (9.6-]5)
with DOP) = fF,
9.6 @ Gray-Scale Morphology 699Similarly, the geodesic erosion of size 1 of f with respect to g is defined as
EMP) = (FOd) vg (9.6-16)where V denotes the point-wise maximum operator. The geodesic erosion of See Problem 9.33 tor a
Hist of dual relationshipssize n is defined as between expressions in
ne this section.
ELL) = EM[EE Xp] (9.6-17) lis section.
with EO(f) = f. .The morphological reconstruction by dilation of a gray-scale mask image, g,
by a gray-scale marker image, f, is defined as the geodesic dilation of f with
respect to g, iterated until stability is reached; that is,Rf) = DE) (9.6-18)with k such that DMF) = Dk *Y(f). The morphological reconstruction by
erosion of g by f is similarly defined asRE(f) = E®(S) (9.6-19)with k such that EM(f) = E&P).As in the binary case, opening by reconstruction of gray-scale images first
erodes the input image and uses it as a marker. The opening by reconstruction
of size 7 of an image f is defined as the reconstruction by dilation of f from
the erosion of size n of f; that is,OU) = RPP Ond)] (96-20)where (f © nb) denotes n erosions of f by b, as explained in Section 9.5.7. Recall from the discussion of Eq. (9.5-27) for binary images that the objective of
opening by reconstruction is to preserve the shape of the image components
that remain after erosion.Similarly, the closing by reconstruction of size n of an image f is defined as
the reconstruction by erosion of f from the dilation of size n of f; that is,CPG) = RF[( ends] (96-21)where (f ® nb) denotes n dilations of f by 6. Because of duality, the closing by
reconstruction of an image can be obtained by complementing the image, obtaining the opening by reconstruction, and complementing the result. Finally,
as the following example shows, a useful technique called rop-hat by reconstruction consists of subtracting from an image its opening by reconstruction.# In this example, we illustrate (he use of gray-scale reconstruction in sev- EXAMPLE 9.11:
eral steps to normalize the irregular background of the image in Fig. 9.44(a), Using ;
leaving only the text on a background of constant intensity. The solution of morphological to
this problem is a good illustration of the power of morphological concepts. gatien a complex
We begin by suppressing the horizontai reflection on the top of the keys. The — background.
reflections are wider than any single character in the image, so we should beable to suppress them by performing an opening by reconstruction using aJong horizontal] line in the erosion operation. This operation will yield thebackground containing the keys and their reflections. Subtracting this from
700 Chopter 9 # Morphological Image Processing  ¥
a
gFIGURE 9.44 (3) Original image of size 1134 x 1360 pixels. (b) Opening by reconstruction of (a) using a
horizontal line 71 pixels jong in the erosion. (¢} Opening of (a) using the same Ime. (d) Top-hai by
reconstruction. {e) Top-hat. (f) Opening by reconstruction of (d} using a horizontal line 11 pixels long.
(g) Dilation of (f) using a horizontal ling 21 pixe)s fone. (bh) Minimiwn of (d) and (¢). (3 Pinal reconstruction
result. (Images courtesy of Dr. Steve Eddins, The MathWorks, Ene.) the original image (ie.. performing a top-hat by reconstruction) wall climtpround from the origt
 nate the horizontal reflections and variations in back
nal imageFigure 9.44(b} shows the result of operisi
image using a horizontal Woe oF size Pos
We could have used jist ap opening te ren
ing background would not
ample compure (he regi  2 by reconslructic  pixels in the erasion operation.
but the resi
9 Adc shows (for ex   e the vharacte lorniasy bs   vs Ue aye
9.4¥9.596The following four statements are true. Advance an argument that establishes the reason(s) for their validity. Part (a) is true in general, Parts (b) through
(d) are true only for digital sets. To show the validity of (b) through (d), draw
a discrete, square grid (as shown in Problem 9.1) and give an example for
each case using sets composed of points on this grid. (Hint: Keep the number
of points in each case as small as possible while still establishing the validity
of the statements.)* (a) The erosion of a canvex set by a convex structuring element is a convex set.x (b) The dilation of a convex set by a convex structuring element is not necessar
ily always convex.{c) The points in a convex digital set are not always connected.(d) It is possible to have a set of points in which a line joining every pair of
points in the set lies within the set but the set is not convex.With reference to the image shown, give \he structuring element and morpho
logical operation(s) that produced each of the results shown in images (a)through (d). Show the origin of each structuring element clearly. The dashedlines show the boundary of the original set and are included only for reference.Note that in (d) all corners are rounded. {a) (b) (c) {d)Let A denote the set shown shaded in the following figure, Refer to the struc
turing elements shown (the black dots denote the origin). Sketch the resuil of
the following morphological operations:(a) (AS B’)@ Bt
(b) (AGB) eB
(ce) (ASB) es!
(d) (AOB)OPB@ Problems 703
shows the result of subtracting Fig, 9.44(b) from Fig. 9.44(a). As expected, the
horizontal reflections and variations in background were suppressed. For
comparison, Fig. 9.44(e) shows the result of performing just a top-hat transformation (i.e., subtracting the “standard” opening from the image, as discussed earlier in this section). As expected from the characteristics of the
background in Fig. 9.44(c), the background in Fig. 9.44(e) is not nearly as uniform as in Fig. 9.44(d)}.The next step is to remove the vertical reflections from the edges of keys,
which are quite visible in Fig, 9.44(d), We can do this by performing an opening by reconstruction with a line SE whose width is approximately equal to
the reflections (about 11 pixels in this case). Figure 9.44(f) shows the result of
performing this operation on Fig. 9.44(d). The vertical reflections were suppressed, but so were thin, vertical strokes that are valid characters (for example, the I in SIN}, so we have to find a way to restore the latter. The
suppressed characters are very close to the other characters so, if we dilate
the remaining characters horizontally, the dilated characters will overlap the
area previously occupied by the suppressed characters. Figure 9,44(g), obtained by dilating Fig. 9.44(f) with a line SE of size 1 X 21, shows that indeed
this is case.All that remains at this point is to restore the suppressed characters. Consider an image formed as the point-wise minimum between the dilated image
in Fig. 9.44(g) and the top-hat by reconstruction in Fig. 9.44(d). Figure 9.44(h)
shows the minimum image (although this result appears to be close to our objective, note that the I in SIN is still missing). By using this image as a marker
and the dilated image as the mask in gray-scale reconstruction (Eq. (9.6-18)]}
we obtain the final result in Fig. 9.44(i). This image shows that all characters
were properly extracted from the original, irregular background, including
the background of the keys. The background in Fig. 9.44(i) is uniform
throughout. iSummaryyThe morphological concepts and techniques introGuced in this chapter constitute a
powerful set of tools for extracting features of interest in an image. One of the most appealing aspects of morphological image processing is the extensive set-theoretic foundation from which morphological techniques have evolved. A significant advantage in
terms of implementation is the fact that diJation and erosion are primitive operations
that are the basis for a bread class af morphological algorithms. As shown in the following chapter, morphology can be used as the basis for developing image segmentation procedures with numerous applications. As discussed in Chapter 11, morphological
techniques also play a major role in procedures for image description.References and Further ReadingThe book by Serra [1982] is a fundamental reference on morphological image processing. See also Serra [1988], Giardina and Dougherty [1988], and Haralick and Shapiro
{1992]. Additional early references relevant to our discussion include Blum [£967],
Lantuéjoul [1980], Maragos [1987], and Haralick et al. [1987]. For an overview of both
binary and gray-scale morphology, see Basart and Gonzalez (1992) and Basart et al.m Summary 701
702 Chapter 9 @ Morphological Image Processing  SadDetailed solutions to the
problems marked with a
star can be found in the
book Web site. The site
alya contains suggested
projects based on the material in this chapter.[1992]. This set of references provides ample basic background for the material covered
in Sections 9.1 through 9.4, For a good overview of the material in Sections 9.5 and 9.6,
see the book by Soille [2003].Important issues of implementing morphological algorithms such as the ones
given in Section 9.5 and 9.6 are exemplified in the papers by Jones and Svalbe [1994],
Park and Chin [1995], Sussner and Ritter [1997], Anelti et al. [!998], and Shaked and
Bruckstein [1998]. A paper by Vincent [1993] is especially important in terms of practical details for implementing gray-scale morphological algorithms. See also the book
by Gonzalez, Woods, and Eddins [2004].For additional reading on the theory and applications of morphological image processing, see the book by Goutsias and Bloomberg [2000] and a special issue of Pattern
Recognition [2000]. See also a compilation of references by Rosenfeld [2000]. The
books by Marchand-Maillet and Sharaiha (2000] on binary image processing and by
Ritter and Wilson [2001] on image algebra also are of interest. Current work in the application of morphological techniques for image processing is exemplified in the papers
by Kim [2005] and Evans and.Liu [2006]. .ON
Problems
9.1 Digital images in this book are embedded in square grid arrangements and pixels are allowed to be 4-, 8-, or m-connected. However, other grid arrangements
are possible. Specifically, a hexagonal grid arrangement that leads to 6-connectivity, is used sometimes (see the following figure).
(a} How would you convert an image from a. square grid to a hexagonal grid?(b) Discuss the shape invariance to rotation of objects represented in a square
grid as opposed to a hexagonal grid.(c) Is it possible to have ambiguous diagonal configurations in a hexagonal
grid, as is the case with 8-connectivity? (See Section 2.5.2.) 9.2. & (a) Give a morphological algorithm for converting an 4-connected binary
boundary to an m-connected boundary (see Section 2.5.2). You may assume
that the boundary is fully connected and that it is one pixel thick.(b} Does the operation of your algorithm require more than one iteration with
each structuring element? Explain your reasoning.(c) Is the performance of your algorithm indepeddent of the order in which the
structuring elements are applied? If your answer is yes, prove it; otherwise
give an example that illustrates the dependence of your procedure on the
order of application of the structuring elements.9.3. Erosion of a set A by structuring element B is a subset of A as long as the originof B is contained by 8. Give an example in which the erosion A © B lies outside,or partially outside. A.
704 Chapter 9 m@ Morphological Image Processing*9.798+999.10 (a) What is the limiting effect of repeatedly dilating an image? Assume that a
trivial (one point) structuring element is not used.(b) What is the smallest image from which you can start in order for your answer in part (a) to hold?(a) What is the limiting effect of repeatedly eroding an image? Assume that a
trivial (one point) structuring element is not used.{b) What is the smallest image from which you can start in order for your answer in part (a) to hold?An alternative definition of erosion isAOQB = {weZ?|w + be A, for every be BYShow that this definition is equivalent to the definition in Eq. (92-2).(a) Show that the definition of erosion given in Problem 9.9 is equivalent to yet
another definition of erosion: *AGB = (}(A)-»
beB
(If —5 is replaced with 6, this expression is called the Minkowsky subtrac
tion of two sets.)
(b) Show that the expression in {a) also is equivalent to the definition in
Eq, (9,2-2).
Problems*9.11 An alternative definition of dilation is
A@B = (weZ?|w =a +b, forsome ae A and be B}
Show that this definition and the definition in Eq. (9,2-4) are equivalent.9.12 (a) Show that the definition of dilation given in Problem 9.1 1 is equivalent to
yet another definition of dilation:
A@B =| JA),
. bok
(This expression also is called the Minkowsky addition of lwo sets.)
(b) Show that the expression in (a) also is equivalent to the definition in Eq.
{9.2-4).
9.13 Prove the validity of the duality expression in Eq. (9.2-6) and Eq. (Y.2-5).
*914 Prove the validity of the duality expressions (A ¥ B)° = (A « B) and
(A ° BY = (AT ¥B),
9.15 Prove the validity of the following expressions:
x(a) Ao Bis a subset (subimage) of A.
(b) If C is a subset of D, then C o Bis asubsei of Do B.
{ce} (Ao Blo B=Ac B.9.16 Prove the validity of ihe following expressions (assume that the origin of B is
contained in B and that Problems ¥.14 and 9.15 are true):
(a) A is a subset (subimage) of A ¥ 8B.(b) If C is a subset of D, then C ¥ B is a subset of D ¥ B.
(©) (A¥B) ¥B= A¥B.9.17 Refer to the image and structuring clement shown, Sketch what the sets C.D.
E, and F would look like in the following sequence of operations: C = AGB:
D=COB, E = DGB, and F = E®B, The initial set A consists of al) the
image components shown in white, with the exception of the structuring clement B. Note that this sequence of operations is simply the opening of A by A,
followed by the closing of that opening by A. You may assume that B is just
large enough to enclose each of the noise components.‘
706 Chepter $ w Morphological Image Processing*9.18 Consider the three binary images shown in the following figure. The image
on the feft is composed of squares of sizes 1,3, 5, 7,9, and 15 pixels on the
side. The image in the middle was generated by croding the image on the left
with a square structuring element of 1s, of size [3 x 13 pixels, with the objective of eliminating all the squares, except the largest ones. Finally, the
image on the right is the result of dilating the image in the center with the
same structuring element, with the objective of restoring the largest squares,
You know that erosion followed by dilation is the opening of an image, and
you know also that opening generally does not restore objects to their original form, Explain why full reconstruction of the large squares was possible in
this case.    9.19 Sketch the result of applying the hit-or-miss transform to the image and struc
turing element shown. Indicate clearly the origin and berder you selected for
the structuring element. Imave Structuring
clementx9.20 = Three features (lake, bay, and line segment) useful for differentiating thinned objects in an image are shown in the following page, Develop a morphological/Jogical
algorithm for differentiating among these shapes. The input to vour algorithm
would be one of these three shapes. Ihe output must be the identity of the input.
You may assume that the features are | pixel thick and that each is felly connecled.
However, they can appear in any orientation
Bay Line segment9.21 Discuss what you would expect the result to be in each of the followingcases:(a) The starting point of the hole filling algorithm of Section 9.5.2 is a point on
the boundary of the object.(b) The starting point in the.hole filling algorithm is outside of the boundary,(c) Sketch what the convex hull of the figure in Problem 9.6 woutd look like as
computed with the algorithm given in Section 9.5.4. Assume that L = 4
pixels.9.22 x(a) Discuss the effect of using the structuring element in Fig. 9,15{c) for boundary extraction, instead of the one shown in Fig. 9.13(b).{b) What would be the effect of using a 2 x 2 structuring element composed of
all 1s in the hole filling algorithm of Eq. (9.5-2), instead of the structuring element shown in Fig. 9.15(c)?9.23 *(a) Propose a method (using any of the techniques from Sections 9.1
through 9.5) for automating the example in Fig. 9.16. You may assume
that the spheres do not touch each other and that none touch the border
of the image.(b) Repeat (a), but allowing the spheres to touch in arbitrary ways, including
touching the border. ~~.&9.24 The algorithm given in Section 9.5.3 for extfatting connected components requires that a point be known in each connected component in order to extract
thers all. Suppose that you are given a binary image containing an arbitrary
{unknown) number of connected components. Propose a completely automated procedure for extracting alf connected components. Assume that points belonging to connected components are labeled 1 and background points are
labeled 0,9.25 Give an expression based on reconstruction by dilation capable of extracting allthe holes in a binary image.9.26 With reference to the hole-filling algorithm in Section 9.5.9:(a) Explain what would happen if all border points of f are J,(b) If the result in (a) gives the result that you would expect, explain why, If it
does not, explain how you would modify the algorithm so that it works as
expected.™ Problems707
708 Chapter § @ Morphological Image Processing% 9.279.28w9.299w9O519.329.339.34Explain what would happen in binary erosion and dilation if the structuring element is a single point, valued 1. Give the reason(s) for your answer.As explained in Eq. (9.5-27) and Section 9.6.4, opening by reconstruction preserves the shape of the image components that remain after erosion. What does
closing by reconstruction do?Show that geodesic erosion and dilation (Section 9.5.9) are duals with respect
to set complementation. That is, show that Ef’ (F) = [Di2[D& PFs] |° and,conversely, that Df? (F) = [E82 [e% 2(F*)]|". Assume that the structuring element is symmetric about its origin.Show that reconstruction by dilation and reconstruction by erosion (Section 9.5.9)are duals with Tespect 10 set complementation. That is, show thatRB(F) = [REF and, vice versa, that RE(F) = [REFS]. Assume that thestructuring element is symmetric about its origin.Advance an argument. showing that:(a) [(F OnB)] = (F°® nB), where (F OnB) indicates n erosions of F by B.(b) ((F @nB)f = (FOO nB).Show that binary closing by reconstruction is the dual of opening by recon
struction with respect to set complementation: OWE y= cy R F *)| , and similarly
cthat CMF) = [oer] . Assume that the structuring element is symmetricwith respect to its origin.Prove the vatidity of the following gray-scale morphology expressions. You may
assume that 0 is a flat structuring element. Recall that f¢(x, y) = —f(x, y) andthat B(x, y) = b(-x - y).%* (a) Duality of erosion and dilation: (f 8 6)* = rob and (f @by = fre 6.{b) (f ¥b)° = fF ° band (f 2 bj = foxb.wo) DPF) = [EP LES MYO]] and BMY) = [DP[DE MG I]]. Assume asymmetric structuring element.
(a) REF) = [RELY and RECA) = [RET(e) [(f @nb)] = (f° @ nb), where (f 6 nb) indicates n erosions of f by b, Also,
(Uf @ nb)]}° = (f° Gab).©) OP FY = [CRUD] and CRA) = [OY]. Assume that the structuring element is symmetric with respect to its origin.In Fig. 9.43, a boundary between distinct texture regions was established withoutdifficulty. Consider the image at the top of the facing page, which shows a regionof small circles enclosed by a region of larger circles.(a) Would the method used to generate Fig. 9.43(d) work with this image as
well? Explain your reasoning, including any assumptions that you need to
make for the method to work(b) If your answer was yes, sketch what the boundary will look like.
9.35 A gray-scale image, f(x, y), is corrupted by nonoverlapping noise spikes that
can be modeled as small, cylindrical artifacts of radii Rijn & 6 = Ryyax aNd amplitude Amin = @ = Amax- max* (a) Developa morphological filtering approach for cleaning up the image.(b) Repeat (a), but now assume that there is overlapping of, at most, four noisespikes. ° .
A preprocessing step in an application of microscopy is concerned with the issue
of isolating individual round particles from similar particles that overlap in
groups of two or more particles (see following image). Assuming that all particles are of the same size, propose a morphological algorithm that produces three
images consisting respectively of .
(a) Only of particles that have merged with the boundary of the image.9.36(b) Onty overlapping particles.
(©) Only nonoverlapping particles. Problems709
710 Ghapter 9 #1 Morphological Image Processing9.37 A high-technology manufacturing plant wins a government contract to manufacture high-precision washers of the form shown in the following figure. The
terms of the contract require that the shape of all washers be inspected by an
imaging system. In this context, shape inspection refers to deviations from
round on the inner and outer edges of the washers. You may assume the following: (1) A “golden” (perfect with respect to the problem) image of an acceptable
washer is available; and (2) the imaging and positioning systems ultimately used
in the system will have an accuracy high enough to allow you to ignore errors
due to digitalization and positioning. You are hired as a consultant to help specify the visual inspection part of the system. Propose a solution based on morphological/logic operations. Your answer should be in the form of a block
diagram.
. Image Segmentation The whole is equal to the sum of its parts.
Euclid
The whole is greater than the sum of its parts.
Max WertheimerPreviewThe material in the previous chapter began a transition from image processing
methods whose inputs and outputs are images, to methods in which the inputs are
images but the outputs are attributes extracted from those images (in the sense
defined in Section 1.1). Segmentation is another major step in that direction.
Segmentation subdivides an image into its constituent regions or objects. The
level of detail to which the subdivision is carried depends on the problem being
solved. That is, segmentation should stop when the objects or regions of interest
in an application have been detected. For example, in the automated inspection
of electronic assemblies, interest lies in analyzing images of products with the
objective of determining the presence or absence of specific anomalies, such as
missing components or broken connection paths. There is no-point in carrying
segmentation past the level of detail required to identify those elements.
Segmentation of nontrivial images is one of the most difficult tasks in image
processing. Segmentation accuracy determines the eventual success or failure
of computerized analysis procedures. For this reason, considerable care should
be taken to improve the probability of accurate segmentation. In some situations, such as in industrial inspection applications, at Jeast some measure of
control over the environment typically is possible. The experienced image processing system designer invariably pays considerable attention to such opportunities, In other applications. such as autonomous target acquisition, the
system designer has no control over the operating environment, and the usual711
712 Chapter 10 % Image SegmentationSve Seetions 6,7 and
18.3.8 for a discussion of
segmentation techniques
based on more than just
gray (intensity) values,Sev Section 2.5.2
regarding connecicd sets.approach is to focus on selecting the types of sensors most likely to enhance
the objects of interest while diminishing the contribution of irrelevant image
detail. A good example is the use of infrared imaging by the military to detect
objects with strong heat signatures, such as equipment and troops in motion.Most of the segmentation algorithms in this chapter are based on one of two
basic properties of intensity values: discontinuity and similarity. In the first category, the approach is to partition an image based on abrupt changes in intensity,
such as edges. The principal approaches in the second category are based on partitioning an image into regions that are similar according to a set of predefined
criteria. Thresholding, region growing, and region splitting and merging are examples of methods in this category. In this chapter, we discuss and illustrate a
number of these approaches and show that improvements in segmentation performance can be achieved by combining methods from distinct categories, such
as techniques in which edge detection is combined with thresholding. We discuss
also image segmentation based on morphology. This approach is particularly attractive because it combines se¥eral of the positive attributes of segmentation
based on the techniques presented in the first part of the chapter. We conclude
the chapter with a brief discussion on the use of motion cues for segmentation.FundamentalsLet R represent the entire spatial region occupied by an image. We may view
image segmentation .as a process that partitions R into m subregions,
R), Ro... , Ry, such that(a) JR = R.
=I
(b) R;is a connected set, i = 1,2,....m.
(©) ROR, = Boral iand j,i # j.
(ad) O(R)) = TRUE fori = 1,2,....9.
(e) Q(R;U R,) = FALSE for any adjacent regions R; and R;.Here, Q(R,) is a logical predicate defined over the points in set R,, and 2 is the
null set. The symbols U and ™ represent set union and intersection, respectively, as defined in Section 2.6.4. Two regions R; and R; are said to be adjacent
if their union forms a connected set, as discussed in Section 2.5.2.Condition (a) indicates that the segmentation must be complete; that is,
every pixe] must be in a region. Condition (b) requires that points in a region be
connected in some predefined sense (e.g., the points must be 4- or 8-connected,
as defined in Section 2.5.2). Condition (c) indicates that the regions must be
disjoint. Condition (d) deals with the properties that must be satisfied by the
pixels in a segmented region—for example, Q(R;) = TRUE if all pixels in R;
have the same intensity level. Finally, condition (e) indicates that two adjacent
regions R; and R; must be different in the sense of predicate QF ‘in general, Q can be a compound expression such as. for example, O(R,) = TRUE if the average intcosity of the pixels in R, is less than m;, AND if the standard deviation of their inter :ity is greater than «7,
where m7, and «; are specified constants.
19.1 % FundamentalsThus, we see that the fundamental problem in segmentation is to partition
an image into regions that satisfy the preceding conditions. Segmentation algorithms for monochrome images generally are based on one of two basic
categories dealing with properties of intensily values: discontinuity and similarity. In the first category, the assumption is that boundaries of regions are
sufficiently different from each other and from the background to allow
boundary detection based on local discontinuities in intensity. Edge-based
segmentation is the principal approach used in this category, Region-based
Segmentation approaches in the second category ate based on partitioning an
image into regions that are similar according to a set of predefined criteria.Figure 10.1 illustrates the preceding concepts. Figure 10.1 (a) shows an image
of a region of constant intensity superimposed on a darker background, also of
constant intensity. These two regions comprise the overall image region. Figure
10.1{b) shows the result of computing the boundary of the inner region based
on intensity discontinuities. Points on the inside and outside of the boundary
are black (zero) because there are no discontinuities in intensity in those regions. To segment the image, we assign one level (say, white) to the pixels on, or
interior to, the boundary and another level (say, black) to all points exterior to
the boundary. Figure 10.1(c) shows the result of such a procedure. We see that
conditions (a) through (c) stated at the beginnitig of this section are satisfied by  det
FIGURE 10.) (a) Image containing a region of constant intensity. (bj Image showing the
boundary of the inner region, obtained from intensity discontinuities. (c) Result of
segmenting the image into two regions. (¢) [mage containing u textured region
(e) Result of edge computations. Noic th : are
connected to the original boundary, making i boundary using
only edge information. (f) Result of seamentation | ries,   
  
    713
714 Chapter 10 m Image SegmentationWhen we refer Lo fines,
we are referring to thin
structures, typically just a
few pixels thick, Such
lines may correspond. for
example, to elements of a
digitized architectural
drawing or coads in a
satellite image.this result. The predicate of condition (d) is: [f a pixel is on, or inside the boundary,
label it white; otherwise label it black. We see that this predicate is TRUE for the
points labeled black and white in Fig. 10.1(c). Similarly, the two segmented
regions (object and background) satisfy condition (e).The next three images illustrate region-based segmentation. Figure 10.1(d)
is similar to Fig. 10.1(a), but the intensities of the inner region form a textured
pattern. Figure 10.i1(¢) shows the result of computing the edges of this image.
Clearly, the numerous spurious changes in intensity make it difficult to identify a unique boundary for the original image because many of the nonzero
intensity changes are connected to the boundary, so edge-based segmentation
is not a suitable approach. We note however, that the outer region is constant,
so all we need to solve this simple segmentation problem is a predicate that
differentiates between textured and constant regions. The standard deviation
of pixel values is a measure that accomplishes this, because it is nonzero in
areas of the texture region and zero otherwise. Figure 10.1(f) shows the result
of dividing the original image into subregions of size 4 x 4. Each subregion
was then labeled white if the standard deviation of its pixels was positive (i.e.,
if the predicate was TRUE) and zero otherwise. The result has a “blocky” appearance around the edge of the region because groups of 4 * 4 squares
were labeled with the same intensity. Finally, note that these results also satisfy
the five conditions stated at the beginning of this section.EEA Point, Line; and Edge DetectionThe focus of this section is on segmentation methods that are based on detecting sharp, /ocaé changes in intensity. The three types of image features in which
we are interested are isolated points, lines, and edges. Edge pixels are pixels at
which the intensity of an image function changes abruptly, and edges (or edge
segments) are sets of connected edge pixels (see Section 2.5.2 regarding connectivity). Edge detectors are local image processing methods designed to detect edge pixels. A line may be viewed as an edge segment in which the
intensity of the background on either side of the line is either much higher or
much lower than the intensity of the line pixels. In fact, as we discuss in the following section and in Section 10.2.4, lines give rise to so-called “roof edges.”
Similarly, an isolated point may be viewed as a line whose length and width are
equal to one pixel.10.2.) BackgroundAs we saw in Sections 2.6.3 and 3.5, local averaging smooths an image. Given
that averaging is analogous to integration, it should come as no surprise that
abrupt, local changes in intensity can be detected using derivatives. For reasons that will become evident shortly, first- and second-order derivatives are
particularly well suited for this purpose.Derivatives of a digital function are defined in terms of differences. There
are various ways to approximate these differences but, as explained in
Section 3.6.1, we require that any approximation used for a first derivative
(1) must be zero in areas of constant intensity; (2) must be nonzero at the onset
of an intensity step or ramp; and (3) must be nonzero at points along an intensity
10.2 ® Point, Line, and Edge Detection 715ramp. Similarly, we require that an approximation used for a second derivative
(1) must be zero in areas of constant intensity; (2) must be nonzero at the
onset and end of an intensity step or ramp; and (3) must be zero along intensity ramps. Because we are dealing with digital quantities whose values are finite, the maximum possible intensity change is also finite, and the shortest
distance over which a change can occur is between adjacent pixels.We obtain an approximation to the first-order derivative at point x of a
one-dimensional function f(x) by expanding the function f(x + Ax) into a
’ Taylor series about x, letting Ax = 1, and keeping only the linear terms (Problem 10.1). The result is the digital differenceaf = f'(x) = fet - fx) (102-1)We used a partial derivative here for consistency in notation when we consider an image function of two variables, f(x, y), at which time we will be dealing
with partial derivatives along the two spatial axes. Clearly, af/ax = df/dx
when f is a function of only one variable.We obtain an expression for the second derivative by differentiating Eq.
(10.2-1) with respect to x:Pf af'(x) _ax2 x
= f(x +2- fat) -f@t) + fx)
= f(x + 2) - 2f(x +1) + f()where the second line follows from Eq. (10.2-1). This expansion is about point
x + 1. Our interest is on the second derivative about point x, so we subtract 1
from the arguments in the preceding expression and obtain the resultF(xt+ I - f'@)e
oF = fe) = fle) + f= 1) 2F@) 0022)
Bo
Tt easily is verified that Eqs. (10.2-1) and (10.2-2) satisfy ihe conditions stated
at the beginning of this section regarding derivatives of the first and second
order, To iltustrate this, and also to highlight the fundamental similarities and
differences between first- and second-order derivatives in the context ofimage processing, consider Fig. 10.2.Figure 10.2(a) shows an image that contains various solid objects, a line, and a
single noise point. Figure 10.2(b) shows a horizontal intensity profile (scan tine)
of the image approximately through its center, including the isolated point. Transitions in intensity between the solid objects and the background along the scan
line show two types of edges: ramp edges (on the left} and step edges (on the
right). As we discuss later, intensity transitions involving thin objects such as
lines often are referred to as roof edges. Figure 10.2(c} shows a simplification of
the profile, with just enough points to make it possible for us to analyze numerically how the first- and second-order derivatives behave as they encounter a
noise point, a line, and the edges of objects. In this simplified diagram theRecall from Section 2.4.2
that increments between
image Samples are
defined as unity for
Aotational clarity. hence
the use of dx = tin the
derivation of Eq. (10.2-1),
716 Chapter 10 @ Image Segmentation transition in the ramp spans four pixels, the noise point is a single pixel, the line
is three pixels thick, and the transition of the intensity step takes place between
adjacent pixels. The number of intensity levels was limited to eight for simplicity.Consider the properties of the first and second derivatives as we traverse the
profile from left to right. Initially, we note that the first-order derivative is
nonzero at the onset and along the entire intensity ramp, while the secondorder derivative is nonzero only at the onset and end of the ramp. Because
edges of digital images resemble this type of transition, we conclude that firstorder derivatives produce “thick” edges and second-order derivatives much
finer ones. Next we encounter the isolated noise point. Here, the magnitude of
the response at the point is much stronger for the second- than for the first-order
derivative. This is not unexpected, because a second-order derivative is much        j
\ |
|
\ ‘
\ t
Lf |
\ } |
Ln |
flog
7 a-— Isolated point owe
6 t
= :
55 +e i Ses, t
54 a Ramp : \ Line Step ~“!
£3 Le ' \ Plat segment ,% 4
2 ~~ roy \ jy t
. t 1 \ t
. 1 . PLY * '
ff] s-e 6 o-oo ‘eee i}
image strip [5 [513] 3] 21; oropolT [sta folololel a7 717] -E |
bhibiialtlaittrt nei tb btye dd
First derivative -P—tet-3-10 06-6000 1 2-2-100079000
pidde dep i ee pebad tt seedy
Second derivative -10 000! f 12600 } |-d4t 1007-700
ab
c  
  
  snsity profile through the center of the image,
implitied profile (the points are joined by dashes
for clarity). The image strip cor (sto the intensity profile. and the numbers in the
boxes are the wieasity values of tin dois shown an the profile. Che derivatives were
obtained using Eqs. (G.2-i} and (2 ayFIGURE 10.2 (2) image. (b) Horico
including the isolated noise paint, (
10.2 m Point, Line, and Edge Detection 717more aggressive than a first-order derivative in enhancing sharp changes. Thus,
we can expect second-order derivatives to enhance fine detail (including noise)
much more than first-order derivatives. The line in this example is rather thin, so
it too is fine detail, and we see again that the second derivative has a larger magnitude. Finaily, note in both the ramp and step edges that the second derivative
has opposite signs (negative to positive or positive to negative) as it transitions
into and out of an edge. This “double-edge” effect is an important characteristic. that, as we show in Section*10.2.6, can be used to locate edges. The sign of the
second derivative is used also to determine whether an edge is a transition from
light to dark (negative second derivative) or from dark to light (positive second
derivative), where the sign is observed as we move into the edge.In summary, we arrive at the following conclusions: (1) First-order derivatives
generally produce thicker edges in an image. (2) Second-order derivatives have
a stronger response to fine detail, such as thin lines, isolated points, and noise.
(3) Second-order derivatives produce a double-edge response at ramp and step
transitions in intensity. (4) The sign of the second derivative can be used to determine whether a transition into an edge is from light to dark or dark to light.The approach of choice for computing first and second derivatives at every
pixel location in an image is to use spatial filters. For the 3 x 3 filter mask in
Fig, 10.3, the procedure is to compute the sum of products of the mask coefficients
with the intensity values in the region encompassed by the mask. That is, with reference to Eq. (3.4.3), the response of the mask at the center point of the region isR= wi, + Wz + +++ + Woe9
Derr
k=]where z; is the intensity of the pixel whose spatial location corresponds to the
location of the kth coefficient in the mask. The details of implementing this operation over all pixels in an image are discussed in detail in Sections 3.4 and
3.6. In other words, computation of derivatives based on spatial masks is spatia] filtering of an image with those masks, as explained in those sections.*(10.2-3)      tAs explained in Section 3.4.3, Eq. (10.2-3) is simplified notation either for spatial correlation, given by
Eq, (3.4-1), or spatial convolution, given by Eq. {3.4-2). Therefore, when 2 is evaluated at all locations in
an image, the result is an array, All spatial fillering in this chapter is done using correlation. In some inStances, we use the term convolving a mask with an image as a matter of convention. However, we use
this terminology only when (he filter masks are symmetric, in which case correlation and convolution
yield the same result.FIGURE 10.3
A general 3 x 3
spatial filter mask.
718  Ceapter 10 @ Image Segmentation10.2.2 Detection of Isolated PointsBased on the conclusions reached in the preceding section, we know that point
detection should be based on the second derivative. From the discussion in
Section 3.6.2, this implies using the Laplacian:V(x, y) = “f + &f (10.2-4)
oywhere the partials are obtained using Eq, (10.2-2):2ae = fet ly) + fle- Ly) Fey) (10.2.5)
and7a” Yo foay y+1)+finy- 1) - f(xy) (10.2-6)The Laplacian is thenWily) = flx+ Ly) + fe- ly) t fay +)
~ + f(x,y ~ 1) ~ af y)(10.2-7)As explained in Section 3.6.2, this expression can be implemented using the
mask in Fig. 3.37{a). Also, as explained in that section, we can extend Eq. (10.2-7)
to include the diagonal terms, and use the mask in Fig. 3.37(b). Using the
Laplacian mask in Fig. 10.4(a), which is the same as the mask in Fig. 3.37(b), we
say that a point has been detected at the location (x, y) on which the mask is
centered if the absolute value of the response of the mask at that point exceeds
a specified threshold. Such points are labeled | in the output image and al]
others are labeled 0, thus producing a binary image. In other words, the output
is obtained using the following expression:_ fi if [RQ y)| = T
8(% 9) = {) otherwise (10.2-8)where g is the output image, 7 is a nonnegative threshold, and R is given by
Eq. (10.2-3). This formulation simply measures the weighted differences between a pixel and its 8-neighbors. Intuitively, the idea is that the intensity of an
isolated point will be quite different from its surroundings and thus will be easily detectable by this type of mask. The only differences in intensity that are
considered of interest are those large enough (as determined by 7) to be considered isolated points, Note that, as usual for a derivative mask, the coefficients sum to zero, indicating that the mask response will be zero in areas of
constant intensity.
10.2 s» Point, Line, and Edge Detection    @ We illustrate segmentation of isolated points in an image with the aid of
Fig, 10.4(b), which is an X-ray image of a turbine blade from a jet engine. The
blade has a porosity in the upper-right quadrant of the image, and there is a
single black pixel embedded within the porosity. Figure 10.4(c) is the result of
applying the point detector mask to the X-ray image, and Fig. 10.4{d) shows
the result of using Eq. (10.2-8) with T equal to 90% of the highest absolute
pixel value of the image in Fig. 10.4(c). The single pixel is clearly visible in this
image (the pixel was enlarged manuaily to eghance its visibility). This type of
detection process is rather specialized, because it is based on abrupt intensity
changes at single-pixel locations that are surrounded by a homogeneous background in the area of the detector mask. When this condition is not satistied.
other methods discussed in this chapter are more suitable for detecting intensity changes. a10.2.3 Line DetectionThe next level of complexity is line detection. Based on the discussion in
Section 10.2.1, we know that for line detection we can expect second derivatives to result in a stronger response and to produce thinner lines than first
derivatives, Thus, we can use the Laplacian mask in Fig. 10.4(a) for line detection
also, keeping in mind that the double-Jinc cffeel of the secand derivalive must
be handled properly. The following : pleustedes fre procedure      719a
bedFIGURE 10.4(a) Point
detection
(Laplacian} mask
(b) X-ray image
of turbine blade
with a porosity.
The porosity
contains a single
black pixel.(c) Result of
convolving the
mask with the
image. (d) Result
of using Eq. (10.2-8)
showing a single
point (the point
was enlarged to
make il easier (o
sec). (Original
image courtesy of
X-TEXK Sysiems,
Ltd.)EXAMPLE 10.1:
Deiection of
isolated points in
an image.
720 Chapter 10 1 Image SegmentationEXAMPLE 10.2:  %& Figure 10.5(a) shows a 486 < 486 (binary) portion of a wire-bond mask for
Using the . an electronic circuit, and Fig. 10.5(b) shows its Laplacian image. Because the
paplacian forline 7 aptacian image contains negative values,’ scaling is necessary for display, As
. the magnified section shows, mid pray represents zero, darker shades of gray
represent negative values, and lighter shades are positive. The double-line effect is clearly visible in the magnified region.
At first, it might appear that the negative values can be handled simply by
taking the absolute value of the Laplacian image. However, as Fig, 10.5(c)
shows, this approach doubles the thickness of the lines. A more suitable approach is to use only the positive values of (he Laplacian (in noisy situations
we use the values that exceed a positive threshold to eliminate random variations about zero caused by the noise). As the image in Fig. 10.5(d) shows,
this approach results in thinner lines, which are considerably more useful.
Note in Figs. 10.5(b)} through (d) that when the lines are wide with respect to
the size of the Laplacian mask, the lines are separated by a zero “valley.”‘ab
cdFIGURE 10.5(a) Original image.
(b) Laplacian
image; the
magnified section
shows the
positive/negative
double-line effect
characteristic of the
Laplacian.(c) Absolute value
of the Laplacian.
(d) Positive values
of the Laplacian.    ihe pixels in the Teswiiny
7 dou pasitwe and negate prveis“When a mask whose coefficients sum to zere ty convolved with a
image will sum to zero also {Problem Alt. samy ing thy existence
in the resull, Sealing so thar wl vulges are aettic   
  seis required fur display purposes
10.2 © Point, Line, and Edge Detection 723 A third mode) of an edge is the so-called roof edge, having the characteris
tics illustrated in Fig. 10.8(c). Roof edges are models af
dvtermewith the base (width) of a roof edge beir
sharpness of the line. In the limit, when its    flaes Uiroe a region, Css AndabcdefFIGURE 10.7(a) Image of a
wire-bond
template.(b) Result of
processing with
the +45° line
detector mask in
Fig. 10.6(c} Zoomed view
of the lop left
region af (b).(d) Zoomed view
of the bottom
right region of
(b). (e) The image
in (b) witb all
negative valucs
set to zero. (f) All
points {in while)
whose watues
satisfied the
condition g = T.
where ¢ is the
image m (e). (The
points in (f) were
enlarged to make
them easier lo
sec.}
10.2 % Point, Line, and Edge DetectionThis is not unexpected. For example, when the 3 X 3 filter is centered on a
line of constant intensity 5 pixels wide, the response will be zero, thus producing the effeci just mentioned. When we talk about line detection, the assumption is that lines are thin with respect to the size of the detector. Lines
that do not satisfy this assumption are best treated as regions and handled by
the edge detection methods discussed later in this section. idThe Laplacian detector in Fig. 10.4{a) is isotropic, so its response is independent of direction (with respect to the four directions of the 3 x 3 Laplacian
mask: vertical, horizontal, and two diagonals). Often, interest lies in detecting
lines in specified directions. Consider the masks in Fig. 10.6. Suppose that an
image with a constant background and containing various lines (oriented at 0°,
445°, and 90°) is filtered with the first mask. The maximum responses would
occur at image locations in which a horizontal line passed through the middle
row of the mask. This is easily verified by sketching a simple array of 1s with a line
of a different intensity (say, 5s) running horizontally through the array. A similar
experiment would reveal that the second mask in Fig. 10.6 responds best to lines
oriented at +45°; the third mask to vertical lines; and the fourth mask to lines in
the —45° direction. The preferred direction of each mask is weighted with a larger coefficient (i.e.,2) than other possible directions, The coefficients in each mask
sum to zero, indicating a zero response in areas of constant intensity.Let R,, R2, R3, and Ry denote the responses of the masks in Fig. 10.6, from
left to right, where the Rs are given by Eq. (10.2-3). Suppose that an image is
filtered (individually) with the four masks. If, at a given point in the image,
\Ry| > |R;l, for all j # &, that point is said to be more likely associated with a
line in the direction of mask &. For example, if at a point in the image,
|R,| > {R,| for j = 2,3, 4, that particular point is said to be more likely associated with a horizontal line. Alternatively, we may be interested in detecting
lines in a specified direction. In this case, we would use the mask associated
with that direction and threshold its output, as in Eq. (10.2-8). In other words,
if we are interested in detecting all the lines in an image in the direction defined by a given mask, we simply run the mas through the image and threshold the absolute value of the result, The points that are left are the strongest
responses which, for lines 1 pixel thick, correspond closest to the direction
defined by the mask. The following example illustrates this procedure.   721Recall from Section 2.4.2
that the image axis convention bas the origin al
the top left, with the positive x-axis pointing
down and the positive
ypoaxis extending to the
right. The angles of the
lines discussed in this
section are measured
with respect to the positive x-axis, For example, a
vertical Jine has an angle
of 0°, and a +45? line
extends downward and
to the right.Do not confuse our use of
Ria designate mask response with the same
symbol to denote regions
in Section 16.1.     +45°Horizontal Vertical FIGURE 10.6 Line detection masks. Angles are with respect to the axis system in Fig. 2.18(b).
722 Chapter 10 mw Image SegmentationEXAMPLE 10.3:
Detection of lines
in specified
directions.@ Figure 10.7(a) shows the image used in the previous example. Suppose that
we are interested in finding all the lines that are 1 pixel thick and oriented at
+45°. For this purpose, we use the second mask in Fig. 10.6. Figure 10.7(b) is
the result of filtering the image with that mask. As before, the shades darker
than the gray background in Fig. 10.7(b) correspond to negative values. There
are two principal segments in the image oriented in the +45° direction, one at
the top left and one at the bottom right. Figures 10.7(c) and (d) show zoomed
sections of Fig. 10.7(b) corresponding to these two areas. Note how much
brighter the straight line segment in Fig. 10.7(d) is than the segment in
Fig. 10.7(c). The reason is that the line segment in the bottom right of
Fig. 10.7(a) is 1 pixel thick, while the one at the top left is not. The mask is
“tuned” to detect |-pixel-thick lines in the +45° direction, so we expect its response to be stronger when such lines are detected. Figure 10.7(e) shows the
positive values of Fig. 10.7(b). Because we are interested in the strongest response, we let T equal the maximum value in Fig. 10.7(e). Figure 10,7(f) shows
in white the points whose valuas satisfied the condition g = T, where g is the
image in Fig. 10.7(e). The isolated points in the figure are points that also had
similarly strong responses to the mask. In the original image, these points and
their immediate neighbors are oriented in such a way that the mask produced
a maximum response at those locations. These isolated points can be detected
using the mask in Fig. 10.4(a) and then deleted, or they can be deleted using
morphological operators, as discussed in the last chapter. *19.2.4 Edge ModelsEdge detection is the approach used most frequently for segmenting images based
on abrupt (local) changes in intensity. We begin by introducing several ways to
model edges and then discuss a number of approaches for edge detection.Edge models are classified according to their intensity profiles. A step edge
involves a transition between two intensity Jevels occurring ideally over the
distance of 1 pixel. Figure 10.8(a) shows a section of a vertical step edge and a
horizontal intensity profile through the edge. Step edges occur, for example, in
images generated by a computer for use in areas such as solid modeling and
animation. These clean, idea/ edges can occur over the distance of 1 pixel, provided that no additiona) processing (such as smoothing) is used to make them
look “reaJ.” Digital step edges are used frequently as edge models in algorithm
development. For example, the Canny edge detection algorithm discussed in
Section 10.2.6 was derived using a step-edge model.In practice, digital images have edges that are blurred and noisy, with the degree of blurring determined principatly by limitations in the focusing mechanism (e.g., lenses in the case of optical images), and the noise leve] determined
principally by the electronic components of the imaging system. In such situations, edges are more closely modeled as having an intensity ramp profile, such
as the edge in Fig. 10.8(b). The slope of the ramp is inversely proportional to the
degree of blurring in the edge. In this model, we no longer have a thin (1 pixel
thick) path. Instead, an edge point now is any point contained in the ramp, and
an edge segment would then be a set of such points thai are connected.
724  Chopter 10 @ Image SegmentationFIGURE 10.8 aFrom left to right, | | | |
i Al
| ;|models (ideal r j
representations) of ! :
a step,a ramp, and | if \
aroof edge, andtheir corresponding
intensity profiles. really nothing more than a 1-pixel-thick line running through a region in an
image. Roof edges arise, for example, in range imaging, when thin objects
(such as pipes) are closer to the sensor than their equidistant background
(such as walls). The pipes appear brighter and thus create an image similar to
the model in Fig. 10.8(c). As mentioned earlier, other areas in which roof edges
appear routinely are in the digitization of line drawings and also im satellite images, where thin features, such as roads, can be modeled by this type of edge.
It is not unusual to find images that contain all three types of edges. Although blurring and noise result in deviations from the ideal shapes, edges in
images that are reasonably sharp and have a moderate amount of noise do
resemble the characteristics of the edge models in Fig. 10.8, as the profiles in
Fig. 10.9 illustrate.’ What the modeis in Fig. 16.8 allow us to do is write mathematical expressions for edges in the development of image processing algorithms. The performance of these algorithms will depend on the differences
between actual edges and the models used in developing the algorithms. FIGURE 10.9 A 1508 x 1970 image showing (zoomed) actual ramp (bottom. left}, step
(top, right), and roof edge profiles. The profiles are from dark to light, in the areas
indicated by the short line segments shown in the small circles. The ramp and “step”
profiles span 9 pixels and 2 pixels, respectively. The base of the rool edge is 3 pixels.
(Original image courtesy of Dr. David R. Pickens, Vanderbilt University.)      ‘Ramp edges with a sharp siope of a lew pixels of € treated as stepthem from ramps in the same image whose slopes are more pradaal
10.2 @ Point, Line, and Edge DetectionFigure 10.10(a) shows the image from which the segment in Fig. 10.8(b) was
extracted. Figure 10.10(b} shows a horizontal intensity profile. This figure
shows also the first and second derivatives of the intensity profile. As in the
discussion in Section 10.2.1, moving from left to right along the intensity profile, we note that the first derivative is positive at the onset of the ramp and at
points on the ramp, and it is zero in areas of constant intensity. The second deTivative is positive at the beginning of the ramp, negative at the end of the‘ ramp, Zero at points on the-ramp, and zero at points of constant intensity, The
signs of the derivatives just discussed would be reversed for an edge that transitions from light to dark. The intersection between the zero intensity axis and
a line extending between the extrema of the second derivative marks a point
called the zero crossing of the second derivative.We conclude from these observations that the magnitude of the first derivative can be used to detect the presence of an edge at a point in an image. Similarly, the sign of the second derivative can be used to determine whether an
edge pixel lies on the dark or light side of an edge. We note two additional
properties of the second derivative around an edge: (1) it produces two values
for every edge in an image (an undesirable feature); and (2) its zero crossings
can be used for locating the centers of thick edges, as we show later in this section. Some edge models make use of a smooth transition into and out of the
ramp (Problem 10.7). However, the conclusions reached using those models
are the same as with an ideal ramp, and working with the latter simplifies theoretical formulations. Finally, although attention thus far has been limited to a
1-D horizontal profile, a similar argument applies to an edge of any orientation in an image. We simply define a profile perpendicular to the edge direction at any desired point and interpret the results in the same manner as for
the vertical edge just discussed.    Horizon{al intensity
profileFurst
derivativeSecond
derivativef
Zero crossing «4
f 725abFIGURE 10.10(a) Two regions of
constant intensity
separated by an
ideal vertical
ramp edge.(b) Detail near
the edge, showing
a horizontal
intensity profile,
together with its
first and second
derivatives,
10.2 « Point, Line, and Edge Detection 727 FIGURE 10.1) First column: Images and intensity profiles of a ramp edge corrupted by
random Gaussian noise of zero mean and standard deviations of 0.0, 0.1, 1.0, and 10.0
intensity levels, respectively. Second column: Mirsi-devivalive images arid intensity
profiles. Third column: Second-derivative images and intensity profiles.
726 Chapter 10 ® Image SegmentationEXAMPLE 10.4:
Behavior of the
first and second
derivatives of a
noisy edge.Computation of the
derivatives for the entire
image segment is
discussed in the following
section, For now, our
interest lies on analyzing
just the intensity profiles.M The edges in Fig. 10.8 are free of noise. The image segments in the first
column in Fig. 10.11 show close-ups of four ramp edges that transition from a
black region on the left to a white region on the right {keep in mind that the entire transition from black to white is a single edge). The image segment at the top
left is free of noise. The other three images in the first column are corrupted by
additive Gaussian noise with zero mean and standard deviation of 0.1, 1.0, and
10.0 intensity levels, respectively. The graph below each image is a horizontal intensity profile passing through the center of the image. All images have 8 bits of
intensity resolution, with 0 and 255 representing black and white, respectively.
Consider the image at the top of the center column. As discussed in connection with Fig. 10.10(b), the derivative of the scan line on the left is zero in the constant areas. These are the two black bands shown in the derivative image. The
derivatives at points on the ramp are constant and equal to the slope of the ramp.
These constant values in the derivative image are shown in gray. As we move
down the center column, the derivatives become increasingly different from the
noiseless case. In fact, it would b€ difficult to associate the last profile in the center column with the first derivative of a ramp edge. What makes these results interesting is that the noise is almost invisible in the images on the left column.
These examples are good illustrations of the sensitivity of derivatives to noise.
As expected, the second derivative is even more sensitive to noise. The second derivative of the noiseless image is shown at the top of the right column.
The thin white and black vertical lines are the positive and negative components of the second derivative, as explained in Fig. 10.10. The gray in these images represents zero (as discussed earlier, scaling causes zero to show as gray).
The only noisy second derivative image that barely resembles the noiseless
case is the one corresponding to noise with a standard deviation of 6.1. The remaining second-derivative images and profifes clearly illustrate that it would
be difficult indeed to detect their positive and negative components, which are
the truly useful features of the second derivative in terms of edge detection.
The fact that such little visual noise can have such a significant impact on
the two key derivatives used for detecting edges is an important issue to keep
in mind. In particular, image smoothing should be a serious consideration
prior to the use of derivatives in applications where noise with levels similar to
those we have just discussed is likely to be present. "We conclude this section by noting that there are three fundamental steps
performed in edge detection:4. Image smoothing for noise reduction. The need for this step is amply
illustrated by the results in the second and third columns of Fig. 10.11.2. Detection of edge points. As mentioned earlitr, this is a local operation
that extracts from an image all points that are potential candidates to
become edge points. .3. Edge localization. The objective of this step is to select from the candidate
edge points only the points that are true members of the set of points comprising an edge.The remainder of this section deals with techniques for achieving these objectives.
728 Chapter 10 m Image SegmentationFor convenience, we
repeat here some
equations from
Section 3.6.4.EXAMPLE 10.5:
Properties of the
gradient.10.2.5 Basic Edge DetectionAs illustrated in the previous section, detecting changes in intensity for the
purpose of finding edges can be accomplished using first- or second-order derivatives. We discuss first-order derivatives in this section and work with secondorder derivatives in Section 10.2.6.The image gradient and its propertiesThe tool of choice for finding edge strength and direction at location (x, y) of
an image, f, is the gradient, denoted by Vf, and defined as the vectoraf
Vf = grad(f) = [é] = a (10.2-9)
¥ —
: yThis vector has the important geometrical property that it points in the direction
of the greatest rate of change of f at location {x, y).
The magnitude (length) of vector Vf, denoted as M(x, y), whereM(x, y) = mag(Vf) = V/8s + 8} (102-10)
is the value of the rate of change in the direction of the gradient vector.
Note that g,, gy, and M(x, y) are images of the same size as the original,
created when x and y are allowed to vary over all pixel locations in f. It is
common practice to refer to the latter image as the gradient image, or simply as the gradient when the meaning is clear. The summation, square, and
square root operations are array operations, as defined in Section 2.6.1.The direction of the gradient vector is given by the anglexa(x, y) = nF (10.2-11)measured with respect to the x-axis. As in the case of the gradient image,
a(x, y) also is an image of the same size as the original created by the array division of image g, by image g,. The direction of an edge at an arbitrary point
(x, y) is orthogonal to the direction, a(x, y), of the gradient vector at the point.@ Figure 10.12(a) shows a zoomed section of an image containing a straight
edge segment. Each square shown corresponds to a pixel, and we are interested in obtaining the strength and direction of the edge at the point highlighted
with a box. The pixels in gray have value 0 and the pixels in white have value 1.
We show following this example that an approach for camputing the derivatives in the x- and y-directions using a3 X 3 neighborhood centered about a
point consists simply of subtracting the pixels in the top row of the neighborhood from the pixels in the bottom row to obtain the partial derivative in the
x-direction. Similarly, we subtract the pixels in the left column from the pixels
in the right column to obtain the partial derivative in the y-direction. It then
730 — Chapter 10 Image SegmentationabFIGURE 10.13
One-dimensional
masks used to
implement Eqs.10.2-12) and . : .
hee ye These two equations can be imp!emented for all pertinent values of x and y byfiltering f(x, y) with the 1-D masks in Fig. 10.13.When diagonal edge direction is of interest, we need a 2-D mask. The Roberts
cross-gradient operators (Roberts [1965]) are one of the earliest attempts to use
2-D masks with a diagonal preference. Consider the 3 X 3 region in Fig. 10.14(a).
The Roberts operators are based on implementing the diagonal differencesof
In the remainder of this Bx = 5 = (a - 2s) (10.214)
section we assume : .
implicitly that f is u and : *Junction of wwe variables,
ard omil the variables 10
simplify the notation.(10.2-15)a
be
de
fe .FIGURE 10.14A3 x 3 region of
an image (the z’s
are intensity
values) and
various masks
used to compute
the gradient at
the point labeled
Zs.    Filter masks used to
compute the derivatives
needed for the gradient
are often called gradient
operators, difference
operators. edge uperdtors,
on edge detectors.
10.2 m Point, Line, and Edge Detection 729         direction     a beeFIGURE 10.12 Using the gradient to determine edge strength and direction at a point.
Note that the edge is perpendicular to the direction of the gradient vector at the point
where the gradient is computed. Each square in the figure represents one pixel.follows, using these differences as our estimates of the partials, that df/ax = —2
and df /ay = 2 at the point in question. Then,
of
ax -2
vf a] |= =
i E 1 of | 2 |
ayfrom which we obtain M(x, y) = 2V2 at that point. Similarly, the direction of
the gradient vector at the same point follows from Eq. (10.2-11):
a(x, y) = tan”/(g,/g,) = —45°, which is the same as 135° measured in the
positive direction with respect to the x-axis, Figure 10.12(b) shows the gradient
vector and its direction angle.Figure 10.12(c) illustrates the important fact mentioned earlier that the
edge at a point is orthogonal to the gradient vector at that point. So the direction angle of the edge in this example is a — 90° = 45°. All edge points in
Fig. 10.12(a) have the same gradient, so the enfire edge segment is in the same
direction. The gradient vector sometimes is called the edge normal. When the
vector is normalized to unit length by dividing it by its magnitude (Eq. (10.2-10}],
the resulting vector is commonly referred to as the edge unit normal. JGradient operatorsObtaining the gradient of an image requires computing the partial derivatives
of jax and df /ay at every pixel location in the image. We are dealing with digital quantities, so a digital approximation of the partial derivatives over a
neighborhood about a point is required. From Section 10.2.1 we know that(= FED = fe +19) = fos) (10.212)
and
&y = HEY) - f(xy + 1) - f(xy) (20.2-13)dyRecall from Section 2.4.2
that the origin of the
image coordinate system
is at the 10p left, with the
positive x- and y-axes.
exiending down and to
the right, respectively,
16.2 @ Point, Line, and Edge Detection 731These derivatives can be implemented by filtering an image with the masks in
Figs. 10,14(b) and (c).Masks of size 2 X 2 are simple conceptually, but they are not as useful for
computing edge direction as masks that are symmetric about the center point,
the smallest of which are of size 3 X 3. These masks take into account the nature of the data on opposite sides of the center point and thus carry more information regarding the direction of an edge. The simplest digital approximations
to the partial derivatives using masks of size 3 xX 3 are given byoFBr = oy = (ar + 2 + By) ~ (41 + + 23) (10.2-16)
and
F)
&y = = (23 + % + 2%) — (Z) + 24 + 2) {10.2-17)In these formulations, the difference between the third and first rows of the
3 X 3 region approximates the derivative in the x-direction, and the difference
between the third and first columns approximate the derivate in the y-direction.
Intuitively, we would expect these approximations to be more accurate than
the approximations obtained using the Roberts operators. Equations (10.2-16)
and (10.2-17) can be implemented over an entire image by filiering f with the
two masks in Figs. 10.14(d) and (e). These masks are called the Prewitt operators
(Prewitt [1970}).A slight variation of the preceding two equations uses a weight of 2 in the
center coefficient:af& = ax = (zz + 2% + 29) ~ (2; + 222 + 23) (10.2-18)and
.
Xoa
gy = x = (Za + 2z6 + Zo) — (zy + 2Z4 + 27) (10.2-19)It can be shown (Problem 10.10) that using a 2 in the center location provides
image smoothing. Figures 10.14(f) and (g) show the masks used to implement
Eqs. (10.2-18) and (10.2-19). These masks are called the Sobel operators
{Sobel (1970}).The Prewitt masks are simpler to implement than the Sobel masks, but,
the slight computational difference between them typically is not an issue.
The fact that the Sobel masks have better noise-suppression (smoothing)
characteristics makes them preferable because, as mentioned in the previous section, noise suppression is an important issue when dealing with derivatives. Note that the coefficients of all the masks in Fig. 10.14 sum to zero,
thus giving a response of zero in areas of constant intensity, as expected of a
derivative operator.Although these
equations encompass a
larger neighborhood, we
are still dealing with
differences between
intensity values, so the
conclusions from earlier
discussions regarding
first-order derivatives
still apply.
732 Chapter 10 # Image Segmentationab
edFIGURE 10,15
Prewitt and Sobel
masks fordetecting diagonaledges.EXAMPLE 10.6:
Illustration of the
2-D gradient
magnitude and
angle.         < SobelThe masks just discussed are used to obtain the gradient components g, and
8, at every pixel location in an image. These two partial derivatives are then
used to estimate edge strength and direction. Computing the magnitude of the
gradient requires that g, and g, be combined in the manner shown in Eq. (10.210). However, this implementation is not always desirable because of the computational burden required by squares and square roots. An approach used
frequently is to approximate the magnitude of the gradient by absolute values:M(x, y) * lgel + Igy (10.2-20)This equation is more attractive computationally, and it still preserves relative
changes in intensity levels. The price paid for this advantage is that the resulting filters will not be isotropic (invariant to rotation) in general. However, this
is not an issue when masks such as the Prewitt and Sobel masks are used to
compute g, and gy, because these masks give isotropic results only for vertical
and horizontal edges. Results would be isotropic only for edges in those two
directions, regardless of which of the two equations is used, In addition, Eqs.
(10.2-10) and (10.2-20) give identical results for vertical and horizontal edges
when the Sobel or Prewitt masks are used (Problem 10.8).It is possible to modify the 3 x 3 masks in Fig. 10.14 so that they have their
strongest responses along the diagonal directions. Figure 10.15 shows the two
additional Prewitt and Sobel masks needed for detecting edges in the diagonal
directions. .@ Figure 10.16 illustrates the absolute value response of the two components
of the gradient, |&| and |8y|, as well as the gradient image formed from the
sum of these two components. The directionality of the horizontal and vettical components of the gradient is evident in Figs. 10.16(b) and (c). Note, for
example, how strong the roof tile, horizontal brick joints, and horizontal segments of the windows are in Fig, 10.16(b) compared lo other edges. By contrast.
10.2 @ Point, Line, and Edge Detection Fig. 10,16(c) favors features such as the vertical components of the fagade and
windows. It is common terminology to use the term edge map when referring
to an image whose principal features are edges, such as gradient magnitude
images. The intensities of the image in Fig. 10.16(a) were scaled to the range
[0, 1]. We use values in this range to simplify parameter selection in the various methods for edge detection discussed in this section.Figure 10.17 shows the gradient angle image computed using Eq. (10.2-11).
In general, angle images are not as useful as gradient magnitude images for
edge detection, but they do complement the information extracted from an
image using the magnitude of the gradient. Fog instance, the constant intensity
areas in Fig, 10.16(a), such as the front edge of the sloping roof and top horizontal bands of the front wall, are constant in Fig. 10.17, indicating that the
gradient vector direction at all the pixel locations in those regions is the same. 733ab
edFIGURE 10.76(a) Original image
of size834 < 1114 pixels,
with intensity
values scaled to
the range [0, 1].
(b) |r|, the
component of the
gradient in the
x-direction,
obtained using
the Sobel mask in
Fig. 10.14(£) to
filter the image.
(c) |8y|, obtained
using the mask in
Fig. 10.14{g).(da) The gradient
image, |8:| + |8yl.FIGURE 10.17
Gradient angle
image computed
usingEy. (102-14).f constant intcasity in this
iinage indicate
that the direction
of the gradient  
 c pixel
sions in thoseOns
734 Chapter 10 # Image SegmentationThe maximum edge
strength (magnitude) of
asmoothed image
decreases inversely as a
function of the size of the
smoothing mask (Problem 10.13).ab
cdFIGURE 10.18
Same sequence as
in Fig. 10,16, but
with the original
image smoothed
usingaS x 5
averaging filter
prior to edge
detection.As we show in Section 10,2.6, angle information plays a key supporting rele in
the implementation of the Canny edge detection algorithm, the most advanced edge detection method we discuss in this chapter.The original image in Fig. 10.16(a) is of reasonably high resolution
(834 X 1114 pixels), and at the distance the image was acquired, the contribution made to image detail by the wall bricks is significant. This level of fine detail often is undesirable in edge detection because it tends to act as noise,
which is enhanced by derivative computations and thus complicates detection
of the principal edges in an image. One way to reduce fine detail is to smooth
the image. Figure 10.18 shows the same sequence of images as in Fig. 10.16, but
with the original image smoothed first using a 5 X 5 averaging filter (see
Section 3.5 regarding smoothing filters). The response of each mask now
shows almost no contribution due to the bricks, with the results being dominated mostly by the principal edges. ,It is evident in Figs. 10.16 aiid 10.18 that the horizontal and vertical Sobel
masks do not differentiate between edges oriented in the +45° directions. If it
is important to emphasize edges along the diagonal directions, then one of the
masks in Fig, 10.15 should be used. Figures 10.19{a) and (b) show the absolute
responses of the 45° and —45° Sobel masks, respectively. The stronger diagonal
response of these masks is evident in these figures. Both diagonal masks have
similar response to horizontal and vertical edges but, as expected, their response
in these directions is weaker than the response of the horizontal and vertical
masks, as discussed earlier.
40.2 & Point, Line, and Edge Detection 735abFIGURE 10.19
Diagonal edge
detection(a) Result of
using the mask in
Fig. 10.15(c}.(b) Result of
using the mask in
Fig. 10,13(d). The
input image in
both cases was
Fig. 10,18(a). Combining the gradient with thresholdingThe results in Fig. 10.18 show that edge detection can be made more selective
by smoothing the image prior to computing the gradient. Another approach
aimed at achieving the same basic objective is to threshold the gradient image.
For example, Fig. 10.20(a) shows the gradient image from Fig. 10.16(d) thresh~ the ihreshold used toolded, in the sense that pixels with values greater than or equal to 33% of the — crate Fie. 10.208)
was selected su that mostmaximum value of the gradient image are shown in white, while pixels — ofthe smailedgescaused
below the threshold value are shown in black. Comparing this image with Ds te bscks were elimi:
Fig. 10.18(d), we see that there are fewer edges in the thresholded image, — wacthe originat onjective
and that the edges in this image are much sharper (see, for example. the edges fates wee
in the roof tile). On the other hand, numerous edges, such as the 45° line defining, — computing the gradient
the far edge of the roof, are broken in the thresholded image.When interest lies both in highlighting the principal edges and on maintaining as much connectivity as possible, it is common practice to use both
smoothing and thresholding. Figure 10.20(b) shows the result of thresholdingFig. 10.18(d), which is the gradient of the smoothed image. This resull shows a   EB.
FIGURE 10.20 (a) Thresholded version of (he image in Fig. 10.16(d), with the threshaleselected as 33% of the highest value in the image: this threshold was just high ¢
eliminate most of the brick edges in the pradiont in (b} Vhresholded ¥
image in Fig. 10.18(d), obtained using a threshold equal ta 244 of Ube bin
that image. 
 
 
 dugh fo
102 & Point, Line, and Edge Detection 737Collecting terms gives the final expression: 24 2-22
VG(x, y) = pel (10,2-23)This expression is called the Laplacian of a Gaussian (LoG).Figures 10.21(a) througir(c) show a 3-D plot, image, and cross section of the
negative of the LoG function (note that the zero crossings of the LoG occur at
x? + y? = 297, which defines a circle of radius V20 centered on the origin).
Because of the shape illustrated in Fig, 10.21(a), the LoG function sometimes
is called the Mexican hat operator. Figure 10.21(d) shows a 5 X 5 mask that
approximates the shape in Fig. 10.21(a) (in practice we would use the negative
of this mask). This approximation is not unique. Its purpose is to capture the
essential shape of the LoG function; in terms of Fig. 10.21(a), this means a positive, central term surrounded by an adjacent, negative region whose values increase as a function of distance from the origin, and a zero outer region. The
coefficients must sum to zero so that the response of the mask is zero in areas
of constant intensity.Masks of arbitrary size can be generated by sampling Eq. (10.2-23) and scaling the coefficients so that they sum to zero. A more effective approach for
generating a LoG filter is to sample Eq. (10.2-21) to the desired  X n size andVG         Zero crossing \ ia Zero crossing      Qe a ee reNote the simlanty betaveen the cross section in
Fag. 10.21 {¢) and uic
highpass filter in Fig
4.37(d). Thus, we can exPeet the LoG to behave
as a highpass filterabcadFIGURE 10.21(a) Threedimensional plot
of the negative of
the LoG, (b)
Negative of the
LoG displayed as
an image. (c)
Cross section of
(a) showing zero
crossings.id} § X S mask
approximation to
\he shape in (a).
The negative of
this mask would
be used in
practtee
736 Chapter 10 @ Image SegmentationTo convince yourself that
edge detection is not independent af scale,consider, for example, the
roof edge in Fig. 10.8(c).
If the scale of the image
is reduced, the edge will
appeer thinner.It is customary torEq. (10.2-21) to differ
from the definition of «
2-D Gaussian PDF by
the constant term
{/2z0?. If an exact
expression is desired in a
given application, then
the multiplying constairt
ean be appended tothe
Gnal result inEq. (10.2-23)reduced number of broken edges; for instance, compare the 45° edges in Figs.
10.20(a) and (b). Of course, edges whose intensity values were severely attenuated
due to blurring (e.g., the edges in the tile roof) are likely to be totally eliminated
by thresholding. We return to the problem of broken edges in Section 10.2.7.10.2.6 More Advanced Techniques for Edge DetectionThe edge-detection methods discussed in the previous section are based simply on filtering an image with one or more masks, with no provisions being
made for edge characteristics and noise content. In this section, we discuss
more advanced techniques that make an attempt to improve on simple edgedetection methods by taking into account factors such as image notse and the
nature of edges themselves.The Marr-Hildreth edge detectorOne of the earliest successful attempts at incorporating more sophisticated
analysis into the edge-finding process is attributed to Marr and Hildreth [1980].
Edge-detection methods in use at the time were based on using small operators
(such as the Sobel masks), as discussed in the previous section. Marr and Hildreth
argued (1) that intensity changes are not jndependent of image scale and so their
detection requires the use of operators of different sizes; and (2) that a sudden intensity change will give rise to a peak or trough in the first derivative or, equivalently, to a zero crossing in the second derivative (as we saw in Fig, 10.10).These ideas suggest that an operator used for edge detection should have
two salient features. First and foremost, it should be a differential operator capable of computing a digital approximation of the first or second derivative at
every point in the image. Second, it should be capable of being “tuned” to act
at any desired scale, so that latge operators can be used to detect blurry edges
and small operators to detect sharply focused fine detail.Marr and Hildreth argued that the most satisfactory operator fulfilling
these conditions is the filter V°G where, as defined in Section 3.6.2, V? is the
Laplacian operator, (67/ax” + 8°/ay’), and G is the 2-D Gaussian functionG(x, 9) = (102-21)with standard deviation ¢ (sometimes a is called the space constant). To find
an expression for V°G we perform the following differentiations:_ PG(x, y) ; #G(x, y)
x? ayV?G(x, ¥)3 ~(10.2-22)
738 Chapter 10 m Image SegmentationThis expression is
implemented in the
spatial domain using
Eq. (3.4-2). Itcan be
implemented also in the
Srequency domain using
Eq. (4.7.1).then convolvet the resulting array with a Laplacian mask, such as the mask in
Fig. 10.4(a). Because convolving an image array with a mask whose coefficients sum to zero yields a result whose elements also sum to zero (see Problems 3.16 and 10,14), this approach automatically satisfies the requirement
that the sum of the LoG filter coefficients be zero. We discuss the issue of selecting the size of LoG filter later in this section.There are two fundamental ideas behind the selection of the operator V°G.
First, the Gaussian part of the operator blurs the image, thus reducing the intensity of structures (including noise) at scales much smailer than o. Unlike
averaging of the form discussed in Section 3.5 and used in Fig. 10.18, the
Gaussian function is smooth in both the spatial and frequency domains (see
Section 4.8.3), and is thus less likely to introduce artifacts (e.g., ringing) not
present in the original image. The other idea concerns V’, the second derivative part of the filter. Although first derivatives can be used for detecting
abrupt changes in intensity, they are directional operators. The Laplacian, on
the other hand, has the important advantage of being isotropic (invariant to
rotation), which not only corresponds to characteristics of the human visual
system (Marr [1982]) but also responds equally to changes in intensity in any
mask direction, thus avoiding having to use multiple masks to calculate the
strongest response at any point in the image.The Marr-Hildreth algorithm consists of convolving the LoG filter with aninput image, f(x, y),
g(x,y) = [P°G(x, y)*f(, y) (10.2-24)and then finding the zero crossings of g(x, y) to determine the locations of
edges in f(x, y). Because these are linear processes, Eq. (10.2-24) can be written
also as .a(x, y) = VIG(x, ye f(x, »)] (10.2-25)indicating that we can smooth the image first with a Gaussian filter and then
compute the Laplacian of the result. These two equations give identical results.
The Marr-Hildreth edge-detection algorithm may be summarized as follows:L. Filter the input image with an n < n Gaussian lowpass filter obtained by
sampling Eq. (10.2-21).2. Compute the Laplacian of the image resulting from Step f using, for example,
the 3 X 3 mask in Fig, 10.4(a), [Steps 1 and 2 implement Eq. (10.2-25).}3. Find the zero crossings of the image from Step2.To specify the size of the Gaussian filter, recall that about 99.7% of the volume
under a 2-D Gaussian surface lies between +3¢ about the mean. Thus, as a rule‘The LoG is a symmetric filter, so spatial filtering using correlation or convolution yields the same result.
We use the convolution terminology here to indicate linear filtering for consistency with the literature
on this topic. Also, this gives you exposure to terminology that you will encounter in other contexts. [t is
important that you kcep in mind the camments made at the end of Section 3.4.2 regarding this topic.
740 Chapter 10 & Image SegmentationabcdFIGURE 10.22(a) Original image
of size 834 x 1114
pixels, with
intensity values
scaled to the range
(0, 1]. (b) Results
of Steps 1 and 2 of
the Marr-Hildreth
algorithm usingo =4andn = 25.
(c) Zero crossings
of (b) using a
threshold of 0
(note the closedloop edges).(d) Zero crossings
found using a
threshold equal 1o
4% of the
maximum value of
the image in (b).
Note the thin
edges.The difference of
Gaussians is a highpass
fier, as discussed in
Section 4.7.4, useful information, but, due to its complexity, it is used in practice mostly as a
design too! for selecting an appropriate value of o to use with a single filter.Marr and Hildreth [1980] noted that it is possible to approximate the LoG
filter in Eq. (10.2-23) by a difference of Gaussians (DoG):(10.226) with o, > o2. Experimental results suggest that certain “channels” in the
human vision system are selective with respect to orientation and frequency.
and can be modeled using Eq. (10.2-26) with a ratio of standard deviations of
L.75:1. Marr and Hildreth suggested that using the ratio 16:1 preserves the
basic characteristics of these observations and also provides a closer “engi:
neering” approximation to the Cots fenction. To make meaningful comparisons between the Loi and Doi. the value of « for the LoG must be selected
as in the following equation so that the LoG and DoG have the same zero
crossings (Problem 10.17);! (10.2-27) uy rs i OFAlthough the zero crossings of ihe LoG and DuoG wilh be the same wheu this
value of « is used, their amplitude scales will be differcit. We can make them
compatible by scaling beth functions so that they have ihe samo vale al the
origin.
10.2 @ Point, Line, and Edge Detection 739of thumb, the size of an 2 X n LoG discrete filter should be such that n is the
smallest odd integer greater than or equal to 6a. Choosing a filter mask smaller than this will tend 10 “truncate” the LoG function, with the degree of truncation being inversely proportional to the size of the mask; using a larger mask
would make little difference in the result.One approach for finding the zero crossings at any pixel, p, of the filtered
image, g(x, y), is based on using a 3 X 3 neighborhood centered at p. A zero- crossing at p implies that the signs of at least two of its opposing neighboring
pixels must differ. There are four cases to test: left/right, up/down, and the two
diagonals. If the values of g(x, y) are being compared against a threshold (a
common approach), then not only must the signs of opposing neighbors be different, but the absolute value of their numerical difference must also exceed
the threshold before we can call p a zero-crossing pixel. We illustrate this approach in Example 10.7 below.Zero crossings are the key feature of the Marr-Hildreth edge-detection
method. The approach discussed in the previous paragraph is attractive because of its simplicity of implementation and because it generally gives good
results. If the accuracy of the zero-crossing locations found using this method
is inadequate in a particular application, then a technique proposed by Huertas
and Medioni [1986] for finding zero crossings with subpixel accuracy can be
employed.B Figure 10.22(a) shows the original building image used earlier and
Fig. 10.22(b) is the result of Steps 1 and 2 of the Marr-Hildreth algorithm, using
o = 4 (approximately 0.5% of the short dimension of the image) and n = 25
(the smailest odd integer greater than or equal to 6a, as discussed earlier). As
in Fig. 10.5, the gray tones in this image are due to scaling. Figure 10.22(c)
shows the zero crossings obtained using the 3 x 3 neighborhood approach
discussed above with a threshold of zero, Note that all the edges form closed
loops. This so-called “spaghetti” effect is a serious drawback of this method
when a threshold value of zero is used ( Problem 10. 15). We avoid closed-loop
edges by using a positive threshold.Figure 10.22(d) shows the result of using a threshold approximately equal
to 4% of the maximum value of the LoG image. Note that the majority of the
principal edges were readily detected and “irrelevant” features, such as the
edges due to the bricks and the tile roof, were filtered out. As we show in the next
section, this type of performance is virtually impossible to obtain using the
gradient-based edge-detection techniques discussed in the previous section.
Another important consequence of using zero crossings for edge detection is
that the resulting edges are 1 pixel thick. This property simplifies subsequent
stages of processing, such as edge linking. iwA procedure used sometimes to take into account the fact mentioned earlier
that intensity changes are scale dependent is to filter an image with various
values of o. The resulting zero-crossings edge maps are then combined by
keeping only the edges that are common to al! maps. This approach can yieldAttempting to find the
zero crossings by finding
the coordinates (x. y),
such thal g(x, y) = Ois
impractical because of
noise andior
computational
inaccuracies.EXAMPLE 10,7:
Ifustration of the
Marr-Hildreth
edge-detection
method.
10.2 m Point, Line, and Edge Detection 741abFIGURE 10.23(a) Negatives of the
LoG (solid) and
DoG (dotted)
profiles using a
standard deviationbt ratio of 1.75:1.The profiles in Figs. 10,23(a) and (b) were generated with standard deviation
ratios of 1:1.75 and 1:1.6, respectively (by convention, the curves shown are
inverted, as in Fig. 10.21). The LoG profiles are shown as solid lines while the
DoG profiles are dotted. The curves shown are intensity profiles through the
center of LoG and DoG arrays generated by sampling Eq. (10.2-23) (with
the constant in 1/270* in front) and Eq. (10.2-26), respectively. The amplitude
of all curves at the origin were normalized to 1. As Fig. 10.23(b) shows, the ratio
1:1.6 yielded a closer approximation between the LoG and DoG functions,Both the LoG and the DoG filtering operations can be implemented with
1-D convolutions instead of using 2-D convolutions directly (Problem 10.19),
For an image of size M X N and a filter of size n X n, doing so reduces the
number of multiplications and additions for each convolution from being proportional to n2MN for 2-D convolations to being proportional to nMN for
1-D convolutions. This implementation difference is significant. For example, if
n = 25, a 1-D implementation will require on the order of 12 times fewer
multiplication and addition operations than using 2-D convolution.The Canny edge detectorAlthough the algorithm is more complex, the performance of the Canny edge
detector (Canny [1986]) discussed in this sectiomis superior in general to the edge
detectors discussed thus far. Canny’s approach is based on three basic objectives:1. Low error rate. All edges should be found, and there should be no spurious
responses. That is, the edges detected must be as close as possible to the
true edges.2. Edge points should be well localized. The edges located must be as close as
possible to the true edges, That is, the distance between a point marked as an
edge by the detector and the center of the true edge should be minimum.3. Single edge point response. The detector should return only one point for
each true edge point. That is, the number of local maxima around the true
edge should be minimum, This means that the detector should not identify
multiple edge pixels where only a single edge point exists.The essence of Canny’s work was in expressing the preceding three criteria
mathematically and then attempting to find optimal solutions to these formulations. In general, it is difficult (or impossible) to find a closed-form solution{b) Profiles obtained
using a ratio of 1.6:1.
742 Chapter 10 a Image SegmentationRecall that white noise is
noise haying a frequency
spectrum thar is Continuous and uniform over aspecified frequency hand.White Gaussian noise is
white noise in which the
distribution of amplitude
valucs is Gaussian.
Gaussian white noise is 4
goud approximation of
many real-world situations and generates
mathematically tractable
models, [1 has the useful
Property that its values
are statistically
independent,that satisfies all the preceding objectives. However, using numerical optimization with 1-D step edges corrupted by additive white Gaussian noise led to the
conclusion that a good approximation’ to the optimal step edge detector is the
first derivative of a Gaussian:d@oo2 -x
—€ w = —~edx Pd (10.2-28)Generalizing this result to 2~D involves recognizing that the 1-D approach still
applies in the direction of the edge normal (see Fig. 10.12). Because the direction of the normal is unknown beforehand, this would require applying the
1-D edge detector in all possible directions. This task can be approximated by
first smoothing the image with a circular 2-D Gaussian function, computing
the gradient of the result, and then using the gradient magnitude and direction
to estimate edge strength and direction at every point.Let f(x, y) denote the input image and G(x, y) denote the Gaussian function:veyG(x, y) = ee (10.2-29)
We form a smoothed image, f,(x, y), by convolving G and f:
> fey) = G(x, ye f(x, y) (10.2-30)This operation is followed by computing the gradient magnitude and direction
(angle), as discussed in Section 10.2.5:M(x, y) = Vat + (10.2-31)
and
=} By
a(x, y) = tan | (10.2-32)
8xwith g, = of/ax and g, = af,/dy. Any of the filter mask pairs in Fig. 10.14 can
be used to obtain g, and g,, Equation (10.2-30) is implemented using an X n
Gaussian mask whose size is discussed below, Keep in mind that A¢(x, y) and
a(x, y) are arrays of the same size as the image from which they are computed.Because it is generated using the gradient, M(x, y) typically contains wide
ridges around local maxima (recall the discussion Jn Section 10.2.1 regarding
edges obtained using the gradient). The next step is to thin those ridges. One
approach is to use nonmaxima suppression. This can be done in several ways,
but the essence of the approach is to specify a number of discrete orientations “Canny [1986] showed that using a Gaussian approximation proved only about 20%. worse than using the
optimized numericat solution. A difference of this magnitude generally is imperceptible in most applicauens,

[Finished in 0.2s]